---
title: covariance and correlation coefficient
author: Dan Li
date: '2020-09-05'
slug: covariance-and-correlation-coefficient
categories:
  - statistics
tags:
  - Regression
output:
  pdf_document:
    pandoc_args: --lua-filter=color-text.lua
    keep_tex: yes
  html_document:
    pandoc_args: --lua-filter=color-text.lua
header-includes:
- \usepackage{bm,amsmath,amsfonts}
---

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<p>We define the covariance of any two random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, written <span class="math inline">\(Cov(X,Y)\)</span>, as: <span class="math display">\[\begin{align}
Cov(X,Y) &amp;= E(X-\mu_X)(Y-\mu_Y)\\
&amp;= E(XY-X\mu_Y-Y\mu_X+\mu_X\mu_Y)\\
&amp;= E(XY)-\mu_X\mu_Y-\mu_X\mu_Y+\mu_X\mu_Y\\
&amp;= E(XY) - \mu_X\mu_Y\\
&amp;= E(XY) − E(X)E(Y)\\
\end{align}\]</span>.
If <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent random variables,
<span class="math display">\[\begin{align}
E(XY)&amp;=\int\int xy\cdot f_{X,Y}(x,y)dxdy\\
&amp;=\int\int xy\cdot f_X(x)f_Y(y)dxdy\\
&amp;=\int x\cdot f_X(x)dx\int y\cdot f_Y(y)dy\\
&amp;=E(X)E(Y)
\end{align}\]</span>, then <span class="math inline">\(Cov(X,Y) = E(XY) − E(X)E(Y)=0\)</span></p>
<p>The Variance of the sum of two random variables <span class="math inline">\(aX + bY\)</span> is:
<span class="math display">\[\begin{align}
Var(aX + bY) &amp;= E(aX + bY)^2-(E(aX + bY))^2\\
&amp;=E(aX + bY)^2-(a\mu_X+b\mu_Y)^2\\
&amp;=E(a^2X^2+2aXbY+b^2Y^2)-a^2\mu_X^2-2a\mu_Xb\mu_Y-b^2\mu_Y^2\\
&amp;=a^2(E(X^2)-\mu_X^2)+b^2(E(Y^2)-\mu_Y^2)+2ab(E(XY)-\mu_X\mu_Y)\\
&amp;=a^2Var(X)+b^2Var(Y)+2abCov(X,Y)
\end{align}\]</span>.<br />
Then the Variance of the sum of <span class="math inline">\(n\)</span> random variables <span class="math inline">\(W_1,W_2, . . . ,W_n\)</span> is:
<span class="math display">\[Var(\sum_{i=1}^{n}a_iW_i)=\sum_{i=1}^{n}a_i^2Var(W_i)+2\sum_{i&lt;j}a_ia_jCov(W_i,W_j)\]</span></p>
<p>The correlation coefficient of any two random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, is denoted with <span class="math inline">\(\rho(X,Y)\)</span>, and given by: <span class="math inline">\(\rho(X,Y)=\frac{Cov(X,Y)}{\sigma_X\sigma_Y}=Cov(\frac{X-\mu_X}{\sigma_X}, \frac{Y-\mu_Y}{\sigma_Y})\)</span></p>
<p>For any two random variables <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>, if we define the <span class="math inline">\(Cov(X_1,X_2)\)</span> as <span class="math inline">\(\sigma_{12}\)</span>, then
<span class="math display">\[\begin{align}
Cov(aX_1 + bX_2)&amp;=E(aX_1-a\mu_1)(bX_2-b\mu_2)\\
&amp;=abE(X_1-\mu_1)(X_2-\mu_2)\\
&amp;=abCov(X_1,X_2)\\
&amp;=ab\sigma_{12}
\end{align}\]</span> And <span class="math display">\[Var(aX_1 + bX_2)=a^2\sigma_{11}+b^2\sigma_{22}+2ab\sigma_{12}\]</span></p>
<p>We can use vector and matrix to denote <span class="math inline">\(aX_1 + bX_2\)</span> as:
<span class="math display">\[\mathbf c^T\mathbf X=\begin{bmatrix}
a&amp;b
\end{bmatrix}\begin{bmatrix}
X_1\\
X_2
\end{bmatrix}\]</span> and denote the variance-covariance matrix as <span class="math display">\[\mathbf \Sigma=\begin{bmatrix}
\sigma_{11}&amp;\sigma_{12}\\
\sigma_{12}&amp;\sigma_{22}
\end{bmatrix}\]</span>, then <span class="math display">\[\begin{align}
Var(aX_1 + bX_2)&amp;=Var(\mathbf c^T\mathbf X)\\
&amp;=a^2\sigma_{11}+b^2\sigma_{22}+2ab\sigma_{12}\\
&amp;=\mathbf c^T\mathbf \Sigma\mathbf c
\end{align}\]</span></p>
<p><span class="math inline">\(p\)</span> random variables <span class="math display">\[\mathbf X=\begin{bmatrix}
X_{1}\\
X_{2}\\
\vdots\\
X_{p}\\
\end{bmatrix}\]</span> has the expectation matrix <span class="math display">\[E(\mathbf X)=\boldsymbol\mu_{\mathbf X}=\begin{bmatrix}
\mu_{1}\\
\mu_{2}\\
\vdots\\
\mu_{p}\\
\end{bmatrix}\]</span> and the covariance matrix <span class="math display">\[\mathbf \Sigma_{\mathbf X}=\begin{bmatrix}
\sigma_{11}&amp;\sigma_{12}&amp;\cdots&amp;\sigma_{1p}\\
\sigma_{12}&amp;\sigma_{22}&amp;\cdots&amp;\sigma_{2p}\\
\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\
\sigma_{1p}&amp;\sigma_{2p}&amp;\cdots&amp;\sigma_{pp}\\
\end{bmatrix}
\]</span>
In general, the <span class="math inline">\(q\)</span> linear combinations of the <span class="math inline">\(p\)</span> random variables <span class="math inline">\(X_1,\cdots,X_p\)</span>:
<span class="math display">\[\begin{array}{c}
Z_1&amp;=c_{11}X_1+c_{12}X_2+\cdots+c_{1p}X_p\\
Z_2&amp;=c_{21}X_1+c_{22}X_2+\cdots+c_{2p}X_p\\
&amp;\vdots\\
Z_q&amp;=c_{q1}X_1+c_{q2}X_2+\cdots+c_{qp}X_p\\
\end{array}\]</span> or <span class="math display">\[\mathbf Z=\begin{bmatrix}
Z_1\\
Z_2\\
\vdots\\
Z_q
\end{bmatrix}=\begin{bmatrix}
c_{11}&amp;c_{12}&amp;\cdots&amp;c_{1p}\\
c_{21}&amp;c_{22}&amp;\cdots&amp;c_{2p}\\
\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\
c_{q1}&amp;c_{q2}&amp;\cdots&amp;c_{qp}
\end{bmatrix}\begin{bmatrix}
X_1\\
X_2\\
\vdots\\
X_q
\end{bmatrix}=\mathbf C \mathbf X
\]</span> The linear combinations <span class="math inline">\(\mathbf Z=\mathbf C \mathbf X\)</span> have:<br />
<span class="math inline">\(\boldsymbol\mu_{\mathbf Z}=E(\mathbf Z)=E(\mathbf C \mathbf X)=\mathbf C \boldsymbol\mu_{\mathbf X}\)</span><br />
<span class="math inline">\(\mathbf \Sigma_{\mathbf Z}=Cov(\mathbf Z)=Cov(\mathbf C\mathbf X)=\mathbf C\mathbf \Sigma_{\mathbf X}\mathbf C^T\)</span></p>
<p>If we collect <span class="math inline">\(n\)</span> sets of measurements on <span class="math inline">\(p\)</span> variables, and treat the measurements as random variables, the <em>random sample</em> can be defined as:
<span class="math display">\[\mathbf X=\begin{bmatrix}
X_{11}&amp;X_{12}&amp;\cdots&amp;X_{1p}\\
X_{21}&amp;X_{22}&amp;\cdots&amp;X_{2p}\\
\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\
X_{n1}&amp;X_{n2}&amp;\cdots&amp;X_{np}
\end{bmatrix}=\begin{bmatrix}
\mathbf X_1^T\\
\mathbf X_2^T\\
\mathbf \vdots\\
\mathbf X_n^T
\end{bmatrix}
\]</span>, with each set of measurements <span class="math inline">\(\mathbf X_j^T\)</span> on <span class="math inline">\(p\)</span> variables is a random vector and represent <strong>independent</strong> observations from a common joint distribution with density function <span class="math inline">\(f(\mathbf x)=f(x_1,x_2,\ldots,x_p)\)</span>. Then <span class="math inline">\(\mathbf {\overline X}=\frac{1}{n}\displaystyle\sum_{j=1}^n\mathbf X_j\)</span><br />
<span class="math display">\[\begin{align}
E(\mathbf {\overline X})&amp;=E(\frac{1}{n}\displaystyle\sum_{j=1}^n\mathbf X_j)\\
&amp;=E(\frac{1}{n}\mathbf X_1)+\ldots +E(\frac{1}{n}\mathbf X_n)\\
&amp;=\frac{1}{n}\boldsymbol\mu+\ldots +\frac{1}{n}\boldsymbol\mu\\
&amp;=\boldsymbol\mu
\end{align}\]</span> <span class="math inline">\(\boldsymbol\mu\)</span> is the <strong>population mean vector</strong>.<br />
<span class="math display">\[\begin{align}
(\mathbf {\overline X}-\boldsymbol\mu)(\mathbf {\overline X}-\boldsymbol\mu)^T&amp;=(\frac{1}{n}\displaystyle\sum_{j=1}^n\mathbf X_j-\boldsymbol\mu)(\frac{1}{n}\displaystyle\sum_{j=1}^n\mathbf X_j-\boldsymbol\mu)^T\\
&amp;=\Biggl(\frac{1}{n}\displaystyle\sum_{j=1}^n(\mathbf X_j-\boldsymbol\mu)\Biggr)\Biggl(\frac{1}{n}\displaystyle\sum_{k=1}^n(\mathbf X_k-\boldsymbol\mu)\Biggr)^T\\
&amp;=\frac{1}{n^2}\sum_{j=1}^n\sum_{k=1}^n(\mathbf X_j-\boldsymbol\mu)(\mathbf X_k-\boldsymbol\mu)^T\\
\end{align}\]</span> then
<span class="math display">\[Cov(\mathbf {\overline X})=E((\mathbf {\overline X}-\boldsymbol\mu)(\mathbf {\overline X}-\boldsymbol\mu)^T)=\frac{1}{n^2}\sum_{j=1}^n\sum_{k=1}^nE\Bigl((\mathbf X_j-\boldsymbol\mu)(\mathbf X_k-\boldsymbol\mu)^T\Bigr)
\]</span> because for <span class="math inline">\(j\ne k\)</span>, <span class="math inline">\(\mathbf X_j\)</span> and <span class="math inline">\(\mathbf X_k\)</span> are independent, so <span class="math inline">\(E\Bigl((\mathbf X_j-\boldsymbol\mu)(\mathbf X_k-\boldsymbol\mu)^T\Bigr)=0\)</span>, then <span class="math display">\[Cov(\mathbf {\overline X})=\frac{1}{n^2}\sum_{j=1}^nE\Bigl((\mathbf X_j-\boldsymbol\mu)(\mathbf X_j-\boldsymbol\mu)^T\Bigr)=\frac{1}{n^2}\sum_{j=1}^n\mathbf\Sigma=\frac{1}{n}\mathbf\Sigma\]</span> <span class="math inline">\(\mathbf\Sigma\)</span> is the <strong>population variance–covariance matrix</strong>.</p>
<p>To obtain the expected value of <span class="math inline">\(\mathbf S_n\)</span>:
<span class="math display">\[\begin{align}
E(n\mathbf S_n)=E\sum_{j=1}^n\Bigl((\mathbf X_j-\mathbf{\overline X})(\mathbf X_j-\mathbf{\overline X})^T\Bigr)&amp;=E\sum_{j=1}^n\Bigl((\mathbf X_j-\mathbf{\overline X})(\mathbf X_j^T-\mathbf{\overline X}^T)\Bigr)\\
&amp;=E\sum_{j=1}^n\Bigl((\mathbf X_j-\mathbf{\overline X})\mathbf X_j^T-(\mathbf X_j-\mathbf{\overline X})\mathbf{\overline X}^T)\Bigr)\\
&amp;=E\sum_{j=1}^n(\mathbf X_j-\mathbf{\overline X})\mathbf X_j^T-\sum_{j=1}^n(\mathbf X_j-\mathbf{\overline X})\mathbf{\overline X}^T)\\
&amp;=E\sum_{j=1}^n(\mathbf X_j-\mathbf{\overline X})\mathbf X_j^T\\
&amp;=E\Bigl(\sum_{j=1}^n\mathbf X_j\mathbf X_j^T-\sum_{j=1}^n\mathbf{\overline X}\mathbf X_j^T\Bigr)\\
&amp;=E\Bigl(\sum_{j=1}^n\mathbf X_j\mathbf X_j^T-n\mathbf{\overline X}\mathbf{\overline X}^T\Bigr)\\
&amp;=\sum_{j=1}^nE(\mathbf X_j\mathbf X_j^T)-nE(\mathbf{\overline X}\mathbf{\overline X}^T)\\
&amp;=\sum_{j=1}^n(\mathbf\Sigma+\boldsymbol\mu\boldsymbol\mu^T)-n(\frac{1}{n}\mathbf\Sigma+\boldsymbol\mu\boldsymbol\mu^T)\\
&amp;=(n-1)\mathbf\Sigma
\end{align}\]</span>, then <span class="math inline">\(E(\mathbf S_n)=\frac{n-1}{n}\mathbf\Sigma\)</span> and the <span style="color: red;"><strong>Unbiased Sample Variance–Covariance Matrix <span class="math inline">\(\mathbf S=\frac{1}{n-1}\sum_{j=1}^n(\mathbf X_j-\mathbf{\overline X})(\mathbf X_j-\mathbf{\overline X})^T\)</span></strong></span> is the Unbiased estimator of population variance–covariance matrix <span class="math inline">\(\mathbf \Sigma\)</span> <span class="math display">\[\underset{(p,p)}{\mathbf S}=\frac{n}{n-1}\mathbf S_n=\frac{1}{n-1}\sum_{j=1}^n(\mathbf X_j-\mathbf{\overline X})(\mathbf X_j-\mathbf{\overline X})^T\]</span>, here <span class="math inline">\(\underset{(p,p)}{\mathbf S}\)</span> has <span class="math inline">\((i,k)^{th}\)</span> entry <span class="math display">\[s_{i,k}=\frac{1}{n-1}\sum_{j=1}^{n}(X_{ji}-\overline X_i)(X_{jk}-\overline X_k),\quad 0\le i,k\le p\]</span>, and contains <span class="math inline">\(p\)</span> variances and <span class="math inline">\(\frac{1}{2}(p^2-p)\)</span> covariances.</p>
<p>We can also use matrix operations on the data matrix <span class="math display">\[\mathbf X=\begin{bmatrix}
x_{11}&amp;x_{12}&amp;\cdots&amp;x_{1p}\\
x_{21}&amp;x_{22}&amp;\cdots&amp;x_{2p}\\
\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\
x_{n1}&amp;x_{n2}&amp;\cdots&amp;x_{np}
\end{bmatrix}\]</span> to calculate <span class="math inline">\(\overline {\mathbf X}\)</span> and <span class="math inline">\(\mathbf S\)</span>:
let <span class="math display">\[\mathbf 1=\begin{bmatrix}
1\\
1\\
\vdots\\
1
\end{bmatrix}\]</span> then <span class="math display">\[\overline {\mathbf X}=\frac{1}{n}\mathbf X^T\mathbf 1\]</span> and <span class="math display">\[\mathbf 1\overline {\mathbf X}^T=\mathbf 1(\frac{1}{n}\mathbf X^T\mathbf 1)^T=\frac{1}{n}\mathbf 1\mathbf 1^T\mathbf X=\begin{bmatrix}
\bar{x_1}&amp;\bar{x_2}&amp;\cdots&amp;\bar{x_p}\\
\bar{x_1}&amp;\bar{x_2}&amp;\cdots&amp;\bar{x_p}\\
\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\
\bar{x_1}&amp;\bar{x_2}&amp;\cdots&amp;\bar{x_p}
\end{bmatrix}\]</span> Subtracting this result from <span class="math inline">\(\mathbf X\)</span> produces the matrix of deviations:<span class="math display">\[\mathbf X-\frac{1}{n}\mathbf 1\mathbf 1^T\mathbf X=\begin{bmatrix}
x_{11}-\bar{x_1}&amp;x_{12}-\bar{x_2}&amp;\cdots&amp;x_{1p}-\bar{x_p}\\
x_{21}-\bar{x_1}&amp;x_{22}-\bar{x_2}&amp;\cdots&amp;x_{2p}-\bar{x_p}\\
\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\
x_{n1}-\bar{x_1}&amp;x_{n2}-\bar{x_2}&amp;\cdots&amp;x_{np}-\bar{x_p}
\end{bmatrix}\]</span> Now, the matrix <span class="math inline">\((n-1)\mathbf S\)</span> representing sums of squares and cross products: <span class="math display">\[\begin{align}
(n-1)\mathbf S&amp;=(\mathbf X-\frac{1}{n}\mathbf 1\mathbf 1^T\mathbf X)^T(\mathbf X-\frac{1}{n}\mathbf 1\mathbf 1^T\mathbf X)\\
&amp;=\Bigl((\mathbf 1-\frac{1}{n}\mathbf 1\mathbf 1^T)\mathbf X\Bigr)^T\Bigl((\mathbf 1-\frac{1}{n}\mathbf 1\mathbf 1^T)\mathbf X\Bigr)\\
&amp;=\mathbf X^T(\mathbf 1-\frac{1}{n}\mathbf 1\mathbf 1^T)^T(\mathbf 1-\frac{1}{n}\mathbf 1\mathbf 1^T)\mathbf X\\
&amp;=\mathbf X^T(\mathbf 1-\frac{1}{n}\mathbf 1\mathbf 1^T)\mathbf X\\
\end{align}\]</span> then <span class="math display">\[\mathbf S=\frac{1}{n-1}\mathbf X^T(\mathbf 1-\frac{1}{n}\mathbf 1\mathbf 1^T)\mathbf X\]</span></p>
<p>The <span class="math inline">\(p\times p\)</span> sample <span style="color: red;"><strong>standard deviation matrix</strong></span> is <span class="math display">\[
\underset{(p\times p)}{\mathbf D^{\frac{1}{2}}}=\begin{bmatrix}
\sqrt{s_{11}}&amp;0&amp;\cdots&amp;0\\
0&amp;\sqrt{s_{22}}&amp;\cdots&amp;0\\
\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\
0&amp;0&amp;\cdots&amp;\sqrt{s_{pp}}
\end{bmatrix}
\]</span> <span class="math display">\[
\underset{(p\times p)}{\mathbf D^{-\frac{1}{2}}}=\begin{bmatrix}
\frac{1}{\sqrt{s_{11}}}&amp;0&amp;\cdots&amp;0\\
0&amp;\frac{1}{\sqrt{s_{22}}}&amp;\cdots&amp;0\\
\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\
0&amp;0&amp;\cdots&amp;\frac{1}{\sqrt{s_{pp}}}
\end{bmatrix}
\]</span> since <span class="math display">\[
\underset{(p\times p)}{\mathbf S}=\begin{bmatrix}
s_{11}&amp;s_{12}&amp;\cdots&amp;s_{1p}\\
s_{12}&amp;s_{22}&amp;\cdots&amp;s_{2p}\\
\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\
s_{1p}&amp;s_{2p}&amp;\cdots&amp;s_{pp}
\end{bmatrix}
\]</span> and the <strong>sample correlation matrix</strong> <span class="math display">\[
\underset{(p\times p)}{\mathbf R}=\begin{bmatrix}
\frac{s_{11}}{\sqrt{s_{11}}\sqrt{s_{11}}}&amp;\frac{s_{12}}{\sqrt{s_{11}}\sqrt{s_{22}}}&amp;\cdots&amp;\frac{s_{1p}}{\sqrt{s_{11}}\sqrt{s_{pp}}}\\
\frac{s_{12}}{\sqrt{s_{11}}\sqrt{s_{22}}}&amp;\frac{s_{22}}{\sqrt{s_{22}}\sqrt{s_{22}}}&amp;\cdots&amp;\frac{s_{2p}}{\sqrt{s_{22}}\sqrt{s_{pp}}}\\
\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\
\frac{s_{1p}}{\sqrt{s_{11}}\sqrt{s_{pp}}}&amp;\frac{s_{2p}}{\sqrt{s_{22}}\sqrt{s_{pp}}}&amp;\cdots&amp;\frac{s_{pp}}{\sqrt{s_{pp}}\sqrt{s_{pp}}}\\
\end{bmatrix}=\begin{bmatrix}
1&amp;r_{12}&amp;\cdots&amp;r_{1p}\\
r_{12}&amp;1&amp;\cdots&amp;r_{2p}\\
\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\
r_{1p}&amp;r_{2p}&amp;\cdots&amp;1\\
\end{bmatrix}
\]</span> we have <span class="math display">\[\underset{(p\times p)}{\mathbf R}=\underset{(p\times p)}{\mathbf D^{-\frac{1}{2}}}\underset{(p\times p)}{\mathbf S}\underset{(p\times p)}{\mathbf D^{-\frac{1}{2}}}\]</span> and <span class="math display">\[\mathbf S=\mathbf D^{\frac{1}{2}}\mathbf R\mathbf D^{\frac{1}{2}}\]</span></p>
