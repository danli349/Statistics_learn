<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.80.0" />


<title>Factor analysis - A Hugo website</title>
<meta property="og:title" content="Factor analysis - A Hugo website">


  <link href='../../../../favicon.ico' rel='icon' type='image/x-icon'/>



  








<link href='//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css' rel='stylesheet' type='text/css' />



<link rel="stylesheet" href="../../../../css/fonts.css" media="all">
<link rel="stylesheet" href="../../../../css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="../../../../" class="nav-logo">
    <img src="../../../../images/logo.png"
         width="50"
         height="50"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="../../../../about/">about</a></li>
    
    <li><a href="../../../../vitae/">CV</a></li>
    
    <li><a href="https://github.com/danli349">GitHub</a></li>
    
    <li><a href="https://scholar.google.com/citations?user=mNjLK8EAAAAJ&amp;hl=en">Googlr Scholar</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">9 min read</span>
    

    <h1 class="article-title">Factor analysis</h1>

    
    <span class="article-date">2020-10-11</span>
    

    <div class="article-content">
      
<script src="../../../../rmarkdown-libs/header-attrs/header-attrs.js"></script>


<p>Let <span class="math inline">\(\mathbf X\)</span> is drawn from a <span class="math inline">\(p\)</span>-variate normal distribution with <span class="math inline">\(N_p(\boldsymbol\mu, \boldsymbol\Sigma)\)</span> distribution. The matrix of factor loadings <span class="math display">\[\mathbf L=\begin{bmatrix}
\ell_{11}&amp;\ell_{12}&amp;\cdots&amp;\ell_{1m}\\
\ell_{21}&amp;\ell_{22}&amp;\cdots&amp;\ell_{2m}\\
\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\
\ell_{p1}&amp;\ell_{p2}&amp;\cdots&amp;\ell_{pm}\\
\end{bmatrix}\]</span> with <span class="math inline">\(\ell_{ij}\)</span> is the <em>loading</em> of the <span class="math inline">\(i^{th}\)</span> variable on the <span class="math inline">\(j^{th}\)</span> factor.</p>
<p>The common factor is <span class="math display">\[\mathbf F=\begin{bmatrix}
F_1\\
F_2\\
\vdots\\
F_m\\
\end{bmatrix}\]</span> with <span class="math inline">\(E(\mathbf F)=\underset{(m\times 1)}{\mathbf0}\)</span>, <span class="math inline">\(Var(F_j)=1,\quad (j=1,2,\cdots,m)\)</span> and <span class="math inline">\(Cov(\mathbf F)=E(\mathbf F\mathbf F^T)=\underset{(m\times m)}{\mathbf I}\)</span> Then the <strong>Orthogonal factor model</strong> is <span class="math display">\[\underset{(p\times1)}{\mathbf X-\boldsymbol\mu}=\underset{(p\times m)}{\mathbf L}\underset{(m\times1)}{\mathbf F}+\underset{(p\times1)}{\boldsymbol\epsilon}\]</span> with <span class="math inline">\(E(\boldsymbol\epsilon)=\underset{(p\times 1)}{\mathbf0}\)</span> and <span class="math display">\[Cov(\boldsymbol\epsilon)=E(\boldsymbol\epsilon\boldsymbol\epsilon^T)=\boldsymbol\Psi=\begin{bmatrix}
\psi_1&amp;0&amp;\cdots&amp;0\\
0&amp;\psi_2&amp;\cdots&amp;0\\
\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\
0&amp;0&amp;\cdots&amp;\psi_p\\
\end{bmatrix}\]</span> with <span class="math inline">\(Var(\epsilon_i)=\psi_i\)</span> and <span class="math inline">\(\mathbf F\)</span> and <span class="math inline">\(\boldsymbol\epsilon\)</span> are independent with <span class="math inline">\(Cov(\boldsymbol\epsilon,\mathbf F)=E(\boldsymbol\epsilon\mathbf F^T)=\underset{(p\times m)}{\mathbf0}\)</span>
Because <span class="math inline">\(\mathbf L\)</span> is fixed, then <span class="math display">\[\begin{align}
\boldsymbol\Sigma=Cov(\mathbf X)&amp;=E(\mathbf X-\boldsymbol\mu)(\mathbf X-\boldsymbol\mu)^T\\
&amp;=E(\mathbf L\mathbf F+\boldsymbol\epsilon)(\mathbf L\mathbf F+\boldsymbol\epsilon)^T\\
&amp;=E(\mathbf L\mathbf F+\boldsymbol\epsilon)((\mathbf L\mathbf F)^T+\boldsymbol\epsilon^T)\\
&amp;=E\Bigl(\mathbf L\mathbf F(\mathbf L\mathbf F)^T+\boldsymbol\epsilon(\mathbf L\mathbf F)^T+\mathbf L\mathbf F\boldsymbol\epsilon^T+\boldsymbol\epsilon\boldsymbol\epsilon^T\Bigr)\\
&amp;=\mathbf LE(\mathbf F\mathbf F^T)\mathbf L^T+\mathbf0+\mathbf0+E(\boldsymbol\epsilon\boldsymbol\epsilon^T)\\
&amp;=\mathbf L\mathbf L^T+\boldsymbol\Psi
\end{align}\]</span> or <span class="math display">\[Var(X_i)=\underset{Var(X_i)}{\underbrace{\sigma_{ii}}}=\mathbf L_i\mathbf L_i^T+\psi_i=\underset{\text{communality}}{\underbrace{\ell_{i1}^2+\ell_{i2}^2+\cdots+\ell_{im}^2}}+\underset{\text{specific variance}}{\underbrace{\psi_i}}\]</span> with <span class="math inline">\(\mathbf L_i\)</span> is the <span class="math inline">\(i^{th}\)</span> row of <span class="math inline">\(\mathbf L\)</span> We can denote the <span class="math inline">\(i^{th}\)</span> <strong>communality</strong> as <span class="math inline">\(h_i^2=\ell_{i1}^2+\ell_{i2}^2+\cdots+\ell_{im}^2,\quad (i=1,2,\cdots,p)\)</span>, which is the sum of squares of the loadings of the <span class="math inline">\(i^{th}\)</span> variable on the <span class="math inline">\(m\)</span> common factors, and the total variance of the <span class="math inline">\(i^{th}\)</span> variable is the sum of <strong>communality</strong> and <strong>specific variance</strong> <span class="math inline">\(\sigma_{ii}=h_i^2+\psi_i\)</span>
<span class="math display">\[Cov(X_i,X_k)=E(\mathbf L_i^T\mathbf F+\epsilon_i)(\mathbf L_k^T\mathbf F+\epsilon_k)^T=\mathbf L_i^T\mathbf L_k=\ell_{i1}\ell_{k1}+\ell_{i2}\ell_{k2}+\cdots+\ell_{im}\ell_{km}\]</span>
<span class="math display">\[Cov(\mathbf X,\mathbf F)=E(\mathbf X-\boldsymbol\mu)\mathbf F^T=E(\mathbf L\mathbf F+\boldsymbol\epsilon)\mathbf F^T=\mathbf LE(\mathbf F\mathbf F^T)+E(\boldsymbol\epsilon\mathbf F^T)=\mathbf L\]</span> or <span class="math display">\[Cov(X_i,F_j)=E(X_i-\mu_i)\mathbf F_j^T=E(\mathbf L_i^T\mathbf F+\epsilon_i)\mathbf F_j^T=\ell_{ij}\]</span></p>
<ol style="list-style-type: decimal">
<li><p>The Principal Component (and Principal Factor) Method:<br />
Let <span class="math inline">\(\boldsymbol\Sigma\)</span> has eigenvalue–eigenvector pairs <span class="math inline">\((\lambda_i,\mathbf e_i)\)</span> with <span class="math inline">\(\lambda_1\ge\lambda_2\ge\cdots\ge\lambda_p\ge0\)</span> Then <span class="math display">\[\boldsymbol\Sigma=\sum_{i=1}^{p}\lambda_i\mathbf e_i\mathbf e_i^T=\begin{bmatrix}
\sqrt{\lambda_1}\mathbf e_1&amp;\sqrt{\lambda_2}\mathbf e_2&amp;\cdots&amp;\sqrt{\lambda_p}\mathbf e_p\\
\end{bmatrix}\begin{bmatrix}
\sqrt{\lambda_1}\mathbf e_1^T\\
\sqrt{\lambda_2}\mathbf e_2^T\\
\vdots\\
\sqrt{\lambda_p}\mathbf e_p^T\\
\end{bmatrix}\]</span> This factor analysis model has as many factors as variables <span class="math inline">\((m=p)\)</span> and specific variances <span class="math inline">\(\psi_i=0\)</span> for all <span class="math inline">\(i\)</span>. The loading matrix <span class="math inline">\(\mathbf L\)</span> has <span class="math inline">\(j^{th}\)</span> column given by <span class="math inline">\(\sqrt{\lambda_j}\mathbf e_j\)</span> and <span class="math inline">\(\boldsymbol\Sigma=\mathbf L\mathbf L^T\)</span>. when the last <span class="math inline">\(p-m\)</span> eigenvalues are small we can neglect them, then <span class="math display">\[\boldsymbol\Sigma\approx\sum_{i=1}^{m}\lambda_i\mathbf e_i\mathbf e_i^T+\boldsymbol\Psi=\begin{bmatrix}
\sqrt{\lambda_1}\mathbf e_1&amp;\sqrt{\lambda_2}\mathbf e_2&amp;\cdots&amp;\sqrt{\lambda_m}\mathbf e_m\\
\end{bmatrix}\begin{bmatrix}
\sqrt{\lambda_1}\mathbf e_1^T\\
\sqrt{\lambda_2}\mathbf e_2^T\\
\vdots\\
\sqrt{\lambda_m}\mathbf e_m^T\\
\end{bmatrix}+\boldsymbol\Psi\\
=\underset{(p\times m)}{\mathbf L}\underset{(m\times p)}{\mathbf L^T}+\begin{bmatrix}
\psi_1&amp;0&amp;\cdots&amp;0\\
0&amp;\psi_2&amp;\cdots&amp;0\\
\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\
0&amp;0&amp;\cdots&amp;\psi_p\\
\end{bmatrix}\]</span> where <span class="math display">\[\psi_i=\sigma_{ii}-\sum_{j=1}^{m}\ell_{ij}^2,\quad (i=1,2,\cdots,p)\]</span> When the sample covariance matrix <span class="math inline">\(\mathbf S\)</span> has eigenvalue–eigenvector pairs <span class="math inline">\((\hat{\lambda}_i,\hat{\mathbf e}_i)\)</span> with <span class="math inline">\(\hat{\lambda}_1\ge\hat{\lambda}_2\ge\cdots\ge\hat{\lambda}_p\ge0\)</span> <span class="math display">\[\mathbf S\approx\begin{bmatrix}
\sqrt{\hat{\lambda}_1}\hat{\mathbf e}_1&amp;\sqrt{\hat{\lambda}_2}\hat{\mathbf e}_2&amp;\cdots&amp;\sqrt{\hat{\lambda}_m}\hat{\mathbf e}_m\\
\end{bmatrix}\begin{bmatrix}
\sqrt{\hat{\lambda}_1}\hat{\mathbf e}_1^T\\
\sqrt{\hat{\lambda}_2}\hat{\mathbf e}_2^T\\
\vdots\\
\sqrt{\hat{\lambda}_m}\hat{\mathbf e}_m^T\\
\end{bmatrix}+\widetilde{\boldsymbol\Psi}\\
=\underset{(p\times m)}{\widetilde{\mathbf L}}\underset{(m\times p)}{\widetilde{\mathbf L}^T}+\begin{bmatrix}
\widetilde{\psi}_1&amp;0&amp;\cdots&amp;0\\
0&amp;\widetilde{\psi}_2&amp;\cdots&amp;0\\
\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\
0&amp;0&amp;\cdots&amp;\widetilde{\psi}_p\\
\end{bmatrix}\]</span> <span class="math inline">\(\widetilde{\mathbf L}\)</span> is the the matrix of estimated factor loadings and <span class="math display">\[\widetilde{\psi}_i=s_{ii}-\sum_{j=1}^{m}\widetilde{\ell}_{ij}^2\]</span> <span class="math display">\[\widetilde{h}_i^2=\widetilde{\ell}_{i1}^2+\widetilde{\ell}_{i2}^2+\cdots+\widetilde{\ell}_{im}^2\]</span> <span class="math display">\[Var(X_i)=s_{ii}=\widetilde{h}_i^2+\widetilde{\psi}_i\]</span> The residual matrix is <span class="math inline">\(\mathbf S-\widetilde{\mathbf L}\widetilde{\mathbf L}^T-\widetilde{\boldsymbol\Psi}\)</span> The contribution to the total sample variance <span class="math inline">\(s_{11}+s_{22}+\cdots+s_{pp}=tr(\mathbf S)\)</span> from the first common factor is <span class="math display">\[\widetilde{\ell}_{11}^2+\widetilde{\ell}_{21}^2+\cdots+\widetilde{\ell}_{p1}^2=(\sqrt{\hat{\lambda}_1}\hat{\mathbf e}_1)^T(\sqrt{\hat{\lambda}_1}\hat{\mathbf e}_1)=\hat{\lambda}_1\]</span> Then proportion of total sample variance due to <span class="math inline">\(j^{th}\)</span> factor is <span class="math display">\[\frac{\hat{\lambda}_j}{tr(\mathbf S)}\]</span> The number of common factors in the model are increased until a <strong>suitable proportion</strong> of the total sample variance has been explained.</p></li>
<li><p>If the factor model of correlation matrix is <span class="math inline">\(\mathbf R=\mathbf L\mathbf L^T+\boldsymbol\Psi\)</span> then the <span class="math inline">\(m\)</span> common factors should account for the <strong>communality portions</strong> <span class="math inline">\(h_i^2\)</span> of the diagonal elements <span class="math inline">\(r_{ii}=1=h_i^2+\psi_i\)</span> as well as the off-diagonal elements of <span class="math inline">\(\mathbf R\)</span>. Then the <strong>reduced</strong> sample correlation matrix <span class="math inline">\(\mathbf R_r\)</span> is: <span class="math display">\[\mathbf R_r=\mathbf R-\boldsymbol\Psi=\begin{bmatrix}
h_1^{*2}&amp;r_{12}&amp;\cdots&amp;r_{1p}\\
r_{12}&amp;h_2^{*2}&amp;\cdots&amp;r_{2p}\\
\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\
r_{1p}&amp;r_{2p}&amp;\cdots&amp;h_p^{*2}\\
\end{bmatrix}\]</span> with <span class="math inline">\(\psi_i\)</span> are the specific variances and the <span class="math inline">\(i^{th}\)</span> diagonal element of <span class="math inline">\(\mathbf R\)</span> is <span class="math inline">\(h_i^{*2}=1-\psi^{*}_i\)</span>. The reduced sample correlation matrix <span class="math inline">\(\mathbf R_r\)</span> should be accounted for by the <span class="math inline">\(m\)</span> common factors <span class="math inline">\(\mathbf R\doteq\mathbf L_r^*\mathbf L_r^{*T}\)</span> <span class="math display">\[\mathbf L_r^*=\begin{bmatrix}
\sqrt{\hat{\lambda}_1^*}\hat{\mathbf e}_1^*&amp;\sqrt{\hat{\lambda}_2^*}\hat{\mathbf e}_2^*&amp;\cdots&amp;\sqrt{\hat{\lambda}_m^*}\hat{\mathbf e}_m^*\\
\end{bmatrix}\]</span> <span class="math display">\[\psi_i^*=1-\sum_{j=1}^{m}\ell_{ij}^{*2}=1-\widetilde{h}_i^{*2}\]</span> with <span class="math inline">\(\widetilde{h}_i^{*2}\)</span> the estimated communalities.</p></li>
<li><p>The maximum likelihood method: When <span class="math inline">\(\mathbf X_j\)</span> are random sample from <span class="math inline">\(N_p(\boldsymbol\mu, \boldsymbol\Sigma)\)</span>, and <span class="math inline">\(\mathbf F_j\)</span> and <span class="math inline">\(\boldsymbol\epsilon_j\)</span> are jointly normal, then the observations <span class="math inline">\(\underset{(p\times1)}{\mathbf X_j-\boldsymbol\mu}=\underset{(p\times m)}{\mathbf L}\underset{(m\times1)}{\mathbf F_j}+\underset{(p\times1)}{\boldsymbol\epsilon_j}\)</span> are normal distributed, where <span class="math inline">\(Cov(\mathbf X)=\boldsymbol\Sigma=\mathbf L\mathbf L^T+\boldsymbol\Psi\)</span> is the covariance matrix for the <span class="math inline">\(m\)</span> common factor model. Then the likelihood is <span class="math display">\[\begin{align}
L(\boldsymbol\mu, \boldsymbol\Sigma)&amp;=\prod_{j=1}^{n}\Biggl[\frac{1}{(2\pi)^{p/2}|\mathbf\Sigma|^{1/2}}e^{-\frac{1}{2}(\mathbf x_j-\boldsymbol \mu)^T\mathbf\Sigma^{-1}(\mathbf x_j-\boldsymbol \mu)}\Biggr]\\
&amp;=\frac{1}{(2\pi)^{\frac{np}{2}}|\mathbf\Sigma|^{n/2}}e^{-\frac{1}{2}\sum_{j=1}^{n}(\mathbf x_j-\boldsymbol \mu)^T\mathbf\Sigma^{-1}(\mathbf x_j-\boldsymbol \mu)}\\
&amp;=\frac{1}{(2\pi)^{\frac{np}{2}}|\mathbf\Sigma|^{n/2}}e^{-\frac{1}{2}tr\Biggl[\mathbf\Sigma^{-1}\sum_{j=1}^{n}(\mathbf x_j-\boldsymbol \mu)^T(\mathbf x_j-\boldsymbol\mu)\Biggr]}\\
&amp;=\frac{1}{(2\pi)^{\frac{np}{2}}|\mathbf\Sigma|^{n/2}}e^{-\frac{1}{2}tr\Biggl[\mathbf\Sigma^{-1}\Biggl(\sum_{j=1}^{n}(\mathbf x_j-\overline{\mathbf x})(\mathbf x_j-\overline{\mathbf x})^T+n(\overline{\mathbf x}-\boldsymbol\mu)(\overline{\mathbf x}-\boldsymbol\mu)^T\Biggr)\Biggr]}\\
\end{align}\]</span> with <span class="math inline">\(\boldsymbol\Sigma=\mathbf L\mathbf L^T+\boldsymbol\Psi\)</span> We can define <span class="math inline">\(\mathbf L\)</span> by imposing the <strong>uniqueness</strong> condition <span class="math inline">\(\mathbf L^T\boldsymbol\Psi^{-1}\mathbf L=\boldsymbol\Delta\)</span> with <span class="math inline">\(\boldsymbol\Delta\)</span> a diagonal matrix. The maximum likelihood estimators of the likelihood is <span class="math inline">\(\hat{\mathbf L}\)</span>,<span class="math inline">\(\hat{\boldsymbol\Psi}\)</span> and <span class="math inline">\(\boldsymbol\mu=\bar{\mathbf x}\)</span> subject to <span class="math inline">\(\hat{\mathbf L}^T\hat{\boldsymbol\Psi}^{-1}\hat{\mathbf L}=\boldsymbol\Delta\)</span>. The maximum likelihood estimates of the communalities are <span class="math inline">\(\hat{h_i^2}=\hat{\ell}_{i1}^2+\hat{\ell}_{i2}^2+\cdots+\hat{\ell}_{im}^2 \quad(i=1,2,\cdots,p)\)</span> and the proportion of total sample variance due to <span class="math inline">\(j^{th}\)</span> factor is <span class="math display">\[\frac{\hat{\ell}_{1j}^2+\hat{\ell}_{2j}^2+\cdots+\hat{\ell}_{pj}^2}{tr(\mathbf S)}\]</span> If the random variables are standardized with <span class="math display">\[Z_i=\frac{X_i-\mu_i}{\sqrt{\sigma_{ii}}}\]</span>, with matrix notation <span class="math inline">\(\mathbf Z=(\mathbf D^{\frac{1}{2}})^{-1}(\mathbf X-\boldsymbol\mu)\)</span> where <span class="math display">\[\mathbf D=\begin{bmatrix}
\sigma_{11}&amp;0&amp;0&amp;\cdots&amp;0\\
0&amp;\sigma_{22}&amp;0&amp;\cdots&amp;0\\
\vdots&amp;\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\
0&amp;0&amp;0&amp;\cdots&amp;\sigma_{pp}\\
\end{bmatrix}\]</span> Then <span class="math inline">\(E(\mathbf Z)=\mathbf0\)</span> and <span class="math display">\[Cov(\mathbf Z)=(\mathbf D^{-\frac{1}{2}})\boldsymbol\Sigma(\mathbf D^{-\frac{1}{2}})=\begin{bmatrix}
\frac{\sigma_{11}}{\sqrt{\sigma_{11}}\sqrt{\sigma_{11}}}&amp;\frac{\sigma_{12}}{\sqrt{\sigma_{11}}\sqrt{\sigma_{22}}}&amp;\frac{\sigma_{13}}{\sqrt{\sigma_{11}}\sqrt{\sigma_{33}}}&amp;\cdots&amp;\frac{\sigma_{1p}}{\sqrt{\sigma_{11}}\sqrt{\sigma_{pp}}}\\
\frac{\sigma_{12}}{\sqrt{\sigma_{11}}\sqrt{\sigma_{22}}}&amp;\frac{\sigma_{22}}{\sqrt{\sigma_{22}}\sqrt{\sigma_{22}}}&amp;\frac{\sigma_{23}}{\sqrt{\sigma_{22}}\sqrt{\sigma_{33}}}&amp;\cdots&amp;\frac{\sigma_{2p}}{\sqrt{\sigma_{22}}\sqrt{\sigma_{pp}}}\\
\vdots&amp;\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\
\frac{\sigma_{1p}}{\sqrt{\sigma_{11}}\sqrt{\sigma_{pp}}}&amp;\frac{\sigma_{2p}}{\sqrt{\sigma_{22}}\sqrt{\sigma_{pp}}}&amp;\frac{\sigma_{3p}}{\sqrt{\sigma_{33}}\sqrt{\sigma_{pp}}}&amp;\cdots&amp;\frac{\sigma_{pp}}{\sqrt{\sigma_{pp}}\sqrt{\sigma_{pp}}}\\
\end{bmatrix}=\boldsymbol\rho\\
=(\mathbf D^{-\frac{1}{2}}\mathbf L)(\mathbf D^{-\frac{1}{2}}\mathbf L)^T+\mathbf D^{-\frac{1}{2}}\boldsymbol\Psi\mathbf D^{-\frac{1}{2}}\]</span> Thus <span class="math inline">\(\boldsymbol\rho\)</span> can be factorized with loading matrix <span class="math inline">\(\mathbf L_z=\mathbf D^{-\frac{1}{2}}\mathbf L\)</span> and specific variance matrix <span class="math inline">\(\boldsymbol\Psi_z=\mathbf D^{-\frac{1}{2}}\boldsymbol\Psi\mathbf D^{-\frac{1}{2}}\)</span>. If <span class="math inline">\(\hat{\mathbf D}^{-\frac{1}{2}}\)</span> and <span class="math inline">\(\hat{\mathbf L}\)</span> are the maximum likelihood estimators of <span class="math inline">\(\mathbf D^{-\frac{1}{2}}\)</span> and <span class="math inline">\(\mathbf L\)</span>, then the maximum likelihood estimators of <span class="math inline">\(\boldsymbol\rho\)</span> is <span class="math display">\[\hat{\boldsymbol\rho}=(\hat{\mathbf D}^{-\frac{1}{2}}\hat{\mathbf L})(\hat{\mathbf D}^{-\frac{1}{2}}\hat{\mathbf L})^T+\hat{\mathbf D}^{-\frac{1}{2}}\hat{\boldsymbol\Psi}\hat{\mathbf D}^{-\frac{1}{2}}=\hat{\mathbf L}_z\hat{\mathbf L}_z^T+\hat{\boldsymbol\Psi}_z\]</span> The proportion of total standardized sample variance due to <span class="math inline">\(j^{th}\)</span> factor is <span class="math display">\[\frac{\hat{\ell}_{1j}^2+\hat{\ell}_{2j}^2+\cdots+\hat{\ell}_{pj}^2}{tr(\boldsymbol\rho)}=\frac{\hat{\ell}_{1j}^2+\hat{\ell}_{2j}^2+\cdots+\hat{\ell}_{pj}^2}{p}\]</span></p></li>
<li><p>Test for the Number of Common Factors:<br />
To test the adequacy of the <span class="math inline">\(m\)</span> common factor model <span class="math display">\[\underset{(p\times1)}{\mathbf X-\boldsymbol\mu}=\underset{(p\times m)}{\mathbf L}\underset{(m\times1)}{\mathbf F}+\underset{(p\times1)}{\boldsymbol\epsilon}\]</span>
The NULL hypothesis is <span class="math display">\[H_0:\underset{(p\times p)}{\boldsymbol\Sigma}=Cov(\mathbf X)=E(\mathbf X-\boldsymbol\mu)(\mathbf X-\boldsymbol\mu)^T=\underset{(p\times m)}{\mathbf L}\underset{(m\times p)}{\mathbf L^T}+\boldsymbol\Psi\]</span> versus <span class="math display">\[H_1: \underset{(p\times p)}{\boldsymbol\Sigma}=Cov(\mathbf X)=\underset{(p\times q)}{\mathbf L}\underset{(q\times p)}{\mathbf L^T}+\boldsymbol\Psi, \quad (q\ne m)\]</span> Under <span class="math inline">\(H_0\)</span>, the maximum likelihood estimates of <span class="math inline">\(\mathbf\Sigma\)</span> is <span class="math inline">\(\hat{\mathbf\Sigma}=\hat{\mathbf L}\hat{\mathbf L}^T+\hat{\boldsymbol\Psi}\)</span> and <span class="math inline">\(\hat{\mathbf L}\)</span>,<span class="math inline">\(\hat{\boldsymbol\Psi}\)</span> are the maximum likelihood estimates of <span class="math inline">\(\mathbf L\)</span>,<span class="math inline">\(\boldsymbol\Psi\)</span>. Then the maximum likelihood function is <span class="math display">\[\frac{1}{(2\pi)^{\frac{np}{2}}|\mathbf\Sigma|^{n/2}}e^{-\frac{1}{2}tr\Biggl[\mathbf\Sigma^{-1}\Biggl(\sum_{j=1}^{n}(\mathbf x_j-\overline{\mathbf x})(\mathbf x_j-\overline{\mathbf x})^T+n(\overline{\mathbf x}-\boldsymbol\mu)(\overline{\mathbf x}-\boldsymbol\mu)^T\Biggr)\Biggr]}\\
=\frac{1}{(2\pi)^{\frac{np}{2}}|\hat{\mathbf L}\hat{\mathbf L}^T+\hat{\boldsymbol\Psi}|^{n/2}}e^{-\frac{1}{2}tr\Biggl[(\hat{\mathbf L}\hat{\mathbf L}^T+\hat{\boldsymbol\Psi})^{-1}\Biggl(\sum_{j=1}^{n}(\mathbf x_j-\overline{\mathbf x})(\mathbf x_j-\overline{\mathbf x})^T\Biggr)\Biggr]}\\
=\frac{1}{(2\pi)^{\frac{np}{2}}|\hat{\mathbf L}\hat{\mathbf L}^T+\hat{\boldsymbol\Psi}|^{n/2}}e^{-\frac{1}{2}tr\Biggl[(\hat{\mathbf L}\hat{\mathbf L}^T+\hat{\boldsymbol\Psi})^{-1}n\mathbf S_n\Biggr]}\]</span> with the maximum likelihood estimates of <span class="math inline">\(\boldsymbol\mu\)</span> is <span class="math inline">\(\boldsymbol\mu=\bar{\mathbf x}\)</span></p></li>
<li><p>Factor Rotation<br />
The <span class="math inline">\((p\times m)\)</span> matrix of rotated loadings is <span class="math inline">\(\hat{\mathbf L}^*=\hat{\mathbf L}\mathbf T\)</span>, where <span class="math inline">\(\hat{\mathbf L}\)</span> is the <span class="math inline">\((p\times m)\)</span> matrix of estimated factor loadings and <span class="math inline">\(\mathbf T\mathbf T^T=\mathbf T^T\mathbf T=\mathbf I\)</span>. If <span class="math inline">\(\beta\)</span> is the rotation angle, <span class="math display">\[cos(\alpha+\beta)=cos\alpha cos\beta-sin\alpha sin\beta=xcos\beta-ysin\beta=\begin{bmatrix}
x&amp;y
\end{bmatrix}\begin{bmatrix}
cos\beta\\
-sin\beta
\end{bmatrix}\]</span> and <span class="math display">\[sin(\alpha+\beta)=sin\alpha cos\beta+cos\alpha sin\beta=xsin\beta+ycos\beta=\begin{bmatrix}
x&amp;y
\end{bmatrix}\begin{bmatrix}
sin\beta\\
cos\beta
\end{bmatrix}\]</span> Then <span class="math display">\[\mathbf T=\begin{bmatrix}
cos\beta&amp;sin\beta\\
-sin\beta&amp;cos\beta
\end{bmatrix}\]</span> which denotes the counterclockwise rotation of the <span class="math inline">\((x,y)\)</span> dots or clockwise rotation of the coordinate axes.
Then <span class="math display">\[\hat{\boldsymbol\Sigma}=\mathbf S_n=\hat{\mathbf L}\hat{\mathbf L}^T+\hat{\boldsymbol\Psi}=\hat{\mathbf L}\mathbf T\mathbf T^T\hat{\mathbf L}^T+\hat{\boldsymbol\Psi}=\hat{\mathbf L}^*\hat{\mathbf L}^{*T}+\hat{\boldsymbol\Psi}\]</span>, which indicates that the residual matrix remains unchanged <span class="math display">\[\mathbf S_n-\hat{\mathbf L}\hat{\mathbf L}^T-\hat{\boldsymbol\Psi}=\mathbf S_n-\hat{\mathbf L}^*\hat{\mathbf L}^{*T}-\hat{\boldsymbol\Psi}\]</span> and the specific variances <span class="math inline">\(\hat{\psi}_i\)</span> and the communalities <span class="math inline">\(\hat{h^2}_i\)</span> are unaltered. The <span style="color: red;"><strong>varimax (or normal varimax) criterion</strong></span>: <span class="math display">\[V=\frac{1}{p}\sum_{j=1}^{m}\Biggl[\sum_{i=1}^{p}\Biggl(\frac{\hat{\ell}_{ij}^*}{\hat{h}_i}\Biggr)^4-\Biggl(\sum_{i=1}^{p}\frac{\hat{\ell}_{ij}^{*2}}{\hat{h}_i^2}\Biggr)^2/p\Biggr]\]</span> which selects the orthogonal transformation <span class="math inline">\(\mathbf T\)</span> that makes <span class="math inline">\(V\)</span> as large as possible.</p></li>
<li><p>Factor Scores<br />
The estimated values of the <span class="math inline">\(j^{th}\)</span> common factors <span class="math display">\[\mathbf F=\begin{bmatrix}
F_1\\
F_2\\
\vdots\\
F_m\\
\end{bmatrix}\]</span> are <span class="math inline">\(\hat{\mathbf f_j},\quad j=1,2,\cdots,n\)</span> which are called factor scores.</p></li>
<li><p>PCA and SVD decomposition<br />
Consider an <span class="math inline">\(n\times m\)</span> data matrix <span class="math inline">\(\mathbf X\)</span>. The singular value decomposition is <span class="math inline">\(\underset{(n\times m)}{\mathbf X}=\underset{(n\times n)}{\mathbf U}\underset{(n\times m)}{\mathbf S}\underset{(m\times m)}{\mathbf V^T}\)</span>, where where <span class="math inline">\(\mathbf V\)</span> has as its columns the (normalized) eigenvectors of <span class="math inline">\(\mathbf X^T\mathbf X\)</span>. A rotation is a change of coordinates and amounts to writing the above equality as: <span class="math display">\[\mathbf X=\mathbf U\mathbf S(\mathbf T\mathbf T^T)\mathbf V^T=(\mathbf U\mathbf S\mathbf T)(\mathbf T^T\mathbf V^T)=\underset{(n\times m)}{\mathbf U^*}\underset{(m\times m)}{\mathbf V^*}\]</span> with <span class="math inline">\(\mathbf T\)</span> being an orthogonal matrix chosen to achieve a <span class="math inline">\(\mathbf V^*\)</span> close to sparse (maximum contrast between entries). Because we never rotate all PC. Rather, we consider a subset of <span class="math inline">\(k&lt;m\)</span> which provides still a decent rank-<span class="math inline">\(k\)</span> approximation of <span class="math inline">\(\mathbf X\)</span> <span class="math display">\[\mathbf X\approx\mathbf U_k\mathbf S_k\mathbf V_k^T\]</span>, so the rotated solution is now <span class="math display">\[\mathbf X\approx(\underset{(n\times n)}{\mathbf U_k}\underset{(n\times k)}{\mathbf S_k}\underset{(k\times k)}{\mathbf T_k})(\underset{(k\times k)}{\mathbf T_k^T}\underset{(k\times n)}{\mathbf V_k^T})=\underset{(n\times k)}{\mathbf U_k^*}\underset{(k\times n)}{\mathbf V_k^*}\]</span> where <span class="math inline">\(\mathbf V_k^*\)</span> is a <span class="math inline">\(k\times n\)</span> matrix.</p></li>
</ol>
<p>If rotation is applied to loadings, then there are three easy ways to compute varimax-rotated PCs in R:
(1)</p>
<pre class="r bg-success"><code>irisX &lt;- iris[,1:4]      # Iris data
ncomp &lt;- 2
irisX[1:5,]</code></pre>
<pre><code>##   Sepal.Length Sepal.Width Petal.Length Petal.Width
## 1          5.1         3.5          1.4         0.2
## 2          4.9         3.0          1.4         0.2
## 3          4.7         3.2          1.3         0.2
## 4          4.6         3.1          1.5         0.2
## 5          5.0         3.6          1.4         0.2</code></pre>
<pre class="r bg-success"><code>#ran PCA on 4 variables and selected the top 2 PCs using psych::principal
#Note that it returns standardized scores, i.e. all PCs have unit variance
pca_iris_rotated &lt;- psych::principal(irisX, rotate=&quot;varimax&quot;, nfactors=ncomp, scores=TRUE)
print(pca_iris_rotated$scores[1:5,])  # Scores returned by principal()</code></pre>
<pre><code>##            RC1        RC2
## [1,] -1.083475  0.9067262
## [2,] -1.377536 -0.2648876
## [3,] -1.419832  0.1165198
## [4,] -1.471607 -0.1474634
## [5,] -1.095296  1.0949536</code></pre>
<ol start="2" style="list-style-type: decimal">
<li></li>
</ol>
<pre class="r bg-success"><code>#ran PCA on 4 variables and selected the top 2 PCs using prcomp
pca_iris &lt;- prcomp(irisX, center=T, scale=T)
#   the standard deviations of the principal components (i.e., the square roots of the eigenvalues of the # covariance/correlation matrix, though the calculation is actually done with the singular values of the data matrix).
pca_iris$sdev</code></pre>
<pre><code>## [1] 1.7083611 0.9560494 0.3830886 0.1439265</code></pre>
<pre class="r bg-success"><code>sqrt(eigen(cor(irisX))$values)</code></pre>
<pre><code>## [1] 1.7083611 0.9560494 0.3830886 0.1439265</code></pre>
<pre class="r bg-success"><code># $rotation is the matrix of variable loadings (i.e., a matrix whose columns contain the eigenvectors). The function princomp returns this in the element loadings.
pca_iris$rotation</code></pre>
<pre><code>##                     PC1         PC2        PC3        PC4
## Sepal.Length  0.5210659 -0.37741762  0.7195664  0.2612863
## Sepal.Width  -0.2693474 -0.92329566 -0.2443818 -0.1235096
## Petal.Length  0.5804131 -0.02449161 -0.1421264 -0.8014492
## Petal.Width   0.5648565 -0.06694199 -0.6342727  0.5235971</code></pre>
<pre class="r bg-success"><code>eigen(cor(irisX))$vectors</code></pre>
<pre><code>##            [,1]        [,2]       [,3]       [,4]
## [1,]  0.5210659 -0.37741762  0.7195664  0.2612863
## [2,] -0.2693474 -0.92329566 -0.2443818 -0.1235096
## [3,]  0.5804131 -0.02449161 -0.1421264 -0.8014492
## [4,]  0.5648565 -0.06694199 -0.6342727  0.5235971</code></pre>
<pre class="r bg-success"><code>#raw loading are eigenvectors scaled by the square roots of the respective eigenvalues.
(rawLoadings &lt;- pca_iris$rotation[,1:ncomp] %*% diag(pca_iris$sdev, ncomp, ncomp))</code></pre>
<pre><code>##                    [,1]        [,2]
## Sepal.Length  0.8901688 -0.36082989
## Sepal.Width  -0.4601427 -0.88271627
## Petal.Length  0.9915552 -0.02341519
## Petal.Width   0.9649790 -0.06399985</code></pre>
<pre class="r bg-success"><code>#We can manually use varimax function to rotate the loadings, 
(rotatedLoadings &lt;- varimax(rawLoadings, normalize = TRUE)$loadings)</code></pre>
<pre><code>## 
## Loadings:
##              [,1]   [,2]  
## Sepal.Length  0.959       
## Sepal.Width  -0.145 -0.985
## Petal.Length  0.944  0.304
## Petal.Width   0.932  0.257
## 
##                 [,1]  [,2]
## SS loadings    2.702 1.130
## Proportion Var 0.676 0.283
## Cumulative Var 0.676 0.958</code></pre>
<pre class="r bg-success"><code># then use the new rotated loadings to obtain the scores; one needs to multiple the data with the transposed pseudo-inverse of the rotated #loadings using pracma::pinv() function. This will also yield standardized scores.
invLoadings &lt;- t(pracma::pinv(rotatedLoadings))
scores &lt;- scale(irisX) %*% invLoadings
print(scores[1:5,])                   # Scores computed via rotated loadings</code></pre>
<pre><code>##           [,1]       [,2]
## [1,] -1.083475 -0.9067262
## [2,] -1.377536  0.2648876
## [3,] -1.419832 -0.1165198
## [4,] -1.471607  0.1474634
## [5,] -1.095296 -1.0949536</code></pre>
<ol start="3" style="list-style-type: decimal">
<li></li>
</ol>
<pre class="r bg-success"><code>pca_iris &lt;- prcomp(irisX, center=T, scale=T)
rawLoadings &lt;- pca_iris$rotation[,1:ncomp] %*% diag(pca_iris$sdev, ncomp, ncomp)
#One can use varimax function to rotate the loadings, and then use the $rotmat rotation matrix to #rotate the standardized scores obtained with prcomp.
#pca_iris$x contains the PCs 
scores &lt;- scale(pca_iris$x[,1:2]) %*% varimax(rawLoadings, normalize = TRUE)$rotmat
print(scores[1:5,])                   # Scores computed via rotating the scores</code></pre>
<pre><code>##           [,1]       [,2]
## [1,] -1.083475 -0.9067262
## [2,] -1.377536  0.2648876
## [3,] -1.419832 -0.1165198
## [4,] -1.471607  0.1474634
## [5,] -1.095296 -1.0949536</code></pre>

    </div>
  </article>

  


</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
          </li>
          <li>
          </li>
        </ul>
      </footer>

    </div>
    



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/r.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/yaml.min.js"></script>
<script>hljs.configure({languages: []}); hljs.initHighlightingOnLoad();</script>



    
<script src="../../../../js/math-code.js"></script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
  </body>
</html>

