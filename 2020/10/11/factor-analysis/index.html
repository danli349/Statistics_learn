<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.75.1" />


<title>Factor analysis - A Hugo website</title>
<meta property="og:title" content="Factor analysis - A Hugo website">


  <link href='../../../../favicon.ico' rel='icon' type='image/x-icon'/>



  







<link rel="stylesheet" href="../../../../css/fonts.css" media="all">
<link rel="stylesheet" href="../../../../css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="../../../../" class="nav-logo">
    <img src="../../../../images/logo.png"
         width="50"
         height="50"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="../../../../vitae/">CV</a></li>
    
    <li><a href="https://github.com/danli349">GitHub</a></li>
    
    <li><a href="https://scholar.google.com/citations?user=mNjLK8EAAAAJ&amp;hl=en">Googlr Scholar</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">5 min read</span>
    

    <h1 class="article-title">Factor analysis</h1>

    
    <span class="article-date">2020-10-11</span>
    

    <div class="article-content">
      
<script src="../../../../rmarkdown-libs/header-attrs/header-attrs.js"></script>


<p>Let <span class="math inline">\(\mathbf X\)</span> is drawn from a <span class="math inline">\(p\)</span>-variate normal distribution with <span class="math inline">\(N_p(\boldsymbol\mu, \boldsymbol\Sigma)\)</span> distribution. The matrix of factor loadings <span class="math display">\[\mathbf L=\begin{bmatrix}
\ell_{11}&amp;\ell_{12}&amp;\cdots&amp;\ell_{1m}\\
\ell_{21}&amp;\ell_{22}&amp;\cdots&amp;\ell_{2m}\\
\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\
\ell_{p1}&amp;\ell_{p2}&amp;\cdots&amp;\ell_{pm}\\
\end{bmatrix}\]</span> with <span class="math inline">\(\ell_{ij}\)</span> is the <em>loading</em> of the <span class="math inline">\(i^{th}\)</span> variable on the <span class="math inline">\(j^{th}\)</span> factor.</p>
<p>The common factor is <span class="math display">\[\mathbf F=\begin{bmatrix}
F_1\\
F_2\\
\vdots\\
F_m\\
\end{bmatrix}\]</span> with <span class="math inline">\(E(\mathbf F)=\underset{(m\times 1)}{\mathbf0}\)</span>, <span class="math inline">\(Var(F_j)=1,\quad (j=1,2,\cdots,m)\)</span> and <span class="math inline">\(Cov(\mathbf F)=E(\mathbf F\mathbf F^T)=\underset{(m\times m)}{\mathbf I}\)</span> Then the <strong>Orthogonal factor model</strong> is <span class="math display">\[\underset{(p\times1)}{\mathbf X-\boldsymbol\mu}=\underset{(p\times m)}{\mathbf L}\underset{(m\times1)}{\mathbf F}+\underset{(p\times1)}{\boldsymbol\epsilon}\]</span> with <span class="math inline">\(E(\boldsymbol\epsilon)=\underset{(p\times 1)}{\mathbf0}\)</span> and <span class="math display">\[Cov(\boldsymbol\epsilon)=E(\boldsymbol\epsilon\boldsymbol\epsilon^T)=\boldsymbol\Psi=\begin{bmatrix}
\psi_1&amp;0&amp;\cdots&amp;0\\
0&amp;\psi_2&amp;\cdots&amp;0\\
\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\
0&amp;0&amp;\cdots&amp;\psi_p\\
\end{bmatrix}\]</span> with <span class="math inline">\(Var(\epsilon_i)=\psi_i\)</span> and <span class="math inline">\(\mathbf F\)</span> and <span class="math inline">\(\boldsymbol\epsilon\)</span> are independent with <span class="math inline">\(Cov(\boldsymbol\epsilon,\mathbf F)=E(\boldsymbol\epsilon\mathbf F^T)=\underset{(p\times m)}{\mathbf0}\)</span>
Because <span class="math inline">\(\mathbf L\)</span> is fixed, then <span class="math display">\[\begin{align}
\boldsymbol\Sigma=Cov(\mathbf X)&amp;=E(\mathbf X-\boldsymbol\mu)(\mathbf X-\boldsymbol\mu)^T\\
&amp;=E(\mathbf L\mathbf F+\boldsymbol\epsilon)(\mathbf L\mathbf F+\boldsymbol\epsilon)^T\\
&amp;=E(\mathbf L\mathbf F+\boldsymbol\epsilon)((\mathbf L\mathbf F)^T+\boldsymbol\epsilon^T)\\
&amp;=E\Bigl(\mathbf L\mathbf F(\mathbf L\mathbf F)^T+\boldsymbol\epsilon(\mathbf L\mathbf F)^T+\mathbf L\mathbf F\boldsymbol\epsilon^T+\boldsymbol\epsilon\boldsymbol\epsilon^T\Bigr)\\
&amp;=\mathbf LE(\mathbf F\mathbf F^T)\mathbf L^T+\mathbf0+\mathbf0+E(\boldsymbol\epsilon\boldsymbol\epsilon^T)\\
&amp;=\mathbf L\mathbf L^T+\boldsymbol\Psi
\end{align}\]</span> or <span class="math display">\[Var(X_i)=\underset{Var(X_i)}{\underbrace{\sigma_{ii}}}=\mathbf L_i\mathbf L_i^T+\psi_i=\underset{\text{communality}}{\underbrace{\ell_{i1}^2+\ell_{i2}^2+\cdots+\ell_{im}^2}}+\underset{\text{specific variance}}{\underbrace{\psi_i}}\]</span> with <span class="math inline">\(\mathbf L_i\)</span> is the <span class="math inline">\(i^{th}\)</span> row of <span class="math inline">\(\mathbf L\)</span> We can denote the <span class="math inline">\(i^{th}\)</span> <strong>communality</strong> as <span class="math inline">\(h_i^2=\ell_{i1}^2+\ell_{i2}^2+\cdots+\ell_{im}^2,\quad (i=1,2,\cdots,p)\)</span>, which is the sum of squares of the loadings of the <span class="math inline">\(i^{th}\)</span> variable on the <span class="math inline">\(m\)</span> common factors, and the total variance of the <span class="math inline">\(i^{th}\)</span> variable is the sum of <strong>communality</strong> and <strong>specific variance</strong> <span class="math inline">\(\sigma_{ii}=h_i^2+\psi_i\)</span>
<span class="math display">\[Cov(X_i,X_k)=E(\mathbf L_i^T\mathbf F+\epsilon_i)(\mathbf L_k^T\mathbf F+\epsilon_k)^T=\mathbf L_i^T\mathbf L_k=\ell_{i1}\ell_{k1}+\ell_{i2}\ell_{k2}+\cdots+\ell_{im}\ell_{km}\]</span>
<span class="math display">\[Cov(\mathbf X,\mathbf F)=E(\mathbf X-\boldsymbol\mu)\mathbf F^T=E(\mathbf L\mathbf F+\boldsymbol\epsilon)\mathbf F^T=\mathbf LE(\mathbf F\mathbf F^T)+E(\boldsymbol\epsilon\mathbf F^T)=\mathbf L\]</span> or <span class="math display">\[Cov(X_i,F_j)=E(X_i-\mu_i)\mathbf F_j^T=E(\mathbf L_i^T\mathbf F+\epsilon_i)\mathbf F_j^T=\ell_{ij}\]</span></p>
<ol style="list-style-type: decimal">
<li><p>The Principal Component (and Principal Factor) Method:<br />
Let <span class="math inline">\(\boldsymbol\Sigma\)</span> has eigenvalue–eigenvector pairs <span class="math inline">\((\lambda_i,\mathbf e_i)\)</span> with <span class="math inline">\(\lambda_1\ge\lambda_2\ge\cdots\ge\lambda_p\ge0\)</span> Then <span class="math display">\[\boldsymbol\Sigma=\sum_{i=1}^{p}\lambda_i\mathbf e_i\mathbf e_i^T=\begin{bmatrix}
\sqrt{\lambda_1}\mathbf e_1&amp;\sqrt{\lambda_2}\mathbf e_2&amp;\cdots&amp;\sqrt{\lambda_p}\mathbf e_p\\
\end{bmatrix}\begin{bmatrix}
\sqrt{\lambda_1}\mathbf e_1^T\\
\sqrt{\lambda_2}\mathbf e_2^T\\
\vdots\\
\sqrt{\lambda_p}\mathbf e_p^T\\
\end{bmatrix}\]</span> This factor analysis model has as many factors as variables <span class="math inline">\((m=p)\)</span> and specific variances <span class="math inline">\(\psi_i=0\)</span> for all <span class="math inline">\(i\)</span>. The loading matrix <span class="math inline">\(\mathbf L\)</span> has <span class="math inline">\(j^{th}\)</span> column given by <span class="math inline">\(\sqrt{\lambda_j}\mathbf e_j\)</span> and <span class="math inline">\(\boldsymbol\Sigma=\mathbf L\mathbf L^T\)</span>. when the last <span class="math inline">\(p-m\)</span> eigenvalues are small we can neglect them, then <span class="math display">\[\boldsymbol\Sigma\approx\sum_{i=1}^{m}\lambda_i\mathbf e_i\mathbf e_i^T+\boldsymbol\Psi=\begin{bmatrix}
\sqrt{\lambda_1}\mathbf e_1&amp;\sqrt{\lambda_2}\mathbf e_2&amp;\cdots&amp;\sqrt{\lambda_m}\mathbf e_m\\
\end{bmatrix}\begin{bmatrix}
\sqrt{\lambda_1}\mathbf e_1^T\\
\sqrt{\lambda_2}\mathbf e_2^T\\
\vdots\\
\sqrt{\lambda_m}\mathbf e_m^T\\
\end{bmatrix}+\boldsymbol\Psi\\
=\underset{(p\times m)}{\mathbf L}\underset{(m\times p)}{\mathbf L^T}+\begin{bmatrix}
\psi_1&amp;0&amp;\cdots&amp;0\\
0&amp;\psi_2&amp;\cdots&amp;0\\
\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\
0&amp;0&amp;\cdots&amp;\psi_p\\
\end{bmatrix}\]</span> where <span class="math display">\[\psi_i=\sigma_{ii}-\sum_{j=1}^{m}\ell_{ij}^2,\quad (i=1,2,\cdots,p)\]</span> When the sample covariance matrix <span class="math inline">\(\mathbf S\)</span> has eigenvalue–eigenvector pairs <span class="math inline">\((\hat{\lambda}_i,\hat{\mathbf e}_i)\)</span> with <span class="math inline">\(\hat{\lambda}_1\ge\hat{\lambda}_2\ge\cdots\ge\hat{\lambda}_p\ge0\)</span> <span class="math display">\[\mathbf S\approx\begin{bmatrix}
\sqrt{\hat{\lambda}_1}\hat{\mathbf e}_1&amp;\sqrt{\hat{\lambda}_2}\hat{\mathbf e}_2&amp;\cdots&amp;\sqrt{\hat{\lambda}_m}\hat{\mathbf e}_m\\
\end{bmatrix}\begin{bmatrix}
\sqrt{\hat{\lambda}_1}\hat{\mathbf e}_1^T\\
\sqrt{\hat{\lambda}_2}\hat{\mathbf e}_2^T\\
\vdots\\
\sqrt{\hat{\lambda}_m}\hat{\mathbf e}_m^T\\
\end{bmatrix}+\widetilde{\boldsymbol\Psi}\\
=\underset{(p\times m)}{\widetilde{\mathbf L}}\underset{(m\times p)}{\widetilde{\mathbf L}^T}+\begin{bmatrix}
\widetilde{\psi}_1&amp;0&amp;\cdots&amp;0\\
0&amp;\widetilde{\psi}_2&amp;\cdots&amp;0\\
\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\
0&amp;0&amp;\cdots&amp;\widetilde{\psi}_p\\
\end{bmatrix}\]</span> <span class="math inline">\(\widetilde{\mathbf L}\)</span> is the the matrix of estimated factor loadings and <span class="math display">\[\widetilde{\psi}_i=s_{ii}-\sum_{j=1}^{m}\widetilde{\ell}_{ij}^2\]</span> <span class="math display">\[\widetilde{h}_i^2=\widetilde{\ell}_{i1}^2+\widetilde{\ell}_{i2}^2+\cdots+\widetilde{\ell}_{im}^2\]</span> <span class="math display">\[Var(X_i)=s_{ii}=\widetilde{h}_i^2+\widetilde{\psi}_i\]</span> The residual matrix is <span class="math inline">\(\mathbf S-\widetilde{\mathbf L}\widetilde{\mathbf L}^T-\widetilde{\boldsymbol\Psi}\)</span> The contribution to the total sample variance <span class="math inline">\(s_{11}+s_{22}+\cdots+s_{pp}=tr(\mathbf S)\)</span> from the first common factor is <span class="math display">\[\widetilde{\ell}_{11}^2+\widetilde{\ell}_{21}^2+\cdots+\widetilde{\ell}_{p1}^2=(\sqrt{\hat{\lambda}_1}\hat{\mathbf e}_1)^T(\sqrt{\hat{\lambda}_1}\hat{\mathbf e}_1)=\hat{\lambda}_1\]</span> Then proportion of total sample variance due to <span class="math inline">\(j^{th}\)</span> factor is <span class="math display">\[\frac{\hat{\lambda}_j}{tr(\mathbf S)}\]</span> The number of common factors in the model are increased until a <strong>suitable proportion</strong> of the total sample variance has been explained.</p></li>
<li><p>If the factor model of correlation matrix is <span class="math inline">\(\mathbf R=\mathbf L\mathbf L^T+\boldsymbol\Psi\)</span> then the <span class="math inline">\(m\)</span> common factors should account for the <strong>communality portions</strong> <span class="math inline">\(h_i^2\)</span> of the diagonal elements <span class="math inline">\(r_{ii}=1=h_i^2+\psi_i\)</span> as well as the off-diagonal elements of <span class="math inline">\(\mathbf R\)</span>. Then the <strong>reduced</strong> sample correlation matrix <span class="math inline">\(\mathbf R_r\)</span> is: <span class="math display">\[\mathbf R_r=\mathbf R-\boldsymbol\Psi=\begin{bmatrix}
h_1^{*2}&amp;r_{12}&amp;\cdots&amp;r_{1p}\\
r_{12}&amp;h_2^{*2}&amp;\cdots&amp;r_{2p}\\
\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\
r_{1p}&amp;r_{2p}&amp;\cdots&amp;h_p^{*2}\\
\end{bmatrix}\]</span> with <span class="math inline">\(\psi_i\)</span> are the specific variances and the <span class="math inline">\(i^{th}\)</span> diagonal element of <span class="math inline">\(\mathbf R\)</span> is <span class="math inline">\(h_i^{*2}=1-\psi^{*}_i\)</span>. The reduced sample correlation matrix <span class="math inline">\(\mathbf R_r\)</span> should be accounted for by the <span class="math inline">\(m\)</span> common factors <span class="math inline">\(\mathbf R\doteq\mathbf L_r^*\mathbf L_r^{*T}\)</span> <span class="math display">\[\mathbf L_r^*=\begin{bmatrix}
\sqrt{\hat{\lambda}_1^*}\hat{\mathbf e}_1^*&amp;\sqrt{\hat{\lambda}_2^*}\hat{\mathbf e}_2^*&amp;\cdots&amp;\sqrt{\hat{\lambda}_m^*}\hat{\mathbf e}_m^*\\
\end{bmatrix}\]</span> <span class="math display">\[\psi_i^*=1-\sum_{j=1}^{m}\ell_{ij}^{*2}=1-\widetilde{h}_i^{*2}\]</span> with <span class="math inline">\(\widetilde{h}_i^{*2}\)</span> the estimated communalities.</p></li>
<li><p>The maximum likelihood method: When <span class="math inline">\(\mathbf X_j\)</span> are random sample from <span class="math inline">\(N_p(\boldsymbol\mu, \boldsymbol\Sigma)\)</span>, and <span class="math inline">\(\mathbf F_j\)</span> and <span class="math inline">\(\boldsymbol\epsilon_j\)</span> are jointly normal, then the observations <span class="math inline">\(\underset{(p\times1)}{\mathbf X_j-\boldsymbol\mu}=\underset{(p\times m)}{\mathbf L}\underset{(m\times1)}{\mathbf F_j}+\underset{(p\times1)}{\boldsymbol\epsilon_j}\)</span> are normal distributed, where <span class="math inline">\(Cov(\mathbf X)=\boldsymbol\Sigma=\mathbf L\mathbf L^T+\boldsymbol\Psi\)</span> is the covariance matrix for the <span class="math inline">\(m\)</span> common factor model. Then the likelihood is <span class="math display">\[\begin{align}
L(\boldsymbol\mu, \boldsymbol\Sigma)&amp;=\prod_{j=1}^{n}\Biggl[\frac{1}{(2\pi)^{p/2}|\mathbf\Sigma|^{1/2}}e^{-\frac{1}{2}(\mathbf x_j-\boldsymbol \mu)^T\mathbf\Sigma^{-1}(\mathbf x_j-\boldsymbol \mu)}\Biggr]\\
&amp;=\frac{1}{(2\pi)^{\frac{np}{2}}|\mathbf\Sigma|^{n/2}}e^{-\frac{1}{2}\sum_{j=1}^{n}(\mathbf x_j-\boldsymbol \mu)^T\mathbf\Sigma^{-1}(\mathbf x_j-\boldsymbol \mu)}\\
&amp;=\frac{1}{(2\pi)^{\frac{np}{2}}|\mathbf\Sigma|^{n/2}}e^{-\frac{1}{2}tr\Biggl[\mathbf\Sigma^{-1}\sum_{j=1}^{n}(\mathbf x_j-\boldsymbol \mu)^T(\mathbf x_j-\boldsymbol\mu)\Biggr]}\\
&amp;=\frac{1}{(2\pi)^{\frac{np}{2}}|\mathbf\Sigma|^{n/2}}e^{-\frac{1}{2}tr\Biggl[\mathbf\Sigma^{-1}\Biggl(\sum_{j=1}^{n}(\mathbf x_j-\overline{\mathbf x})(\mathbf x_j-\overline{\mathbf x})^T+n(\overline{\mathbf x}-\boldsymbol\mu)(\overline{\mathbf x}-\boldsymbol\mu)^T\Biggr)\Biggr]}\\
\end{align}\]</span> with <span class="math inline">\(\boldsymbol\Sigma=\mathbf L\mathbf L^T+\boldsymbol\Psi\)</span> We can define <span class="math inline">\(\mathbf L\)</span> by imposing the <strong>uniqueness</strong> condition <span class="math inline">\(\mathbf L^T\boldsymbol\Psi^{-1}\mathbf L=\boldsymbol\Delta\)</span> with <span class="math inline">\(\boldsymbol\Delta\)</span> a diagonal matrix. The maximum likelihood estimators of the likelihood is <span class="math inline">\(\hat{\mathbf L}\)</span>,<span class="math inline">\(\hat{\boldsymbol\Psi}\)</span> and <span class="math inline">\(\boldsymbol\mu=\bar{\mathbf x}\)</span> subject to <span class="math inline">\(\hat{\mathbf L}^T\hat{\boldsymbol\Psi}^{-1}\hat{\mathbf L}=\boldsymbol\Delta\)</span>. The maximum likelihood estimates of the communalities are <span class="math inline">\(\hat{h_i^2}=\hat{\ell}_{i1}^2+\hat{\ell}_{i2}^2+\cdots+\hat{\ell}_{im}^2 \quad(i=1,2,\cdots,p)\)</span> and the proportion of total sample variance due to <span class="math inline">\(j^{th}\)</span> factor is <span class="math display">\[\frac{\hat{\ell}_{1j}^2+\hat{\ell}_{2j}^2+\cdots+\hat{\ell}_{pj}^2}{tr(\mathbf S)}\]</span> If the random variables are standardized with <span class="math display">\[Z_i=\frac{X_i-\mu_i}{\sqrt{\sigma_{ii}}}\]</span>, with matrix notation <span class="math inline">\(\mathbf Z=(\mathbf D^{\frac{1}{2}})^{-1}(\mathbf X-\boldsymbol\mu)\)</span> where <span class="math display">\[\mathbf D=\begin{bmatrix}
\sigma_{11}&amp;0&amp;0&amp;\cdots&amp;0\\
0&amp;\sigma_{22}&amp;0&amp;\cdots&amp;0\\
\vdots&amp;\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\
0&amp;0&amp;0&amp;\cdots&amp;\sigma_{pp}\\
\end{bmatrix}\]</span> Then <span class="math inline">\(E(\mathbf Z)=\mathbf0\)</span> and <span class="math display">\[Cov(\mathbf Z)=(\mathbf D^{-\frac{1}{2}})\boldsymbol\Sigma(\mathbf D^{-\frac{1}{2}})=\begin{bmatrix}
\frac{\sigma_{11}}{\sqrt{\sigma_{11}}\sqrt{\sigma_{11}}}&amp;\frac{\sigma_{12}}{\sqrt{\sigma_{11}}\sqrt{\sigma_{22}}}&amp;\frac{\sigma_{13}}{\sqrt{\sigma_{11}}\sqrt{\sigma_{33}}}&amp;\cdots&amp;\frac{\sigma_{1p}}{\sqrt{\sigma_{11}}\sqrt{\sigma_{pp}}}\\
\frac{\sigma_{12}}{\sqrt{\sigma_{11}}\sqrt{\sigma_{22}}}&amp;\frac{\sigma_{22}}{\sqrt{\sigma_{22}}\sqrt{\sigma_{22}}}&amp;\frac{\sigma_{23}}{\sqrt{\sigma_{22}}\sqrt{\sigma_{33}}}&amp;\cdots&amp;\frac{\sigma_{2p}}{\sqrt{\sigma_{22}}\sqrt{\sigma_{pp}}}\\
\vdots&amp;\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\
\frac{\sigma_{1p}}{\sqrt{\sigma_{11}}\sqrt{\sigma_{pp}}}&amp;\frac{\sigma_{2p}}{\sqrt{\sigma_{22}}\sqrt{\sigma_{pp}}}&amp;\frac{\sigma_{3p}}{\sqrt{\sigma_{33}}\sqrt{\sigma_{pp}}}&amp;\cdots&amp;\frac{\sigma_{pp}}{\sqrt{\sigma_{pp}}\sqrt{\sigma_{pp}}}\\
\end{bmatrix}=\boldsymbol\rho\\
=(\mathbf D^{-\frac{1}{2}}\mathbf L)(\mathbf D^{-\frac{1}{2}}\mathbf L)^T+\mathbf D^{-\frac{1}{2}}\boldsymbol\Psi\mathbf D^{-\frac{1}{2}}\]</span> Thus <span class="math inline">\(\boldsymbol\rho\)</span> can be factorized with loading matrix <span class="math inline">\(\mathbf L_z=\mathbf D^{-\frac{1}{2}}\mathbf L\)</span> and specific variance matrix <span class="math inline">\(\boldsymbol\Psi_z=\mathbf D^{-\frac{1}{2}}\boldsymbol\Psi\mathbf D^{-\frac{1}{2}}\)</span>. If <span class="math inline">\(\hat{\mathbf D}^{-\frac{1}{2}}\)</span> and <span class="math inline">\(\hat{\mathbf L}\)</span> are the maximum likelihood estimators of <span class="math inline">\(\mathbf D^{-\frac{1}{2}}\)</span> and <span class="math inline">\(\mathbf L\)</span>, then the maximum likelihood estimators of <span class="math inline">\(\boldsymbol\rho\)</span> is <span class="math display">\[\hat{\boldsymbol\rho}=(\hat{\mathbf D}^{-\frac{1}{2}}\hat{\mathbf L})(\hat{\mathbf D}^{-\frac{1}{2}}\hat{\mathbf L})^T+\hat{\mathbf D}^{-\frac{1}{2}}\hat{\boldsymbol\Psi}\hat{\mathbf D}^{-\frac{1}{2}}=\hat{\mathbf L}_z\hat{\mathbf L}_z^T+\hat{\boldsymbol\Psi}_z\]</span> The proportion of total standardized sample variance due to <span class="math inline">\(j^{th}\)</span> factor is <span class="math display">\[\frac{\hat{\ell}_{1j}^2+\hat{\ell}_{2j}^2+\cdots+\hat{\ell}_{pj}^2}{tr(\boldsymbol\rho)}=\frac{\hat{\ell}_{1j}^2+\hat{\ell}_{2j}^2+\cdots+\hat{\ell}_{pj}^2}{p}\]</span></p></li>
<li><p>Test for the Number of Common Factors:<br />
To test the adequacy of the <span class="math inline">\(m\)</span> common factor model <span class="math display">\[\underset{(p\times1)}{\mathbf X-\boldsymbol\mu}=\underset{(p\times m)}{\mathbf L}\underset{(m\times1)}{\mathbf F}+\underset{(p\times1)}{\boldsymbol\epsilon}\]</span>
The NULL hypothesis is <span class="math display">\[H_0:\underset{(p\times p)}{\boldsymbol\Sigma}=Cov(\mathbf X)=E(\mathbf X-\boldsymbol\mu)(\mathbf X-\boldsymbol\mu)^T=\underset{(p\times m)}{\mathbf L}\underset{(m\times p)}{\mathbf L^T}+\boldsymbol\Psi\]</span> versus <span class="math display">\[H_1: \underset{(p\times p)}{\boldsymbol\Sigma}=Cov(\mathbf X)=\underset{(p\times q)}{\mathbf L}\underset{(q\times p)}{\mathbf L^T}+\boldsymbol\Psi, \quad (q\ne m)\]</span> Under <span class="math inline">\(H_0\)</span>, the maximum likelihood estimates of <span class="math inline">\(\mathbf\Sigma\)</span> is <span class="math inline">\(\hat{\mathbf\Sigma}=\hat{\mathbf L}\hat{\mathbf L}^T+\hat{\boldsymbol\Psi}\)</span> and <span class="math inline">\(\hat{\mathbf L}\)</span>,<span class="math inline">\(\hat{\boldsymbol\Psi}\)</span> are the maximum likelihood estimates of <span class="math inline">\(\mathbf L\)</span>,<span class="math inline">\(\boldsymbol\Psi\)</span>. Then the maximum likelihood function is <span class="math display">\[\frac{1}{(2\pi)^{\frac{np}{2}}|\mathbf\Sigma|^{n/2}}e^{-\frac{1}{2}tr\Biggl[\mathbf\Sigma^{-1}\Biggl(\sum_{j=1}^{n}(\mathbf x_j-\overline{\mathbf x})(\mathbf x_j-\overline{\mathbf x})^T+n(\overline{\mathbf x}-\boldsymbol\mu)(\overline{\mathbf x}-\boldsymbol\mu)^T\Biggr)\Biggr]}\\
=\frac{1}{(2\pi)^{\frac{np}{2}}|\hat{\mathbf L}\hat{\mathbf L}^T+\hat{\boldsymbol\Psi}|^{n/2}}e^{-\frac{1}{2}tr\Biggl[(\hat{\mathbf L}\hat{\mathbf L}^T+\hat{\boldsymbol\Psi})^{-1}\Biggl(\sum_{j=1}^{n}(\mathbf x_j-\overline{\mathbf x})(\mathbf x_j-\overline{\mathbf x})^T\Biggr)\Biggr]}\\
=\frac{1}{(2\pi)^{\frac{np}{2}}|\hat{\mathbf L}\hat{\mathbf L}^T+\hat{\boldsymbol\Psi}|^{n/2}}e^{-\frac{1}{2}tr\Biggl[(\hat{\mathbf L}\hat{\mathbf L}^T+\hat{\boldsymbol\Psi})^{-1}n\mathbf S_n\Biggr]}\]</span> with the maximum likelihood estimates of <span class="math inline">\(\boldsymbol\mu\)</span> is <span class="math inline">\(\boldsymbol\mu=\bar{\mathbf x}\)</span></p></li>
<li><p>Factor Rotation<br />
The <span class="math inline">\((p\times m)\)</span> matrix of rotated loadings is <span class="math inline">\(\hat{\mathbf L}^*=\hat{\mathbf L}\mathbf T\)</span>, where <span class="math inline">\(\hat{\mathbf L}\)</span> is the <span class="math inline">\((p\times m)\)</span> matrix of estimated factor loadings and <span class="math inline">\(\mathbf T\mathbf T^T=\mathbf T^T\mathbf T=\mathbf I\)</span>. Then <span class="math display">\[\hat{\boldsymbol\Sigma}=\mathbf S_n=\hat{\mathbf L}\hat{\mathbf L}^T+\hat{\boldsymbol\Psi}=\hat{\mathbf L}\mathbf T\mathbf T^T\hat{\mathbf L}^T+\hat{\boldsymbol\Psi}=\hat{\mathbf L}^*\hat{\mathbf L}^{*T}+\hat{\boldsymbol\Psi}\]</span>, which indicates that the residual matrix remains unchanged <span class="math display">\[\mathbf S_n-\hat{\mathbf L}\hat{\mathbf L}^T-\hat{\boldsymbol\Psi}=\mathbf S_n-\hat{\mathbf L}^*\hat{\mathbf L}^{*T}-\hat{\boldsymbol\Psi}\]</span> and the specific variances <span class="math inline">\(\hat{\psi}_i\)</span> and the communalities <span class="math inline">\(\hat{h^2}_i\)</span> are unaltered.</p></li>
</ol>

    </div>
  </article>

  


</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
          </li>
          <li>
          </li>
        </ul>
      </footer>

    </div>
    

    
<script src="../../../../js/math-code.js"></script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
  </body>
</html>

