<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.75.1" />


<title>Classification - A Hugo website</title>
<meta property="og:title" content="Classification - A Hugo website">


  <link href='../../../../favicon.ico' rel='icon' type='image/x-icon'/>



  







<link rel="stylesheet" href="../../../../css/fonts.css" media="all">
<link rel="stylesheet" href="../../../../css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="../../../../" class="nav-logo">
    <img src="../../../../images/logo.png"
         width="50"
         height="50"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="../../../../about/">about</a></li>
    
    <li><a href="../../../../vitae/">CV</a></li>
    
    <li><a href="https://github.com/danli349">GitHub</a></li>
    
    <li><a href="https://scholar.google.com/citations?user=mNjLK8EAAAAJ&amp;hl=en">Googlr Scholar</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">9 min read</span>
    

    <h1 class="article-title">Classification</h1>

    
    <span class="article-date">2020-10-22</span>
    

    <div class="article-content">
      
<script src="index_files/header-attrs/header-attrs.js"></script>
<link href="index_files/anchor-sections/anchor-sections.css" rel="stylesheet" />
<script src="index_files/anchor-sections/anchor-sections.js"></script>


<p>Two classes <span class="math inline">\(\pi_1\)</span> and <span class="math inline">\(\pi_2\)</span> have the prior probability <span class="math inline">\(p_1\)</span> and <span class="math inline">\(p_2\)</span> separately and <span class="math inline">\(p_1+p_2=1\)</span>. The probabilities of the random variable <span class="math inline">\(\mathbf x\)</span> belong to the 2 classes follow the density function <span class="math inline">\(f_1(\mathbf x)\)</span> and <span class="math inline">\(f_2(\mathbf x)\)</span> over the region <span class="math inline">\(R_1+R_2\)</span> and <span class="math inline">\(\underset{R_1}{\int} f_1(\mathbf x)dx=P(1|1)\)</span>, <span class="math inline">\(\underset{R_2}{\int} f_1(\mathbf x)dx=P(2|1)\)</span>, <span class="math inline">\(\underset{R_2}{\int} f_2(\mathbf x)dx=P(2|2)\)</span>, <span class="math inline">\(\underset{R_1}{\int} f_2(\mathbf x)dx=P(1|2)\)</span>. Then the probability of observation <span class="math inline">\(\mathbf x\)</span> which comes from class <span class="math inline">\(\pi_1\)</span> and is correctly classified as <span class="math inline">\(\pi_1\)</span> is the conditional probability <span class="math display">\[P(\mathbf x\in R_1|\pi_1)P(\pi_1)=P(1|1)p_1\]</span>, and observation <span class="math inline">\(\mathbf x\)</span> is misclassified as <span class="math inline">\(\pi_1\)</span> is <span class="math display">\[P(\mathbf x\in R_1|\pi_2)P(\pi_2)=P(1|2)p_2\]</span> Similarly, observation is correctly classified as <span class="math inline">\(\pi_2\)</span> is the conditional probability <span class="math display">\[P(\mathbf x\in R_2|\pi_2)P(\pi_2)=P(2|2)p_2\]</span>, and observation is misclassified as <span class="math inline">\(\pi_2\)</span> is <span class="math display">\[P(\mathbf x\in R_2|\pi_1)P(\pi_1)=P(2|1)p_1\]</span> The costs of misclassification can be defined by a cost matrix <span class="math display">\[\begin{array}{cc|cc}
&amp;&amp;\text{Classify as:}\\
&amp;&amp;\pi_1&amp;\pi_2\\
\hline\\
\text{True populations:}&amp;\pi_1&amp;0&amp;c(2|1)\\
&amp;\pi_2&amp;c(1|2)&amp;0\\
\end{array}\]</span> Then the <span style="color: red;"><strong>Expected Cost of Misclassification (ECM)</strong></span> is provided by <span class="math display">\[\begin{bmatrix}
P(2|1)&amp;P(1|2)\\
\end{bmatrix}\begin{bmatrix}
0&amp;c(2|1)\\
c(1|2)&amp;0\\
\end{bmatrix}\begin{bmatrix}
p_2\\
p_1\\
\end{bmatrix}=P(1|2)c(1|2)p_2+P(2|1)c(2|1)p_1\]</span> A reasonable classification rule should have an ECM as small as possible. The regions boundary between <span class="math inline">\(R_1\)</span> and <span class="math inline">\(R_2\)</span> that minimize the ECM are defined by the values <span class="math inline">\(\mathbf x\)</span> for which <span class="math display">\[\frac{f_1(\mathbf x)}{f_2(\mathbf x)}=\frac{c(1|2)p_2}{c(2|1)p_1}\]</span> We classify a new observation <span class="math inline">\(\mathbf x_0\)</span> into <span class="math inline">\(\pi_1\)</span> if <span class="math display">\[\frac{f_1(\mathbf x_0)}{f_2(\mathbf x_0)}\ge\frac{c(1|2)p_2}{c(2|1)p_1}\]</span> or into <span class="math inline">\(\pi_2\)</span> if <span class="math display">\[\frac{f_1(\mathbf x_0)}{f_2(\mathbf x_0)}\le\frac{c(1|2)p_2}{c(2|1)p_1}\]</span><br />
The <span style="color: red;"><strong>Total Probability of Misclassification (TPM)</strong></span> is provided by <span class="math inline">\(\text{TPM}=P(2|1)p_1+P(1|2)p_2\)</span></p>
<ol style="list-style-type: decimal">
<li><p>Assume that <span class="math inline">\(f_1(\mathbf x)\)</span> and <span class="math inline">\(f_2(\mathbf x)\)</span> are multivariate normal densities share the same covariance matrix <span class="math inline">\(\boldsymbol\Sigma\)</span> and are distributed as <span class="math inline">\(N(\boldsymbol\mu_1, \boldsymbol\Sigma)\)</span> and <span class="math inline">\(N(\boldsymbol\mu_2, \boldsymbol\Sigma)\)</span> The joint densities of a <span class="math inline">\(p\)</span>-dimensional normal random vector <span class="math inline">\(\mathbf X^T=[X_1,X_2,\cdots,X_p]\)</span> for populations <span class="math inline">\(\pi_1\)</span> and <span class="math inline">\(\pi_2\)</span> has the form <span class="math display">\[
f_i(\mathbf x)=\frac{1}{(2\pi)^{p/2}|\mathbf\Sigma|^{1/2}}e^{-\frac{1}{2}(\mathbf x-\boldsymbol \mu_i)^T(\mathbf\Sigma)^{-1}(\mathbf x-\boldsymbol \mu_i)}, \quad i=1,2\]</span> then <span class="math display">\[R_1: \quad\frac{f_1(\mathbf x)}{f_2(\mathbf x)}=exp\Bigl[-\frac{1}{2}(\mathbf x-\boldsymbol \mu_1)^T(\mathbf\Sigma)^{-1}(\mathbf x-\boldsymbol \mu_1)+\frac{1}{2}(\mathbf x-\boldsymbol \mu_2)^T(\mathbf\Sigma)^{-1}(\mathbf x-\boldsymbol \mu_2)\Bigr]\ge\frac{c(1|2)p_2}{c(2|1)p_1}\]</span> <span class="math display">\[R_2: \quad\frac{f_1(\mathbf x)}{f_2(\mathbf x)}=exp\Bigl[-\frac{1}{2}(\mathbf x-\boldsymbol \mu_1)^T(\mathbf\Sigma)^{-1}(\mathbf x-\boldsymbol \mu_1)+\frac{1}{2}(\mathbf x-\boldsymbol \mu_2)^T(\mathbf\Sigma)^{-1}(\mathbf x-\boldsymbol \mu_2)\Bigr]\le\frac{c(1|2)p_2}{c(2|1)p_1}\]</span> And because <span class="math display">\[-\frac{1}{2}(\mathbf x-\boldsymbol \mu_1)^T(\mathbf\Sigma)^{-1}(\mathbf x-\boldsymbol \mu_1)+\frac{1}{2}(\mathbf x-\boldsymbol \mu_2)^T(\mathbf\Sigma)^{-1}(\mathbf x-\boldsymbol \mu_2)=(\boldsymbol \mu_1-\boldsymbol \mu_2)^T(\mathbf\Sigma)^{-1}\mathbf x-\frac{1}{2}(\boldsymbol \mu_1-\boldsymbol \mu_2)^T(\mathbf\Sigma)^{-1}(\boldsymbol \mu_1+\boldsymbol \mu_2)\]</span> then <span class="math display">\[R_1: \quad (\boldsymbol \mu_1-\boldsymbol \mu_2)^T(\mathbf\Sigma)^{-1}\mathbf x-\frac{1}{2}(\boldsymbol \mu_1-\boldsymbol \mu_2)^T(\mathbf\Sigma)^{-1}(\boldsymbol \mu_1+\boldsymbol \mu_2)\ge ln(\frac{c(1|2)p_2}{c(2|1)p_1})\]</span> <span class="math display">\[R_2: \quad (\boldsymbol \mu_1-\boldsymbol \mu_2)^T(\mathbf\Sigma)^{-1}\mathbf x-\frac{1}{2}(\boldsymbol \mu_1-\boldsymbol \mu_2)^T(\mathbf\Sigma)^{-1}(\boldsymbol \mu_1+\boldsymbol \mu_2)&lt; ln(\frac{c(1|2)p_2}{c(2|1)p_1})\]</span></p></li>
<li><p>When we have <span class="math inline">\(n_1\)</span> observations of the multivariate random variable from <span class="math inline">\(\pi_1\)</span> and <span class="math inline">\(n_2\)</span> observations of the multivariate random variable from <span class="math inline">\(\pi_2\)</span> <span class="math display">\[\underset{n_1\times p}{\mathbf X_1}=\begin{bmatrix}
\mathbf x_{11}^T\\
\mathbf x_{12}^T\\
\vdots\\
\mathbf x_{1n_1}^T
\end{bmatrix}\]</span> and <span class="math display">\[\underset{n_2\times p}{\mathbf X_2}=\begin{bmatrix}
\mathbf x_{21}^T\\
\mathbf x_{22}^T\\
\vdots\\
\mathbf x_{2n_2}^T
\end{bmatrix}\]</span> <span class="math display">\[\underset{(p\times 1)}{\bar{\mathbf x}_1}=\frac{1}{n_1}\sum_{j=1}^{n_1}\mathbf x_{1j}\]</span> is the unbiased estimate of <span class="math inline">\(\boldsymbol \mu_1\)</span>, <span class="math display">\[\underset{(p\times p)}{\mathbf S_1}=\frac{1}{n_1-1}\sum_{j=1}^{n_1}(\mathbf x_{1j}-\bar{\mathbf x}_1)(\mathbf x_{1j}-\bar{\mathbf x}_1)^T\]</span> <span class="math display">\[\underset{(p\times 1)}{\bar{\mathbf x}_2}=\frac{1}{n_2}\sum_{j=1}^{n_2}\mathbf x_{2j}\]</span> is the unbiased estimate of <span class="math inline">\(\boldsymbol \mu_2\)</span>, <span class="math display">\[\underset{(p\times p)}{\mathbf S_2}=\frac{1}{n_2-1}\sum_{j=1}^{n_2}(\mathbf x_{2j}-\bar{\mathbf x}_2)(\mathbf x_{2j}-\bar{\mathbf x}_2)^T\]</span> Let <span class="math display">\[\mathbf S=\frac{n_1-1}{n_1-1+n_2-1}\mathbf S_1+\frac{n_2-1}{n_1-1+n_2-1}\mathbf S_2\]</span>, which is the unbiased estimate of <span class="math inline">\(\mathbf\Sigma\)</span> Then the estimated minimum <span style="color: red;"><strong>Expected Cost of Misclassification (ECM)</strong></span> rule for two Normal populations <span class="math display">\[R_1: \quad (\bar{\mathbf x}_1-\bar{\mathbf x}_2)^T\mathbf S^{-1}\mathbf x_0-\frac{1}{2}(\bar{\mathbf x}_1-\bar{\mathbf x}_2)^T\mathbf S^{-1}(\bar{\mathbf x}_1+\bar{\mathbf x}_2)\ge ln(\frac{c(1|2)p_2}{c(2|1)p_1})\]</span> <span class="math display">\[R_2: \quad (\bar{\mathbf x}_1-\bar{\mathbf x}_2)^T\mathbf S^{-1}\mathbf x_0-\frac{1}{2}(\bar{\mathbf x}_1-\bar{\mathbf x}_2)^T\mathbf S^{-1}(\bar{\mathbf x}_1+\bar{\mathbf x}_2)&lt; ln(\frac{c(1|2)p_2}{c(2|1)p_1})\]</span></p></li>
<li><p>Fisherâ€™s approach to Classification with Two Populations with the same covariance matrix : A fixed linear combination of the <span class="math inline">\(n_1\)</span> observations of the multivariate random variable from <span class="math inline">\(\pi_1\)</span> and <span class="math inline">\(n_2\)</span> observations of the multivariate random variable from <span class="math inline">\(\pi_2\)</span> <span class="math display">\[\underset{(n_1\times p)}{\mathbf X_1}\underset{(p\times 1)}{\mathbf a}=\begin{bmatrix}
\mathbf x_{11}^T\\
\mathbf x_{12}^T\\
\vdots\\
\mathbf x_{1n_1}^T
\end{bmatrix}\mathbf a=\begin{bmatrix}
y_{11}\\
y_{12}\\
\vdots\\
y_{1n_1}\\
\end{bmatrix}\]</span> <span class="math display">\[\underset{(n_2\times p)}{\mathbf X_2}\underset{(p\times 1)}{\mathbf a}=\begin{bmatrix}
\mathbf x_{21}^T\\
\mathbf x_{22}^T\\
\vdots\\
\mathbf x_{2n_2}^T
\end{bmatrix}\mathbf a=\begin{bmatrix}
y_{21}\\
y_{22}\\
\vdots\\
y_{2n_2}\\
\end{bmatrix}\]</span> The objective is to select the linear combination of the <span class="math inline">\(\mathbf x\)</span> to achieve maximum separation of the sample means <span class="math inline">\(\bar{y}_1\)</span> and <span class="math inline">\(\bar{y}_2\)</span>, that is to make <span class="math display">\[\text{separation}=\frac{|\bar{y}_1-\bar{y}_2|}{s_y}\]</span> as large as possible, where <span class="math display">\[s_y^2=\frac{\displaystyle\sum_{j=1}^{n_1}(y_{1j}-\bar{y}_1)^2+\displaystyle\sum_{j=1}^{n_2}(y_{2j}-\bar{y}_2)^2}{n_1+n_2-2}\]</span> is the pooled estimate of the variance. Because <span class="math inline">\(\bar{y}_1=\mathbf a^T\bar{\mathbf x}_1\)</span> <span class="math inline">\(\bar{y}_2=\mathbf a^T\bar{\mathbf x}_2\)</span> <span class="math inline">\(s_y^2=\mathbf a^T\mathbf S\mathbf a\)</span> then <span class="math display">\[\text{separation}^2=\frac{(\bar{y}_1-\bar{y}_2)^2}{s_y^2}=\frac{(\mathbf a^T\bar{\mathbf x}_1-\mathbf a^T\bar{\mathbf x}_2)^2}{\mathbf a^T\mathbf S\mathbf a}=\frac{(\mathbf a^T(\bar{\mathbf x}_1-\bar{\mathbf x}_2))^2}{\mathbf a^T\mathbf S\mathbf a}\le (\bar{\mathbf x}_1-\bar{\mathbf x}_2)^T\mathbf S^{-1}(\bar{\mathbf x}_1-\bar{\mathbf x}_2)\]</span> The maximum is achieved when <span class="math inline">\(\mathbf a=\mathbf S^{-1}(\bar{\mathbf x}_1-\bar{\mathbf x}_2)\)</span> and allocate new observation <span class="math inline">\(\mathbf x_0\)</span> to <span class="math display">\[R_1: \quad (\bar{\mathbf x}_1-\bar{\mathbf x}_2)^T\mathbf S^{-1}\mathbf x_0 \ge\frac{1}{2}(\bar{\mathbf x}_1-\bar{\mathbf x}_2)^T\mathbf S^{-1}(\bar{\mathbf x}_1+\bar{\mathbf x}_2)\]</span> <span class="math display">\[R_2: \quad (\bar{\mathbf x}_1-\bar{\mathbf x}_2)^T\mathbf S^{-1}\mathbf x_0&lt;\frac{1}{2}(\bar{\mathbf x}_1-\bar{\mathbf x}_2)^T\mathbf S^{-1}(\bar{\mathbf x}_1+\bar{\mathbf x}_2)\]</span> Fisherâ€™s classification rule is equivalent to the <strong>minimum ECM</strong> rule with equal prior probabilities and equal costs of misclassification.</p></li>
<li><p>Classification with several populations <span class="math inline">\(\pi_i,i=1,2,\cdots,g\)</span> with the prior probability <span class="math inline">\(p_i\)</span> and the cost of allocating an item to <span class="math inline">\(\pi_k\)</span> when it belongs <span class="math inline">\(\pi_i\)</span> is <span class="math inline">\(c(k|i), k,i=1,2,\cdots,g\)</span> and the probability classifying item as <span class="math inline">\(\pi_k\)</span> when it belongs <span class="math inline">\(\pi_i\)</span> is <span class="math inline">\(P(k|i)=\int_{R_k}f_i(\mathbf x)d\mathbf x\)</span>. Then the <strong>Expected Cost of Misclassification (ECM)</strong> of <span class="math inline">\(\pi_k\)</span> is <span class="math display">\[\text{ECM}(k)=P(1|k)c(1|k)+P(2|k)c(2|k)+\cdots+P(k-1|k)c(k-1|k)+P(k+1|k)c(k+1|k)+\cdots+P(g|k)c(g|k)\\
=\sum_{j=1, j\ne k}^{g}P(j|k)c(j|k)\]</span> Multiply each conditional ECM by its prior probability and summing gives the overall ECM <span class="math display">\[\text{ECM}=\sum_{k=1}^{g}p_k\text{ECM}(k)=\sum_{k=1}^{g}p_k\Biggl(\sum_{j=1, j\ne k}^{g}P(j|k)c(j|k)\Biggr)\]</span> We will choose the classification regions <span class="math inline">\(R_k\)</span> to minimize the total ECM. We can assign new observation <span class="math inline">\(\mathbf x\)</span> to region <span class="math inline">\(R_k\)</span> that has the smallest <span class="math display">\[\sum_{j=1, j\ne k}^{g}p_jf_j(\mathbf x)c(k|j)\]</span> or largest <span class="math display">\[p_kf_k(\mathbf x)\]</span> when all the misclassification costs are equal. When <span class="math inline">\(f_i(\mathbf x)\)</span> follow normal distributions <span class="math display">\[
f_i(\mathbf x)=\frac{1}{(2\pi)^{p/2}|\mathbf\Sigma_i|^{1/2}}e^{-\frac{1}{2}(\mathbf x-\boldsymbol \mu_i)^T\mathbf\Sigma_i^{-1}(\mathbf x-\boldsymbol \mu_i)}\]</span> we can allocate <span class="math inline">\(\mathbf x\)</span> to <span class="math inline">\(\pi_k\)</span> if <span class="math display">\[ln(p_kf_k(\mathbf x))=ln(p_k)-\frac{p}{2}ln(2\pi)-\frac{1}{2}ln|\mathbf\Sigma_k|-\frac{1}{2}(\mathbf x-\boldsymbol \mu_k)^T\mathbf\Sigma_k^{-1}(\mathbf x-\boldsymbol \mu_k)\]</span> is the largest. <span style="color: red;"><strong>quadratic discrimination score</strong></span> for the <span class="math inline">\(k^{th}\)</span> population is defined as <span class="math display">\[d_k^Q(\mathbf x)=ln(p_k)-\frac{1}{2}ln|\mathbf\Sigma_k|-\frac{1}{2}(\mathbf x-\boldsymbol \mu_k)^T\mathbf\Sigma_k^{-1}(\mathbf x-\boldsymbol \mu_k)\]</span> Then we can allocate <span class="math inline">\(\mathbf x\)</span> to <span class="math inline">\(\pi_k\)</span> if it has the largest quadratic score <span class="math inline">\(d_k^Q(\mathbf x)\)</span> The estimate of the quadratic discrimination score is <span class="math display">\[\hat{d}_k^Q(\mathbf x)=ln(p_k)-\frac{1}{2}ln|\mathbf S_k|-\frac{1}{2}(\mathbf x-\bar{\mathbf x}_k)^T\mathbf S_k^{-1}(\mathbf x-\bar{\mathbf x}_k)\]</span> Then we can allocate <span class="math inline">\(\mathbf x\)</span> to <span class="math inline">\(\pi_k\)</span> if it has the largest estimate quadratic score <span class="math inline">\(\hat{d}_k^Q(\mathbf x)\)</span> And if the population covariance matrices <span class="math inline">\(\mathbf\Sigma_k\)</span> are equal, it can be estimated using the pooled <span class="math display">\[\mathbf S=\frac{1}{n_1+n_2+\cdots+n_g-g}\Biggl((n_1-1)\mathbf S_1+(n_2-1)\mathbf S_2+\cdots+(n_g-1)\mathbf S_g\Biggr)\]</span> Then the estimate of the quadratic discrimination score is <span class="math display">\[\hat{d}_k^Q(\mathbf x)=ln(p_k)-\frac{1}{2}ln|\mathbf S|-\frac{1}{2}(\mathbf x-\bar{\mathbf x}_k)^T\mathbf S^{-1}(\mathbf x-\bar{\mathbf x}_k)\\
=ln(p_k)-\frac{1}{2}ln|\mathbf S|-\frac{1}{2}D_k^2(\mathbf x)\]</span> with <span class="math inline">\(D_k^2(\mathbf x)=(\mathbf x-\bar{\mathbf x}_k)^T\mathbf S^{-1}(\mathbf x-\bar{\mathbf x}_k)\)</span>, which is the squared distances between the new observation <span class="math inline">\(\mathbf x\)</span> and sample mean <span class="math inline">\(\bar{\mathbf x}_k\)</span>. Then we can allocate <span class="math inline">\(\mathbf x\)</span> to <span class="math inline">\(\pi_k\)</span> if the quadratic discrimination score <span class="math inline">\(\hat{d}_k^Q(\mathbf x)\)</span> is the largest or the squared distances <span class="math inline">\(D_k^2(\mathbf x)\)</span> is smallest. Or equivalently, we can allocate <span class="math inline">\(\mathbf x\)</span> to <span class="math inline">\(\pi_k\)</span> when <span class="math display">\[\hat{d}_k^Q(\mathbf x)-\hat{d}_i^Q(\mathbf x)=ln(\frac{p_k}{p_i})-\frac{1}{2}\Biggl[(\mathbf x-\bar{\mathbf x}_k)^T\mathbf S^{-1}(\mathbf x-\bar{\mathbf x}_k)-(\mathbf x-\bar{\mathbf x}_i)^T\mathbf S^{-1}(\mathbf x-\bar{\mathbf x}_i)\Biggr]\\
=ln(\frac{p_k}{p_i})-\Biggl[-\bar{\mathbf x}_k^T\mathbf S^{-1}\mathbf x+\frac{1}{2}\bar{\mathbf x}_k^T\mathbf S^{-1}\bar{\mathbf x}_k+\bar{\mathbf x}_i^T\mathbf S^{-1}\mathbf x-\frac{1}{2}\bar{\mathbf x}_i^T\mathbf S^{-1}\bar{\mathbf x}_i\Biggr]\\
=ln(\frac{p_k}{p_i})+\Biggl[(\bar{\mathbf x}_k-\bar{\mathbf x}_i)^T\mathbf S^{-1}\mathbf x-\frac{1}{2}(\bar{\mathbf x}_k-\bar{\mathbf x}_i)^T\mathbf S^{-1}(\bar{\mathbf x}_k+\bar{\mathbf x}_i)\Biggr]\ge 0\]</span> for all <span class="math inline">\(i=1,2,\cdots,g\)</span></p></li>
<li><p>Fisherâ€™s approach to Classification with <span class="math inline">\(g\)</span> Populations with the same covariance matrix <span class="math inline">\(\mathbf\Sigma\)</span> which is full rank: A fixed linear combination of the <span class="math inline">\(n_i\)</span> observations of the multivariate random variable from <span class="math inline">\(\pi_i, i=1,2,\cdots,g\)</span> is <span class="math display">\[\underset{(n_i\times p)}{\mathbf X_i}\underset{(p\times 1)}{\mathbf a}=\begin{bmatrix}
\mathbf x_{i1}^T\\
\mathbf x_{i2}^T\\
\vdots\\
\mathbf x_{in_i}^T
\end{bmatrix}\mathbf a=\begin{bmatrix}
y_{i1}\\
y_{i2}\\
\vdots\\
y_{in_i}\\
\end{bmatrix}=Y_i\]</span> <span class="math inline">\(E(\mathbf X_i)=\boldsymbol\mu_i\)</span> and <span class="math inline">\(Cov(\mathbf X_i)=\mathbf\Sigma\)</span> then <span class="math inline">\(E(Y_i)=\mu_{iY}=\mathbf a^T\boldsymbol\mu_i\)</span> and <span class="math inline">\(Cov(Y_i)=\mathbf a^T\mathbf\Sigma\mathbf a\)</span> which is the same for all populations. The the overall mean of all populations is <span class="math display">\[\bar{\boldsymbol\mu}=\frac{1}{g}\sum_{i=1}^{g}\boldsymbol\mu_i\]</span> and the overall mean of all <span class="math inline">\(Y_i\)</span> is <span class="math display">\[\bar{\mu}_Y=\mathbf a^T\bar{\boldsymbol\mu}\]</span> Then for the squared separation <span class="math display">\[\text{separation}^2=\frac{\displaystyle\sum_{i=1}^{g}(\mu_{iY}-\bar{\mu}_Y)^2}{\sigma_Y^2}=\frac{\displaystyle\sum_{i=1}^{g}(\mathbf a^T\boldsymbol\mu_i-\mathbf a^T\bar{\boldsymbol\mu})^2}{\mathbf a^T\mathbf \Sigma\mathbf a}=\frac{\mathbf a^T\Biggl(\displaystyle\sum_{i=1}^{g}(\boldsymbol\mu_i-\bar{\boldsymbol\mu})(\boldsymbol\mu_i-\bar{\boldsymbol\mu})^T\Biggr)\mathbf a}{\mathbf a^T\mathbf \Sigma\mathbf a}\]</span> The squared separation measures the variability between the groups of <span class="math inline">\(Y\)</span>-values relative to the common variability within groups. We can then select <span class="math inline">\(\mathbf a\)</span> to maximize this ratio. For the sample mean vectors <span class="math display">\[\bar{\mathbf x}_i=\frac{1}{n_i}\sum_{j=1}^{n_i}\mathbf x_{ij}\]</span> the mean vector is <span class="math display">\[\bar{\mathbf x}=\frac{1}{g}\sum_{i=1}^{g}\bar{\mathbf x}_i\]</span> and <span class="math display">\[\mathbf S=\frac{\displaystyle\sum_{i=1}^{g}\sum_{j=1}^{n_i}(\mathbf x_{ij}-\bar{\mathbf x}_i)(\mathbf x_{ij}-\bar{\mathbf x}_i)^T}{n_1+n_2+\cdots+n_g-g}\]</span> is the estimate of <span class="math inline">\(\mathbf\Sigma\)</span> Then for the squared separation <span class="math display">\[\text{separation}^2=\frac{\mathbf a^T\Biggl(\displaystyle\sum_{i=1}^{g}(\bar{\mathbf x}_i-\bar{\mathbf x})(\bar{\mathbf x}_i-\bar{\mathbf x})^T\Biggr)\mathbf a}{\mathbf a^T\mathbf S\mathbf a}\]</span> Or <span class="math display">\[\frac{\text{separation}^2}{n_1+n_2+\cdots+n_g-g}=\frac{\mathbf a^T\Biggl(\displaystyle\sum_{i=1}^{g}(\bar{\mathbf x}_i-\bar{\mathbf x})(\bar{\mathbf x}_i-\bar{\mathbf x})^T\Biggr)\mathbf a}{\mathbf a^T\Biggl(\displaystyle\sum_{i=1}^{g}\sum_{j=1}^{n_i}(\mathbf x_{ij}-\bar{\mathbf x}_i)(\mathbf x_{ij}-\bar{\mathbf x}_i)^T\Biggr)\mathbf a}\]</span> Let <span class="math inline">\((\lambda_1, \mathbf e_1),(\lambda_2, \mathbf e_2),\cdots,(\lambda_s, \mathbf e_s), s\le \text{min}(g-1,p)\)</span> are the eigenvalue-eigenvector pairs of matrix <span class="math display">\[\Biggl(\displaystyle\sum_{i=1}^{g}\sum_{j=1}^{n_i}(\mathbf x_{ij}-\bar{\mathbf x}_i)(\mathbf x_{ij}-\bar{\mathbf x}_i)^T\Biggr)^{-1}\Biggl(\displaystyle\sum_{i=1}^{g}(\bar{\mathbf x}_i-\bar{\mathbf x})(\bar{\mathbf x}_i-\bar{\mathbf x})^T\Biggr)\]</span> Then the vector of coefficients <span class="math inline">\(\mathbf a\)</span> that maximizes the ratio <span class="math display">\[\frac{\mathbf a^T\Biggl(\displaystyle\sum_{i=1}^{g}(\bar{\mathbf x}_i-\bar{\mathbf x})(\bar{\mathbf x}_i-\bar{\mathbf x})^T\Biggr)\mathbf a}{\mathbf a^T\Biggl(\displaystyle\sum_{i=1}^{g}\sum_{j=1}^{n_i}(\mathbf x_{ij}-\bar{\mathbf x}_i)(\mathbf x_{ij}-\bar{\mathbf x}_i)^T\Biggr)\mathbf a}\]</span> is given by <span class="math inline">\(\mathbf e_1\)</span> and the linear combination <span class="math inline">\(\mathbf e_1^T\mathbf x\)</span> is called the <span style="color: red;"><strong>sample first discriminant</strong></span>, and the linear combination <span class="math inline">\(\mathbf e_k^T\mathbf x\)</span> is called the <span style="color: red;"><strong>sample <span class="math inline">\(k^{th}\)</span> discriminant</strong></span>, <span class="math inline">\(k\le s\)</span>. <span class="math inline">\(\mathbf W=\displaystyle\sum_{i=1}^{g}\sum_{j=1}^{n_i}(\mathbf x_{ij}-\bar{\mathbf x}_i)(\mathbf x_{ij}-\bar{\mathbf x}_i)^T\)</span> is the <span style="color: red;">sample <strong>Within</strong> groups matrix</span>, and <span class="math inline">\(\mathbf B=\displaystyle\sum_{i=1}^{g}(\bar{\mathbf x}_i-\bar{\mathbf x})(\bar{\mathbf x}_i-\bar{\mathbf x})^T\)</span> is the <span style="color: red;">sample <strong>Between</strong> groups matrix</span>. Let <span class="math display">\[\mathbf Y=\begin{bmatrix}
\mathbf e_1^T\mathbf x\\
\mathbf e_2^T\mathbf x\\
\vdots\\
\mathbf e_s^T\mathbf x\\
\end{bmatrix}(s\le \text{min}(g-1,p))\]</span> contains all the <strong>sample discriminants</strong>, then population <span class="math inline">\(\mathbf X_i\)</span> with <span class="math inline">\(n_i\)</span> observations have <strong>sample discriminants</strong> <span class="math display">\[\mathbf Y_i=\underset{(s\times p)}{\begin{bmatrix}
\mathbf e_1^T\\
\mathbf e_2^T\\
\vdots\\
\mathbf e_s^T\\
\end{bmatrix}}\underset{(p\times n_i)}{\mathbf X_i}=\underset{(s\times n_i)}{\begin{bmatrix}
\mathbf Y_1\\
\mathbf Y_2\\
\vdots\\
\mathbf Y_s\\
\end{bmatrix}}(s\le \text{min}(g-1,p))\]</span> and it has mean vector <span class="math display">\[\boldsymbol\mu_{iY}=\begin{bmatrix}
\mu_{iY_1}\\
\mu_{iY_2}\\
\vdots\\
\mu_{iY_s}\\
\end{bmatrix}=\begin{bmatrix}
\mathbf e_1^T\\
\mathbf e_2^T\\
\vdots\\
\mathbf e_s^T\\
\end{bmatrix}\boldsymbol\mu_i=\begin{bmatrix}
\mathbf e_1^T\boldsymbol\mu_i\\
\mathbf e_2^T\boldsymbol\mu_i\\
\vdots\\
\mathbf e_s^T\boldsymbol\mu_i\\
\end{bmatrix}\]</span>. The squared distance from column components of <span class="math inline">\(\mathbf Y_i\)</span> to its column mean <span class="math inline">\(\boldsymbol\mu_{iY}\)</span> is <span class="math display">\[(\mathbf y-\boldsymbol\mu_{iY})^T(\mathbf y-\boldsymbol\mu_{iY})=\sum_{j=1}^{s}[\mathbf a^T(\mathbf x-\boldsymbol\mu_i)]^2\]</span>, we can assigns <span class="math inline">\(\mathbf y\)</span> to population <span class="math inline">\(\pi_k\)</span> if the square of the distance from <span class="math inline">\(\mathbf y\)</span> to <span class="math inline">\(\boldsymbol\mu_{kY}\)</span> is smaller than the square of the distance from <span class="math inline">\(\mathbf y\)</span> to <span class="math inline">\(\boldsymbol\mu_{iY}\)</span> for <span class="math inline">\(i\ne k\)</span> <span class="math display">\[(\mathbf y-\boldsymbol\mu_{kY})^T(\mathbf y-\boldsymbol\mu_{kY})=\sum_{j=1}^{s}(y_j-\mu_{iY_j})^2=\sum_{j=1}^{s}[\mathbf a_j^T(\mathbf x-\boldsymbol\mu_k)]^2\le \sum_{j=1}^{s}[\mathbf a_j^T(\mathbf x-\boldsymbol\mu_i)]^2\]</span> If we only use the first <span class="math inline">\(r, r\le s\)</span> discriminants then <strong>Fisherâ€™s Classification Procedure</strong> based on sample discriminants is allocate <span class="math inline">\(\mathbf x\)</span> to <span class="math inline">\(\pi_k\)</span> if <span class="math display">\[\sum_{j=1}^{r}(\hat{y}_j-\bar{y}_{kj})^2=\sum_{j=1}^{r}[\hat{\mathbf a}_j^T(\mathbf x-\bar{\mathbf x}_k)]^2\le \sum_{j=1}^{r}[\mathbf a_j^T(\mathbf x-\bar{\mathbf x}_i)]^2\]</span> where <span class="math inline">\(\hat{\mathbf a}_j\)</span> is the eigenvectors of <span class="math inline">\((p\times p)\)</span> matrix <span class="math display">\[\mathbf W^{-1}\mathbf B=\Biggl(\displaystyle\sum_{i=1}^{g}\sum_{j=1}^{n_i}(\mathbf x_{ij}-\bar{\mathbf x}_i)(\mathbf x_{ij}-\bar{\mathbf x}_i)^T\Biggr)^{-1}\Biggl(\displaystyle\sum_{i=1}^{g}(\bar{\mathbf x}_i-\bar{\mathbf x})(\bar{\mathbf x}_i-\bar{\mathbf x})^T\Biggr)\]</span></p></li>
<li><p><span class="math inline">\(\displaystyle\sum_{i=1}^{g}(\boldsymbol\mu_i-\bar{\boldsymbol\mu})^T\boldsymbol\Sigma^{-1}(\boldsymbol\mu_i-\bar{\boldsymbol\mu})\)</span> is the sum of the squared statistical distance from the <span class="math inline">\(i^{th}\)</span> population mean <span class="math inline">\(\boldsymbol\mu_i\)</span> to the centroid <span class="math inline">\(\bar{\boldsymbol\mu}\)</span>. Let <span class="math inline">\((\lambda_1, \mathbf e_1),(\lambda_2, \mathbf e_2),\cdots,(\lambda_p, \mathbf e_p)\)</span> are the eigenvalue-eigenvector pairs of matrix <span class="math inline">\(\mathbf \Sigma^{-1}\mathbf B=\displaystyle\sum_{i=1}^{g}(\boldsymbol\mu_i-\bar{\boldsymbol\mu})\boldsymbol\Sigma^{-1}(\boldsymbol\mu_i-\bar{\boldsymbol\mu})^T\)</span> Then the squared separation <span class="math display">\[\text{separation}^2=\frac{\mathbf a^T\Biggl(\displaystyle\sum_{i=1}^{g}(\boldsymbol\mu_i-\bar{\boldsymbol\mu})\boldsymbol\Sigma^{-1}(\boldsymbol\mu_i-\bar{\boldsymbol\mu})^T\Biggr)\mathbf a}{\mathbf a^T\mathbf a}\]</span> get its uncorrelated maximums <span class="math inline">\(\lambda_1,\cdots,\lambda_p\)</span> when <span class="math inline">\(\mathbf a=\mathbf e_1,\cdots,\mathbf e_p\)</span>. Then the squared statistical distance of the new observation <span class="math inline">\(\mathbf x\)</span> to the mean vector <span class="math inline">\(\boldsymbol\mu_i\)</span> of population <span class="math inline">\(\pi_i\)</span> is <span class="math display">\[(\mathbf x-\boldsymbol\mu_i)^T\boldsymbol\Sigma^{-1}(\mathbf x-\boldsymbol\mu_i)=(\mathbf x-\boldsymbol\mu_i)^T\boldsymbol\Sigma^{-\frac{1}{2}}\boldsymbol\Sigma^{-\frac{1}{2}}(\mathbf x-\boldsymbol\mu_i)\\
=(\mathbf x-\boldsymbol\mu_i)^T\boldsymbol\Sigma^{-\frac{1}{2}}\mathbf E\mathbf E^T\boldsymbol\Sigma^{-\frac{1}{2}}(\mathbf x-\boldsymbol\mu_i)\]</span> where <span class="math inline">\(\mathbf E=[\mathbf e_1,\mathbf e_2,\cdots,\mathbf e_p]\)</span> is the orthogonal matrix whose columns are eigenvectors of matrix <span class="math inline">\(\mathbf \Sigma^{-1}\mathbf B\)</span> When <span class="math inline">\(\mathbf a_i=\boldsymbol\Sigma^{-\frac{1}{2}}\mathbf e_i\)</span> or <span class="math inline">\(\mathbf a_i^T=\mathbf e_i^T\boldsymbol\Sigma^{-\frac{1}{2}}\)</span>, <span class="math display">\[\mathbf E^T\boldsymbol\Sigma^{-\frac{1}{2}}(\mathbf x-\boldsymbol\mu_i)=\begin{bmatrix}
\mathbf a_1^T(\mathbf x-\boldsymbol\mu_i)\\
\mathbf a_2^T(\mathbf x-\boldsymbol\mu_i)\\
\vdots\\
\mathbf a_p^T(\mathbf x-\boldsymbol\mu_i)\\
\end{bmatrix}\]</span> and <span class="math display">\[(\mathbf x-\boldsymbol\mu_i)^T\boldsymbol\Sigma^{-\frac{1}{2}}\mathbf E\mathbf E^T\boldsymbol\Sigma^{-\frac{1}{2}}(\mathbf x-\boldsymbol\mu_i)=\begin{bmatrix}
\mathbf a_1^T(\mathbf x-\boldsymbol\mu_i)\\
\mathbf a_2^T(\mathbf x-\boldsymbol\mu_i)\\
\vdots\\
\mathbf a_p^T(\mathbf x-\boldsymbol\mu_i)\\
\end{bmatrix}^T\begin{bmatrix}
\mathbf a_1^T(\mathbf x-\boldsymbol\mu_i)\\
\mathbf a_2^T(\mathbf x-\boldsymbol\mu_i)\\
\vdots\\
\mathbf a_p^T(\mathbf x-\boldsymbol\mu_i)\\
\end{bmatrix}\\
=\sum_{j=1}^{p}[\mathbf a_j^T(\mathbf x-\boldsymbol\mu_i)]^2\]</span> The first discriminant <span class="math inline">\(Y_1=\mathbf e_1^T\boldsymbol\Sigma^{-\frac{1}{2}}\mathbf X\)</span> has mean <span class="math inline">\(\mu_{iY_1}=\mathbf e_1^T\boldsymbol\Sigma^{-\frac{1}{2}}\boldsymbol\mu_i\)</span>, then the squared distance from group means to central value <span class="math inline">\(\bar{\mu}_{Y_1}=\mathbf e_1^T\boldsymbol\Sigma^{-\frac{1}{2}}\bar{\boldsymbol\mu}\)</span> is <span class="math display">\[\sum_{i=1}^{g}(\mu_{iY_1}-\bar{\mu}_{Y_1})^2=\lambda_1\]</span> Then <span class="math display">\[\sum_{j=1}^{p}\sum_{i=1}^{g}(\mu_{iY_j}-\bar{\mu}_{Y_j})^2=\sum_{j=1}^{p}\lambda_j\]</span></p></li>
</ol>

    </div>
  </article>

  


</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
          </li>
          <li>
          </li>
        </ul>
      </footer>

    </div>
    

    
<script src="../../../../js/math-code.js"></script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
  </body>
</html>

