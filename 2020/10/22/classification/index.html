<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.75.1" />


<title>Classification - A Hugo website</title>
<meta property="og:title" content="Classification - A Hugo website">


  <link href='../../../../favicon.ico' rel='icon' type='image/x-icon'/>



  







<link rel="stylesheet" href="../../../../css/fonts.css" media="all">
<link rel="stylesheet" href="../../../../css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="../../../../" class="nav-logo">
    <img src="../../../../images/logo.png"
         width="50"
         height="50"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="../../../../vitae/">CV</a></li>
    
    <li><a href="https://github.com/danli349">GitHub</a></li>
    
    <li><a href="https://scholar.google.com/citations?user=mNjLK8EAAAAJ&amp;hl=en">Googlr Scholar</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">5 min read</span>
    

    <h1 class="article-title">Classification</h1>

    
    <span class="article-date">2020-10-22</span>
    

    <div class="article-content">
      
<script src="index_files/header-attrs/header-attrs.js"></script>
<link href="index_files/anchor-sections/anchor-sections.css" rel="stylesheet" />
<script src="index_files/anchor-sections/anchor-sections.js"></script>


<p>2 classes <span class="math inline">\(\pi_1\)</span> and <span class="math inline">\(\pi_2\)</span> have the prior probability <span class="math inline">\(p_1\)</span> and <span class="math inline">\(p_2\)</span> separately and <span class="math inline">\(p_1+p_2=1\)</span>. The probabilities of the random variable <span class="math inline">\(\mathbf x\)</span> belong to the 2 classes follow the density function <span class="math inline">\(f_1(\mathbf x)\)</span> and <span class="math inline">\(f_2(\mathbf x)\)</span> over the region <span class="math inline">\(R_1+R_2\)</span> and <span class="math inline">\(\underset{R_1}{\int} f_1(\mathbf x)dx=P(1|1)\)</span>, <span class="math inline">\(\underset{R_2}{\int} f_1(\mathbf x)dx=P(2|1)\)</span>, <span class="math inline">\(\underset{R_2}{\int} f_2(\mathbf x)dx=P(2|2)\)</span>, <span class="math inline">\(\underset{R_1}{\int} f_2(\mathbf x)dx=P(1|2)\)</span>. Then the probability of observation <span class="math inline">\(\mathbf x\)</span> which comes from class <span class="math inline">\(\pi_1\)</span> and is correctly classified as <span class="math inline">\(\pi_1\)</span> is the conditional probability <span class="math display">\[P(\mathbf x\in R_1|\pi_1)P(\pi_1)=P(1|1)p_1\]</span>, and observation <span class="math inline">\(\mathbf x\)</span> is misclassified as <span class="math inline">\(\pi_1\)</span> is <span class="math display">\[P(\mathbf x\in R_1|\pi_2)P(\pi_2)=P(1|2)p_2\]</span> Similarly, observation is correctly classified as <span class="math inline">\(\pi_2\)</span> is the conditional probability <span class="math display">\[P(\mathbf x\in R_2|\pi_2)P(\pi_2)=P(2|2)p_2\]</span>, and observation is misclassified as <span class="math inline">\(\pi_2\)</span> is <span class="math display">\[P(\mathbf x\in R_2|\pi_1)P(\pi_1)=P(2|1)p_1\]</span> The costs of misclassification can be defined by a cost matrix <span class="math display">\[\begin{array}{cc|cc}
&amp;&amp;\text{Classify as:}\\
&amp;&amp;\pi_1&amp;\pi_2\\
\hline\\
\text{True populations:}&amp;\pi_1&amp;0&amp;c(2|1)\\
&amp;\pi_2&amp;c(1|2)&amp;0\\
\end{array}\]</span> Then the <span style="color: red;"><strong>Expected Cost of Misclassification (ECM)</strong></span> is provided by <span class="math display">\[\begin{bmatrix}
P(2|1)&amp;P(1|2)\\
\end{bmatrix}\begin{bmatrix}
0&amp;c(2|1)\\
c(1|2)&amp;0\\
\end{bmatrix}\begin{bmatrix}
p_2\\
p_1\\
\end{bmatrix}=P(1|2)c(1|2)p_2+P(2|1)c(2|1)p_1\]</span> A reasonable classification rule should have an ECM as small as possible. The regions boundary between <span class="math inline">\(R_1\)</span> and <span class="math inline">\(R_2\)</span> that minimize the ECM are defined by the values <span class="math inline">\(\mathbf x\)</span> for which <span class="math display">\[\frac{f_1(\mathbf x)}{f_2(\mathbf x)}=\frac{c(1|2)p_2}{c(2|1)p_1}\]</span> We classify a new observation <span class="math inline">\(\mathbf x_0\)</span> into <span class="math inline">\(\pi_1\)</span> if <span class="math display">\[\frac{f_1(\mathbf x_0)}{f_2(\mathbf x_0)}\ge\frac{c(1|2)p_2}{c(2|1)p_1}\]</span> or into <span class="math inline">\(\pi_2\)</span> if <span class="math display">\[\frac{f_1(\mathbf x_0)}{f_2(\mathbf x_0)}\le\frac{c(1|2)p_2}{c(2|1)p_1}\]</span><br />
The <span style="color: red;"><strong>Total Probability of Misclassification (TPM)</strong></span> is provided by <span class="math inline">\(\text{TPM}=P(2|1)p_1+P(1|2)p_2\)</span></p>
<ol style="list-style-type: decimal">
<li><p>Assume that <span class="math inline">\(f_1(\mathbf x)\)</span> and <span class="math inline">\(f_2(\mathbf x)\)</span> are multivariate normal densities share the same covariance matrix <span class="math inline">\(\boldsymbol\Sigma\)</span> and are distributed as <span class="math inline">\(N(\boldsymbol\mu_1, \boldsymbol\Sigma)\)</span> and <span class="math inline">\(N(\boldsymbol\mu_2, \boldsymbol\Sigma)\)</span> The joint densities of a <span class="math inline">\(p\)</span>-dimensional normal random vector <span class="math inline">\(\mathbf X^T=[X_1,X_2,\cdots,X_p]\)</span> for populations <span class="math inline">\(\pi_1\)</span> and <span class="math inline">\(\pi_2\)</span> has the form <span class="math display">\[
f_i(\mathbf x)=\frac{1}{(2\pi)^{p/2}|\mathbf\Sigma|^{1/2}}e^{-\frac{1}{2}(\mathbf x-\boldsymbol \mu_i)^T(\mathbf\Sigma)^{-1}(\mathbf x-\boldsymbol \mu_i)}, \quad i=1,2\]</span> then <span class="math display">\[R_1: \quad\frac{f_1(\mathbf x)}{f_2(\mathbf x)}=exp\Bigl[-\frac{1}{2}(\mathbf x-\boldsymbol \mu_1)^T(\mathbf\Sigma)^{-1}(\mathbf x-\boldsymbol \mu_1)+\frac{1}{2}(\mathbf x-\boldsymbol \mu_2)^T(\mathbf\Sigma)^{-1}(\mathbf x-\boldsymbol \mu_2)\Bigr]\ge\frac{c(1|2)p_2}{c(2|1)p_1}\]</span> <span class="math display">\[R_2: \quad\frac{f_1(\mathbf x)}{f_2(\mathbf x)}=exp\Bigl[-\frac{1}{2}(\mathbf x-\boldsymbol \mu_1)^T(\mathbf\Sigma)^{-1}(\mathbf x-\boldsymbol \mu_1)+\frac{1}{2}(\mathbf x-\boldsymbol \mu_2)^T(\mathbf\Sigma)^{-1}(\mathbf x-\boldsymbol \mu_2)\Bigr]\le\frac{c(1|2)p_2}{c(2|1)p_1}\]</span> And because <span class="math display">\[-\frac{1}{2}(\mathbf x-\boldsymbol \mu_1)^T(\mathbf\Sigma)^{-1}(\mathbf x-\boldsymbol \mu_1)+\frac{1}{2}(\mathbf x-\boldsymbol \mu_2)^T(\mathbf\Sigma)^{-1}(\mathbf x-\boldsymbol \mu_2)=(\boldsymbol \mu_1-\boldsymbol \mu_2)^T(\mathbf\Sigma)^{-1}\mathbf x-\frac{1}{2}(\boldsymbol \mu_1-\boldsymbol \mu_2)^T(\mathbf\Sigma)^{-1}(\boldsymbol \mu_1+\boldsymbol \mu_2)\]</span> then <span class="math display">\[R_1: \quad (\boldsymbol \mu_1-\boldsymbol \mu_2)^T(\mathbf\Sigma)^{-1}\mathbf x-\frac{1}{2}(\boldsymbol \mu_1-\boldsymbol \mu_2)^T(\mathbf\Sigma)^{-1}(\boldsymbol \mu_1+\boldsymbol \mu_2)\ge ln(\frac{c(1|2)p_2}{c(2|1)p_1})\]</span> <span class="math display">\[R_2: \quad (\boldsymbol \mu_1-\boldsymbol \mu_2)^T(\mathbf\Sigma)^{-1}\mathbf x-\frac{1}{2}(\boldsymbol \mu_1-\boldsymbol \mu_2)^T(\mathbf\Sigma)^{-1}(\boldsymbol \mu_1+\boldsymbol \mu_2)&lt; ln(\frac{c(1|2)p_2}{c(2|1)p_1})\]</span></p></li>
<li><p>When we have <span class="math inline">\(n_1\)</span> observations of the multivariate random variable from <span class="math inline">\(\pi_1\)</span> and <span class="math inline">\(n_2\)</span> observations of the multivariate random variable from <span class="math inline">\(\pi_2\)</span> <span class="math display">\[\underset{n_1\times p}{\mathbf X_1}=\begin{bmatrix}
\mathbf x_{11}^T\\
\mathbf x_{12}^T\\
\vdots\\
\mathbf x_{1n_1}^T
\end{bmatrix}\]</span> and <span class="math display">\[\underset{n_2\times p}{\mathbf X_2}=\begin{bmatrix}
\mathbf x_{21}^T\\
\mathbf x_{22}^T\\
\vdots\\
\mathbf x_{2n_2}^T
\end{bmatrix}\]</span> <span class="math display">\[\underset{(p\times 1)}{\bar{\mathbf x}_1}=\frac{1}{n_1}\sum_{j=1}^{n_1}\mathbf x_{1j}\]</span> is the unbiased estimate of <span class="math inline">\(\boldsymbol \mu_1\)</span>, <span class="math display">\[\underset{(p\times p)}{\mathbf S_1}=\frac{1}{n_1-1}\sum_{j=1}^{n_1}(\mathbf x_{1j}-\bar{\mathbf x}_1)(\mathbf x_{1j}-\bar{\mathbf x}_1)^T\]</span> <span class="math display">\[\underset{(p\times 1)}{\bar{\mathbf x}_2}=\frac{1}{n_2}\sum_{j=1}^{n_2}\mathbf x_{2j}\]</span> is the unbiased estimate of <span class="math inline">\(\boldsymbol \mu_2\)</span>, <span class="math display">\[\underset{(p\times p)}{\mathbf S_2}=\frac{1}{n_2-1}\sum_{j=1}^{n_2}(\mathbf x_{2j}-\bar{\mathbf x}_2)(\mathbf x_{2j}-\bar{\mathbf x}_2)^T\]</span> Let <span class="math display">\[\mathbf S=\frac{n_1-1}{n_1-1+n_2-1}\mathbf S_1+\frac{n_2-1}{n_1-1+n_2-1}\mathbf S_2\]</span>, which is the unbiased estimate of <span class="math inline">\(\mathbf\Sigma\)</span> Then the estimated minimum <span style="color: red;"><strong>Expected Cost of Misclassification (ECM)</strong></span> rule for two Normal populations <span class="math display">\[R_1: \quad (\bar{\mathbf x}_1-\bar{\mathbf x}_2)^T\mathbf S^{-1}\mathbf x_0-\frac{1}{2}(\bar{\mathbf x}_1-\bar{\mathbf x}_2)^T\mathbf S^{-1}(\bar{\mathbf x}_1+\bar{\mathbf x}_2)\ge ln(\frac{c(1|2)p_2}{c(2|1)p_1})\]</span> <span class="math display">\[R_2: \quad (\bar{\mathbf x}_1-\bar{\mathbf x}_2)^T\mathbf S^{-1}\mathbf x_0-\frac{1}{2}(\bar{\mathbf x}_1-\bar{\mathbf x}_2)^T\mathbf S^{-1}(\bar{\mathbf x}_1+\bar{\mathbf x}_2)&lt; ln(\frac{c(1|2)p_2}{c(2|1)p_1})\]</span></p></li>
<li><p>Fisher’s approach to Classification with Two Populations with the same covariance matrix : A fixed linear combination of the <span class="math inline">\(n_1\)</span> observations of the multivariate random variable from <span class="math inline">\(\pi_1\)</span> and <span class="math inline">\(n_2\)</span> observations of the multivariate random variable from <span class="math inline">\(\pi_2\)</span> <span class="math display">\[\underset{(n_1\times p)}{\mathbf X_1}\underset{(p\times 1)}{\mathbf a}=\begin{bmatrix}
\mathbf x_{11}^T\\
\mathbf x_{12}^T\\
\vdots\\
\mathbf x_{1n_1}^T
\end{bmatrix}\mathbf a=\begin{bmatrix}
y_{11}\\
y_{12}\\
\vdots\\
y_{1n_1}\\
\end{bmatrix}\]</span> <span class="math display">\[\underset{(n_2\times p)}{\mathbf X_2}\underset{(p\times 1)}{\mathbf a}=\begin{bmatrix}
\mathbf x_{21}^T\\
\mathbf x_{22}^T\\
\vdots\\
\mathbf x_{2n_2}^T
\end{bmatrix}\mathbf a=\begin{bmatrix}
y_{21}\\
y_{22}\\
\vdots\\
y_{2n_2}\\
\end{bmatrix}\]</span> The objective is to select the linear combination of the <span class="math inline">\(\mathbf x\)</span> to achieve maximum separation of the sample means <span class="math inline">\(\bar{y}_1\)</span> and <span class="math inline">\(\bar{y}_2\)</span>, that is to make <span class="math display">\[\text{separation}=\frac{|\bar{y}_1-\bar{y}_2|}{s_y}\]</span> as large as possible, where <span class="math display">\[s_y^2=\frac{\displaystyle\sum_{j=1}^{n_1}(y_{1j}-\bar{y}_1)^2+\displaystyle\sum_{j=1}^{n_2}(y_{2j}-\bar{y}_2)^2}{n_1+n_2-2}\]</span> is the pooled estimate of the variance. Because <span class="math inline">\(\bar{y}_1=\mathbf a^T\bar{\mathbf x}_1\)</span> <span class="math inline">\(\bar{y}_2=\mathbf a^T\bar{\mathbf x}_2\)</span> <span class="math inline">\(s_y^2=\mathbf a^T\mathbf S\mathbf a\)</span> then <span class="math display">\[\text{separation}^2=\frac{(\bar{y}_1-\bar{y}_2)^2}{s_y^2}=\frac{(\mathbf a^T\bar{\mathbf x}_1-\mathbf a^T\bar{\mathbf x}_2)^2}{\mathbf a^T\mathbf S\mathbf a}=\frac{(\mathbf a^T(\bar{\mathbf x}_1-\bar{\mathbf x}_2))^2}{\mathbf a^T\mathbf S\mathbf a}\le (\bar{\mathbf x}_1-\bar{\mathbf x}_2)^T\mathbf S^{-1}(\bar{\mathbf x}_1-\bar{\mathbf x}_2)\]</span> The maximum is achieved when <span class="math inline">\(\mathbf a=\mathbf S^{-1}(\bar{\mathbf x}_1-\bar{\mathbf x}_2)\)</span> and allocate new observation <span class="math inline">\(\mathbf x_0\)</span> to <span class="math display">\[R_1: \quad (\bar{\mathbf x}_1-\bar{\mathbf x}_2)^T\mathbf S^{-1}\mathbf x_0 \ge\frac{1}{2}(\bar{\mathbf x}_1-\bar{\mathbf x}_2)^T\mathbf S^{-1}(\bar{\mathbf x}_1+\bar{\mathbf x}_2)\]</span> <span class="math display">\[R_2: \quad (\bar{\mathbf x}_1-\bar{\mathbf x}_2)^T\mathbf S^{-1}\mathbf x_0&lt;\frac{1}{2}(\bar{\mathbf x}_1-\bar{\mathbf x}_2)^T\mathbf S^{-1}(\bar{\mathbf x}_1+\bar{\mathbf x}_2)\]</span> Fisher’s classification rule is equivalent to the <strong>minimum ECM</strong> rule with equal prior probabilities and equal costs of misclassification.</p></li>
<li><p>Classification with several populations <span class="math inline">\(\pi_i,i=1,2,\cdots,g\)</span> with the prior probability <span class="math inline">\(p_i\)</span> and the cost of allocating an item to <span class="math inline">\(\pi_k\)</span> when it belongs <span class="math inline">\(\pi_i\)</span> is <span class="math inline">\(c(k|i), k,i=1,2,\cdots,g\)</span> and the probability classifying item as <span class="math inline">\(\pi_k\)</span> when it belongs <span class="math inline">\(\pi_i\)</span> is <span class="math inline">\(P(k|i)=\int_{R_k}f_i(\mathbf x)d\mathbf x\)</span>. Then the <strong>Expected Cost of Misclassification (ECM)</strong> of <span class="math inline">\(\pi_k\)</span> is <span class="math display">\[\text{ECM}(k)=P(1|k)c(1|k)+P(2|k)c(2|k)+\cdots+P(k-1|k)c(k-1|k)+P(k+1|k)c(k+1|k)+\cdots+P(g|k)c(g|k)\\
=\sum_{j=1, j\ne k}^{g}P(j|k)c(j|k)\]</span> Multiply each conditional ECM by its prior probability and summing gives the overall ECM <span class="math display">\[\text{ECM}=\sum_{k=1}^{g}p_k\text{ECM}(k)=\sum_{k=1}^{g}p_k\Biggl(\sum_{j=1, j\ne k}^{g}P(j|k)c(j|k)\Biggr)\]</span> We will choose the classification regions <span class="math inline">\(R_k\)</span> to minimize the total ECM. We can assign new observation <span class="math inline">\(\mathbf x\)</span> to region <span class="math inline">\(R_k\)</span> that has the smallest <span class="math display">\[\sum_{j=1, j\ne k}^{g}p_jf_j(\mathbf x)c(k|j)\]</span> or largest <span class="math display">\[p_kf_k(\mathbf x)\]</span> when all the misclassification costs are equal. When <span class="math inline">\(f_i(\mathbf x)\)</span> follow normal distributions <span class="math display">\[
f_i(\mathbf x)=\frac{1}{(2\pi)^{p/2}|\mathbf\Sigma_i|^{1/2}}e^{-\frac{1}{2}(\mathbf x-\boldsymbol \mu_i)^T\mathbf\Sigma_i^{-1}(\mathbf x-\boldsymbol \mu_i)}\]</span> we can allocate <span class="math inline">\(\mathbf x\)</span> to <span class="math inline">\(\pi_k\)</span> if <span class="math display">\[ln(p_kf_k(\mathbf x))=ln(p_k)-\frac{p}{2}ln(2\pi)-\frac{1}{2}ln|\mathbf\Sigma_k|-\frac{1}{2}(\mathbf x-\boldsymbol \mu_k)^T\mathbf\Sigma_k^{-1}(\mathbf x-\boldsymbol \mu_k)\]</span> is the largest. <span style="color: red;"><strong>quadratic discrimination score</strong></span> for the <span class="math inline">\(k^{th}\)</span> population is defined as <span class="math display">\[d_k^Q(\mathbf x)=ln(p_k)-\frac{1}{2}ln|\mathbf\Sigma_k|-\frac{1}{2}(\mathbf x-\boldsymbol \mu_k)^T\mathbf\Sigma_k^{-1}(\mathbf x-\boldsymbol \mu_k)\]</span> Then we can allocate <span class="math inline">\(\mathbf x\)</span> to <span class="math inline">\(\pi_k\)</span> if it has the largest quadratic score <span class="math inline">\(d_k^Q(\mathbf x)\)</span> The estimate of the quadratic discrimination score is <span class="math display">\[\hat{d}_k^Q(\mathbf x)=ln(p_k)-\frac{1}{2}ln|\mathbf S_k|-\frac{1}{2}(\mathbf x-\bar{\mathbf x}_k)^T\mathbf S_k^{-1}(\mathbf x-\bar{\mathbf x}_k)\]</span> Then we can allocate <span class="math inline">\(\mathbf x\)</span> to <span class="math inline">\(\pi_k\)</span> if it has the largest estimate quadratic score <span class="math inline">\(\hat{d}_k^Q(\mathbf x)\)</span> And if the population covariance matrices <span class="math inline">\(\mathbf\Sigma_k\)</span> are equal, it can be estimated using the pooled <span class="math display">\[\mathbf S=\frac{1}{n_1+n_2+\cdots+n_g-g}\Biggl((n_1-1)\mathbf S_1+(n_2-1)\mathbf S_2+\cdots+(n_g-1)\mathbf S_g\Biggr)\]</span> Then the estimate of the quadratic discrimination score is <span class="math display">\[\hat{d}_k^Q(\mathbf x)=ln(p_k)-\frac{1}{2}ln|\mathbf S|-\frac{1}{2}(\mathbf x-\bar{\mathbf x}_k)^T\mathbf S^{-1}(\mathbf x-\bar{\mathbf x}_k)\\
=ln(p_k)-\frac{1}{2}ln|\mathbf S|-\frac{1}{2}D_k^2(\mathbf x)\]</span> with <span class="math inline">\(D_k^2(\mathbf x)=(\mathbf x-\bar{\mathbf x}_k)^T\mathbf S^{-1}(\mathbf x-\bar{\mathbf x}_k)\)</span>, which is the squared distances between the new observation <span class="math inline">\(\mathbf x\)</span> and sample mean <span class="math inline">\(\bar{\mathbf x}_k\)</span>. Then we can allocate <span class="math inline">\(\mathbf x\)</span> to <span class="math inline">\(\pi_k\)</span> if the quadratic discrimination score <span class="math inline">\(\hat{d}_k^Q(\mathbf x)\)</span> is the largest or the squared distances <span class="math inline">\(D_k^2(\mathbf x)\)</span> is smallest. Or equivalently, we can allocate <span class="math inline">\(\mathbf x\)</span> to <span class="math inline">\(\pi_k\)</span> when <span class="math display">\[\hat{d}_k^Q(\mathbf x)-\hat{d}_i^Q(\mathbf x)=ln(\frac{p_k}{p_i})-\frac{1}{2}\Biggl[(\mathbf x-\bar{\mathbf x}_k)^T\mathbf S^{-1}(\mathbf x-\bar{\mathbf x}_k)-(\mathbf x-\bar{\mathbf x}_i)^T\mathbf S^{-1}(\mathbf x-\bar{\mathbf x}_i)\Biggr]\\
=ln(\frac{p_k}{p_i})-\Biggl[-\bar{\mathbf x}_k^T\mathbf S^{-1}\mathbf x+\frac{1}{2}\bar{\mathbf x}_k^T\mathbf S^{-1}\bar{\mathbf x}_k+\bar{\mathbf x}_i^T\mathbf S^{-1}\mathbf x-\frac{1}{2}\bar{\mathbf x}_i^T\mathbf S^{-1}\bar{\mathbf x}_i\Biggr]\\
=ln(\frac{p_k}{p_i})+\Biggl[(\bar{\mathbf x}_k-\bar{\mathbf x}_i)^T\mathbf S^{-1}\mathbf x-\frac{1}{2}(\bar{\mathbf x}_k-\bar{\mathbf x}_i)^T\mathbf S^{-1}(\bar{\mathbf x}_k+\bar{\mathbf x}_i)\Biggr]\ge 0\]</span> for all <span class="math inline">\(i=1,2,\cdots,g\)</span></p></li>
</ol>

    </div>
  </article>

  


</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
          </li>
          <li>
          </li>
        </ul>
      </footer>

    </div>
    

    
<script src="../../../../js/math-code.js"></script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
  </body>
</html>

