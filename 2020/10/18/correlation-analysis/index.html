<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.80.0" />


<title>Correlation Analysis - A Hugo website</title>
<meta property="og:title" content="Correlation Analysis - A Hugo website">


  <link href='../../../../favicon.ico' rel='icon' type='image/x-icon'/>



  








<link href='//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css' rel='stylesheet' type='text/css' />



<link rel="stylesheet" href="../../../../css/fonts.css" media="all">
<link rel="stylesheet" href="../../../../css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="../../../../" class="nav-logo">
    <img src="../../../../images/logo.png"
         width="50"
         height="50"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="../../../../about/">about</a></li>
    
    <li><a href="../../../../vitae/">CV</a></li>
    
    <li><a href="https://github.com/danli349">GitHub</a></li>
    
    <li><a href="https://scholar.google.com/citations?user=mNjLK8EAAAAJ&amp;hl=en">Googlr Scholar</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">8 min read</span>
    

    <h1 class="article-title">Correlation Analysis</h1>

    
    <span class="article-date">2020-10-18</span>
    

    <div class="article-content">
      
<script src="../../../../2020/10/18/correlation-analysis/index_files/header-attrs/header-attrs.js"></script>


<p><strong>Canonical-correlation analysis (CCA)</strong>, also called <strong>canonical variates analysis</strong>, is a way of inferring information from cross-covariance matrices. If we have two groups of variables <span class="math inline">\(\mathbf X\)</span> has <span class="math inline">\(p\)</span> variables <span class="math display">\[\mathbf X=\begin{bmatrix}
X_1\\
X_2\\
\vdots\\
X_p\\
\end{bmatrix}\]</span> and <span class="math inline">\(\mathbf Y\)</span> has <span class="math inline">\(q\)</span> variables <span class="math display">\[\mathbf Y=\begin{bmatrix}
Y_1\\
Y_2\\
\vdots\\
Y_q\\
\end{bmatrix}\]</span> <span class="math display">\[E(\mathbf X)=\boldsymbol\mu_X\]</span> <span class="math display">\[Cov(\mathbf X)=\boldsymbol\Sigma_{XX}\]</span> and <span class="math display">\[E(\mathbf Y)=\boldsymbol\mu_Y\]</span> <span class="math display">\[Cov(\mathbf Y)=\boldsymbol\Sigma_{YY}\]</span> and <span class="math display">\[Cov(\mathbf X,\mathbf Y)=\boldsymbol\Sigma_{XY}=\boldsymbol\Sigma_{YX}^T=E(\mathbf X-\boldsymbol\mu_X)(\mathbf Y-\boldsymbol\mu_Y)^T=\begin{bmatrix}
\sigma_{X_1Y_1}&amp;\sigma_{X_1Y_2}&amp;\cdots&amp;\sigma_{X_1Y_q}\\
\sigma_{X_2Y_1}&amp;\sigma_{X_2Y_2}&amp;\cdots&amp;\sigma_{X_2Y_q}\\
\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\
\sigma_{X_pY_1}&amp;\sigma_{X_pY_2}&amp;\cdots&amp;\sigma_{X_pY_q}\\
\end{bmatrix}\]</span> Linear combinations provide simple summary measures of a set of variables. Let <span class="math display">\[U=\mathbf a^T\mathbf X\]</span> <span class="math display">\[V=\mathbf b^T\mathbf Y\]</span> Then <span class="math display">\[Var(U)=Var(\mathbf a^T\mathbf X)=\mathbf a^TCov(\mathbf X)\mathbf a=\mathbf a^T\boldsymbol\Sigma_{XX}\mathbf a\]</span> <span class="math display">\[Var(V)=Var(\mathbf b^T\mathbf Y)=\mathbf b^TCov(\mathbf Y)\mathbf b=\mathbf b^T\boldsymbol\Sigma_{YY}\mathbf b\]</span> <span class="math display">\[Cov(U,V)=Cov(\mathbf a^T\mathbf X,\mathbf b^T\mathbf Y)=\mathbf a^TCov(\mathbf X,\mathbf Y)\mathbf b=\mathbf a^T\boldsymbol\Sigma_{XY}\mathbf b\]</span> We shall seek coefficient vectors <span class="math inline">\(\mathbf a\)</span> and <span class="math inline">\(\mathbf b\)</span> such that <span class="math display">\[Cor(U,V)=Cor(\mathbf a^T\mathbf X,\mathbf b^T\mathbf Y)=\frac{Cov(\mathbf a^T\mathbf X,\mathbf b^T\mathbf Y)}{\sqrt{Var(\mathbf a^T\mathbf X)}\sqrt{Var(\mathbf b^T\mathbf Y)}}=\frac{\mathbf a^T\boldsymbol\Sigma_{XY}\mathbf b}{\sqrt{\mathbf a^T\boldsymbol\Sigma_{XX}\mathbf a}\sqrt{\mathbf b^T\boldsymbol\Sigma_{YY}\mathbf b}}\]</span> is as large as possible. The <span style="color: red;"><strong>first pair of canonical variables</strong></span> is the pair of linear combinations <span class="math inline">\((U_1,V_1)\)</span> which maximize the correlation <span class="math inline">\(Cor(U,V)\)</span> and has unit variances. The <span style="color: red;"><strong><span class="math inline">\(k^{th}\)</span> pair of canonical variables</strong></span> is the pair of linear combinations <span class="math inline">\((U_k,V_k)\)</span> which maximize the correlation <span class="math inline">\(Cor(U,V)\)</span> and has unit variances and uncorrelated with all of the previous <span class="math inline">\(k-1\)</span> canonical variable pairs.</p>
<ol style="list-style-type: decimal">
<li><p>If variables <span class="math inline">\(x_1,x_2,\cdots,x_p\)</span> and are uncorrelated, the statistical distance from <span class="math inline">\(\mathbf X\)</span> to origin <span class="math inline">\(\mathbf O\)</span> is <span class="math display">\[d(\mathbf X,\mathbf O)=\sqrt{\frac{x_1^2}{s_{11}}+\frac{x_2^2}{s_{22}}+\cdots+\frac{x_p^2}{s_{pp}}}\]</span> If variables <span class="math inline">\(x_1,x_2,\cdots,x_p\)</span> and are correlated, we can rotate the original
coordinate system through the angle <span class="math inline">\(\theta\)</span> while keeping the scatters fixed and label the variables in the rotated axes as <span class="math inline">\(\widetilde x_1,\widetilde x_2,\cdots,\widetilde x_p\)</span>, which are uncorrelated. Then the statistical distance from <span class="math inline">\(\mathbf X\)</span> to origin <span class="math inline">\(\mathbf O\)</span> in the rotated axes is <span class="math display">\[d(\mathbf X,\mathbf O)=\sqrt{\frac{\widetilde x_1^2}{\widetilde s_{11}}+\frac{\widetilde x_2^2}{\widetilde s_{22}}+\cdots+\frac{\widetilde x_p^2}{\widetilde s_{pp}}}=\sqrt{\mathbf x^T\mathbf A\mathbf x}\]</span> with <span class="math inline">\(\mathbf A\)</span> is the rotation matrix: <span class="math display">\[\mathbf A=\begin{bmatrix}
a_{11}&amp;a_{12}&amp;\cdots&amp;a_{1p}\\
a_{12}&amp;a_{22}&amp;\cdots&amp;a_{2p}\\
\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\
a_{1p}&amp;a_{2p}&amp;\cdots&amp;a_{pp}\\
\end{bmatrix}\]</span> By the spectral decomposition <span class="math display">\[\mathbf A=\sum_{i=1}^{p}\lambda_i\mathbf e_i\mathbf e_i^T\quad (i=1,2,\cdots,p)\]</span> so <span class="math display">\[d^2=\mathbf x^T\mathbf A\mathbf x=\mathbf x^T(\sum_{i=1}^{p}\lambda_i\mathbf e_i\mathbf e_i^T)\mathbf x=\sum_{i=1}^{p}\lambda_i(\mathbf x^T\mathbf e_i)^2\]</span> Let <span class="math inline">\(\lambda_1\ge\lambda_2\ge\cdots\ge\lambda_p\)</span>, then <span class="math inline">\(\mathbf x=d\lambda_1^{-1/2}\mathbf e_1\)</span> satisfies <span class="math display">\[\mathbf x^T\mathbf A\mathbf x=\sum_{i=1}^{p}\lambda_i(\mathbf x^T\mathbf e_i)^2=\lambda_1\Bigl[(d\lambda_1^{-1/2}\mathbf e_1)^T\mathbf e_1\Bigr]^2=d^2\]</span> and all of the uncorrelated <span class="math inline">\(\mathbf x_i=d\lambda_i^{-1/2}\mathbf e_i\)</span> satisfies <span class="math inline">\(\mathbf x^T\mathbf A\mathbf x=d^2\)</span> Thus, the points at distance <span class="math inline">\(d\)</span> lie on an hyperellipsoids whose axes are given by the eigenvectors of <span class="math inline">\(\mathbf A\)</span> with lengths proportional to the reciprocals of the square roots of the eigenvalues. The constant of proportionality is <span class="math inline">\(d\)</span> and the half-length of the axes of the hyperellipsoid in the direction <span class="math inline">\(\mathbf e_i\)</span> is equal to <span class="math inline">\(\frac{d}{\sqrt{\lambda_i}}\)</span></p></li>
<li><p>Let <span class="math inline">\(\underset{p\times 1}{\mathbf b}\)</span> and <span class="math inline">\(\underset{p\times 1}{\mathbf d}\)</span> be any two vectors, and <span class="math inline">\(\underset{p\times p}{\mathbf B}\)</span> is a positive definite matrix. Then <span class="math display">\[(\mathbf b^T\mathbf d)^2=(\mathbf b^T\mathbf B^{1/2}\mathbf B^{-1/2}\mathbf d)^2=\Bigl((\mathbf B^{1/2}\mathbf b)^T(\mathbf B^{-1/2}\mathbf d)\Bigr)^2\\
\le\Bigl((\mathbf B^{1/2}\mathbf b)^T(\mathbf B^{1/2}\mathbf b)\Bigr)\Bigl((\mathbf B^{-1/2}\mathbf d)^T(\mathbf B^{-1/2}\mathbf d)\Bigr)\\
=(\mathbf b^T\mathbf B\mathbf b)(\mathbf d^T\mathbf B^{-1}\mathbf d)\]</span> with equality if and only if <span class="math inline">\(\mathbf b=c\mathbf B^{-1}\mathbf d\)</span> for some constant c.Â So <span class="math display">\[\underset{\mathbf b\ne\mathbf 0}{\text{max}}\Bigl(\frac{\mathbf b^T\mathbf d}{\sqrt{\mathbf b^T\mathbf B\mathbf b}}\Bigr)=\sqrt{\mathbf d^T\mathbf B^{-1}\mathbf d}\]</span> <strong>The meaning of this maximum is square of the inner product of two vectors is no more than the product of 1) the inner product of one vector with itself rotated by a positive definite matrix and 2) the inner product of the other vector with itself rotated by the reverse matrix. The maximum is achieved when one vector rotated by matrix <span class="math inline">\(\mathbf B\)</span> will be the same direction with another vector</strong>.</p></li>
<li><p>By the spectral decomposition, the hyperellipsoids <span class="math display">\[d^2=\mathbf a^T\boldsymbol\Sigma_{XX}\mathbf a=\mathbf a^T(\sum_{i=1}^{p}\lambda_i\mathbf e_i\mathbf e_i^T)\mathbf a=\sum_{i=1}^{p}\lambda_i(\mathbf a^T\mathbf e_i)^2\quad (i=1,2,\cdots,p)\]</span> where <span class="math inline">\((\lambda_i,\mathbf e_i)\)</span> are the eigenvalue-eigenvector pairs of <span class="math inline">\(\boldsymbol\Sigma_{XX}\)</span> and <span class="math inline">\(\mathbf a_i=d\lambda_i^{-1/2}\mathbf e_i\)</span> is on the hyperellipsoid with the distance to origin <span class="math inline">\(\mathbf O\)</span> in the <span class="math inline">\(\mathbf e_i\)</span> direction is <span class="math inline">\(\frac{d}{\sqrt{\lambda_i}}\)</span> Then <span class="math display">\[U_i=\mathbf a_i^T\mathbf X=(d\lambda_i^{-1/2}\mathbf e_i)^T\mathbf X=d\lambda_i^{-1/2}\mathbf e_i^T\mathbf X=d\mathbf e_i^T\boldsymbol\Sigma_{XX}^{-1/2}\mathbf X\]</span> and <span class="math display">\[V_j=\mathbf b_j^T\mathbf Y=c\kappa_j^{-1/2}\mathbf f_j^T\mathbf Y=c\mathbf f_j^T\boldsymbol\Sigma_{YY}^{-1/2}\mathbf Y\]</span> are the maximum uncorrelated values, where <span class="math inline">\((\kappa_j,\mathbf f_j)\)</span> are the eigenvalue-eigenvector pairs of <span class="math inline">\(\boldsymbol\Sigma_{YY}\)</span> and <span class="math inline">\(d=\sqrt{\mathbf a^T\boldsymbol\Sigma_{XX}\mathbf a}\)</span> and <span class="math inline">\(c=\sqrt{\mathbf b^T\boldsymbol\Sigma_{YY}\mathbf b}\)</span> Then the maximum correlation is <span class="math display">\[Cor(U_k,V_k)=\frac{\mathbf a_k^T\boldsymbol\Sigma_{XY}\mathbf b_k}{\sqrt{\mathbf a_k^T\boldsymbol\Sigma_{XX}\mathbf a_k}\sqrt{\mathbf b_k^T\boldsymbol\Sigma_{YY}\mathbf b_k}}=\frac{d\mathbf e_k^T\boldsymbol\Sigma_{XX}^{-1/2}\boldsymbol\Sigma_{XY}c\boldsymbol\Sigma_{YY}^{-1/2}\mathbf f_k}{dc}=\mathbf e_k^T\boldsymbol\Sigma_{XX}^{-1/2}\boldsymbol\Sigma_{XY}\boldsymbol\Sigma_{YY}^{-1/2}\mathbf f_k\]</span> and <span class="math display">\[Cor^2(U_i,V_j)=\mathbf e_k^T\boldsymbol\Sigma_{XX}^{-1/2}\boldsymbol\Sigma_{XY}\boldsymbol\Sigma_{YY}^{-1/2}\mathbf f_k\mathbf f_k^T\boldsymbol\Sigma_{YY}^{-1/2}\boldsymbol\Sigma_{YX}\boldsymbol\Sigma_{XX}^{-1/2}\mathbf e_k=\mathbf e_k^T\boldsymbol\Sigma_{XX}^{-1/2}\boldsymbol\Sigma_{XY}\boldsymbol\Sigma_{YY}^{-1}\boldsymbol\Sigma_{YX}\boldsymbol\Sigma_{XX}^{-1/2}\mathbf e_k=\rho_k^2\]</span> where <span class="math inline">\(\rho_k^2\)</span> are the eigenvalues of <span class="math inline">\(\boldsymbol\Sigma_{XX}^{-1/2}\boldsymbol\Sigma_{XY}\boldsymbol\Sigma_{YY}^{-1}\boldsymbol\Sigma_{YX}\boldsymbol\Sigma_{XX}^{-1/2}\)</span> also <span class="math display">\[Cor^2(U_i,V_j)=\mathbf f_k^T\boldsymbol\Sigma_{YY}^{-1/2}\boldsymbol\Sigma_{YX}\boldsymbol\Sigma_{XX}^{-1/2}\mathbf e_k\mathbf e_k^T\boldsymbol\Sigma_{XX}^{-1/2}\boldsymbol\Sigma_{XY}\boldsymbol\Sigma_{YY}^{-1/2}\mathbf f_k=\mathbf f_k^T\boldsymbol\Sigma_{YY}^{-1/2}\boldsymbol\Sigma_{YX}\boldsymbol\Sigma_{XX}^{-1}\boldsymbol\Sigma_{XY}\boldsymbol\Sigma_{YY}^{-1/2}\mathbf f_k=\rho_k^{*2}\]</span> where <span class="math inline">\(\rho_k^{*2}\)</span> are the eigenvalues of <span class="math inline">\(\boldsymbol\Sigma_{YY}^{-1/2}\boldsymbol\Sigma_{YX}\boldsymbol\Sigma_{XX}^{-1}\boldsymbol\Sigma_{XY}\boldsymbol\Sigma_{YY}^{-1/2}\)</span> Usually, we normalize <span class="math inline">\(\mathbf a\)</span> and <span class="math inline">\(\mathbf b\)</span> to make <span class="math inline">\(d=\sqrt{\mathbf a^T\boldsymbol\Sigma_{XX}\mathbf a}=1\)</span> and <span class="math inline">\(c=\sqrt{\mathbf b^T\boldsymbol\Sigma_{YY}\mathbf b}=1\)</span> So <span class="math display">\[Var(U)=\mathbf a^T\boldsymbol\Sigma_{XX}\mathbf a=1\]</span> <span class="math display">\[Var(V)=\mathbf b^T\boldsymbol\Sigma_{YY}\mathbf b=1\]</span></p></li>
<li><p>If the original variables are standardized with <span class="math display">\[\mathbf Z_X=\begin{bmatrix}
\mathbf Z_{X1}\\
\mathbf Z_{X2}\\
\vdots\\
\mathbf Z_{Xp}\\
\end{bmatrix}\]</span> and <span class="math display">\[\mathbf Z_Y=\begin{bmatrix}
\mathbf Z_{Y1}\\
\mathbf Z_{Y2}\\
\vdots\\
\mathbf Z_{Yq}\\
\end{bmatrix}\]</span> the <strong>canonical variates</strong> are of the form <span class="math display">\[U_k=\mathbf a_k^T\mathbf Z_X=\mathbf e_k^T\boldsymbol\rho_{XX}^{-1/2}\mathbf Z_X\]</span> and <span class="math display">\[V_k=\mathbf b_k^T\mathbf Z_Y=\mathbf f_k^T\boldsymbol\rho_{YY}^{-1/2}\mathbf Z_Y\]</span> Here <span class="math inline">\(Cov(\mathbf Z_X)=\boldsymbol\rho_{XX}\)</span> and
<span class="math inline">\(Cov(\mathbf Z_Y)=\boldsymbol\rho_{YY}\)</span> and <span class="math inline">\(Cov(\mathbf Z_X, \mathbf Z_Y)=\boldsymbol\rho_{XY}=\boldsymbol\rho_{YX}^T\)</span> The <strong>canonical correlations</strong> <span class="math display">\[Cor^2(U_k,V_k)=\rho_k^2\]</span> where <span class="math inline">\(\rho_k^2\)</span> are the eigenvalues of the matrix <span class="math display">\[\boldsymbol\rho_{XX}^{-1/2}\boldsymbol\rho_{XY}\boldsymbol\rho_{YY}^{-1}\boldsymbol\rho_{YX}\boldsymbol\rho_{XX}^{-1/2}\]</span> or the eigenvalues of the matrix <span class="math display">\[\boldsymbol\rho_{YY}^{-1/2}\boldsymbol\rho_{YX}\boldsymbol\rho_{XX}^{-1}\boldsymbol\rho_{XY}\boldsymbol\rho_{YY}^{-1/2}\]</span> Then the <strong>canonical coefficients</strong> of the original variables are <span class="math inline">\(\mathbf e_i^T\boldsymbol\Sigma_{XX}^{-1/2}\)</span> and <span class="math inline">\(\mathbf f_i^T\boldsymbol\Sigma_{YY}^{-1/2}\)</span> and are the same as the <strong>canonical coefficients</strong> of the standardized variables is <span class="math inline">\(\mathbf e_i^T\boldsymbol\rho_{XX}^{-1/2}\)</span> and <span class="math inline">\(\mathbf f_i^T\boldsymbol\rho_{YY}^{-1/2}\)</span> The correlations are unaffected by the standardization.</p></li>
<li><p>The first <span class="math inline">\(p\)</span> canonical variables in <span class="math inline">\(\mathbf U\)</span> are <span class="math display">\[\underset{(p\times 1)}{\mathbf U}=\mathbf A\mathbf X=\begin{bmatrix}
\mathbf a_1^T\\
\mathbf a_2^T\\
\vdots\\
\mathbf a_p^T\\
\end{bmatrix}\begin{bmatrix}
X_1\\
X_2\\
\vdots\\
X_p\\
\end{bmatrix}\]</span> where <span class="math inline">\(\mathbf A\)</span> is the matrix whose rows contain the <strong>canonical coefficients</strong>. Then <span class="math display">\[Cov(\mathbf U, \mathbf X)=Cov(\mathbf A\mathbf X, \mathbf X)=\mathbf A\boldsymbol\Sigma_{XX}\]</span>. Because <span class="math inline">\(Var(U_i)=1, (i=1,2,\cdots,p)\)</span> the <strong>canonical correlations</strong> between <strong>canonical variates</strong> and their <strong>component variables</strong> are <span class="math display">\[Cor(U_i,X_k)=\frac{Cov(U_i,X_k)}{\sqrt{1}\sqrt{Var(X_k)}}=\frac{Cov(U_i,X_k)}{\sigma_{kk}^{1/2}}=Cov(U_i,X_k)\sigma_{kk}^{-1/2}=Cov(U_i,\sigma_{kk}^{-1/2}X_k)\]</span>. Let diagonal matrix <span class="math display">\[\mathbf D_{XX}^{-1/2}=\begin{bmatrix}
\sigma_{11}^{-1/2}&amp;0&amp;\cdots&amp;0\\
0&amp;\sigma_{22}^{-1/2}&amp;\cdots&amp;0\\
\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\
0&amp;0&amp;\cdots&amp;\sigma_{pp}^{-1/2}\\
\end{bmatrix}\]</span> Then the <strong>canonical correlations</strong> between <strong>canonical variates</strong> and the <strong>original variables</strong> are <span class="math display">\[\underset{(p\times p)}{\boldsymbol\rho_{\mathbf U, \mathbf X}}=Cor(\mathbf U, \mathbf X)=Cov(\mathbf U, \mathbf D_{XX}^{-1/2}\mathbf X)=Cov(\mathbf A\mathbf X, \mathbf D_{XX}^{-1/2}\mathbf X)=\mathbf A\boldsymbol\Sigma_{XX}\mathbf D_{XX}^{-1/2}\]</span>
The first <span class="math inline">\(q\)</span> canonical variables in <span class="math inline">\(\mathbf V\)</span> are <span class="math display">\[\underset{(q\times 1)}{\mathbf V}=\mathbf B\mathbf Y=\begin{bmatrix}
\mathbf b_1^T\\
\mathbf b_2^T\\
\vdots\\
\mathbf b_q^T\\
\end{bmatrix}\begin{bmatrix}
Y_1\\
Y_2\\
\vdots\\
Y_q\\
\end{bmatrix}\]</span> where <span class="math inline">\(\mathbf B\)</span> is the matrix whose rows contain the <strong>canonical coefficients</strong>. Then the <strong>canonical correlations</strong> between <strong>canonical variates</strong> and the <strong>original variables</strong> are <span class="math display">\[\underset{(q\times q)}{\boldsymbol\rho_{\mathbf V, \mathbf Y}}=Cor(\mathbf V, \mathbf Y)=Cov(\mathbf V, \mathbf D_{YY}^{-1/2}\mathbf Y)=Cov(\mathbf B\mathbf Y, \mathbf D_{YY}^{-1/2}\mathbf Y)=\mathbf B\boldsymbol\Sigma_{YY}\mathbf D_{YY}^{-1/2}\]</span> Similarly, the <strong>canonical correlations</strong> between <strong>canonical variates</strong> <span class="math inline">\(\mathbf U\)</span> and the <strong>original variables</strong> <span class="math inline">\(\mathbf Y\)</span> are <span class="math display">\[\underset{(p\times q)}{\boldsymbol\rho_{\mathbf U, \mathbf Y}}=Cor(\mathbf U, \mathbf Y)=Cov(\mathbf U, \mathbf D_{YY}^{-1/2}\mathbf Y)=Cov(\mathbf A\mathbf X, \mathbf D_{YY}^{-1/2}\mathbf Y)=\mathbf A\boldsymbol\Sigma_{XY}\mathbf D_{YY}^{-1/2}\]</span> and <span class="math display">\[\underset{(q\times p)}{\boldsymbol\rho_{\mathbf V, \mathbf X}}=Cor(\mathbf V, \mathbf X)=Cov(\mathbf V, \mathbf D_{XX}^{-1/2}\mathbf X)=Cov(\mathbf B\mathbf Y, \mathbf D_{XX}^{-1/2}\mathbf X)=\mathbf B\boldsymbol\Sigma_{YX}\mathbf D_{XX}^{-1/2}\]</span></p></li>
<li><p>If the original variables are standardized with <span class="math display">\[\mathbf Z_X=\begin{bmatrix}
\mathbf Z_{X1}\\
\mathbf Z_{X2}\\
\vdots\\
\mathbf Z_{Xp}\\
\end{bmatrix}\]</span> Then <span class="math display">\[\boldsymbol\rho_{\mathbf U, \mathbf Z_X}=Cor(\mathbf U, \mathbf Z_X)=Cov(\mathbf A_Z\mathbf Z_X, \mathbf Z_X)=\mathbf A_Z\boldsymbol\rho_{XX}=\mathbf A_Z\mathbf D_{XX}^{-1/2}\boldsymbol\Sigma_{XX}\mathbf D_{XX}^{-1/2}\\
=\mathbf A\mathbf D_{XX}^{1/2}\mathbf D_{XX}^{-1/2}\boldsymbol\Sigma_{XX}\mathbf D_{XX}^{-1/2}=\mathbf A\boldsymbol\Sigma_{XX}\mathbf D_{XX}^{-1/2}=\underset{(p\times p)}{\boldsymbol\rho_{\mathbf U, \mathbf X}}\]</span> and <span class="math inline">\(\mathbf A_Z=\mathbf A\mathbf D_{XX}^{1/2}\)</span> are the matrices whose rows contain the canonical coefficients for <span class="math inline">\(\mathbf Z_X\)</span>. The <strong>canonical correlations</strong> are unaffected by the standardization. Similarly, <span class="math display">\[\boldsymbol\rho_{\mathbf V, \mathbf Z_Y}=\mathbf B_Z\boldsymbol\rho_{YY}\]</span></p></li>
<li><p>Because the correlation between the components in <span class="math inline">\(\mathbf X\)</span> and <span class="math inline">\(\mathbf Y\)</span> is <span class="math inline">\(|\rho_{ik}|=|cor(X_i,Y_k)|=|cor(aX_i,bY_k)|\)</span>, then the <span style="color: red;"><strong>first canonical correlation</strong></span> is larger than the absolute value of any entry in matrix <span class="math inline">\(Cov(\mathbf Z_X, \mathbf Z_Y)=\boldsymbol\rho_{XY}=\mathbf D_{XX}^{-1/2}\boldsymbol\Sigma_{XY}\mathbf D_{YY}^{-1/2}\)</span></p></li>
<li><p>The change of coordinates from <span class="math inline">\(\mathbf X\)</span> to <span class="math inline">\(\mathbf U=\mathbf A\mathbf X\)</span> and from <span class="math inline">\(\mathbf Y\)</span> to <span class="math inline">\(\mathbf V=\mathbf B\mathbf Y\)</span> is chosen to maximize <span class="math inline">\(Cor(U_1,V_1)\)</span> and, successively <span class="math inline">\(Cor(U_i,V_i)\)</span> where <span class="math inline">\((U_i,V_i)\)</span> have zero correlation with the previous pairs. The canonical variables <span class="math inline">\(\mathbf U\)</span> have covariance matrix <span class="math display">\[Cov(\mathbf U)=Cov(\mathbf A\mathbf X)=\mathbf A\boldsymbol\Sigma_{XX}\mathbf A^T=\mathbf I\]</span> and <span class="math display">\[\mathbf A=\mathbf E^T\boldsymbol\Sigma_{XX}^{-1/2}=\mathbf E^T\mathbf P_X\boldsymbol\Lambda_{X}^{-1/2}\mathbf P_X^T\]</span> where <span class="math inline">\(\mathbf E\)</span> is an orthogonal matrix with rows <span class="math inline">\(\mathbf e^T\)</span> and <span class="math inline">\(\mathbf P_X\)</span> is an orthogonal matrix with columns are the eigenvectors of <span class="math inline">\(\boldsymbol\Sigma_{XX}\)</span> and <span class="math inline">\(\boldsymbol\Lambda_{X}\)</span> is a diagonal matrix with the eigenvalues of <span class="math inline">\(\boldsymbol\Sigma_{XX}\)</span> on its main diagonal. <span class="math inline">\(\mathbf P_X^T\mathbf X\)</span> is the set of principal components derived from <span class="math inline">\(\mathbf X\)</span> and <span class="math inline">\(\boldsymbol\Lambda_{X}^{-1/2}\mathbf P_X^T\mathbf X\)</span> has <span class="math inline">\(i^{th}\)</span> row <span class="math inline">\(\frac{1}{\sqrt{\lambda_i}}\mathbf p_i^T\mathbf X\)</span> which is the <span class="math inline">\(i^{th}\)</span> principal component scaled to have unit variance. The <span class="math inline">\(Cov(\boldsymbol\Lambda_{X}^{-1/2}\mathbf P_X^T\mathbf X)=\mathbf 1\)</span> Then the canonical variables <span class="math display">\[\mathbf U=\mathbf A\mathbf X=\mathbf E^T\mathbf P_X\boldsymbol\Lambda_{X}^{-1/2}\mathbf P_X^T\mathbf X\]</span> means (1) a transformation of <span class="math inline">\(\mathbf X\)</span> to uncorrelated standardized principal components followed by (2) a rigid (orthogonal) rotation <span class="math inline">\(\mathbf P_X\)</span> determined by <span class="math inline">\(\boldsymbol\Sigma_{XX}\)</span> and then (3) another rotation <span class="math inline">\(\mathbf E^T\)</span> determined from the full covariance matrix <span class="math display">\[\boldsymbol\Sigma_{XX}^{-1/2}\boldsymbol\Sigma_{XY}\boldsymbol\Sigma_{YY}^{-1}\boldsymbol\Sigma_{YX}\boldsymbol\Sigma_{XX}^{-1/2}\]</span></p></li>
</ol>
<pre class="r bg-success"><code>## signs of results are random
pop &lt;- LifeCycleSavings[, 2:3]
oec &lt;- LifeCycleSavings[, -(2:3)]
cancor(pop, oec)</code></pre>
<pre><code>## $cor
## [1] 0.8247966 0.3652762
## 
## $xcoef
##               [,1]        [,2]
## pop15 -0.009110856 -0.03622206
## pop75  0.048647514 -0.26031158
## 
## $ycoef
##              [,1]          [,2]          [,3]
## sr   0.0084710221  3.337936e-02 -5.157130e-03
## dpi  0.0001307398 -7.588232e-05  4.543705e-06
## ddpi 0.0041706000 -1.226790e-02  5.188324e-02
## 
## $xcenter
##   pop15   pop75 
## 35.0896  2.2930 
## 
## $ycenter
##        sr       dpi      ddpi 
##    9.6710 1106.7584    3.7576</code></pre>
<pre class="r bg-success"><code>x &lt;- matrix(rnorm(150), 50, 3)
y &lt;- matrix(rnorm(250), 50, 5)
(cxy &lt;- cancor(x, y))</code></pre>
<pre><code>## $cor
## [1] 0.4648744 0.3393778 0.2497012
## 
## $xcoef
##             [,1]         [,2]        [,3]
## [1,]  0.13681854 -0.011328223  0.07117195
## [2,] -0.06953136  0.001488554  0.11053153
## [3,] -0.01805799 -0.124911518 -0.01680600
## 
## $ycoef
##             [,1]        [,2]        [,3]         [,4]        [,5]
## [1,]  0.04242329  0.01471038 -0.09464761  0.003304399 -0.12610091
## [2,] -0.13756352 -0.06554034  0.01593199 -0.006577211 -0.05380149
## [3,]  0.08329928 -0.02767489  0.11328385  0.083475987 -0.05480750
## [4,]  0.06465024 -0.07792535 -0.01358597 -0.116063204  0.00723675
## [5,] -0.01178345  0.07447101  0.13377486 -0.053148728 -0.02503208
## 
## $xcenter
## [1]  0.09045249 -0.15751434 -0.08311951
## 
## $ycenter
## [1]  0.088946456  0.004213401  0.107837528 -0.133090644 -0.004320256</code></pre>
<pre class="r bg-success"><code>all(abs(cor(x %*% cxy$xcoef,
            y %*% cxy$ycoef)[,1:3] - diag(cxy $ cor)) &lt; 1e-15)</code></pre>
<pre><code>## [1] TRUE</code></pre>
<pre class="r bg-success"><code>all(abs(cor(x %*% cxy$xcoef) - diag(3)) &lt; 1e-15)</code></pre>
<pre><code>## [1] TRUE</code></pre>
<pre class="r bg-success"><code>all(abs(cor(y %*% cxy$ycoef) - diag(5)) &lt; 1e-15)</code></pre>
<pre><code>## [1] TRUE</code></pre>

    </div>
  </article>

  


</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
          </li>
          <li>
          </li>
        </ul>
      </footer>

    </div>
    



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/r.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/yaml.min.js"></script>
<script>hljs.configure({languages: []}); hljs.initHighlightingOnLoad();</script>



    
<script src="../../../../js/math-code.js"></script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
  </body>
</html>

