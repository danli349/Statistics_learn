<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.75.1" />


<title>Linear Regression - A Hugo website</title>
<meta property="og:title" content="Linear Regression - A Hugo website">


  <link href='../../../../favicon.ico' rel='icon' type='image/x-icon'/>



  







<link rel="stylesheet" href="../../../../css/fonts.css" media="all">
<link rel="stylesheet" href="../../../../css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="../../../../" class="nav-logo">
    <img src="../../../../images/logo.png"
         width="50"
         height="50"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="../../../../vitae/">CV</a></li>
    
    <li><a href="https://github.com/danli349">GitHub</a></li>
    
    <li><a href="https://scholar.google.com/citations?user=mNjLK8EAAAAJ&amp;hl=en">Googlr Scholar</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">9 min read</span>
    

    <h1 class="article-title">Linear Regression</h1>

    
    <span class="article-date">2020-09-03</span>
    

    <div class="article-content">
      
<script src="../../../../rmarkdown-libs/header-attrs/header-attrs.js"></script>


<p>If there are <span class="math inline">\(n\)</span> points <span class="math inline">\((x_1,y_1),(x_2,y_3),...,(x_n,y_n)\)</span>, the straight line <span class="math inline">\(y=a+bx\)</span> minimizing the sum of the squares of the <span style="color: red;"><strong>vertical distances</strong></span> from the data points to the line <span class="math inline">\(L=\sum_{i=1}^{n}(y_i-a-bx_i)^2\)</span>, then we take partial derivatives of L with respect to <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> and let them equal to <span class="math inline">\(0\)</span> to get least squares coefficients <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>:
<span class="math display">\[\frac{\partial L}{\partial b}=-2\sum_{i=1}^{n}(y_i-a-bx_i)x_i=0\]</span>, then <span class="math display">\[\sum_{i=1}^{n}x_iy_i=a\sum_{i=1}^{n}x_i+b\sum_{i=1}^{n}x_i^2\]</span><br />
And, <span class="math display">\[\frac{\partial L}{\partial a}=-2\sum_{i=1}^{n}(y_i-a-bx_i)=0\]</span>, then
<span class="math display">\[\sum_{i=1}^{n}y_i=na+b\sum_{i=1}^{n}x_i\]</span>
these 2 equations are:
<span class="math display">\[
\begin{bmatrix}
\displaystyle\sum_{i=1}^{n}x_i &amp; \displaystyle\sum_{i=1}^{n}x_i^2\\
n &amp; \displaystyle\sum_{i=1}^{n}x_i\\
\end{bmatrix}
\begin{bmatrix}
a\\
b
\end{bmatrix}
=
\begin{bmatrix}
\displaystyle\sum_{i=1}^{n}x_iy_i\\
\displaystyle\sum_{i=1}^{n}y_i
\end{bmatrix}
\]</span>
then, using <span style="color: red;"><strong>Cramerâ€™s rule</strong></span>
<span class="math display">\[\begin{align}
b&amp;=\frac{\begin{bmatrix}
\displaystyle\sum_{i=1}^{n}x_i &amp; \displaystyle\sum_{i=1}^{n}x_iy_i\\
n &amp; \displaystyle\sum_{i=1}^{n}y_i\\
\end{bmatrix}}{\begin{bmatrix}
\displaystyle\sum_{i=1}^{n}x_i &amp; \displaystyle\sum_{i=1}^{n}x_i^2\\
n &amp; \displaystyle\sum_{i=1}^{n}x_i\\
\end{bmatrix}}\\
&amp;=\frac{(\displaystyle\sum_{i=1}^{n}x_i)(\displaystyle\sum_{i=1}^{n}y_i)-n(\displaystyle\sum_{i=1}^{n}x_iy_i)}{(\displaystyle\sum_{i=1}^{n}x_i)^2-n\displaystyle\sum_{i=1}^{n}x_i^2}\\
&amp;=\frac{n(\displaystyle\sum_{i=1}^{n}x_iy_i)-(\displaystyle\sum_{i=1}^{n}x_i)(\displaystyle\sum_{i=1}^{n}y_i)}{n\displaystyle\sum_{i=1}^{n}x_i^2-(\displaystyle\sum_{i=1}^{n}x_i)^2}\\
&amp;=\frac{(\displaystyle\sum_{i=1}^{n}x_iy_i)-\frac{1}{n}(\displaystyle\sum_{i=1}^{n}x_i)(\displaystyle\sum_{i=1}^{n}y_i)}{\displaystyle\sum_{i=1}^{n}x_i^2-\frac{1}{n}(\displaystyle\sum_{i=1}^{n}x_i)^2}
\end{align}\]</span>, and, <span class="math inline">\(a=\frac{\displaystyle\sum_{i=1}^{n}y_i-b\sum_{i=1}^{n}x_i}{n}=\bar y-b\bar x\)</span>, which shows point <span class="math inline">\((\bar x, \bar y)\)</span> is in the line.</p>
<p>We call <span class="math inline">\(\hat y_i=a+bx_i\)</span> the <strong>predicted value</strong> of <span class="math inline">\(y_i\)</span>, and <span class="math inline">\(y_i-\hat y_i\)</span> the <span class="math inline">\(i^{th}\)</span> <strong>residual</strong>.</p>
<ul>
<li><p>univariate linear regression models:<br />
When there are <span class="math inline">\(n\)</span> response variables <span class="math inline">\(Y_i, i=1,2,\cdots,n\)</span> and <span class="math inline">\(r\)</span> predictor variables <span class="math inline">\(Z_{ij},j=1,2,\cdots,r\)</span> for each response variable:
<span class="math display">\[\begin{bmatrix}
Y_1\\
Y_2\\
\vdots\\
Y_n\\
\end{bmatrix}=\begin{bmatrix}
1&amp;Z_{11}&amp;Z_{12}&amp;\cdots&amp;Z_{1r}\\
1&amp;Z_{21}&amp;Z_{22}&amp;\cdots&amp;Z_{2r}\\
\vdots&amp;\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\
1&amp;Z_{n1}&amp;Z_{n2}&amp;\cdots&amp;Z_{nr}\\
\end{bmatrix}\begin{bmatrix}
\beta_0\\
\beta_1\\
\vdots\\
\beta_r\\
\end{bmatrix}+\begin{bmatrix}
\epsilon_1\\
\epsilon_2\\
\vdots\\
\epsilon_n\\
\end{bmatrix}\]</span> or
<span class="math inline">\(\mathbf Y=\mathbf Z\boldsymbol\beta+\boldsymbol\epsilon\)</span>, where <span class="math inline">\(E(\boldsymbol\epsilon)=\boldsymbol0\)</span>, <span class="math inline">\(Cov(\boldsymbol\epsilon)=E(\boldsymbol\epsilon\boldsymbol\epsilon^T)=\sigma^2\mathbf I\)</span>. We have to find the regression coefficients <span class="math inline">\(\boldsymbol\beta\)</span> and the error variance <span class="math inline">\(\sigma^2\)</span> that are consistent with the available data. Denote <span class="math inline">\(\hat{\boldsymbol\beta}\)</span> as least squares estimate of <span class="math inline">\(\boldsymbol\beta\)</span>, then <span class="math inline">\(\hat{\boldsymbol y}=\mathbf Z\hat{\boldsymbol\beta}\)</span> and <span class="math inline">\(\hat{\boldsymbol y}\)</span> is the projection of vector <span class="math inline">\(\boldsymbol y\)</span> on the column space of <span class="math inline">\(\mathbf Z\)</span>, so <span class="math inline">\(\boldsymbol y-\hat{\boldsymbol y}=\boldsymbol y-\mathbf Z\hat{\boldsymbol\beta}\)</span> is perpendicular to column space of <span class="math inline">\(\mathbf Z\)</span>, so <span class="math inline">\(\mathbf Z^T(\boldsymbol y-\mathbf Z\hat{\boldsymbol\beta})=\mathbf Z^T\boldsymbol y-\mathbf Z^T\mathbf Z\hat{\boldsymbol\beta}=\boldsymbol0\Rightarrow\mathbf Z^T\boldsymbol y=\mathbf Z^T\mathbf Z\hat{\boldsymbol\beta}\)</span>. <span class="math inline">\(\mathbf Z^T\mathbf Z\)</span> is a <span class="math inline">\((r+1)(r+1)\)</span> asymmetric matrix. Because the column space of <span class="math inline">\(\mathbf Z^T\mathbf Z\)</span> must in the column space of <span class="math inline">\(\mathbf Z\)</span> and row space of <span class="math inline">\(\mathbf Z^T\mathbf Z\)</span> must in the row space of <span class="math inline">\(\mathbf Z\)</span> so the rank of <span class="math inline">\(rank(\mathbf Z^T\mathbf Z)\le r+1\)</span> and <span class="math inline">\(rank(\mathbf Z^T\mathbf Z)\le n\)</span>, because <span class="math inline">\(\mathbf Z^T\mathbf Z\)</span> is a <span class="math inline">\((r+1)(r+1)\)</span> matrix, so only when <span class="math inline">\(r+1\le n\)</span>, it will be possible that <span class="math inline">\(rank(\mathbf Z^T\mathbf Z)=r+1\)</span> and <span class="math inline">\(\mathbf Z^T\mathbf Z\)</span> is invertible.<br />
Then <span class="math inline">\(\hat{\boldsymbol\beta}=(\mathbf Z^T\mathbf Z)^{-1}\mathbf Z^T\boldsymbol y\)</span> and <span class="math inline">\(\mathbf Z(\mathbf Z^T\mathbf Z)^{-1}\mathbf Z^T\boldsymbol y=\hat{\boldsymbol y}\)</span> , so <span class="math inline">\(\boldsymbol y-\hat{\boldsymbol y}=(\mathbf 1-\mathbf Z(\mathbf Z^T\mathbf Z)^{-1}\mathbf Z^T)\boldsymbol y\)</span> It is clear that every column vector of <span class="math inline">\((\mathbf 1-\mathbf Z(\mathbf Z^T\mathbf Z)^{-1}\mathbf Z^T)\)</span> is perpendicular to column space of <span class="math inline">\(\mathbf Z\)</span> caused by <span class="math inline">\(\mathbf Z^T(\mathbf 1-\mathbf Z(\mathbf Z^T\mathbf Z)^{-1}\mathbf Z^T)=\mathbf0\)</span>. And <span class="math inline">\((\mathbf 1-\mathbf Z(\mathbf Z^T\mathbf Z)^{-1}\mathbf Z^T)(\mathbf 1-\mathbf Z(\mathbf Z^T\mathbf Z)^{-1}\mathbf Z^T)=\mathbf 1-\mathbf Z(\mathbf Z^T\mathbf Z)^{-1}\mathbf Z^T\)</span>, so <span class="math inline">\(\mathbf 1-\mathbf Z(\mathbf Z^T\mathbf Z)^{-1}\mathbf Z^T\)</span> is an <span style="color: red;"><strong>Idempotent matrix</strong></span>. So <span class="math display">\[\begin{align}
tr[\mathbf 1-\mathbf Z(\mathbf Z^T\mathbf Z)^{-1}\mathbf Z^T]&amp;=tr[\mathbf 1]-tr[\mathbf Z(\mathbf Z^T\mathbf Z)^{-1}\mathbf Z^T]\\
&amp;=tr[\mathbf 1]-tr[\mathbf Z^T\mathbf Z(\mathbf Z^T\mathbf Z)^{-1}]\\
&amp;=tr[\underset{n\times n}{\mathbf 1}]-tr[\underset{(r+1)\times (r+1)}{\mathbf 1}]\\
&amp;=n-r-1
\end{align}\]</span><br />
Because <span class="math inline">\(\hat{\boldsymbol\epsilon}=\boldsymbol y-\hat{\boldsymbol y}\)</span> is perpendicular to column space of <span class="math inline">\(\mathbf Z\)</span>, then <span class="math inline">\(\mathbf Z^T\hat{\boldsymbol\epsilon}=\boldsymbol0\)</span> and <span class="math inline">\(\boldsymbol1^T\hat{\boldsymbol\epsilon}=\displaystyle\sum_{i=1}^{n}\hat{\boldsymbol\epsilon}_i=\displaystyle\sum_{i=1}^{n}y_i-\displaystyle\sum_{i=1}^{n}\hat y_i=n\bar y-n\bar{\hat y}=0\)</span>. Then <span class="math inline">\(\bar y=\bar{\hat y}\)</span> And because <span class="math inline">\(\mathbf y^T\mathbf y=(\hat{\mathbf y}+\mathbf y-\hat{\mathbf y})^T(\hat{\mathbf y}+\mathbf y-\hat{\mathbf y})=(\hat{\mathbf y}+\hat{\boldsymbol\epsilon})^T(\hat{\mathbf y}+\hat{\boldsymbol\epsilon})=\hat{\mathbf y}^T\hat{\mathbf y}+\hat{\boldsymbol\epsilon}^T\hat{\boldsymbol\epsilon}\)</span>, this also can be get using Pythagorean theorem. Then sum of squares decomposition can be: <span class="math inline">\(\mathbf y^T\mathbf y-n(\bar y)^2=\hat{\mathbf y}^T\hat{\mathbf y}-n(\bar{\hat y})^2+\hat{\boldsymbol\epsilon}^T\hat{\boldsymbol\epsilon}\)</span> or <span class="math inline">\(\underset{SS_{total}}{\displaystyle\sum_{i=1}^{n}(y_i-\bar y)^2}=\underset{SS_{regression}}{\displaystyle\sum_{i=1}^{n}(\hat y_i-\bar{\hat y})^2}+\underset{SS_{error}}{\displaystyle\sum_{i=1}^{n}\hat\epsilon_i^2}\)</span> The quality of the models fit can be measured by the <strong>coefficient of determination</strong> <span class="math inline">\(R^2=\frac{\displaystyle\sum_{i=1}^{n}(\hat y_i-\bar{\hat y})^2}{\displaystyle\sum_{i=1}^{n}(y_i-\bar y)^2}\)</span></p></li>
<li><p>Inferences About the Regression Model:<br />
</p></li>
</ul>
<ol style="list-style-type: decimal">
<li>Because <span class="math inline">\(E(\hat{\boldsymbol\beta})=E((\mathbf Z^T\mathbf Z)^{-1}\mathbf Z^T\mathbf Y)\)</span>, so <span class="math inline">\(E(\mathbf Z\hat{\boldsymbol\beta})=E(\mathbf Z(\mathbf Z^T\mathbf Z)^{-1}\mathbf Z^T\mathbf Y)=E(\hat{\mathbf Y})\)</span> and because<br />
<span class="math inline">\(E(\mathbf Z\boldsymbol\beta)=E(\mathbf Y-\boldsymbol\epsilon)=E(\mathbf Y)=E(\hat{\mathbf Y})\)</span>, so <span class="math inline">\(E(\mathbf Z\hat{\boldsymbol\beta})=E(\mathbf Z\boldsymbol\beta)\)</span> and <span class="math inline">\(E(\mathbf Z(\hat{\boldsymbol\beta}-\boldsymbol\beta))=\boldsymbol0\)</span> so <span class="math inline">\(E(\hat{\boldsymbol\beta})=E(\boldsymbol\beta)\)</span><br />
</li>
<li>If <span class="math inline">\(\boldsymbol\epsilon\)</span> is distributed as <span class="math inline">\(N_n(\boldsymbol0,\sigma^2\mathbf I)\)</span>, then <span class="math inline">\(Cov(\mathbf Z\hat{\boldsymbol\beta})=\mathbf Z^T\mathbf ZCov(\hat{\boldsymbol\beta})=Cov(\hat{\mathbf Y})=Cov(\mathbf Y+\boldsymbol\epsilon)=Cov(\boldsymbol\epsilon)=\sigma^2\mathbf I\)</span>, then <span class="math inline">\(Cov(\hat{\boldsymbol\beta})=\sigma^2(\mathbf Z^T\mathbf Z)^{-1}\)</span>, then <span class="math inline">\(\hat{\boldsymbol\beta}\)</span> is distributed as <span class="math inline">\(N_{(r+1)}(\boldsymbol\beta,\sigma^2(\mathbf Z^T\mathbf Z)^{-1})\)</span><br />
Because <span class="math inline">\(E(\hat{\boldsymbol\epsilon})=\boldsymbol 0\)</span> and <span class="math inline">\(Cov(\hat{\boldsymbol\epsilon})=Cov(\boldsymbol y-\hat{\boldsymbol y})=Cov((\mathbf 1-\mathbf Z(\mathbf Z^T\mathbf Z)^{-1}\mathbf Z^T)\boldsymbol y)=(\mathbf 1-\mathbf Z(\mathbf Z^T\mathbf Z)^{-1}\mathbf Z^T)\sigma^2\)</span> <span class="math display">\[\begin{align}
E(\hat{\boldsymbol\epsilon}^T\hat{\boldsymbol\epsilon})&amp;=E(\boldsymbol y-\hat{\boldsymbol y})^T(\boldsymbol y-\hat{\boldsymbol y})\\
&amp;=E((\mathbf 1-\mathbf Z(\mathbf Z^T\mathbf Z)^{-1}\mathbf Z^T)\boldsymbol y)^T((\mathbf 1-\mathbf Z(\mathbf Z^T\mathbf Z)^{-1}\mathbf Z^T)\boldsymbol y)\\
&amp;=E(\boldsymbol y^T(\mathbf 1-\mathbf Z(\mathbf Z^T\mathbf Z)^{-1}\mathbf Z^T))((\mathbf 1-\mathbf Z(\mathbf Z^T\mathbf Z)^{-1}\mathbf Z^T)\boldsymbol y)\\
&amp;=E(\boldsymbol y^T(\mathbf 1-\mathbf Z(\mathbf Z^T\mathbf Z)^{-1}\mathbf Z^T)\boldsymbol y)\\
&amp;=E(\boldsymbol y^T\boldsymbol y)-E(\boldsymbol y^T\mathbf Z(\mathbf Z^T\mathbf Z)^{-1}\mathbf Z^T\boldsymbol y)\\
&amp;=n\sigma^2-(r+1)\sigma^2\\
&amp;=(n-r-1)\sigma^2\\
\end{align}\]</span> so defining <span class="math display">\[s^2=\frac{\hat{\boldsymbol\epsilon}^T\hat{\boldsymbol\epsilon}}{n-r-1}=\frac{\boldsymbol y^T(\mathbf 1-\mathbf Z(\mathbf Z^T\mathbf Z)^{-1}\mathbf Z^T)\boldsymbol y}{n-r-1}\]</span> Then <span class="math inline">\(E(s^2)=\sigma^2\)</span></li>
<li>If <span class="math inline">\(\hat\sigma^2\)</span> is the maximum likelihood estimator of <span class="math inline">\(\sigma^2\)</span>, then <span class="math inline">\(n\hat\sigma^2=(n-r-1)s^2=\hat{\boldsymbol\epsilon}^T\hat{\boldsymbol\epsilon}\)</span> is distributed as <span class="math inline">\(\sigma^2\chi_{n-r-1}^2\)</span>. The likelihood associated with the parameters <span class="math inline">\(\boldsymbol\beta\)</span> and <span class="math inline">\(\sigma^2\)</span> is <span class="math inline">\(L(\boldsymbol\beta,\sigma^2)=\frac{1}{(2\pi)^{n/2}\sigma^n}exp\Bigl[-\frac{(\mathbf y-\mathbf Z\boldsymbol\beta)^T(\mathbf y-\mathbf Z\boldsymbol\beta)}{2\sigma^2}\Bigr]\)</span> with the maximum occurring at <span class="math inline">\(\hat{\boldsymbol\beta}=(\mathbf Z^T\mathbf Z)^{-1}\mathbf Z^T\boldsymbol y\)</span>. For the maximum of <span class="math inline">\(\hat\sigma^2=(\mathbf y-\mathbf Z\hat{\boldsymbol\beta})^T(\mathbf y-\mathbf Z\hat{\boldsymbol\beta})/n\)</span>, then the maximum likelihood is <span class="math inline">\(L(\hat{\boldsymbol\beta},\hat\sigma^2)=\displaystyle\frac{1}{(2\pi)^{n/2}\hat\sigma^n}e^{-n/2}\)</span><br />
</li>
<li>Let <span class="math inline">\(\mathbf V=(\mathbf Z^T\mathbf Z)^{1/2}(\hat{\boldsymbol\beta}-\boldsymbol\beta)\)</span>, then <span class="math inline">\(E(\mathbf V)=\mathbf0\)</span> and <span class="math inline">\(Cov(\mathbf V)=(\mathbf Z^T\mathbf Z)^{1/2}Cov(\hat{\boldsymbol\beta}-\boldsymbol\beta)(\mathbf Z^T\mathbf Z)^{1/2}=(\mathbf Z^T\mathbf Z)^{1/2}Cov(\hat{\boldsymbol\beta})(\mathbf Z^T\mathbf Z)^{1/2}=(\mathbf Z^T\mathbf Z)^{1/2}\sigma^2(\mathbf Z^T\mathbf Z)^{-1}(\mathbf Z^T\mathbf Z)^{1/2}=\sigma^2\mathbf I\)</span>, so <span class="math inline">\(\mathbf V\)</span> is normally distributed and <span class="math inline">\(\mathbf V^T\mathbf V=(\hat{\boldsymbol\beta}-\boldsymbol\beta)^T(\mathbf Z^T\mathbf Z)^{1/2}(\mathbf Z^T\mathbf Z)^{1/2}(\hat{\boldsymbol\beta}-\boldsymbol\beta)=(\hat{\boldsymbol\beta}-\boldsymbol\beta)^T(\mathbf Z^T\mathbf Z)(\hat{\boldsymbol\beta}-\boldsymbol\beta)\)</span> is distributed as <span class="math inline">\(\sigma^2\chi_{r+1}^2\)</span>, then <span class="math inline">\(\frac{\chi_{r+1}^2/(r+1)}{\chi_{n-r-1}^2/(n-r-1)}=\frac{\mathbf V^T\mathbf V}{(r+1)s^2}\)</span> has an <span class="math inline">\(F_{(r+1),(n-r-1)}\)</span> distribution. Then a <span class="math inline">\(100(1-\alpha)\%\)</span> confidence region for <span class="math inline">\(\boldsymbol\beta\)</span> is given by: <span class="math inline">\((\boldsymbol\beta-\hat{\boldsymbol\beta})^T(\mathbf Z^T\mathbf Z)(\boldsymbol\beta-\hat{\boldsymbol\beta})\le(r+1)s^2F_{(r+1),(n-r-1)}(\alpha)\)</span>. Also for each component of <span class="math inline">\(\boldsymbol\beta\)</span>, <span class="math inline">\(|\beta_i-\hat{\beta}_i|\le\sqrt{(r+1)F_{(r+1),(n-r-1)}(\alpha)}\sqrt{s^2(\mathbf Z^T\mathbf Z)_{ii}^{-1}}\)</span>, where <span class="math inline">\((\mathbf Z^T\mathbf Z)_{ii}^{-1}\)</span> is the <span class="math inline">\(i^{th}\)</span> diagonal element of <span class="math inline">\((\mathbf Z^T\mathbf Z)^{-1}\)</span>. Then, the simultaneous <span class="math inline">\(100(1-\alpha)\%\)</span> confidence intervals for the <span class="math inline">\(\beta_i\)</span> are given by: <span class="math inline">\(\hat{\beta}_i\pm\sqrt{(r+1)F_{(r+1),(n-r-1)}(\alpha)}\sqrt{s^2(\mathbf Z^T\mathbf Z)_{ii}^{-1}}\)</span><br />
</li>
<li>If some of <span class="math inline">\(\mathbf Z=[z_0,z_1,z_2,\cdots,z_q,z_{q+1},z_{q+2},\cdots,z_{r}]\)</span> parameters <span class="math inline">\([z_{q+1},z_{q+2},\cdots,z_{r}]\)</span> do not influence <span class="math inline">\(\mathbf y\)</span>, which means the hypothesis <span class="math inline">\(H_0:\beta_{q+1}=\beta_{q+2}=\cdots=\beta_{r}=0\)</span>. We can express the general linear model as <span class="math display">\[\mathbf y=\mathbf Z\boldsymbol\beta+\boldsymbol\epsilon=\left[\begin{array}{c:c}
\underset{n\times(q+1)}{\mathbf Z_1}&amp;\underset{n\times(r-q)}{\mathbf Z_2}
\end{array}
\right]\begin{bmatrix}
\underset{(q+1)\times1}{\boldsymbol \beta_{(1)}}\\
\hdashline
\underset{(r-q)\times1}{\boldsymbol \beta_{(2)}}\\
\end{bmatrix}+\boldsymbol\epsilon=\mathbf Z_1\boldsymbol\beta_{(1)}+\mathbf Z_2\boldsymbol\beta_{(2)}+\boldsymbol\epsilon\]</span> Now the hypothesis is <span class="math inline">\(H_0:\boldsymbol\beta_{(2)}=\boldsymbol0,\mathbf y=\mathbf Z_1\boldsymbol\beta_{(1)}+\boldsymbol\epsilon\)</span> The <span class="math display">\[\begin{align}
\text{Extra sum of squares}&amp;=SS_{\text{error}}(\mathbf Z_1)-SS_{\text{error}}(\mathbf Z)\\
&amp;=(\mathbf y-\mathbf Z_1\hat{\boldsymbol\beta}_{(1)})^T(\mathbf y-\mathbf Z_1\hat{\boldsymbol\beta}_{(1)})-(\mathbf y-\mathbf Z\hat{\boldsymbol\beta})^T(\mathbf y-\mathbf Z\hat{\boldsymbol\beta})
\end{align}\]</span> where <span class="math inline">\(\hat{\boldsymbol\beta}_{(1)}=(\mathbf Z_1^T\mathbf Z_1)^{-1}\mathbf Z_1^T\mathbf y\)</span>. Under the restriction of the null hypothesis, the maximum likelihood is <span class="math inline">\(\underset{\boldsymbol\beta_{(1)},\sigma^2}{\text{max }}L(\boldsymbol\beta_{(1)},\sigma^2)=\displaystyle\frac{1}{(2\pi)^{n/2}\hat\sigma_1^n}e^{-n/2}\)</span> where the maximum occurs at <span class="math inline">\(\hat{\boldsymbol\beta}_{(1)}=(\mathbf Z_1^T\mathbf Z_1)^{-1}\mathbf Z_1^T\boldsymbol y\)</span> and <span class="math inline">\(\hat\sigma_1^2=(\mathbf y-\mathbf Z_1\hat{\boldsymbol\beta}_{(1)})^T(\mathbf y-\mathbf Z_1\hat{\boldsymbol\beta}_{(1)})/n\)</span> Reject <span class="math inline">\(H_0:\boldsymbol\beta_{(2)}=\boldsymbol0\)</span> for small values of the <strong>likelihood ratio test</strong> : <span class="math display">\[\frac{\underset{\boldsymbol\beta_{(1)},\sigma^2}{\text{max }}L(\boldsymbol\beta_{(1)},\sigma^2)}{\underset{\boldsymbol\beta,\sigma^2}{\text{max }}L(\boldsymbol\beta,\sigma^2)}=\Bigl(\frac{\hat\sigma_1}{\hat\sigma}\Bigr)^{-n}=\Bigl(\frac{\hat\sigma^2_1}{\hat\sigma^2}\Bigr)^{-n/2}=\Bigl(1+\frac{\hat\sigma^2_1-\hat\sigma^2}{\hat\sigma^2}\Bigr)^{-n/2}\]</span> Which is equivalent to rejecting <span class="math inline">\(H_0\)</span> when <span class="math inline">\(\frac{\hat\sigma^2_1-\hat\sigma^2}{\hat\sigma^2}\)</span> is large or when <span class="math display">\[\frac{n(\hat\sigma^2_1-\hat\sigma^2)/(r-q)}{n\hat\sigma^2/(n-r-1)}=\frac{(SS_{error}(\mathbf Z_1)-SS_{error}(\mathbf Z))/(r-q)}{s^2}=F_{(r-q),(n-r-1)}\]</span> is large.<br />
</li>
<li>Let <span class="math inline">\(y_0\)</span> denote the response value when the predictor variables have values <span class="math inline">\(\mathbf z^T_0=[1,z_{01},z_{02},\cdots,z_{0r}]\)</span>, then <span class="math inline">\(E(y_0|\mathbf z_0)=\beta_0+z_{01}\beta_1+\cdots+z_{0r}\beta_r=\mathbf z^T_0\boldsymbol\beta\)</span>, its least squares estimate is <span class="math inline">\(\mathbf z^T_0\hat{\boldsymbol\beta}\)</span>. Because <span class="math inline">\(Cov(\hat{\boldsymbol\beta})=\sigma^2(\mathbf Z^T\mathbf Z)^{-1}\)</span> and <span class="math inline">\(\hat{\boldsymbol\beta}\)</span> is distributed as <span class="math inline">\(N_{(r+1)}(\boldsymbol\beta,\sigma^2(\mathbf Z^T\mathbf Z)^{-1})\)</span>, so <span class="math inline">\(Var(\mathbf z^T_0\hat{\boldsymbol\beta})=\mathbf z^T_0Cov(\hat{\boldsymbol\beta})\mathbf z_0=\sigma^2\mathbf z^T_0(\mathbf Z^T\mathbf Z)^{-1}\mathbf z_0\)</span>. Because <span class="math inline">\((n-r-1)s^2=\hat{\boldsymbol\epsilon}^T\hat{\boldsymbol\epsilon}\)</span> is distributed as <span class="math inline">\(\sigma^2\chi_{n-r-1}^2\)</span>, so <span class="math inline">\(\frac{s^2}{\sigma^2}=\chi_{n-r-1}^2/(n-r-1)\)</span>. Consequently, the linear combination <span class="math inline">\(\mathbf z^T_0\hat{\boldsymbol\beta}\)</span> is distributed as <span class="math inline">\(N(\mathbf z^T_0\boldsymbol\beta,\sigma^2\mathbf z^T_0(\mathbf Z^T\mathbf Z)^{-1}\mathbf z_0)\)</span> and <span class="math inline">\(\displaystyle\frac{(\mathbf z^T_0\hat{\boldsymbol\beta}-\mathbf z^T_0\boldsymbol\beta)/\sqrt{\sigma^2\mathbf z^T_0(\mathbf Z^T\mathbf Z)^{-1}\mathbf z_0}}{\sqrt{\frac{s^2}{\sigma^2}}}\)</span> is distributed as <span class="math inline">\(t_{n-r-1}\)</span>. Then a <span class="math inline">\(100(1-\alpha)\%\)</span> confidence interval for <span class="math inline">\(E(y_0|\mathbf z_0)=\mathbf z^T_0\boldsymbol\beta\)</span> is given by: <span class="math inline">\(\mathbf z^T_0\hat{\boldsymbol\beta}\pm t_{n-r-1}(\frac{\alpha}{2})\sqrt{\frac{s^2}{\sigma^2}}\sqrt{\sigma^2\mathbf z^T_0(\mathbf Z^T\mathbf Z)^{-1}\mathbf z_0}=\mathbf z^T_0\hat{\boldsymbol\beta}\pm t_{n-r-1}(\frac{\alpha}{2})\sqrt{s^2\mathbf z^T_0(\mathbf Z^T\mathbf Z)^{-1}\mathbf z_0}\)</span></li>
</ol>
<ul>
<li><p>Multivariate linear regression:<br />
When there are <span class="math inline">\(m\)</span> regression coefficients vectors <span class="math inline">\(\boldsymbol\beta_1,\cdots,\boldsymbol\beta_m\)</span>, each regression coefficients vector <span class="math inline">\(\boldsymbol\beta\)</span> has <span class="math inline">\(r+1\)</span> components <span class="math inline">\([\beta_0,\beta_1,\cdots,\beta_r]\)</span>, then the response variables are:
<span class="math display">\[\underset{(n\times m)}{\underbrace{\begin{bmatrix}
Y_{11}&amp;Y_{12}&amp;\cdots&amp;Y_{1m}\\
Y_{21}&amp;Y_{22}&amp;\cdots&amp;Y_{2m}\\
\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\
Y_{n1}&amp;Y_{n2}&amp;\cdots&amp;Y_{nm}\\
\end{bmatrix}}}=\underset{(n\times (r+1))}{\underbrace{\begin{bmatrix}
1&amp;Z_{11}&amp;Z_{12}&amp;\cdots&amp;Z_{1r}\\
1&amp;Z_{21}&amp;Z_{22}&amp;\cdots&amp;Z_{2r}\\
\vdots&amp;\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\
1&amp;Z_{n1}&amp;Z_{n2}&amp;\cdots&amp;Z_{nr}\\
\end{bmatrix}}}\underset{((r+1)\times m)}{\underbrace{\begin{bmatrix}
\beta_{01}&amp;\beta_{02}&amp;\cdots&amp;\beta_{0m}\\
\beta_{11}&amp;\beta_{12}&amp;\cdots&amp;\beta_{1m}\\
\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\
\beta_{r1}&amp;\beta_{r2}&amp;\cdots&amp;\beta_{rm}\\
\end{bmatrix}}}+\underset{(n\times m)}{\underbrace{\begin{bmatrix}
\epsilon_{11}&amp;\epsilon_{12}&amp;\cdots&amp;\epsilon_{1m}\\
\epsilon_{21}&amp;\epsilon_{22}&amp;\cdots&amp;\epsilon_{2m}\\
\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\
\epsilon_{n1}&amp;\epsilon_{n2}&amp;\cdots&amp;\epsilon_{nm}\\
\end{bmatrix}}}\]</span> or
<span class="math inline">\(\mathbf Y=\mathbf Z\boldsymbol\beta+\boldsymbol\epsilon\)</span>, the <span class="math inline">\(j^{th}\)</span> row of <span class="math inline">\(\mathbf Y\)</span> is the <span class="math inline">\(m\)</span> observations on the <span class="math inline">\(j^{th}\)</span> trial and regressed using <span class="math inline">\(m\)</span> regression coefficients vectors. The <span class="math inline">\(i^{th}\)</span> column of <span class="math inline">\(\mathbf Y\)</span> is the <span class="math inline">\(n\)</span> observations on all of the <span class="math inline">\(n\)</span> trials and regressed using the <span class="math inline">\(i^{th}\)</span> regression coefficients vector. The <span class="math inline">\(i^{th}\)</span> column of <span class="math inline">\(\boldsymbol\epsilon\)</span> is the <span class="math inline">\(n\)</span> errors of the <span class="math inline">\(n\)</span> observations regressed using the <span class="math inline">\(i^{th}\)</span> regression coefficients vector <span class="math inline">\(\boldsymbol\beta_i\)</span>, <span class="math inline">\(\mathbf Y_i=\mathbf Z\boldsymbol\beta_i+\boldsymbol\epsilon_i\)</span><br />
The <span class="math inline">\(2\)</span> columns of <span class="math inline">\(\boldsymbol\epsilon\)</span>, <span class="math inline">\(\boldsymbol\epsilon_i\)</span> and <span class="math inline">\(\boldsymbol\epsilon_k\)</span> are independent, with <span class="math inline">\(E(\boldsymbol\epsilon_i)=\boldsymbol0\)</span> and <span class="math inline">\(Cov(\boldsymbol\epsilon_i,\boldsymbol\epsilon_k)=\sigma_{ik}\mathbf I, \quad i,(k=1,2,\cdots,m)\)</span>. The <span class="math inline">\(m\)</span> observations on the <span class="math inline">\(j^{th}\)</span> trial with <span class="math inline">\(2\)</span> regression coefficients vectors <span class="math inline">\(\boldsymbol\beta_i,\boldsymbol\beta_k\)</span> have covariance matrix <span class="math inline">\(\underset{m\times m}{\boldsymbol\Sigma}=\{\sigma_{ik}\}\)</span></p></li>
<li><p>Like in the univariate linear regression models, <span class="math inline">\(\hat{\boldsymbol\beta}_i=(\mathbf Z^T\mathbf Z)^{-1}\mathbf Z^T\mathbf Y_i\)</span> and <span class="math inline">\(\hat{\boldsymbol\beta}=(\mathbf Z^T\mathbf Z)^{-1}\mathbf Z^T\mathbf Y\)</span><br />
The Predicted values:<span class="math inline">\(\hat{\mathbf Y}=\mathbf Z(\mathbf Z^T\mathbf Z)^{-1}\mathbf Z^T\mathbf Y\)</span>,<br />
Residuals: <span class="math inline">\(\hat{\boldsymbol\epsilon}=\mathbf Y-\hat{\mathbf Y}=(\mathbf 1-\mathbf Z(\mathbf Z^T\mathbf Z)^{-1}\mathbf Z^T)\mathbf Y\)</span> also because <span class="math inline">\(\mathbf Z^T\hat{\boldsymbol\epsilon}=\mathbf Z^T(\mathbf 1-\mathbf Z(\mathbf Z^T\mathbf Z)^{-1}\mathbf Z^T)\mathbf Y=\mathbf0\)</span>, so the residuals <span class="math inline">\(\hat{\boldsymbol\epsilon}_i\)</span> are perpendicular to the columns of <span class="math inline">\(\mathbf Z\)</span>, also because <span class="math inline">\(\hat{\mathbf Y}^T\hat{\boldsymbol\epsilon}=(\mathbf Z\hat{\boldsymbol\beta})^T(\mathbf 1-\mathbf Z(\mathbf Z^T\mathbf Z)^{-1}\mathbf Z^T)\mathbf Y=\hat{\boldsymbol\beta}^T\mathbf Z^T(\mathbf 1-\mathbf Z(\mathbf Z^T\mathbf Z)^{-1}\mathbf Z^T)\mathbf Y=\mathbf0\)</span>, so <span class="math inline">\(\hat{\mathbf Y}_i\)</span> are perpendicular to all residual vectors <span class="math inline">\(\hat{\boldsymbol\epsilon}_i\)</span> So <span class="math inline">\(\mathbf Y^T\mathbf Y=(\hat{\mathbf Y}+\hat{\boldsymbol\epsilon})^T(\hat{\mathbf Y}+\hat{\boldsymbol\epsilon})=\hat{\mathbf Y}^T\hat{\mathbf Y}+\hat{\boldsymbol\epsilon}^T\hat{\boldsymbol\epsilon}+\mathbf0+\mathbf0\)</span> or <span class="math inline">\(\mathbf Y^T\mathbf Y=\hat{\mathbf Y}^T\hat{\mathbf Y}+\hat{\boldsymbol\epsilon}^T\hat{\boldsymbol\epsilon}\)</span></p></li>
</ul>
<ol style="list-style-type: decimal">
<li><p>Because <span class="math inline">\(\mathbf Y_i=\mathbf Z\boldsymbol\beta_i+\boldsymbol\epsilon_i\)</span>, then <span class="math inline">\(\hat{\boldsymbol\beta}_i-\boldsymbol\beta_i=(\mathbf Z^T\mathbf Z)^{-1}\mathbf Z^T\mathbf Y_i-\boldsymbol\beta_i=(\mathbf Z^T\mathbf Z)^{-1}\mathbf Z^T(\mathbf Z\boldsymbol\beta_i+\boldsymbol\epsilon_i)-\boldsymbol\beta_i=(\mathbf Z^T\mathbf Z)^{-1}\mathbf Z^T\boldsymbol\epsilon_i\)</span> so <span class="math inline">\(E(\hat{\boldsymbol\beta}_i-\boldsymbol\beta_i)=(\mathbf Z^T\mathbf Z)^{-1}\mathbf Z^TE(\boldsymbol\epsilon_i)=\mathbf0\)</span>, or <span class="math inline">\(E(\hat{\boldsymbol\beta}_i)=\boldsymbol\beta_i\)</span>
and because <span class="math inline">\(\hat{\boldsymbol\epsilon}_i=\mathbf Y_i-\hat{\mathbf Y}_i=(\mathbf 1-\mathbf Z(\mathbf Z^T\mathbf Z)^{-1}\mathbf Z^T)\mathbf Y_i=(\mathbf 1-\mathbf Z(\mathbf Z^T\mathbf Z)^{-1}\mathbf Z^T)(\mathbf Z\boldsymbol\beta_i+\boldsymbol\epsilon_i)=(\mathbf 1-\mathbf Z(\mathbf Z^T\mathbf Z)^{-1}\mathbf Z^T)\boldsymbol\epsilon_i\)</span>, so <span class="math inline">\(E(\hat{\boldsymbol\epsilon}_i)=\mathbf0\)</span> And <span class="math display">\[\begin{align}
Cov(\hat{\boldsymbol\beta}_i,\hat{\boldsymbol\beta}_k)&amp;=E(\hat{\boldsymbol\beta}_i-\boldsymbol\beta_i)(\hat{\boldsymbol\beta}_k-\boldsymbol\beta_k)^T\\
&amp;=E((\mathbf Z^T\mathbf Z)^{-1}\mathbf Z^T\boldsymbol\epsilon_i)((\mathbf Z^T\mathbf Z)^{-1}\mathbf Z^T\boldsymbol\epsilon_k)^T\\
&amp;=(\mathbf Z^T\mathbf Z)^{-1}\mathbf Z^TE(\boldsymbol\epsilon_i\boldsymbol\epsilon_k^T)\mathbf Z(\mathbf Z^T\mathbf Z)^{-1}\\
&amp;=\sigma_{ik}(\mathbf Z^T\mathbf Z)^{-1}
\end{align}\]</span> Then <span class="math display">\[\begin{align}
E(\hat{\boldsymbol\epsilon}_i^T\hat{\boldsymbol\epsilon}_k)&amp;=E\Biggl(((\mathbf 1-\mathbf Z(\mathbf Z^T\mathbf Z)^{-1}\mathbf Z^T)\boldsymbol\epsilon_i)^T((\mathbf 1-\mathbf Z(\mathbf Z^T\mathbf Z)^{-1}\mathbf Z^T)\boldsymbol\epsilon_k)\Biggr)\\
&amp;=E(\boldsymbol\epsilon_i^T(\mathbf 1-\mathbf Z(\mathbf Z^T\mathbf Z)^{-1}\mathbf Z^T)\boldsymbol\epsilon_k)\\
&amp;=E(tr(\boldsymbol\epsilon_i^T(\mathbf 1-\mathbf Z(\mathbf Z^T\mathbf Z)^{-1}\mathbf Z^T)\boldsymbol\epsilon_k))\\
&amp;=E(tr((\mathbf 1-\mathbf Z(\mathbf Z^T\mathbf Z)^{-1}\mathbf Z^T)\boldsymbol\epsilon_k\boldsymbol\epsilon_i^T))\\
&amp;=tr[(\mathbf 1-\mathbf Z(\mathbf Z^T\mathbf Z)^{-1}\mathbf Z^T)E(\boldsymbol\epsilon_k\boldsymbol\epsilon_i^T)]\\
&amp;=tr[(\mathbf 1-\mathbf Z(\mathbf Z^T\mathbf Z)^{-1}\mathbf Z^T)\sigma_{ik}\mathbf I]\\
&amp;=\sigma_{ik}tr[\mathbf 1-\mathbf Z(\mathbf Z^T\mathbf Z)^{-1}\mathbf Z^T]\\
&amp;=\sigma_{ik}(n-r-1)
\end{align}\]</span>
<span class="math display">\[\begin{align}
Cov(\hat{\boldsymbol\beta}_i,\hat{\boldsymbol\epsilon}_k)&amp;=E(\hat{\boldsymbol\beta}_i-E(\hat{\boldsymbol\beta}_i))(\hat{\boldsymbol\epsilon}_k-E(\hat{\boldsymbol\epsilon}_k))^T\\
&amp;=E(\hat{\boldsymbol\beta}_i-\boldsymbol\beta_i)(\hat{\boldsymbol\epsilon}_k-\boldsymbol\epsilon_k)^T\\
&amp;=E((\mathbf Z^T\mathbf Z)^{-1}\mathbf Z^T\boldsymbol\epsilon_i)(\hat{\boldsymbol\epsilon}_k)^T\\
&amp;=E((\mathbf Z^T\mathbf Z)^{-1}\mathbf Z^T\boldsymbol\epsilon_i)((\mathbf 1-\mathbf Z(\mathbf Z^T\mathbf Z)^{-1}\mathbf Z^T)\boldsymbol\epsilon_k)^T\\
&amp;=E((\mathbf Z^T\mathbf Z)^{-1}\mathbf Z^T\boldsymbol\epsilon_i)(\boldsymbol\epsilon_k^T(\mathbf 1-\mathbf Z(\mathbf Z^T\mathbf Z)^{-1}\mathbf Z^T))\\
&amp;=(\mathbf Z^T\mathbf Z)^{-1}\mathbf Z^TE(\boldsymbol\epsilon_i\boldsymbol\epsilon_k^T)(\mathbf 1-\mathbf Z(\mathbf Z^T\mathbf Z)^{-1}\mathbf Z^T))\\
&amp;=(\mathbf Z^T\mathbf Z)^{-1}\mathbf Z^T\sigma_{ik}\mathbf I(\mathbf 1-\mathbf Z(\mathbf Z^T\mathbf Z)^{-1}\mathbf Z^T))\\
&amp;=\sigma_{ik}((\mathbf Z^T\mathbf Z)^{-1}\mathbf Z^T-(\mathbf Z^T\mathbf Z)^{-1}\mathbf Z^T)\\
&amp;=\mathbf0
\end{align}\]</span>so each element of <span class="math inline">\(\hat{\boldsymbol\beta}\)</span> is uncorrelated with each element of <span class="math inline">\(\hat{\boldsymbol\epsilon}\)</span>.</p></li>
<li><p><span class="math inline">\(\hat{\boldsymbol\beta}=(\mathbf Z^T\mathbf Z)^{-1}\mathbf Z^T\mathbf Y\)</span> is the maximum likelihood estimator of <span class="math inline">\(\boldsymbol\beta\)</span> and <span class="math inline">\(\hat{\boldsymbol\beta}\)</span> is a normal distribution with <span class="math inline">\(E(\hat{\boldsymbol\beta})=\boldsymbol\beta\)</span> and <span class="math inline">\(Cov(\hat{\boldsymbol\beta}_i,\hat{\boldsymbol\beta}_k)=\sigma_{ik}(\mathbf Z^T\mathbf Z)^{-1}\)</span>. And <span class="math inline">\(\hat{\boldsymbol\Sigma}=\frac{1}{n}\hat{\boldsymbol\epsilon}^T\hat{\boldsymbol\epsilon}=\frac{1}{n}(\mathbf Y-\mathbf Z\hat{\boldsymbol\beta})^T(\mathbf Y-\mathbf Z\hat{\boldsymbol\beta})\)</span> is the maximum likelihood estimator of <span class="math inline">\(\boldsymbol\Sigma\)</span> and <span class="math inline">\(n\hat{\boldsymbol\Sigma}\)</span> is distribution with <span class="math inline">\(W_{p,n-r-1}(\boldsymbol\Sigma)\)</span>. The likelihood associated with the parameters <span class="math inline">\(\boldsymbol\beta\)</span> and <span class="math inline">\(\boldsymbol\Sigma\)</span> is <span class="math display">\[\begin{align}
L(\hat{\boldsymbol\mu},\hat{\boldsymbol\Sigma})&amp;=\prod_{j=1}^{n}\Biggl[\frac{1}{(2\pi)^{m/2}|\hat{\boldsymbol\Sigma}|^{1/2}}e^{-\frac{1}{2}(\mathbf y_j-\boldsymbol\mu)^T\hat{\boldsymbol\Sigma}^{-1}(\mathbf y_j-\boldsymbol\mu)}\Biggr]\\
&amp;=\frac{1}{(2\pi)^{\frac{mn}{2}}|\hat{\mathbf\Sigma}|^{n/2}}e^{-\frac{1}{2}\sum_{j=1}^{n}(\mathbf y_j-\boldsymbol\mu)^T\hat{\boldsymbol\Sigma}^{-1}(\mathbf y_j-\boldsymbol\mu)}\\
&amp;=\frac{1}{(2\pi)^{\frac{mn}{2}}|\hat{\boldsymbol\Sigma}|^{n/2}}e^{-\frac{1}{2}tr\Biggl[\hat{\boldsymbol\Sigma}^{-1}\sum_{j=1}^{n}(\mathbf y_j-\boldsymbol\mu)^T(\mathbf y_j-\boldsymbol\mu)\Biggr]}\\
&amp;=\frac{1}{(2\pi)^{(mn/2)}|\hat{\boldsymbol\Sigma}|^{n/2}}e^{-\frac{1}{2}mn}
\end{align}\]</span></p></li>
<li><p>If some of <span class="math inline">\(\mathbf Z=[z_0,z_1,z_2,\cdots,z_q,z_{q+1},z_{q+2},\cdots,z_{r}]\)</span> parameters <span class="math inline">\([z_{q+1},z_{q+2},\cdots,z_{r}]\)</span> do not influence <span class="math inline">\(\mathbf Y\)</span>, which means the hypothesis <span class="math inline">\(H_0:\boldsymbol\beta_{q+1}=\boldsymbol\beta_{q+2}=\cdots=\boldsymbol\beta_{r}=\boldsymbol0\)</span>. We can express the general linear model as <span class="math display">\[\mathbf Y=\mathbf Z\boldsymbol\beta+\boldsymbol\epsilon=\left[\begin{array}{c:c}
\underset{n\times(q+1)}{\mathbf Z_1}&amp;\underset{n\times(r-q)}{\mathbf Z_2}
\end{array}
\right]\begin{bmatrix}
\underset{(q+1)\times m}{\boldsymbol\beta_{(1)}}\\
\hdashline
\underset{(r-q)\times m}{\boldsymbol\beta_{(2)}}\\
\end{bmatrix}+\boldsymbol\epsilon=\mathbf Z_1\boldsymbol\beta_{(1)}+\mathbf Z_2\boldsymbol\beta_{(2)}+\boldsymbol\epsilon\]</span> Now the hypothesis is <span class="math inline">\(H_0:\boldsymbol\beta_{(2)}=\boldsymbol0,\mathbf Y=\mathbf Z_1\boldsymbol\beta_{(1)}+\boldsymbol\epsilon\)</span> The <span class="math display">\[\begin{align}
\text{Extra sum of squares}&amp;=SS_{\text{error}}(\mathbf Z_1)-SS_{\text{error}}(\mathbf Z)\\
&amp;=(\mathbf Y-\mathbf Z_1\hat{\boldsymbol\beta}_{(1)})^T(\mathbf Y-\mathbf Z_1\hat{\boldsymbol\beta}_{(1)})-(\mathbf Y-\mathbf Z\hat{\boldsymbol\beta})^T(\mathbf Y-\mathbf Z\hat{\boldsymbol\beta})
\end{align}\]</span> where <span class="math inline">\(\hat{\boldsymbol\beta}_{(1)}=(\mathbf Z_1^T\mathbf Z_1)^{-1}\mathbf Z_1^T\mathbf Y\)</span>. And <span class="math display">\[\hat{\boldsymbol\Sigma}_1=\frac{1}{n}SS_{\text{error}}(\mathbf Z_1)=\frac{1}{n}(\mathbf Y-\mathbf Z_1\hat{\boldsymbol\beta}_{(1)})^T(\mathbf Y-\mathbf Z_1\hat{\boldsymbol\beta}_{(1)})\]</span> Under the restriction of the null hypothesis, the maximum likelihood is <span class="math inline">\(\underset{\boldsymbol\beta_{(1)},\boldsymbol\Sigma_1}{\text{max }}L(\boldsymbol\beta_{(1)},\boldsymbol\Sigma_1)=\displaystyle\frac{1}{(2\pi)^{mn/2}\hat{\boldsymbol\Sigma}_1^{n/2}}e^{-mn/2}\)</span> where the maximum occurs at <span class="math inline">\(\hat{\boldsymbol\beta}_{(1)}=(\mathbf Z_1^T\mathbf Z_1)^{-1}\mathbf Z_1^T\boldsymbol Y\)</span> and <span class="math inline">\(\hat{\boldsymbol\Sigma}_1=\frac{1}{n}(\mathbf Y-\mathbf Z_1\hat{\boldsymbol\beta}_{(1)})^T(\mathbf Y-\mathbf Z_1\hat{\boldsymbol\beta}_{(1)})\)</span> Reject <span class="math inline">\(H_0:\boldsymbol\beta_{(2)}=\boldsymbol0\)</span> for small values of the <strong>likelihood ratio test</strong> : <span class="math display">\[\frac{\underset{\boldsymbol\beta_{(1)},\boldsymbol\Sigma_1}{\text{max }}L(\boldsymbol\beta_{(1)},\boldsymbol\Sigma_1)}{\underset{\boldsymbol\beta,\boldsymbol\Sigma}{\text{max }}L(\boldsymbol\beta,\boldsymbol\Sigma)}=\Bigl(\frac{|\hat{\boldsymbol\Sigma}_1|}{|\hat{\boldsymbol\Sigma}|}\Bigr)^{-n/2}=\Bigl(\frac{|\hat{\boldsymbol\Sigma}|}{|\hat{\boldsymbol\Sigma}_1|}\Bigr)^{n/2}=\Lambda\]</span>, where <span class="math inline">\(\Lambda^{2/n}=\frac{|\hat{\boldsymbol\Sigma}|}{|\hat{\boldsymbol\Sigma}_1|}\)</span> is the <span style="color: red;"><strong>Wilksâ€™ lambda statistic</strong></span>.</p></li>
<li><p>The responses <span class="math inline">\(\mathbf y_0\)</span> corresponding to fixed values <span class="math inline">\(\mathbf z_0\)</span> of the predictor variables and regression coefficients metrix <span class="math inline">\(\underset{n\times m}{\boldsymbol\beta}\)</span> is <span class="math inline">\(\mathbf y_0=\mathbf z_0^T\hat{\boldsymbol\beta}+\boldsymbol\epsilon_0\)</span>, <span class="math inline">\(E(\hat{\boldsymbol\beta}^T\mathbf z_0)=\boldsymbol\beta^T\mathbf z_0\)</span> and <span class="math inline">\(Cov(\hat{\boldsymbol\beta}^T\mathbf z_0)=\mathbf z_0^TCov(\hat{\boldsymbol\beta}^T)\mathbf z_0=\mathbf z_0^T(\mathbf Z^T\mathbf Z)^{-1}\mathbf z_0\boldsymbol\Sigma\)</span>, so <span class="math inline">\(\underset{m\times 1}{(\hat{\boldsymbol\beta}^T\mathbf z_0)}\)</span> is distributed as <span class="math inline">\(N_m(\boldsymbol\beta^T\mathbf z_0,\mathbf z_0^T(\mathbf Z^T\mathbf Z)^{-1}\mathbf z_0\boldsymbol\Sigma)\)</span> and <span class="math inline">\(n\hat{\boldsymbol\Sigma}\)</span> is independently distributed as <span class="math inline">\(W_{n-r-1}(\boldsymbol\Sigma)\)</span>. So the <span class="math inline">\(T^2\)</span>-statistic is <span class="math display">\[T^2=\Biggl(\frac{\hat{\boldsymbol\beta}^T\mathbf z_0-\boldsymbol\beta^T\mathbf z_0}{\sqrt{\mathbf z_0^T(\mathbf Z^T\mathbf Z)^{-1}\mathbf z_0}}\Biggr)^T\Biggl(\frac{n\hat{\boldsymbol\Sigma}}{n-r-1}\Biggr)^{-1}\Biggl(\frac{\hat{\boldsymbol\beta}^T\mathbf z_0-\boldsymbol\beta^T\mathbf z_0}{\sqrt{\mathbf z_0^T(\mathbf Z^T\mathbf Z)^{-1}\mathbf z_0}}\Biggr)\]</span> and <span class="math inline">\(100(1-\alpha)\%\)</span> confidence ellipsoid for <span class="math inline">\(\boldsymbol\beta^T\mathbf z_0\)</span> is provided by the inequality <span class="math display">\[T^2\le\frac{m(n-r-1)}{n-r-m}F_{m,n-r-m}(\alpha)\]</span> The <span class="math inline">\(100(1-\alpha)\%\)</span> simultaneous confidence intervals for the component of <span class="math inline">\(\mathbf z_0^T\boldsymbol\beta\)</span>, <span class="math inline">\(\mathbf z_0^T\boldsymbol\beta_i\)</span> is given by: <span class="math display">\[\mathbf z_0^T\hat{\boldsymbol\beta_i}\pm\sqrt{\frac{m(n-r-1)}{n-r-m}F_{m,n-r-m}(\alpha)}\sqrt{\mathbf z_0^T(\mathbf Z^T\mathbf Z)^{-1}\mathbf z_0}\sqrt{\frac{n\hat{\sigma_{ii}}}{n-r-1}}
\]</span> <span class="math inline">\(i=1,2,\cdots,m\)</span></p></li>
</ol>

    </div>
  </article>

  


</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
          </li>
          <li>
          </li>
        </ul>
      </footer>

    </div>
    

    
<script src="../../../../js/math-code.js"></script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
  </body>
</html>

