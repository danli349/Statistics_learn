<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.80.0" />


<title>Comparisons of several means - A Hugo website</title>
<meta property="og:title" content="Comparisons of several means - A Hugo website">


  <link href='../../../../favicon.ico' rel='icon' type='image/x-icon'/>



  







<link rel="stylesheet" href="../../../../css/fonts.css" media="all">
<link rel="stylesheet" href="../../../../css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="../../../../" class="nav-logo">
    <img src="../../../../images/logo.png"
         width="50"
         height="50"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="../../../../about/">about</a></li>
    
    <li><a href="../../../../vitae/">CV</a></li>
    
    <li><a href="https://github.com/danli349">GitHub</a></li>
    
    <li><a href="https://scholar.google.com/citations?user=mNjLK8EAAAAJ&amp;hl=en">Googlr Scholar</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">7 min read</span>
    

    <h1 class="article-title">Comparisons of several means</h1>

    
    <span class="article-date">2020-09-29</span>
    

    <div class="article-content">
      
<script src="../../../../rmarkdown-libs/header-attrs/header-attrs.js"></script>


<ul>
<li><p>Paired Comparisons:<br />
If there are <span class="math inline">\(2\)</span> treatments over multivariate <span class="math inline">\(\mathbf x_p\)</span>, the difference between treatment <span class="math inline">\(1\)</span> and treatment <span class="math inline">\(2\)</span> is <span class="math inline">\(\mathbf d_j=\mathbf x_{j1}-\mathbf x_{j2},\quad j=1,2,\cdots,n\)</span> if <span class="math inline">\(\mathbf d_j\)</span> are independent <span class="math inline">\(N_p(\boldsymbol\delta, \mathbf\Sigma_d)\)</span> random vectors, inferences
about the vector of mean differences <span class="math inline">\(\boldsymbol\delta\)</span> can be based upon a <span class="math inline">\(T^2\)</span>-statistic: <span class="math inline">\(T^2=n(\overline{\mathbf d}-\boldsymbol\delta)^T\mathbf S_d^{-1}(\overline{\mathbf d}-\boldsymbol\delta)\)</span> is distributed as an <span class="math inline">\(\frac{(n-1)p}{n-p}F_{p,n-p}\)</span> random variable, where <span class="math inline">\(\overline{\mathbf d}=\displaystyle\frac{1}{n}\displaystyle\sum_{j=1}^{n}\mathbf d_j\)</span> and <span class="math inline">\(\mathbf S_d=\displaystyle\frac{1}{n-1}\displaystyle\sum_{j=1}^{n}(\mathbf d_j-\overline{\mathbf d})(\mathbf d_j-\overline{\mathbf d})^T\)</span>, then an <span class="math inline">\(\alpha\)</span>-level hypothesis test of <span class="math inline">\(H_0:\boldsymbol\delta=\mathbf 0\)</span> versus <span class="math inline">\(H_1:\boldsymbol\delta\ne\mathbf 0\)</span>, rejects <span class="math inline">\(H_0\)</span> if the observed <span class="math inline">\(T^2=n\overline{\mathbf d}^T\mathbf S_d^{-1}\overline{\mathbf d}&gt;\frac{(n-1)p}{n-p}F_{p,n-p}(\alpha)\)</span>.<br />
A <span class="math inline">\(100(1-\alpha)\%\)</span> <strong>confidence region</strong> for <span class="math inline">\(\boldsymbol\delta\)</span> is the ellipsoid determined by all <span class="math inline">\(\boldsymbol\delta\)</span> that <span class="math inline">\((\overline{\mathbf d}-\boldsymbol\delta)^T\mathbf S_d^{-1}(\overline{\mathbf d}-\boldsymbol\delta)\le\frac{(n-1)p}{n(n-p)}F_{p,n-p}(\alpha)\)</span> and <span class="math inline">\(100(1-\alpha)\%\)</span> <strong>simultaneous confidence intervals</strong> for the individual mean differences <span class="math inline">\(\delta_i\)</span> are given by <span class="math inline">\(\Biggl(\overline{d_i}-\sqrt{\frac{(n-1)p}{(n-p)}F_{p,n-p}(\alpha)}\sqrt{\frac{s_{ii}}{n}},\quad \overline{d_i}+\sqrt{\frac{(n-1)p}{(n-p)}F_{p,n-p}(\alpha)}\sqrt{\frac{s_{ii}}{n}}\Biggr)\)</span><br />
and the <strong>Bonferroni simultaneous confidence intervals</strong> for the individual mean differences are <span class="math inline">\(\Biggl(\overline{d_i}-t_{n-1}(\frac{\alpha}{2p})\sqrt{\frac{s_{ii}}{n}},\quad \overline{d_i}+t_{n-1}(\frac{\alpha}{2p})\sqrt{\frac{s_{ii}}{n}}\Biggr)\)</span></p></li>
<li><p><em>Repeated Measures Design</em> and many to one Comparisons of univariate variables:<br />
For univariate variables, <span class="math inline">\(q\)</span> treatments and <span class="math inline">\(n\)</span> observations for each treatment, <span class="math display">\[\mathbf X_j=\begin{bmatrix}
X_j1\\
X_j2\\
\vdots\\
X_jq\\
\end{bmatrix}\quad j=1,2,\cdots,n
\]</span>
and the <span class="math inline">\(q-1\)</span> treatments are compared with respect to a single treatment, the contrasts of the components of <span class="math inline">\(\boldsymbol\mu\)</span> is <span class="math display">\[\underset{(q-1)\times 1}{\underbrace{\begin{bmatrix}
\mu_1-\mu_2\\
\mu_1-\mu_3\\
\vdots\\
\mu_1-\mu_q\\
\end{bmatrix}}}=\underset{(q-1)\times q}{\underbrace{\begin{bmatrix}
1&amp;-1&amp;0&amp;\cdots&amp;0\\
1&amp;0&amp;-1&amp;\cdots&amp;0\\
\vdots&amp;\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\
1&amp;0&amp;0&amp;\cdots&amp;-1\\
\end{bmatrix}}}\underset{q\times 1}{\underbrace{\begin{bmatrix}
\mu_1\\
\mu_2\\
\vdots\\
\mu_q\\
\end{bmatrix}}}=\mathbf C_1\boldsymbol\mu\]</span> <span class="math inline">\(\mathbf C_1\)</span> is called contrast matrice. The hypothesis that there are no differences in treatments (equal treatment means) becomes <span class="math inline">\(\mathbf C\boldsymbol\mu=\mathbf 0\)</span> for any choice of the contrast matrix <span class="math inline">\(\mathbf C\)</span>. The contrasts of the observations <span class="math inline">\(\mathbf C\mathbf x_j\)</span> have means <span class="math inline">\(\mathbf C\overline{\mathbf x}\)</span> with <span class="math inline">\((q-1)\)</span> d.f. and covariance matrix <span class="math inline">\(\mathbf C\mathbf S\mathbf C^T\)</span> with <span class="math inline">\((n-q+1)\)</span> d.f., An <span class="math inline">\(\alpha\)</span>-level test of <span class="math inline">\(H_0:\mathbf C\boldsymbol\mu=\mathbf 0\)</span> versus <span class="math inline">\(H_1:\mathbf C\boldsymbol\mu\ne\mathbf 0\)</span> can use <span class="math inline">\(T^2\)</span>-statistic. Reject <span class="math inline">\(H_0\)</span> if: <span class="math inline">\(T^2=n(\mathbf C\overline{\mathbf x})^T(\mathbf C\mathbf S\mathbf C^T)^{-1}(\mathbf C\overline{\mathbf x})&gt;\displaystyle\frac{(n-1)(q-1)}{n-q+1}F_{q-1,n-q+1}(\alpha)\)</span> where <span class="math inline">\(\overline{\mathbf x}=\displaystyle\frac{1}{n}\displaystyle\sum_{j=1}^{n}\mathbf x_j\)</span> and <span class="math inline">\(\mathbf S=\displaystyle\frac{1}{n-1}\displaystyle\sum_{j=1}^{n}(\mathbf x_j-\overline{\mathbf x})(\mathbf x_j-\overline{\mathbf x})^T\)</span>
A <span class="math inline">\(100(1-\alpha)\%\)</span> <strong>confidence region</strong> for contrasts <span class="math inline">\(\mathbf C\boldsymbol\mu\)</span> is determined by the set of all <span class="math inline">\(\mathbf C\boldsymbol\mu\)</span> such that:<span class="math inline">\(n(\mathbf C\overline{\mathbf x}-\mathbf C\boldsymbol\mu)^T(\mathbf C\mathbf S\mathbf C^T)^{-1}(\mathbf C\overline{\mathbf x}-\mathbf C\boldsymbol\mu)\le\displaystyle\frac{(n-1)(q-1)}{n-q+1}F_{q-1,n-q+1}(\alpha)\)</span> and <span class="math inline">\(100(1-\alpha)\%\)</span> <strong>simultaneous confidence intervals</strong> for the individual contrasts <span class="math inline">\(\mathbf c^T\boldsymbol\mu\)</span> are given by <span class="math inline">\(\Biggl(\mathbf c^T\overline{\mathbf x}-\sqrt{\frac{(n-1)(q-1)}{n-q+1}F_{q-1,n-q+1}(\alpha)}\sqrt{\frac{\mathbf c^T\mathbf S\mathbf c}{n}},\quad \mathbf c^T\overline{\mathbf x}+\sqrt{\frac{(n-1)(q-1)}{n-q+1}F_{q-1,n-q+1}(\alpha)}\sqrt{\frac{\mathbf c^T\mathbf S\mathbf c}{n}}\Biggr)\)</span></p></li>
<li><p><span class="math inline">\(p\)</span>-variate two independent population mean vectors comparison:<br />
population <span class="math inline">\(1\)</span>: <span class="math inline">\(\mathbf X_{11},\mathbf X_{12},\cdots,\mathbf X_{1n_1}\)</span> with size <span class="math inline">\(n1\)</span>, mean vector <span class="math inline">\(\boldsymbol\mu_1\)</span> and covariance matrix <span class="math inline">\(\mathbf\Sigma_1\)</span>;<br />
population <span class="math inline">\(2\)</span>: <span class="math inline">\(\mathbf X_{21},\mathbf X_{22},\cdots,\mathbf X_{2n_2}\)</span> with size <span class="math inline">\(n2\)</span>, mean vector <span class="math inline">\(\boldsymbol\mu_2\)</span> and covariance matrix <span class="math inline">\(\mathbf\Sigma_2\)</span></p></li>
<li><p>When <span class="math inline">\(\mathbf\Sigma_1=\mathbf\Sigma_2=\mathbf\Sigma\)</span>, and both populations are multivariate normal, <span class="math inline">\(\displaystyle\sum_{j=1}^{n_1}(\mathbf X_{1j}-\overline{\mathbf X}_{1})(\mathbf X_{1j}-\overline{\mathbf X}_{1})^T\)</span> is an estimate of <span class="math inline">\((n_1-1)\mathbf\Sigma\)</span> and <span class="math inline">\(\displaystyle\sum_{j=1}^{n_2}(\mathbf X_{2j}-\overline{\mathbf X}_{2})(\mathbf X_{2j}-\overline{\mathbf X}_{2})^T\)</span> is an estimate of <span class="math inline">\((n_2-1)\mathbf\Sigma\)</span>
The common covariance <span class="math inline">\(\mathbf\Sigma\)</span> can be estimated using both samples <span class="math display">\[\begin{align}
\mathbf S_{pooled}&amp;=\displaystyle\frac{\displaystyle\sum_{j=1}^{n_1}(\mathbf X_{1j}-\overline{\mathbf X}_{1})(\mathbf X_{1j}-\overline{\mathbf X}_{1})^T+\displaystyle\sum_{j=1}^{n_2}(\mathbf X_{2j}-\overline{\mathbf X}_{2})(\mathbf X_{2j}-\overline{\mathbf X}_{2})^T}{n_1+n_2-2}\\
&amp;=\displaystyle\frac{(n_1-1)\mathbf S_1+(n_2-1)\mathbf S_2}{n_1+n_2-2}\\
&amp;=\displaystyle\frac{\mathbf W_{n_1-1}(\mathbf\Sigma)+\mathbf W_{n_2-1}(\mathbf\Sigma)}{n_1+n_2-2}\\
&amp;=\displaystyle\frac{\mathbf W_{n_1+n_2-2}(\mathbf\Sigma)}{n_1+n_2-2}
\end{align}\]</span>Because <span class="math inline">\(E(\overline{\mathbf X}_{1}-\overline{\mathbf X}_{2})=E(\overline{\mathbf X}_{1})-E(\overline{\mathbf X}_{2})=\boldsymbol\mu_1-\boldsymbol\mu_2\)</span> and <span class="math inline">\(Cov(\overline{\mathbf X}_{1}-\overline{\mathbf X}_{2})=Cov(\overline{\mathbf X}_{1})+Cov(\overline{\mathbf X}_{2})=(\frac{1}{n_1}+\frac{1}{n_2})\mathbf\Sigma\)</span> and <span class="math inline">\(\mathbf S_{pooled}\)</span> is an estimate of <span class="math inline">\(\mathbf\Sigma\)</span>, <span class="math inline">\(T^2\)</span> statistical <span class="math display">\[\begin{align}
T^2&amp;=\Bigl[(\overline{\mathbf X}_{1}-\overline{\mathbf X}_{2})-(\boldsymbol\mu_1-\boldsymbol\mu_2)\Bigr]^T\Bigl[(\frac{1}{n_1}+\frac{1}{n_2})\mathbf S_{pooled}\Bigr]^{-1}\Bigl[(\overline{\mathbf X}_{1}-\overline{\mathbf X}_{2})-(\boldsymbol\mu_1-\boldsymbol\mu_2)\Bigr]\\
&amp;=\Bigl(\frac{1}{n_1}+\frac{1}{n_2}\Bigr)^{-1/2}\Bigl[(\overline{\mathbf X}_{1}-\overline{\mathbf X}_{2})-(\boldsymbol\mu_1-\boldsymbol\mu_2)\Bigr]^T\mathbf S_{pooled}^{-1}\Bigl(\frac{1}{n_1}+\frac{1}{n_2}\Bigr)^{-1/2}\Bigl[(\overline{\mathbf X}_{1}-\overline{\mathbf X}_{2})-(\boldsymbol\mu_1-\boldsymbol\mu_2)\Bigr]\\
&amp;=\Bigl(\text{multivariate normal vector}\Bigr)^T\Bigl(\frac{\text{Wishart random matrix}}{d.f.}\Bigr)^{-1}\Bigl(\text{multivariate normal vector}\Bigr)\\
&amp;=N_p(\mathbf 0, \mathbf\Sigma)^T\Bigl(\frac{\mathbf W_{n_1+n_2-2}(\mathbf\Sigma)}{n_1+n_2-2}\Bigr)^{-1}N_p(\mathbf 0, \mathbf\Sigma)
\end{align}\]</span>is distributed as<span class="math inline">\(\displaystyle\frac{(n_1+n_2-2)p}{n_1+n_2-p-1}F_{p,n_1+n_2-p-1}\)</span> and <span class="math inline">\(P\Biggl[\Bigl[(\overline{\mathbf X}_{1}-\overline{\mathbf X}_{2})-(\boldsymbol\mu_1-\boldsymbol\mu_2)\Bigr]^T\Bigl[(\frac{1}{n_1}+\frac{1}{n_2})\mathbf S_{pooled}\Bigr]^{-1}\Bigl[(\overline{\mathbf X}_{1}-\overline{\mathbf X}_{2})-(\boldsymbol\mu_1-\boldsymbol\mu_2)\Bigr]\le\displaystyle\frac{(n_1+n_2-2)p}{n_1+n_2-p-1}F_{p,n_1+n_2-p-1}(\alpha)\Biggl]=1-\alpha\)</span></p></li>
<li><p>The contrasts of sample means <span class="math inline">\(\mathbf a^T(\overline{\mathbf X}_{1}-\overline{\mathbf X}_{2})\)</span> has <span class="math inline">\(E(\mathbf a^T(\overline{\mathbf X}_{1}-\overline{\mathbf X}_{2}))=\mathbf a^T(\boldsymbol\mu_1-\boldsymbol\mu_2)\)</span> and <span class="math inline">\(Cov(\mathbf a^T(\overline{\mathbf X}_{1}-\overline{\mathbf X}_{2}))=\mathbf a^T(\frac{n_1-1}{n_1+n_2-2}\mathbf S_1+\frac{n_2-1}{n_1+n_2-2}\mathbf S_2)\mathbf a=\mathbf a^T\mathbf S_{pooled}\mathbf a\)</span> and then <span class="math inline">\(t^2=\frac{\Bigl[\mathbf a^T(\overline{\mathbf X}_{1}-\overline{\mathbf X}_{2})-\mathbf a^T(\boldsymbol\mu_1-\boldsymbol\mu_2)\Bigr]^2}{\Bigl(\displaystyle\frac{1}{n_1}+\displaystyle\frac{1}{n_2}\Bigr)\mathbf a^T\mathbf S_{pooled}\mathbf a}=\frac{\Bigl[\mathbf a^T(\overline{\mathbf X}_{1}-\overline{\mathbf X}_{2}-(\boldsymbol\mu_1-\boldsymbol\mu_2)\Bigr]^2}{\Bigl(\displaystyle\frac{1}{n_1}+\displaystyle\frac{1}{n_2}\Bigr)\mathbf a^T\mathbf S_{pooled}\mathbf a}\le\Bigl[(\overline{\mathbf X}_{1}-\overline{\mathbf X}_{2})-(\boldsymbol\mu_1-\boldsymbol\mu_2)\Bigr]^T\Bigl[(\frac{1}{n_1}+\frac{1}{n_2})\mathbf S_{pooled}\Bigr]^{-1}\Bigl[(\overline{\mathbf X}_{1}-\overline{\mathbf X}_{2})-(\boldsymbol\mu_1-\boldsymbol\mu_2)\Bigr]=T^2\)</span> So, <span class="math inline">\((1-\alpha)=P\Bigl[T^2\le\displaystyle\frac{(n_1+n_2-2)p}{n_1+n_2-p-1}F_{p,n_1+n_2-p-1}(\alpha)\Bigr]\)</span> and the <span class="math inline">\(100(1-\alpha)\%\)</span> <strong>simultaneous confidence intervals</strong> for the contrasts <span class="math inline">\(\mathbf a^T(\boldsymbol\mu_1-\boldsymbol\mu_2)\)</span> are given by <span class="math inline">\(\Biggl(\mathbf a^T(\overline{\mathbf X}_{1}-\overline{\mathbf X}_{2})\pm\sqrt{\displaystyle\frac{(n_1+n_2-2)p}{n_1+n_2-p-1}F_{p,n_1+n_2-p-1}(\alpha)}\sqrt{\mathbf a^T\Bigl(\frac{1}{n_1}+\frac{1}{n_2}\Bigr)\mathbf S_{pooled}\mathbf a}\Biggr)\)</span> and the <strong>Bonferroni <span class="math inline">\(100(1-\alpha)\%\)</span> simultaneous confidence intervals</strong> for the individual mean differences are <span class="math inline">\(\mu_{1i}-\mu_{2i}:(\overline{x}_{1i}-\overline{x}_{2i})\pm t_{n_1+n_2-2}(\frac{\alpha}{2p})\sqrt{(\frac{1}{n_1}+\frac{1}{n_2})s_{ii,pooled}}\)</span></p></li>
<li><p>One-way <span style="color: red;"><strong>Multivariate Analysis of Variance (MANOVA)</strong></span>:<br />
In randomized one-way design experiments for multivariate <span class="math inline">\(\mathbf x_p\)</span>, there are <span class="math inline">\(k\)</span> treatments and <span class="math inline">\(n_i\quad (i=1,2,3,\cdots,k)\)</span> samples for each treatment, for treatment <span class="math inline">\(i\)</span> the samples are <span class="math inline">\(\mathbf x_{i1},\mathbf x_{i2},\cdots,\mathbf x_{ij},\cdots,\mathbf x_{in_i}\quad (j=1,2,3,\cdots,n_i)\)</span>. We assume that 1) The random samples from different populations are independent, 2) All populations have a common covariance matrix <span class="math inline">\(\mathbf\Sigma\)</span>, 3) Each population is multivariate normal <span class="math inline">\(N_p(\boldsymbol\mu_i, \mathbf\Sigma)\)</span>. <span class="math inline">\(\underset{i^{th}\text{ population}\\\ \text{mean}}{\boldsymbol\mu_i}=\underset{\text{overall}\\\ \text{mean}}{\boldsymbol\mu}+\underset{i^{th}\text{ treatment}\\\ \text{effect}}{\boldsymbol\tau_i}\)</span> and <span class="math inline">\(\mathbf x_{ij}=\underset{\text{overall}\\\ \text{mean}}{\boldsymbol\mu}+\underset{i^{th}\text{ treatment}\\\ \text{effect}}{\boldsymbol\tau_i}+\underset{\text{random}\\\ \text{error}}{\mathbf e_{ij}}\)</span> and
<span class="math inline">\(\displaystyle\sum_{i=1}^{k}n_i\boldsymbol\tau_i=\displaystyle\sum_{i=1}^{k}n_i(\boldsymbol\mu_i-\boldsymbol\mu)=\displaystyle\sum_{i=1}^{k}n_i\boldsymbol\mu_i-\displaystyle\sum_{i=1}^{k}n_i\boldsymbol\mu=0\)</span> the analysis of variance is based upon an analogous decomposition of the observations, <span class="math inline">\(\underset{\text{observation}}{\mathbf x_{ij}}=\underset{\text{overall mean}}{\overline{\mathbf x}}+\underset{\text{ith treatment effect}}{(\overline{\mathbf x}_i-\overline{\mathbf x})}+\underset{\text{error}}{(\mathbf x_{ij}-\overline{\mathbf x}_i)}\)</span> Then <span class="math inline">\(\mathbf x_{ij}-\overline{\mathbf x}=(\overline{\mathbf x}_i-\overline{\mathbf x})+(\mathbf x_{ij}-\overline{\mathbf x}_i)\)</span>, square both side we have
<span class="math inline">\((\mathbf x_{ij}-\overline{\mathbf x})(\mathbf x_{ij}-\overline{\mathbf x})^T=(\overline{\mathbf x}_i-\overline{\mathbf x})(\overline{\mathbf x}_i-\overline{\mathbf x})^T+2(\overline{\mathbf x}_i-\overline{\mathbf x})(\mathbf x_{ij}-\overline{\mathbf x}_i)+(\mathbf x_{ij}-\overline{\mathbf x}_i)(\mathbf x_{ij}-\overline{\mathbf x}_i)^T\)</span> sum both side over <span class="math inline">\(j\)</span> we have</p></li>
</ul>
<p><span class="math display">\[\begin{align}
\displaystyle\sum_{j=1}^{n_i}(\mathbf x_{ij}-\overline{\mathbf x})(\mathbf x_{ij}-\overline{\mathbf x})^T&amp;=\displaystyle\sum_{j=1}^{n_i}(\overline{\mathbf x}_i-\overline{\mathbf x})(\overline{\mathbf x}_i-\overline{\mathbf x})^T+2\displaystyle\sum_{j=1}^{n_i}(\overline{\mathbf x}_i-\overline{\mathbf x})(\mathbf x_{ij}-\overline{\mathbf x}_i)+\displaystyle\sum_{j=1}^{n_i}(\mathbf x_{ij}-\overline{\mathbf x}_i)(\mathbf x_{ij}-\overline{\mathbf x}_i)^T\\
&amp;=\displaystyle\sum_{j=1}^{n_i}(\overline{\mathbf x}_i-\overline{\mathbf x})(\overline{\mathbf x}_i-\overline{\mathbf x})^T+\displaystyle\sum_{j=1}^{n_i}(\mathbf x_{ij}-\overline{\mathbf x}_i)(\mathbf x_{ij}-\overline{\mathbf x}_i)^T\\
&amp;=n_i(\overline{\mathbf x}_i-\overline{\mathbf x})(\overline{\mathbf x}_i-\overline{\mathbf x})^T+\displaystyle\sum_{j=1}^{n_i}(\mathbf x_{ij}-\overline{\mathbf x}_i)(\mathbf x_{ij}-\overline{\mathbf x}_i)^T
\end{align}\]</span> Next, summing both sides over <span class="math inline">\(i\)</span> we have</p>
<p><span class="math display">\[\begin{align}
\underset{SSTOT}{\displaystyle\sum_{i=1}^{k}\displaystyle\sum_{j=1}^{n_i}(\mathbf x_{ij}-\overline{\mathbf x})(\mathbf x_{ij}-\overline{\mathbf x})^T}=\underset{SSTR}{\displaystyle\sum_{i=1}^{k}n_i(\overline{\mathbf x}_i-\overline{\mathbf x})(\overline{\mathbf x}_i-\overline{\mathbf x})^T}+\underset{SSE}{\displaystyle\sum_{i=1}^{k}\displaystyle\sum_{j=1}^{n_i}(\mathbf x_{ij}-\overline{\mathbf x}_i)(\mathbf x_{ij}-\overline{\mathbf x}_i)^T}
\end{align}\]</span> and also <span class="math inline">\(SSE=\displaystyle\sum_{i=1}^{k}\displaystyle\sum_{j=1}^{n_i}(\mathbf x_{ij}-\overline{\mathbf x}_i)(\mathbf x_{ij}-\overline{\mathbf x}_i)^T=\sum_{i=1}^{k}(n_i-1)\mathbf S_i\)</span> the MANOVA table is
<span class="math display">\[\begin{array}{lccccc}
\hline
Source &amp; d.f. &amp; SS &amp; MS \\
\hline
Treatment &amp; k-1 &amp; SSTR &amp; MSTR  \\
Error &amp; n-k &amp; SSE &amp; MSE &amp;  \\
Total &amp; n-1 &amp; SSTOT\\
\hline
\end{array}
\]</span> with <span class="math inline">\(n=\displaystyle\sum_{i=1}^{k}n_i\)</span> we rejects <span class="math inline">\(H0:\boldsymbol\tau_1 = \boldsymbol\tau_2 = \cdots = \boldsymbol\tau_k\)</span> if the ratio of generalized variances:
<span class="math inline">\(\Lambda^{*}=\frac{|SSE|}{|SSTOT|}\frac{\Biggl|\displaystyle\sum_{i=1}^{k}\displaystyle\sum_{j=1}^{n_i}(\mathbf x_{ij}-\overline{\mathbf x}_i)(\mathbf x_{ij}-\overline{\mathbf x}_i)^T\Biggr|}{\Biggl|\displaystyle\sum_{i=1}^{k}\displaystyle\sum_{j=1}^{n_i}(\mathbf x_{ij}-\overline{\mathbf x})(\mathbf x_{ij}-\overline{\mathbf x})^T\Biggr|}\)</span> is too small and <span class="math inline">\(\Lambda^{*}\)</span> is called <strong>Wilksâ€™ lambda</strong>. If <span class="math inline">\(p=2\)</span> and <span class="math inline">\(k\ge2\)</span> <span class="math inline">\(\Bigl(\displaystyle\frac{n-k-1}{k-1}\Bigr)\Bigl(\displaystyle\frac{1-\sqrt{\Lambda^*}}{\sqrt{\Lambda^*}}\Bigr)\approx F_{2(k-1),2(n-k-1)}\)</span></p>
<ul>
<li><p><strong>Bonferroni simultaneous confidence intervals</strong> for treatment effects: for the <span class="math inline">\(s^{th}\)</span> component of <span class="math inline">\(\boldsymbol\tau_{i}\)</span>, <span class="math inline">\(\tau_{is}, i=1,2,\cdots,k; s=1,2,\cdots,p\)</span>, because <span class="math inline">\(\boldsymbol\tau_{i}\)</span> is estimated by <span class="math inline">\(\hat{\boldsymbol\tau}_{i}=\overline{\mathbf x}_i-\overline{\mathbf x}\)</span> so <span class="math inline">\(\hat{\tau}_{is}=\overline{x}_{is}-\overline{x}_s\)</span> and the estimate of difference of the <span class="math inline">\(s^{th}\)</span> component between two independent samples is the difference of sample means <span class="math inline">\(\hat{\tau}_{as}-\hat{\tau}_{bs}=\overline{x}_{as}-\overline{x}_{bs}\)</span>. The variance is <span class="math inline">\(Var(\hat{\tau}_{as}-\hat{\tau}_{bs})=Var(\overline{x}_{as}-\overline{x}_{bs})=(\frac{1}{n_a}+\frac{1}{n_b})\sigma_{ss}\)</span> and the variance is estimated by dividing the corresponding element of matrix <span class="math inline">\(SSE=\displaystyle\sum_{i=1}^{k}\displaystyle\sum_{j=1}^{n_i}(\mathbf x_{ij}-\overline{\mathbf x}_i)(\mathbf x_{ij}-\overline{\mathbf x}_i)^T\)</span> by its degrees of freedom <span class="math inline">\(\widehat{Var}(\overline{x}_{as}-\overline{x}_{bs})=(\frac{1}{n_a}+\frac{1}{n_b})\frac{w_{ss}}{n-k}\)</span> where <span class="math inline">\(w_{ss}\)</span> is the <span class="math inline">\(s^{th}\)</span> diagonal element of <span class="math inline">\(SSE\)</span>. There are <span class="math inline">\(p\)</span> variables and <span class="math inline">\(\binom{k}{2}=\frac{k(k-1)}{2}\)</span> pairwise differences, so simultaneous confidence intervals will employ the critical value <span class="math inline">\(t_{n-k}(\frac{\alpha}{2m})=t_{n-k}(\frac{\alpha}{2pk(k-1)/2})=t_{n-k}(\frac{\alpha}{pk(k-1)})\)</span> and at <span class="math inline">\(100(1-\alpha)\%\)</span> confidence level, the difference of the <span class="math inline">\(s^{th}\)</span> component between two-sample <span class="math inline">\(\tau_{as}-\tau_{bs}\)</span> belongs to <span class="math inline">\(\overline{x}_{as}-\overline{x}_{bs}\pm t_{n-k}(\frac{\alpha}{pk(k-1)})\sqrt{\frac{w_{ss}}{n-k}(\frac{1}{n_a}+\frac{1}{n_b})}\)</span></p></li>
<li><p>Univariate Two-way Fixed-Effects Analysis of Variance:<br />
Two sets of experimental conditions factor 1 (with <span class="math inline">\(g\)</span> levels) and factor 2 (with <span class="math inline">\(k\)</span> levels), respectively. There are <span class="math inline">\(n\)</span> independent observations for each factor 1 <span class="math inline">\(\times\)</span> factor 2 <span class="math inline">\(=gk\)</span> combinations. Denoting the <span class="math inline">\(r^{th}, r=1,2,\cdots,n\)</span> observation at level <span class="math inline">\(i, i=1,2,\cdots,g\)</span> of factor 1 and level <span class="math inline">\(j, j=1,2,\cdots,k\)</span> of factor 2, as <span class="math inline">\(x_{ijr}=\mu+\tau_i+\beta_j+\gamma_{ij}+e_{ijr}\)</span>, where <span class="math inline">\(\displaystyle\sum_{i=1}^{g}\tau_i=\displaystyle\sum_{j=1}^{k}\beta_j=\displaystyle\sum_{i=1}^{g}\gamma_{ij}=\displaystyle\sum_{j=1}^{k}\gamma_{ij}=0\)</span> and <span class="math inline">\(e_{ijr}\)</span> is <span class="math inline">\(N(0,\sigma^2)\)</span> random variables, <span class="math inline">\(\gamma_{ij}\)</span> is the interaction effect between factor 1 and factor 2. <span class="math inline">\(x_{ijr}-\overline x=(\overline x_{i.}-\overline x)+(\overline x_{.j}-\overline x)+(\overline x_{ij}-\overline x_{i.}-\overline x_{.j}+\overline x)+(x_{ijr}-\overline x_{ij})\)</span> Squaring and summing the equation: <span class="math display">\[\displaystyle\sum_{i=1}^{g}\displaystyle\sum_{j=1}^{k}\displaystyle\sum_{r=1}^{n}(x_{ijr}-\overline x)^2=kn\displaystyle\sum_{i=1}^{g}(\overline x_{i.}-\overline x)^2+gn\displaystyle\sum_{j=1}^{k}(\overline x_{.j}-\overline x)^2\\
+n\displaystyle\sum_{i=1}^{g}\displaystyle\sum_{j=1}^{k}(\overline x_{ij}-\overline x_{i.}-\overline x_{.j}+\overline x)^2+\displaystyle\sum_{i=1}^{g}\displaystyle\sum_{j=1}^{k}\displaystyle\sum_{r=1}^{n}(x_{ijr}-\overline x_{ij})^2\]</span> or <span class="math inline">\(SSTOT=SSTR_{f1}+SSTR_{f2}+SSTR_{f1\times f2}+SSE\)</span> The corresponding degrees of freedom are <span class="math inline">\(gkn-1=(g-1)+(k-1)+(g-1)(k-1)+gk(n-1)\)</span></p></li>
<li><p>Two-Way <strong>Multivariate Analysis of Variance (MANOVA)</strong>:<br />
Each variable is a vector consisting of <span class="math inline">\(p\)</span> components <span class="math inline">\(\mathbf x_{ijr}=\boldsymbol\mu+\boldsymbol\tau_i+\boldsymbol\beta_j+\boldsymbol\gamma_{ij}+\boldsymbol e_{ijr}\)</span> and <span class="math inline">\(\displaystyle\sum_{i=1}^{g}\boldsymbol\tau_i=\displaystyle\sum_{j=1}^{k}\boldsymbol\beta_j=\displaystyle\sum_{i=1}^{g}\boldsymbol\gamma_{ij}=\displaystyle\sum_{j=1}^{k}\boldsymbol\gamma_{ij}=0\)</span> The vectors are all <span class="math inline">\(p\times1\)</span> vectors and <span class="math inline">\(\boldsymbol e_{ijr}\)</span> are independent <span class="math inline">\(N_p(\boldsymbol0,\boldsymbol\Sigma)\)</span> random vectors. The responses consist of <span class="math inline">\(p\)</span> components replicated <span class="math inline">\(n\)</span> times at each of the possible combinations of levels of factors <span class="math inline">\(1\)</span> and <span class="math inline">\(2\)</span>. <span class="math display">\[\displaystyle\sum_{i=1}^{g}\displaystyle\sum_{j=1}^{k}\displaystyle\sum_{r=1}^{n}(\mathbf x_{ijr}-\overline{\mathbf x})(\mathbf x_{ijr}-\overline {\mathbf x})^T=kn\displaystyle\sum_{i=1}^{g}(\overline{\mathbf x}_{i.}-\overline{\mathbf x})(\overline{\mathbf x}_{i.}-\overline{\mathbf x})^T+gn\displaystyle\sum_{j=1}^{k}(\overline{\mathbf x}_{.j}-\overline{\mathbf x})(\overline{\mathbf x}_{.j}-\overline{\mathbf x})^T\\
+n\displaystyle\sum_{i=1}^{g}\displaystyle\sum_{j=1}^{k}(\overline{\mathbf x}_{ij}-\overline{\mathbf x}_{i.}-\overline{\mathbf x}_{.j}+\overline{\mathbf x})(\overline{\mathbf x}_{ij}-\overline{\mathbf x}_{i.}-\overline{\mathbf x}_{.j}+\overline{\mathbf x})^T+\displaystyle\sum_{i=1}^{g}\displaystyle\sum_{j=1}^{k}\displaystyle\sum_{r=1}^{n}(\mathbf x_{ijr}-\overline{\mathbf x}_{ij})(\mathbf x_{ijr}-\overline{\mathbf x}_{ij})^T\]</span> or <span class="math inline">\(SSTOT=SSTR_{f1}+SSTR_{f2}+SSTR_{f1\times f2}+SSE\)</span>. The corresponding degrees of freedom are <span class="math inline">\(gkn-1=(g-1)+(k-1)+(g-1)(k-1)+gk(n-1)\)</span>.</p></li>
</ul>
<ol style="list-style-type: decimal">
<li>A test of the interact effects between factor 1 and factor 2 <span class="math inline">\(H_0:\boldsymbol\gamma_{ij}=\boldsymbol0,\text{ for all i and j}\)</span> versus <span class="math inline">\(H_0:\boldsymbol\gamma_{ij}\ne\boldsymbol0,\text{ At least one i and j}\)</span>, rejecting <span class="math inline">\(H_0\)</span> for small values of the ratio <span class="math inline">\(\Lambda^*=\displaystyle\frac{|SSE|}{|SSTR_{f1\times f2}+SSE|}\)</span> For large samples, <span class="math inline">\(-\Bigl[gk(n-1)-\frac{(p+1)-(g-1)(k-1)}{2}\Bigr]ln\Lambda^*\)</span> is approximate a chi-square <span class="math inline">\(\chi_{(g-1)(k-1)}^2\)</span>, Reject <span class="math inline">\(H_0\)</span> at the <span class="math inline">\(\alpha\)</span> level if <span class="math inline">\(-\Bigl[gk(n-1)-\frac{(p+1)-(g-1)(k-1)}{2}\Bigr]ln\Lambda^*&gt;\chi_{(g-1)(k-1)}^2(\alpha)\)</span></li>
<li>A test of the main effects of factor 1 <span class="math inline">\(H_0:\boldsymbol\tau_{i}=\boldsymbol0,\text{ for all i}\)</span> versus <span class="math inline">\(H_0:\boldsymbol\tau_{i}\ne\boldsymbol0,\text{ At least one i}\)</span>, rejecting <span class="math inline">\(H_0\)</span> for small values of the ratio <span class="math inline">\(\Lambda^*=\displaystyle\frac{|SSE|}{|SSTR_{f1}+SSE|}\)</span> For large samples, <span class="math inline">\(-\Bigl[gk(n-1)-\frac{(p+1)-(g-1)}{2}\Bigr]ln\Lambda^*\)</span> is approximate a chi-square <span class="math inline">\(\chi_{(g-1)p}^2\)</span>, Reject <span class="math inline">\(H_0\)</span> at the <span class="math inline">\(\alpha\)</span> level if <span class="math inline">\(-\Bigl[gk(n-1)-\frac{(p+1)-(g-1)}{2}\Bigr]ln\Lambda^*&gt;\chi_{(g-1)p}^2(\alpha)\)</span></li>
<li>A test of the main effects of factor 2 <span class="math inline">\(H_0:\boldsymbol\beta_{j}=\boldsymbol0,\text{ for all j}\)</span> versus <span class="math inline">\(H_0:\boldsymbol\beta_{j}\ne\boldsymbol0,\text{ At least one j}\)</span>, rejecting <span class="math inline">\(H_0\)</span> for small values of the ratio <span class="math inline">\(\Lambda^*=\displaystyle\frac{|SSE|}{|SSTR_{f2}+SSE|}\)</span> For large samples, <span class="math inline">\(-\Bigl[gk(n-1)-\frac{(p+1)-(k-1)}{2}\Bigr]ln\Lambda^*\)</span> is approximate a chi-square <span class="math inline">\(\chi_{(k-1)p}^2\)</span>, Reject <span class="math inline">\(H_0\)</span> at the <span class="math inline">\(\alpha\)</span> level if <span class="math inline">\(-\Bigl[gk(n-1)-\frac{(p+1)-(k-1)}{2}\Bigr]ln\Lambda^*&gt;\chi_{(k-1)p}^2(\alpha)\)</span></li>
</ol>
<ul>
<li><strong>Bonferroni simultaneous confidence intervals</strong> for contrasts in the model parameters:<br />
The <span class="math inline">\(100(1-\alpha)\%\)</span> simultaneous confidence intervals of each component of the difference vector <span class="math inline">\(\boldsymbol\tau_{\ell}-\boldsymbol\tau_{m}, \ell, m\in(1,2,\cdots,g)\)</span> is <span class="math inline">\(\tau_{\ell s}-\tau_{ms},s\in\{1,2,\cdots,p\}\)</span> belongs to <span class="math inline">\((\overline x_{\ell. s}-\overline x_{m.s})\pm t_{gk(n-1)}(\frac{\alpha}{pg(g-1)})\sqrt{\frac{SSE_{ss}}{gk(n-1)}\frac{2}{kn}}\)</span> Similarly, The <span class="math inline">\(100(1-\alpha)\%\)</span> simultaneous confidence intervals of each component of the difference vector <span class="math inline">\(\boldsymbol\beta_{c}-\boldsymbol\beta_{d}, c, d\in(1,2,\cdots,k)\)</span> is <span class="math inline">\(\beta_{cs}-\beta_{ds},s\in\{1,2,\cdots,p\}\)</span> belongs to <span class="math inline">\((\overline x_{.cs}-\overline x_{.ds})\pm t_{gk(n-1)}(\frac{\alpha}{pk(k-1)})\sqrt{\frac{SSE_{ss}}{gk(n-1)}\frac{2}{gn}}\)</span> <span class="math inline">\(SSE_{ss}\)</span> is the <span class="math inline">\(s^{th}\)</span> diagonal element of <span class="math inline">\(SSE=\displaystyle\sum_{i=1}^{g}\displaystyle\sum_{j=1}^{k}\displaystyle\sum_{r=1}^{n}(\mathbf x_{ijr}-\overline{\mathbf x}_{ij})(\mathbf x_{ijr}-\overline{\mathbf x}_{ij})^T\)</span></li>
</ul>

    </div>
  </article>

  


</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
          </li>
          <li>
          </li>
        </ul>
      </footer>

    </div>
    

    
<script src="../../../../js/math-code.js"></script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
  </body>
</html>

