<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.75.1" />


<title>Inferences about the mean - A Hugo website</title>
<meta property="og:title" content="Inferences about the mean - A Hugo website">


  <link href='../../../../favicon.ico' rel='icon' type='image/x-icon'/>



  







<link rel="stylesheet" href="../../../../css/fonts.css" media="all">
<link rel="stylesheet" href="../../../../css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="../../../../" class="nav-logo">
    <img src="../../../../images/logo.png"
         width="50"
         height="50"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="../../../../vitae/">CV</a></li>
    
    <li><a href="https://github.com/danli349">GitHub</a></li>
    
    <li><a href="https://scholar.google.com/citations?user=mNjLK8EAAAAJ&amp;hl=en">Googlr Scholar</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">4 min read</span>
    

    <h1 class="article-title">Inferences about the mean</h1>

    
    <span class="article-date">2020-09-25</span>
    

    <div class="article-content">
      
<script src="../../../../rmarkdown-libs/header-attrs/header-attrs.js"></script>


<p>The hypothesis testing about the mean is a test of the competing hypotheses: <span class="math inline">\(H_0:\mu=\mu_0\)</span> and <span class="math inline">\(H_1:\mu\ne\mu_0\)</span>. If <span class="math inline">\(X_1,X_2,\cdots,X_n\)</span> denote a random sample from a normal population, the appropriate test <em>statistic</em> is <span class="math inline">\(t=\frac{(\overline X-\mu_0)}{s/\sqrt{n}}\)</span> with <span class="math inline">\(s^2=\frac{1}{(n-1)}\displaystyle\sum_{i=1}^{n}(X_i-\overline X)^2\)</span>. Rejecting <span class="math inline">\(H_0\)</span> when <span class="math inline">\(|t|\)</span> is large is equivalent to rejecting <span class="math inline">\(H_0\)</span> when <span class="math inline">\(t^2=\frac{(\overline X-\mu_0)^2}{s^2/n}=n(\overline X-\mu_0)(s^2)^{-1}(\overline X-\mu_0)\)</span> is large. Then the test becomes reject <span class="math inline">\(H_0\)</span> in favor of <span class="math inline">\(H_1\)</span> at significance level <span class="math inline">\(\alpha\)</span> if <span class="math inline">\(n(\overline X-\mu_0)(s^2)^{-1}(\overline X-\mu_0)&gt;t_{n-1}^2(\alpha/2)\)</span>, its multivariate analog is <span class="math inline">\(T^2=(\overline {\mathbf X}-\boldsymbol\mu_0)^T(\frac{1}{n}\mathbf S)^{-1}(\overline {\mathbf X}-\boldsymbol\mu_0)=n(\overline {\mathbf X}-\boldsymbol\mu_0)^T\mathbf S^{-1}(\overline {\mathbf X}-\boldsymbol\mu_0)\)</span>, where <span class="math inline">\(\overline {\mathbf X}=\frac{1}{n}\displaystyle\sum_{j=1}^{n}\mathbf X_j\)</span>, <span class="math inline">\(\underset{(p\times p)}{\mathbf S}=\frac{1}{n-1}\displaystyle\sum_{j=1}^{n}(\underset{(p\times 1)}{\mathbf X_j}-\underset{(p\times 1)}{\overline {\mathbf X}})(\underset{(p\times 1)}{\mathbf X_j}-\underset{(p\times 1)}{\overline {\mathbf X}})^T\)</span></p>
<p>Because <span class="math inline">\(n(\overline {\mathbf X}-\boldsymbol\mu)^T\mathbf\Sigma^{-1}(\overline {\mathbf X}-\boldsymbol\mu)\)</span> is distributed as <span class="math inline">\(\chi_p^2\)</span>, <span class="math inline">\(T^2=n(\overline {\mathbf X}-\boldsymbol\mu_0)^T\mathbf S^{-1}(\overline {\mathbf X}-\boldsymbol\mu_0)\)</span> is distributed as <span class="math inline">\(\frac{(n-1)p}{n-p}F_{p,n-p}\)</span></p>
<p><span class="math display">\[\begin{align}
T^2&amp;=\sqrt{n}(\overline {\mathbf X}-\boldsymbol\mu_0)^T\Bigl(\frac{1}{n-1}\displaystyle\sum_{j=1}^{n}(\mathbf X_j-\overline {\mathbf X})(\mathbf X_j-\overline {\mathbf X})^T\Bigr)^{-1}\sqrt{n}(\overline {\mathbf X}-\boldsymbol\mu_0)\\
&amp;=N_p(\mathbf 0,\mathbf\Sigma)^T\Bigl(\frac{1}{n-1}\mathbf W_{p,n-1}(\mathbf\Sigma)\Bigr)^{-1}N_p(\mathbf 0,\mathbf\Sigma)
\end{align}\]</span> the <span class="math inline">\(\mathbf W_{p,n-1}(\mathbf\Sigma)\)</span> is <strong>Wishart</strong> random matrix with <span class="math inline">\((n-1)\)</span> d.f.</p>
<ul>
<li><p>Under the hypothesis <span class="math inline">\(H_0:\boldsymbol\mu=\boldsymbol\mu_0\)</span> the normal likelihood specializes to:<span class="math display">\[L(\boldsymbol\mu_0,\mathbf\Sigma)=\frac{1}{(2\pi)^{\frac{np}{2}}|\mathbf\Sigma|^{n/2}}e^{-\frac{1}{2}\sum_{j=1}^{n}(\mathbf x_j-\boldsymbol\mu_0)^T\mathbf\Sigma^{-1}(\mathbf x_j-\boldsymbol\mu_0)}\]</span> The mean <span class="math inline">\(\boldsymbol\mu_0\)</span> now is fixed but <span class="math inline">\(\mathbf\Sigma\)</span> can be varied to find the value that is most likely to lead to the observed sample. This value is obtained by maximizing <span class="math inline">\(L(\boldsymbol\mu_0,\mathbf\Sigma)\)</span> with respect to <span class="math inline">\(\mathbf\Sigma\)</span>:
<span class="math inline">\(\underset{\mathbf\Sigma}{\text{max }}L(\boldsymbol\mu_0,\mathbf\Sigma)=\frac{1}{(2\pi)^{np/2}}\frac{1}{|\hat{\mathbf\Sigma_0}|^{n/2}}e^{-np/2}\)</span> with <span class="math inline">\(\hat{\mathbf\Sigma_0}=\frac{1}{n}\displaystyle\sum_{j=1}^{n}(\mathbf x_j-\boldsymbol\mu_0)(\mathbf x_j-\boldsymbol\mu_0)^T\)</span>. Then the maximum of <span class="math inline">\(L(\boldsymbol\mu_0,\mathbf\Sigma)\)</span> is compared with the unrestricted maximum of <span class="math inline">\(L(\boldsymbol\mu,\mathbf\Sigma)\)</span> to determine whether <span class="math inline">\(\boldsymbol\mu_0\)</span> is a plausible value of <span class="math inline">\(\boldsymbol\mu\)</span>. The resulting ratio is called the <span style="color: red;"><strong>likelihood ratio statistic</strong></span> and this method is called <span style="color: red;"><strong>likelihood ratio test (LRT)</strong></span>. The Likelihood ratio <span class="math inline">\(\Lambda\)</span> is : <span class="math inline">\(\Lambda=\frac{\underset{\mathbf\Sigma}{\text{max }}L(\boldsymbol\mu_0,\mathbf\Sigma)}{\underset{\boldsymbol\mu,\mathbf\Sigma}{\text{max }}L(\boldsymbol\mu,\mathbf\Sigma)}=\frac{|\hat{\mathbf\Sigma}|^{n/2}}{|\hat{\mathbf\Sigma_0}|^{n/2}}=\Biggl(\frac{\Biggl|\displaystyle\sum_{j=1}^{n}(\mathbf x_j-\overline{\mathbf x})(\mathbf x_j-\overline{\mathbf x})^T\Biggr|}{\Biggl|\displaystyle\sum_{j=1}^{n}(\mathbf x_j-\boldsymbol\mu_0)(\mathbf x_j-\boldsymbol\mu_0)^T\Biggr|}\Biggr)^{n/2}\)</span> The <span class="math inline">\(\Lambda^{\frac{2}{n}}=\frac{|\hat{\mathbf\Sigma}|}{|\hat{\mathbf\Sigma_0}|}\)</span> is called <strong>Wilksâ€™ lambda</strong>.</p></li>
<li><p>Because <span class="math display">\[\begin{align}
\Biggl|\displaystyle\sum_{j=1}^{n}(\mathbf x_j-\boldsymbol\mu_0)(\mathbf x_j-\boldsymbol\mu_0)^T\Biggr|&amp;=\Biggl|\displaystyle\sum_{j=1}^{n}(\mathbf x_j-\overline{\mathbf x})(\mathbf x_j-\overline{\mathbf x})^T+n(\overline{\mathbf x}-\boldsymbol\mu_0)(\overline{\mathbf x}-\boldsymbol\mu_0)^T\Biggr|\\
&amp;=\Biggl|\displaystyle\sum_{j=1}^{n}(\mathbf x_j-\overline{\mathbf x})(\mathbf x_j-\overline{\mathbf x})^T\Biggr|\Biggl|1+n(\overline{\mathbf x}-\boldsymbol\mu_0)\Bigl(\displaystyle\sum_{j=1}^{n}(\mathbf x_j-\overline{\mathbf x})(\mathbf x_j-\overline{\mathbf x})^T\Bigr)^{-1}(\overline{\mathbf x}-\boldsymbol\mu_0)^T\Biggr|\\
&amp;=\Biggl|\displaystyle\sum_{j=1}^{n}(\mathbf x_j-\overline{\mathbf x})(\mathbf x_j-\overline{\mathbf x})^T\Biggr|\Bigl(1+\frac{T^2}{(n-1)}\Bigr)
\end{align}\]</span> which means <span class="math inline">\(\Bigl|n\hat{\mathbf\Sigma_0}\Bigr|=\Bigl|n\hat{\mathbf\Sigma}\Bigr|\Bigl(1+\frac{T^2}{(n-1)}\Bigr)\)</span> then <span class="math inline">\(\Lambda^{\frac{2}{n}}=\frac{|\hat{\mathbf\Sigma}|}{|\hat{\mathbf\Sigma_0}|}=\Bigl(1+\frac{T^2}{(n-1)}\Bigr)^{-1}\)</span> Here <span class="math inline">\(H_0\)</span> is rejected for small values of <span class="math inline">\(\Lambda^{\frac{2}{n}}\)</span> or equivalently, large values of <span class="math inline">\(T^2\)</span>.</p></li>
<li><p>Because <span class="math inline">\(T^2=n(\overline {\mathbf X}-\boldsymbol\mu)^T\mathbf S^{-1}(\overline {\mathbf X}-\boldsymbol\mu)\)</span> is distributed as <span class="math inline">\(\frac{(n-1)p}{n-p}F_{p,n-p}\)</span>, a <span class="math inline">\(100(1-\alpha)\%\)</span> confidence region for the mean of a <span class="math inline">\(p\)</span>-dimensional normal distribution is the ellipsoid determined by all <span class="math inline">\(\boldsymbol\mu\)</span> that <span class="math inline">\(n(\overline {\mathbf X}-\boldsymbol\mu)^T\mathbf S^{-1}(\overline {\mathbf X}-\boldsymbol\mu)\le\frac{(n-1)p}{n-p}F_{p,n-p}(\alpha)\)</span>, and we can compare the generalized squared distance of <span class="math inline">\(\boldsymbol\mu_0\)</span> with <span class="math inline">\(\frac{(n-1)p}{n-p}F_{p,n-p}(\alpha)\)</span>, if <span class="math inline">\(n(\overline {\mathbf X}-\boldsymbol\mu_0)^T\mathbf S^{-1}(\overline {\mathbf X}-\boldsymbol\mu_0)&gt;\frac{(n-1)p}{n-p}F_{p,n-p}(\alpha)\)</span>, <span class="math inline">\(\boldsymbol\mu_0\)</span> is not in the confidence region. If <span class="math inline">\(\lambda_i\)</span> and <span class="math inline">\(\mathbf e_i\)</span> are the eigenvalues and eigenvectors of <span class="math inline">\(\mathbf S\)</span>, <span class="math inline">\(\sqrt{(\overline {\mathbf X}-\boldsymbol\mu)^T(\mathbf S)^{-1}(\overline {\mathbf X}-\boldsymbol\mu)}=\sqrt{\frac{(n-1)p}{n(n-p)}F_{p,n-p}(\alpha)}\)</span>, then the axes of the confidence <span class="math inline">\(\alpha\)</span> ellipsoid beginning at the center <span class="math inline">\(\overline {\mathbf X}\)</span> are <span class="math inline">\(\sqrt{\lambda_i}\mathbf e_i\sqrt{\frac{(n-1)p}{n(n-p)}F_{p,n-p}(\alpha)}\)</span>. The <span class="math inline">\(100(1-\alpha)\%\)</span> simultaneous confidence intervals for the individual component means of a
mean vector are <span class="math inline">\(\Biggl(\overline X_i-\sqrt{\frac{(n-1)p}{(n-p)}F_{p,n-p}(\alpha)}\sqrt{\frac{s_{ii}}{n}},\quad \overline X_i+\sqrt{\frac{(n-1)p}{(n-p)}F_{p,n-p}(\alpha)}\sqrt{\frac{s_{ii}}{n}}\Biggl)\)</span></p></li>
<li><p>The linear combination of the components of <span class="math inline">\(\mathbf X\)</span>, which has <span class="math inline">\(N_p(\boldsymbol\mu, \mathbf\Sigma)\)</span> distribution is <span class="math inline">\(Z_j=a_1X_{j1}+a_2X_{j2}+\cdots+a_pX_{jp}=\mathbf a^T\mathbf X_j \quad j=1,2,\cdots,n\)</span>, with sample mean <span class="math inline">\(\overline z=\mathbf a^T\overline {\mathbf X}\)</span> and <span class="math inline">\(s_z^2=\mathbf a^T\mathbf S\mathbf a\)</span>, then <span class="math inline">\(\displaystyle\frac{\overline z-\mu_z}{s_z/\sqrt{n}}, \quad \mu_z=\mathbf a^T\boldsymbol\mu\)</span> will be distributed as <span class="math inline">\(t\)</span>. So a <span class="math inline">\(100(1-\alpha)\%\)</span> confidence region for the mean of <span class="math inline">\(z\)</span>: <span class="math inline">\(\mu_z=\mathbf a^T\boldsymbol\mu\)</span> is based on studentâ€™s <span class="math inline">\(t\)</span>-ratio <span class="math inline">\(t=\displaystyle\frac{\overline z-\mu_z}{s_z/\sqrt{n}}=\displaystyle\frac{\sqrt{n}(\mathbf a^T\overline {\mathbf X}-\mathbf a^T\boldsymbol\mu)}{\sqrt{\mathbf a^T\mathbf S\mathbf a}}\)</span> and leads to the statement <span class="math inline">\(\mathbf a^T\overline {\mathbf X}-t_{n-1}(\alpha/2)\displaystyle\frac{\sqrt{\mathbf a^T\mathbf S\mathbf a}}{\sqrt{n}}\le\mathbf a^T\boldsymbol\mu\le\mathbf a^T\overline {\mathbf X}+t_{n-1}(\alpha/2)\displaystyle\frac{\sqrt{\mathbf a^T\mathbf S\mathbf a}}{\sqrt{n}}\)</span></p></li>
<li><p>Based on the <strong>Cauchyâ€“Schwarz inequality</strong>, <span class="math inline">\(t^2=\Bigl(\displaystyle\frac{\sqrt{n}(\mathbf a^T\overline {\mathbf X}-\mathbf a^T\boldsymbol\mu)}{\sqrt{\mathbf a^T\mathbf S\mathbf a}}\Bigr)^2=n\frac{(\mathbf a^T(\overline {\mathbf X}-\boldsymbol\mu))^2}{\mathbf a^T\mathbf S\mathbf a}\le n\frac{(\mathbf a^T\mathbf S\mathbf a)(\overline {\mathbf X}-\boldsymbol\mu)^T\mathbf S^{-1}(\overline {\mathbf X}-\boldsymbol\mu)}{\mathbf a^T\mathbf S\mathbf a}=n(\overline {\mathbf X}-\boldsymbol\mu)^T\mathbf S^{-1}(\overline {\mathbf X}-\boldsymbol\mu)=T^2\)</span> then <span class="math inline">\(|\mathbf a^T\overline {\mathbf X}-\mathbf a^T\boldsymbol\mu|\le\sqrt{(\overline {\mathbf X}-\boldsymbol\mu)^T\mathbf S^{-1}(\overline {\mathbf X}-\boldsymbol\mu)\mathbf a^T\mathbf S\mathbf a}\le\sqrt{\frac{(n-1)p}{n(n-p)}F_{p,n-p}(\alpha)\mathbf a^T\mathbf S\mathbf a}\)</span> and <span class="math inline">\(\mathbf a^T\boldsymbol\mu\)</span> is contained in the interval <span class="math inline">\(\mathbf a^T\overline {\mathbf X}-\sqrt{\frac{(n-1)p}{n(n-p)}F_{p,n-p}(\alpha)\mathbf a^T\mathbf S\mathbf a}\le\mathbf a^T\boldsymbol\mu\le\mathbf a^T\overline {\mathbf X}+\sqrt{\frac{(n-1)p}{n(n-p)}F_{p,n-p}(\alpha)\mathbf a^T\mathbf S\mathbf a}\)</span> with probability <span class="math inline">\(1-\alpha\)</span>.</p></li>
<li><p>If the confidence about the value of linear combinations of the mean components <span class="math inline">\(\mathbf a_i^T\boldsymbol\mu, \quad i=1,2,\cdots,m\)</span> is <span class="math inline">\(P[\mathbf a_i^T\boldsymbol\mu\text{ is True}]=1-\alpha_i\)</span>, then <span class="math inline">\(P[\text{all }\mathbf a_i^T\boldsymbol\mu\text{ are True}]=\displaystyle\prod_{i=1}^{m}(1-\alpha_i)\ge1-\displaystyle\sum_{i=1}^{m}\alpha_i\)</span>, if the simultaneous confidence interval about the components <span class="math inline">\(\mu_i\)</span> of <span class="math inline">\(\boldsymbol\mu_i\)</span> is <span class="math inline">\(\alpha\)</span>, and <span class="math inline">\(\alpha_i=\alpha/m\)</span>, then the individual <span class="math inline">\(t\)</span>-intervals will be <span class="math inline">\(\overline X_i\pm t_{n-1}(\frac{\alpha_i}{2})\sqrt{\frac{s_{ii}}{n}}\)</span> then <span class="math inline">\(P[\overline X_i\pm t_{n-1}(\frac{\alpha}{2m})\sqrt{\frac{s_{ii}}{n}}\text{ contains }\mu_i]=\displaystyle\prod_{i=1}^{m}(1-\alpha/m)\ge1-\displaystyle\sum_{i=1}^{m}\alpha/m=1-\alpha\)</span> This method is called <span style="color: red;"><strong>Bonferroni Method</strong></span> of Multiple Comparisons.</p></li>
<li><p>Let <span class="math inline">\(\mathbf X_1, \mathbf X_2, \cdots, \mathbf X_n\)</span> be independently distributed as <span class="math inline">\(N_p(\boldsymbol \mu,\mathbf\Sigma)\)</span>, and <span class="math inline">\(\mathbf X\)</span> is a future observation from the same distribution, then <span class="math inline">\(E(\mathbf X-\overline{\mathbf X})=0\)</span> and <span class="math inline">\(Cov(\mathbf X-\overline{\mathbf X})=Cov(\mathbf X)+Cov(\overline{\mathbf X})=\mathbf\Sigma+\frac{1}{n}\mathbf\Sigma=\frac{n+1}{n}\mathbf\Sigma\)</span>, then <span class="math inline">\(\frac{(\mathbf X-\overline{\mathbf X})}{\sqrt{\frac{n+1}{n}}}\)</span> is distributed as <span class="math inline">\(N_p(\mathbf 0,\mathbf\Sigma)\)</span>, now <span class="math inline">\(\sqrt{\frac{n}{n+1}}(\mathbf X-\overline{\mathbf X})^T\mathbf S^{-1}\sqrt{\frac{n}{n+1}}(\mathbf X-\overline{\mathbf X})=\frac{n}{n+1}(\mathbf X-\overline{\mathbf X})^T\mathbf S^{-1}(\mathbf X-\overline{\mathbf X})\)</span> distributed as <span class="math inline">\(\frac{(n-1)p}{(n-p)}F_{p,n-p}(\alpha)\)</span> Then <span class="math inline">\(P\Bigl[(\mathbf X-\overline{\mathbf X})^T\mathbf S^{-1}(\mathbf X-\overline{\mathbf X})\le\frac{(n^2-1)p}{n(n-p)}F_{p,n-p}(\alpha)\Bigr]=1-\alpha\)</span> which is the control ellipse for future observations.</p></li>
</ul>

    </div>
  </article>

  


</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
          </li>
          <li>
          </li>
        </ul>
      </footer>

    </div>
    

    
<script src="../../../../js/math-code.js"></script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
  </body>
</html>

