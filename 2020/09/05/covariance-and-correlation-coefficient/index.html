<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.74.3" />


<title>covariance and correlation coefficient - A Hugo website</title>
<meta property="og:title" content="covariance and correlation coefficient - A Hugo website">


  <link href='../../../../favicon.ico' rel='icon' type='image/x-icon'/>



  







<link rel="stylesheet" href="../../../../css/fonts.css" media="all">
<link rel="stylesheet" href="../../../../css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="../../../../" class="nav-logo">
    <img src="../../../../images/logo.png"
         width="50"
         height="50"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="../../../../vitae/">CV</a></li>
    
    <li><a href="https://github.com/danli349">GitHub</a></li>
    
    <li><a href="https://scholar.google.com/citations?user=mNjLK8EAAAAJ&amp;hl=en">Googlr Scholar</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">3 min read</span>
    

    <h1 class="article-title">covariance and correlation coefficient</h1>

    
    <span class="article-date">2020-09-05</span>
    

    <div class="article-content">
      
<script src="../../../../rmarkdown-libs/header-attrs/header-attrs.js"></script>


<p>We define the covariance of any two random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, written <span class="math inline">\(Cov(X,Y)\)</span>, as: <span class="math display">\[\begin{align}
Cov(X,Y) &amp;= E(X-\mu_X)(Y-\mu_Y)\\
&amp;= E(XY-X\mu_Y-Y\mu_X+\mu_X\mu_Y)\\
&amp;= E(XY)-\mu_X\mu_Y-\mu_X\mu_Y+\mu_X\mu_Y\\
&amp;= E(XY) - \mu_X\mu_Y\\
&amp;= E(XY) − E(X)E(Y)\\
\end{align}\]</span>.
If <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent random variables,
<span class="math display">\[\begin{align}
E(XY)&amp;=\int\int xy\cdot f_{X,Y}(x,y)dxdy\\
&amp;=\int\int xy\cdot f_X(x)f_Y(y)dxdy\\
&amp;=\int x\cdot f_X(x)dx\int y\cdot f_Y(y)dy\\
&amp;=E(X)E(Y)
\end{align}\]</span>, then <span class="math inline">\(Cov(X,Y) = E(XY) − E(X)E(Y)=0\)</span></p>
<p>The Variance of the sum of two random variables <span class="math inline">\(aX + bY\)</span> is:
<span class="math display">\[\begin{align}
Var(aX + bY) &amp;= E(aX + bY)^2-(E(aX + bY))^2\\
&amp;=E(aX + bY)^2-(a\mu_X+b\mu_Y)^2\\
&amp;=E(a^2X^2+2aXbY+b^2Y^2)-a^2\mu_X^2-2a\mu_Xb\mu_Y-b^2\mu_Y^2\\
&amp;=a^2(E(X^2)-\mu_X^2)+b^2(E(Y^2)-\mu_Y^2)+2ab(E(XY)-\mu_X\mu_Y)\\
&amp;=a^2Var(X)+b^2Var(Y)+2abCov(X,Y)
\end{align}\]</span>.<br />
Then the Variance of the sum of <span class="math inline">\(n\)</span> random variables <span class="math inline">\(W_1,W_2, . . . ,W_n\)</span> is:
<span class="math display">\[Var(\sum_{i=1}^{n}a_iW_i)=\sum_{i=1}^{n}a_i^2Var(W_i)+2\sum_{i&lt;j}a_ia_jCov(W_i,W_j)\]</span></p>
<p>The correlation coefficient of any two random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, is denoted with <span class="math inline">\(\rho(X,Y)\)</span>, and given by: <span class="math inline">\(\rho(X,Y)=\frac{Cov(X,Y)}{\sigma_X\sigma_Y}=Cov(\frac{X-\mu_X}{\sigma_X}, \frac{Y-\mu_Y}{\sigma_Y})\)</span></p>
<p>For any two random variables <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>, if we define the <span class="math inline">\(Cov(X_1,X_2)\)</span> as <span class="math inline">\(\sigma_{12}\)</span>, then
<span class="math display">\[\begin{align}
Cov(aX_1 + bX_2)&amp;=E(aX_1-a\mu_1)(bX_2-b\mu_2)\\
&amp;=abE(X_1-\mu_1)(X_2-\mu_2)\\
&amp;=abCov(X_1,X_2)\\
&amp;=ab\sigma_{12}
\end{align}\]</span> And <span class="math display">\[Var(aX_1 + bX_2)=a^2\sigma_{11}+b^2\sigma_{22}+2ab\sigma_{12}\]</span></p>
<p>We can use vector and matrix to denote <span class="math inline">\(aX_1 + bX_2\)</span> as:
<span class="math display">\[\mathbf c^T\mathbf X=\begin{bmatrix}
a&amp;b
\end{bmatrix}\begin{bmatrix}
X_1\\
X_2
\end{bmatrix}\]</span> and denote the variance-covariance matrix as <span class="math display">\[\mathbf \Sigma=\begin{bmatrix}
\sigma_{11}&amp;\sigma_{12}\\
\sigma_{12}&amp;\sigma_{22}
\end{bmatrix}\]</span>, then <span class="math display">\[\begin{align}
Var(aX_1 + bX_2)&amp;=Var(\mathbf c^T\mathbf X)\\
&amp;=a^2\sigma_{11}+b^2\sigma_{22}+2ab\sigma_{12}\\
&amp;=\mathbf c^T\mathbf \Sigma\mathbf c
\end{align}\]</span></p>
<p><span class="math inline">\(p\)</span> random variables <span class="math display">\[\mathbf X=\begin{bmatrix}
X_{1}\\
X_{2}\\
\vdots\\
X_{p}\\
\end{bmatrix}\]</span> has the expectation matrix <span class="math display">\[E(\mathbf X)=\boldsymbol\mu_{\mathbf X}=\begin{bmatrix}
\mu_{1}\\
\mu_{2}\\
\vdots\\
\mu_{p}\\
\end{bmatrix}\]</span> and the covariance matrix <span class="math display">\[\mathbf \Sigma_{\mathbf X}=\begin{bmatrix}
\sigma_{11}&amp;\sigma_{12}&amp;\cdots&amp;\sigma_{1p}\\
\sigma_{12}&amp;\sigma_{22}&amp;\cdots&amp;\sigma_{2p}\\
\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\
\sigma_{1p}&amp;\sigma_{2p}&amp;\cdots&amp;\sigma_{pp}\\
\end{bmatrix}
\]</span>
In general, the <span class="math inline">\(q\)</span> linear combinations of the <span class="math inline">\(p\)</span> random variables <span class="math inline">\(X_1,\cdots,X_p\)</span>:
<span class="math display">\[\begin{array}{c}
Z_1&amp;=c_{11}X_1+c_{12}X_2+\cdots+c_{1p}X_p\\
Z_2&amp;=c_{21}X_1+c_{22}X_2+\cdots+c_{2p}X_p\\
&amp;\vdots\\
Z_q&amp;=c_{q1}X_1+c_{q2}X_2+\cdots+c_{qp}X_p\\
\end{array}\]</span> or <span class="math display">\[\mathbf Z=\begin{bmatrix}
Z_1\\
Z_2\\
\vdots\\
Z_q
\end{bmatrix}=\begin{bmatrix}
c_{11}&amp;c_{12}&amp;\cdots&amp;c_{1p}\\
c_{21}&amp;c_{22}&amp;\cdots&amp;c_{2p}\\
\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\
c_{q1}&amp;c_{q2}&amp;\cdots&amp;c_{qp}
\end{bmatrix}\begin{bmatrix}
X_1\\
X_2\\
\vdots\\
X_q
\end{bmatrix}=\mathbf C \mathbf X
\]</span> The linear combinations <span class="math inline">\(\mathbf Z=\mathbf C \mathbf X\)</span> have:<br />
<span class="math inline">\(\boldsymbol\mu_{\mathbf Z}=E(\mathbf Z)=E(\mathbf C \mathbf X)=\mathbf C \boldsymbol\mu_{\mathbf X}\)</span><br />
<span class="math inline">\(\mathbf \Sigma_{\mathbf Z}=Cov(\mathbf Z)=Cov(\mathbf C\mathbf X)=\mathbf C\mathbf \Sigma_{\mathbf X}\mathbf C^T\)</span></p>
<p>If we collect <span class="math inline">\(n\)</span> sets of measurements on <span class="math inline">\(p\)</span> variables, and treat the measurements as random variables, the <em>random sample</em> can be defined as:
<span class="math display">\[\mathbf X=\begin{bmatrix}
X_{11}&amp;X_{12}&amp;\cdots&amp;X_{1p}\\
X_{21}&amp;X_{22}&amp;\cdots&amp;X_{2p}\\
\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\
X_{n1}&amp;X_{n2}&amp;\cdots&amp;X_{np}
\end{bmatrix}=\begin{bmatrix}
\mathbf X_1^T\\
\mathbf X_2^T\\
\mathbf \vdots\\
\mathbf X_n^T
\end{bmatrix}
\]</span>, with each set of measurements <span class="math inline">\(\mathbf X_j^T\)</span> on <span class="math inline">\(p\)</span> variables is a random vector and represent <strong>independent</strong> observations from a common joint distribution with density function <span class="math inline">\(f(\mathbf x)=f(x_1,x_2,\ldots,x_p)\)</span>. Then <span class="math inline">\(\mathbf {\overline X}=\frac{1}{n}\displaystyle\sum_{j=1}^n\mathbf X_j\)</span><br />
<span class="math display">\[\begin{align}
E(\mathbf {\overline X})&amp;=E(\frac{1}{n}\displaystyle\sum_{j=1}^n\mathbf X_j)\\
&amp;=E(\frac{1}{n}\mathbf X_1)+\ldots +E(\frac{1}{n}\mathbf X_n)\\
&amp;=\frac{1}{n}\boldsymbol\mu+\ldots +\frac{1}{n}\boldsymbol\mu\\
&amp;=\boldsymbol\mu
\end{align}\]</span> <span class="math inline">\(\boldsymbol\mu\)</span> is the <strong>population mean vector</strong>.<br />
<span class="math display">\[\begin{align}
(\mathbf {\overline X}-\boldsymbol\mu)(\mathbf {\overline X}-\boldsymbol\mu)^T&amp;=(\frac{1}{n}\displaystyle\sum_{j=1}^n\mathbf X_j-\boldsymbol\mu)(\frac{1}{n}\displaystyle\sum_{j=1}^n\mathbf X_j-\boldsymbol\mu)^T\\
&amp;=\Biggl(\frac{1}{n}\displaystyle\sum_{j=1}^n(\mathbf X_j-\boldsymbol\mu)\Biggr)\Biggl(\frac{1}{n}\displaystyle\sum_{k=1}^n(\mathbf X_k-\boldsymbol\mu)\Biggr)^T\\
&amp;=\frac{1}{n^2}\sum_{j=1}^n\sum_{k=1}^n(\mathbf X_j-\boldsymbol\mu)(\mathbf X_k-\boldsymbol\mu)^T\\
\end{align}\]</span> then
<span class="math display">\[Cov(\mathbf {\overline X})=E((\mathbf {\overline X}-\boldsymbol\mu)(\mathbf {\overline X}-\boldsymbol\mu)^T)=\frac{1}{n^2}\sum_{j=1}^n\sum_{k=1}^nE\Bigl((\mathbf X_j-\boldsymbol\mu)(\mathbf X_k-\boldsymbol\mu)^T\Bigr)
\]</span> because for <span class="math inline">\(j\ne k\)</span>, <span class="math inline">\(\mathbf X_j\)</span> and <span class="math inline">\(\mathbf X_k\)</span> are independent, so <span class="math inline">\(E\Bigl((\mathbf X_j-\boldsymbol\mu)(\mathbf X_k-\boldsymbol\mu)^T\Bigr)=0\)</span>, then <span class="math display">\[Cov(\mathbf {\overline X})=\frac{1}{n^2}\sum_{j=1}^nE\Bigl((\mathbf X_j-\boldsymbol\mu)(\mathbf X_j-\boldsymbol\mu)^T\Bigr)=\frac{1}{n^2}\sum_{j=1}^n\mathbf\Sigma=\frac{1}{n}\mathbf\Sigma\]</span> <span class="math inline">\(\mathbf\Sigma\)</span> is the <strong>population variance–covariance matrix</strong>.</p>

    </div>
  </article>

  


</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="../../../../index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          <li>
            <a href="https://gohugo.io/" class="footer-links-kudos">Made with <img src="../../../../images/hugo-logo.png" alt="Img link to Hugo website" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>
    

    
<script src="../../../../js/math-code.js"></script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
  </body>
</html>

