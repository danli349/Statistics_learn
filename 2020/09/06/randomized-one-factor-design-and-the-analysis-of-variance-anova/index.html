<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.80.0" />


<title>Randomized one-factor design and the analysis of variance (ANOVA) - A Hugo website</title>
<meta property="og:title" content="Randomized one-factor design and the analysis of variance (ANOVA) - A Hugo website">


  <link href='../../../../favicon.ico' rel='icon' type='image/x-icon'/>



  







<link rel="stylesheet" href="../../../../css/fonts.css" media="all">
<link rel="stylesheet" href="../../../../css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="../../../../" class="nav-logo">
    <img src="../../../../images/logo.png"
         width="50"
         height="50"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="../../../../about/">about</a></li>
    
    <li><a href="../../../../vitae/">CV</a></li>
    
    <li><a href="https://github.com/danli349">GitHub</a></li>
    
    <li><a href="https://scholar.google.com/citations?user=mNjLK8EAAAAJ&amp;hl=en">Googlr Scholar</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">3 min read</span>
    

    <h1 class="article-title">Randomized one-factor design and the analysis of variance (ANOVA)</h1>

    
    <span class="article-date">2020-09-06</span>
    

    <div class="article-content">
      
<script src="../../../../rmarkdown-libs/header-attrs/header-attrs.js"></script>


<p>If we want to compare the average effects elicited by <span class="math inline">\(k\)</span> different levels of some given factor, there will be <span class="math inline">\(k\)</span> <strong>independent</strong> random samples of sizes <span class="math inline">\(n_j\quad (j=1,2,...,k)\)</span>, the total sample size is <span class="math inline">\(n=\displaystyle\sum_{j=1}^{k}n_j\)</span>. Let <span class="math inline">\(Y_{ij}\)</span> represent the <span class="math inline">\(i^{th}\)</span> observation recorded for the <span class="math inline">\(j^{th}\)</span> level.
<span class="math display">\[
  \begin{array}{|c|cccc|}
  \hline
  &amp;&amp;\text{treatment}&amp;\text{levels}&amp;\\
  \hline
    &amp; 1 &amp; 2 &amp; \cdots &amp; k \\
    \hline
    &amp; Y_{11} &amp; Y_{12} &amp; \cdots &amp; Y_{1k} \\
    &amp; Y_{21} &amp; Y_{22} &amp; \cdots &amp; Y_{2k} \\
    &amp;\vdots &amp;\vdots &amp;\cdots&amp;\vdots \\
    &amp;Y_{n_11} &amp;Y_{n_22} &amp;\cdots&amp;Y_{n_kk} \\
  \text{Sample sizes:}&amp;n_1&amp;n_2&amp;\cdots&amp;n_k\\
  \text{Sample totals:}&amp;T_{. 1}&amp;T_{. 2}&amp;\cdots&amp;T_{. k}\\
  \text{Sample means:}&amp;\overline Y_{. 1}&amp;\overline Y_{. 2}&amp;\cdots&amp;\overline Y_{. k}\\
  \text{True means:}&amp;\mu_1&amp;\mu_2&amp;\cdots&amp;\mu_k\\
  \hline
  \end{array}
\]</span></p>
<p>Where <span class="math display">\[T_{.j}=\displaystyle\sum_{i=1}^{n_j}Y_{ij}\]</span>, <span class="math display">\[\overline Y_{.j}=\frac{1}{n_j}\displaystyle\sum_{i=1}^{n_j}Y_{ij}=\frac{T_{.j}}{n_j}\]</span>, and the overall total is <span class="math display">\[T_{..}=\sum_{j=1}^{k}\sum_{i=1}^{n_j}Y_{ij}=\sum_{j=1}^{k}T_{.j}\]</span>, overall mean is <span class="math display">\[\overline Y_{..}=\frac{1}{n}T_{..}\]</span></p>
<p>Now we presume that for each <span class="math inline">\(j\)</span>, <span class="math inline">\(Y_{1j}, Y_{2j}, . . . ,Y_{njj}\)</span> is a random sample from a <strong>normal</strong> distribution and <strong>independent</strong> with each other with mean <span class="math inline">\(\mu_j,j = 1, 2, . . . , k,\)</span> and variance <span class="math inline">\(\sigma^2\)</span> (constant for all <span class="math inline">\(j\)</span>). The maximum likelihood estimator of <span class="math inline">\(μ_j\)</span> is <span class="math inline">\(\overline Y_{.j}\)</span> and the maximum likelihood estimator of <span class="math inline">\(μ\)</span> is <span class="math inline">\(\overline Y_{..}\)</span>.</p>
<p>The <span style="color: red;"><strong>sum of squares of treatment (<span class="math inline">\(SSTR\)</span>)</strong></span> estimates the variation among the <span class="math inline">\(\mu_j\)</span>’s and is defined by
<span class="math display">\[\begin{align}
SSTR=\sum_{j=1}^{k}\sum_{i=1}^{n_j}(\overline Y_{.j}-\overline Y_{..})^2
&amp;=\sum_{j=1}^{k}n_j(\overline Y_{.j}-\overline Y_{..})^2\\
&amp;=\sum_{j=1}^{k}n_j\Bigl[(\overline Y_{.j}-\mu)-(\overline Y_{..}-\mu)\Bigr]^2\\
&amp;=\sum_{j=1}^{k}n_j\Bigl[(\overline Y_{.j}-\mu)^2-2(\overline Y_{.j}-\mu)(\overline Y_{..}-\mu)+(\overline Y_{..}-\mu)^2\Bigr]\\
&amp;=\sum_{j=1}^{k}n_j(\overline Y_{.j}-\mu)^2+\sum_{j=1}^{k}n_j(\overline Y_{..}-\mu)^2-2\sum_{j=1}^{k}n_j(\overline Y_{.j}-\mu)(\overline Y_{..}-\mu)\\
&amp;=\sum_{j=1}^{k}n_j(\overline Y_{.j}-\mu)^2+n(\overline Y_{..}-\mu)^2-2(\overline Y_{..}-\mu)n(\overline Y_{..}-\mu)\\
&amp;=\sum_{j=1}^{k}n_j(\overline Y_{.j}-\mu)^2-n(\overline Y_{..}-\mu)^2\\
&amp;=\sum_{j=1}^{k}n_j\overline Y_{.j}^2-2\sum_{j=1}^{k}n_j\overline Y_{.j}\mu+\sum_{j=1}^{k}n_j\mu^2-n\overline Y_{..}^2+2n\overline Y_{..}\mu-n\mu^2\\
&amp;=\sum_{j=1}^{k}n_j\overline Y_{.j}^2-n\overline Y_{..}^2\\
&amp;=\sum_{j=1}^{k}\frac{\overline T_{.j}^2}{n_j}-\frac{T_{..}^2}{n}
\end{align}\]</span>. Then,
<span class="math display">\[\begin{align}
E(SSTR)&amp;=E\Bigl(\sum_{j=1}^{k}n_j(\overline Y_{.j}-\mu)^2-n(\overline Y_{..}-\mu)^2\Bigr)\\
&amp;=\sum_{j=1}^{k}n_jE(\overline Y_{.j}-\mu)^2-nE(\overline Y_{..}-\mu)^2\\
&amp;=\sum_{j=1}^{k}n_j\Bigl(Var(\overline Y_{.j}-\mu)+(E(\overline Y_{.j}-\mu))^2\Bigr)-n\frac{\sigma^2}{n}\\
&amp;=\sum_{j=1}^{k}n_j(\frac{\sigma^2}{n_j})+\sum_{j=1}^{k}n_j(E(\overline Y_{.j}-\mu))^2-\sigma^2\\
&amp;=(k-1)\sigma^2+\sum_{j=1}^{k}n_j(\mu_j-\mu)^2
\end{align}\]</span>.</p>
<p>When <span class="math inline">\(\sigma^2\)</span> is known, the null hypothesis that the treatment level means are all equal <span class="math inline">\(H0:\mu_1 = \mu_2 = \cdots = \mu_k=\mu\)</span> is true, <span class="math inline">\(E(SSTR)=(k-1)\sigma^2\)</span>, and <span class="math inline">\(\frac{SSTR}{\sigma^2}\)</span> has a <span class="math inline">\(\chi^2\)</span> distribution with <span class="math inline">\(k − 1\)</span> degrees of freedom.</p>
<p>When <span class="math inline">\(\sigma^2\)</span> is unknown, the <span class="math inline">\(j^{th}\)</span> sample variance is:
<span class="math display">\[S_j^2=\frac{1}{n_j-1}\displaystyle\sum_{i=1}^{n_j}(Y_{ij}-\overline Y_{.j})^2\]</span>, and the <span style="color: red;"><strong>sum of squares of error (SSE)</strong></span> is: <span class="math display">\[SSE=\sum_{j=1}^{k}(n_j-1)S_j^2=\sum_{j=1}^{k}\sum_{i=1}^{n_j}(Y_{ij}-\overline Y_{.j})^2\]</span>.</p>
<p>Whether or not <span class="math inline">\(\mu_1 = \mu_2 = \cdots = \mu_k\)</span> is true, <span class="math inline">\((n_j-1)S_j^2/\sigma^2\)</span> has a <span class="math inline">\(\chi^2\)</span> distribution with <span class="math inline">\(n_j − 1\)</span> degrees of freedom, and <span class="math inline">\(SSE/\sigma^2\)</span> has a <span class="math inline">\(\chi^2\)</span> distribution with <span class="math inline">\(n − k\)</span> degrees of freedom.</p>
<p>The <span style="color: red;"><strong>Sum of squares of total (SSTOT)</strong></span> is the variation of the data about the parameter <span class="math inline">\(\mu\)</span>,
<span class="math display">\[\begin{align}
SSTOT&amp;=\sum_{j=1}^{k}\sum_{i=1}^{n_j}(Y_{ij}-\overline Y_{..})^2\\
&amp;=\sum_{j=1}^{k}\sum_{i=1}^{n_j}\Bigl[(Y_{ij}-\overline Y_{.j})+(\overline Y_{.j}-\overline Y_{..})\Bigr]^2\\
&amp;=\sum_{j=1}^{k}\sum_{i=1}^{n_j}(Y_{ij}-\overline Y_{.j})^2+\sum_{j=1}^{k}\sum_{i=1}^{n_j}(\overline Y_{.j}-\overline Y_{..})^2+2\sum_{j=1}^{k}\sum_{i=1}^{n_j}(Y_{ij}-\overline Y_{.j})(\overline Y_{.j}-\overline Y_{..})\\
&amp;=\sum_{j=1}^{k}\sum_{i=1}^{n_j}(Y_{ij}-\overline Y_{.j})^2+\sum_{j=1}^{k}\sum_{i=1}^{n_j}(\overline Y_{.j}-\overline Y_{..})^2+2\sum_{j=1}^{k}(\overline Y_{.j}-\overline Y_{..})\sum_{i=1}^{n_j}(Y_{ij}-\overline Y_{.j})\\
&amp;=\sum_{j=1}^{k}\sum_{i=1}^{n_j}(Y_{ij}-\overline Y_{.j})^2+\sum_{j=1}^{k}\sum_{i=1}^{n_j}(\overline Y_{.j}-\overline Y_{..})^2\\
&amp;=SSE+SSTR
\end{align}\]</span></p>
<p>Because <span class="math inline">\(SSTR/\sigma^2\)</span> and <span class="math inline">\(SSE/\sigma^2\)</span> are independent <span class="math inline">\(\chi^2\)</span> square random variables with <span class="math inline">\(k − 1\)</span> and <span class="math inline">\(n − k\)</span> df,<br />
if <span class="math inline">\(H0:\mu_1 = \mu_2 = \cdots = \mu_k\)</span> is true, <span class="math display">\[F=\frac{SSTR/(k-1)}{SSE/(n-k)}\]</span> has a <span class="math inline">\(F\)</span> distribution with <span class="math inline">\(k − 1\)</span> and <span class="math inline">\(n − k\)</span> df.</p>
<p><strong>ANOVA table</strong> for testing <span class="math inline">\(H0:\mu_1 = \mu_2 = \cdots = \mu_k\)</span>:</p>
<p><span class="math display">\[
\begin{array}{lccccc}
\hline
Source &amp; df &amp; SS &amp; MS &amp; F &amp; P \\
\hline
Treatment &amp; k-1 &amp; SSTR &amp; MSTR &amp; \frac{MSTR}{MSE} &amp; P(F_{k−1,n− k} \ge F(observed)) \\
Error &amp; n-k &amp; SSE &amp; MSE &amp;  \\
Total &amp; n-1 &amp; SSTOT\\
\hline
\end{array}
\]</span> <span class="math inline">\(F\)</span>-test rejects <span class="math inline">\(H0:\mu_1 = \mu_2 = \cdots = \mu_k\)</span> at <span class="math inline">\(\alpha\)</span> if <span class="math inline">\(F=\frac{MSTR}{MSE}&gt;F_{k−1,n− k}(\alpha)\)</span></p>

    </div>
  </article>

  


</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
          </li>
          <li>
          </li>
        </ul>
      </footer>

    </div>
    

    
<script src="../../../../js/math-code.js"></script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
  </body>
</html>

