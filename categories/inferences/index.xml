<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Inferences on A Hugo website</title>
    <link>/categories/inferences/</link>
    <description>Recent content in Inferences on A Hugo website</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 18 Oct 2020 00:00:00 +0000</lastBuildDate><atom:link href="/categories/inferences/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Correlation Analysis</title>
      <link>/2020/10/18/correlation-analysis/</link>
      <pubDate>Sun, 18 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/10/18/correlation-analysis/</guid>
      <description>Two groups of variables \(\mathbf X\) has \(p\) variables \[\mathbf X=\begin{bmatrix}X_1\\X_2\\\vdots\\X_p\\\end{bmatrix}\] and \(\mathbf Y\) has \(q\) variables \[\mathbf Y=\begin{bmatrix}Y_1\\Y_2\\\vdots\\Y_q\\\end{bmatrix}\] \[E(\mathbf X)=\boldsymbol\mu_X\] \[Cov(\mathbf X)=\boldsymbol\Sigma_{XX}\] and \[E(\mathbf Y)=\boldsymbol\mu_Y\] \[Cov(\mathbf Y)=\boldsymbol\Sigma_{YY}\] and \[Cov(\mathbf X,\mathbf Y)=\boldsymbol\Sigma_{XY}=\boldsymbol\Sigma_{YX}^T=E(\mathbf X-\boldsymbol\mu_X)(\mathbf Y-\boldsymbol\mu_Y)^T=\begin{bmatrix}\sigma_{X_1Y_1}&amp;amp;\sigma_{X_1Y_2}&amp;amp;\cdots&amp;amp;\sigma_{X_1Y_q}\\\sigma_{X_2Y_1}&amp;amp;\sigma_{X_2Y_2}&amp;amp;\cdots&amp;amp;\sigma_{X_2Y_q}\\\vdots&amp;amp;\vdots&amp;amp;\ddots&amp;amp;\vdots\\\sigma_{X_pY_1}&amp;amp;\sigma_{X_pY_2}&amp;amp;\cdots&amp;amp;\sigma_{X_pY_q}\\\end{bmatrix}\] Linear combinations provide simple summary measures of a set of variables. Let \[U=\mathbf a^T\mathbf X\] \[V=\mathbf b^T\mathbf Y\] Then \[Var(U)=Var(\mathbf a^T\mathbf X)=\mathbf a^TCov(\mathbf X)\mathbf a=\mathbf a^T\boldsymbol\Sigma_{XX}\mathbf a\] \[Var(V)=Var(\mathbf b^T\mathbf Y)=\mathbf b^TCov(\mathbf Y)\mathbf b=\mathbf b^T\boldsymbol\Sigma_{YY}\mathbf b\] \[Cov(U,V)=Cov(\mathbf a^T\mathbf X,\mathbf b^T\mathbf Y)=\mathbf a^TCov(\mathbf X,\mathbf Y)\mathbf b=\mathbf a^T\boldsymbol\Sigma_{XY}\mathbf b\] We shall seek coefficient vectors \(\mathbf a\) and \(\mathbf b\) such that \[Cor(U,V)=Cor(\mathbf a^T\mathbf X,\mathbf b^T\mathbf Y)=\frac{Cov(\mathbf a^T\mathbf X,\mathbf b^T\mathbf Y)}{\sqrt{Var(\mathbf a^T\mathbf X)}\sqrt{Var(\mathbf b^T\mathbf Y)}}=\frac{\mathbf a^T\boldsymbol\Sigma_{XY}\mathbf b}{\sqrt{\mathbf a^T\boldsymbol\Sigma_{XX}\mathbf a}\sqrt{\mathbf b^T\boldsymbol\Sigma_{YY}\mathbf b}}\] is as large as possible.</description>
    </item>
    
    <item>
      <title>Random Matrix Theory</title>
      <link>/2020/10/12/random-matrix-theory/</link>
      <pubDate>Mon, 12 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/10/12/random-matrix-theory/</guid>
      <description>If \(\mathbf X_j\) is drawn from a p-variate normal distribution with zero mean \(N_p(\boldsymbol0, \boldsymbol\Sigma)\),</description>
    </item>
    
    <item>
      <title>Factor analysis</title>
      <link>/2020/10/11/factor-analysis/</link>
      <pubDate>Sun, 11 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/10/11/factor-analysis/</guid>
      <description>Let \(\mathbf X\) is drawn from a \(p\)-variate normal distribution with \(N_p(\boldsymbol\mu, \boldsymbol\Sigma)\) distribution. The matrix of factor loadings \[\mathbf L=\begin{bmatrix}\ell_{11}&amp;amp;\ell_{12}&amp;amp;\cdots&amp;amp;\ell_{1m}\\\ell_{21}&amp;amp;\ell_{22}&amp;amp;\cdots&amp;amp;\ell_{2m}\\\vdots&amp;amp;\vdots&amp;amp;\ddots&amp;amp;\vdots\\\ell_{p1}&amp;amp;\ell_{p2}&amp;amp;\cdots&amp;amp;\ell_{pm}\\\end{bmatrix}\] with \(\ell_{ij}\) is the loading of the \(i^{th}\) variable on the \(j^{th}\) factor.
The common factor is \[\mathbf F=\begin{bmatrix}F_1\\F_2\\\vdots\\F_m\\\end{bmatrix}\] with \(E(\mathbf F)=\underset{(m\times 1)}{\mathbf0}\), \(Var(F_j)=1,\quad (j=1,2,\cdots,m)\) and \(Cov(\mathbf F)=E(\mathbf F\mathbf F^T)=\underset{(m\times m)}{\mathbf I}\) Then the Orthogonal factor model is \[\underset{(p\times1)}{\mathbf X-\boldsymbol\mu}=\underset{(p\times m)}{\mathbf L}\underset{(m\times1)}{\mathbf F}+\underset{(p\times1)}{\boldsymbol\epsilon}\] with \(E(\boldsymbol\epsilon)=\underset{(p\times 1)}{\mathbf0}\) and \[Cov(\boldsymbol\epsilon)=E(\boldsymbol\epsilon\boldsymbol\epsilon^T)=\boldsymbol\Psi=\begin{bmatrix}\psi_1&amp;amp;0&amp;amp;\cdots&amp;amp;0\\0&amp;amp;\psi_2&amp;amp;\cdots&amp;amp;0\\\vdots&amp;amp;\vdots&amp;amp;\ddots&amp;amp;\vdots\\0&amp;amp;0&amp;amp;\cdots&amp;amp;\psi_p\\\end{bmatrix}\] with \(Var(\epsilon_i)=\psi_i\) and \(\mathbf F\) and \(\boldsymbol\epsilon\) are independent with \(Cov(\boldsymbol\epsilon,\mathbf F)=E(\boldsymbol\epsilon\mathbf F^T)=\underset{(p\times m)}{\mathbf0}\)Because \(\mathbf L\) is fixed, then \[\begin{align}\boldsymbol\Sigma=Cov(\mathbf X)&amp;amp;=E(\mathbf X-\boldsymbol\mu)(\mathbf X-\boldsymbol\mu)^T\\&amp;amp;=E(\mathbf L\mathbf F+\boldsymbol\epsilon)(\mathbf L\mathbf F+\boldsymbol\epsilon)^T\\&amp;amp;=E(\mathbf L\mathbf F+\boldsymbol\epsilon)((\mathbf L\mathbf F)^T+\boldsymbol\epsilon^T)\\&amp;amp;=E\Bigl(\mathbf L\mathbf F(\mathbf L\mathbf F)^T+\boldsymbol\epsilon(\mathbf L\mathbf F)^T+\mathbf L\mathbf F\boldsymbol\epsilon^T+\boldsymbol\epsilon\boldsymbol\epsilon^T\Bigr)\\&amp;amp;=\mathbf LE(\mathbf F\mathbf F^T)\mathbf L^T+\mathbf0+\mathbf0+E(\boldsymbol\epsilon\boldsymbol\epsilon^T)\\&amp;amp;=\mathbf L\mathbf L^T+\boldsymbol\Psi\end{align}\] or \[Var(X_i)=\underset{Var(X_i)}{\underbrace{\sigma_{ii}}}=\mathbf L_i\mathbf L_i^T+\psi_i=\underset{\text{communality}}{\underbrace{\ell_{i1}^2+\ell_{i2}^2+\cdots+\ell_{im}^2}}+\underset{\text{specific variance}}{\underbrace{\psi_i}}\] with \(\mathbf L_i\) is the \(i^{th}\) row of \(\mathbf L\) We can denote the \(i^{th}\) communality as \(h_i^2=\ell_{i1}^2+\ell_{i2}^2+\cdots+\ell_{im}^2,\quad (i=1,2,\cdots,p)\), which is the sum of squares of the loadings of the \(i^{th}\) variable on the \(m\) common factors, and the total variance of the \(i^{th}\) variable is the sum of communality and specific variance \(\sigma_{ii}=h_i^2+\psi_i\)\[Cov(X_i,X_k)=E(\mathbf L_i^T\mathbf F+\epsilon_i)(\mathbf L_k^T\mathbf F+\epsilon_k)^T=\mathbf L_i^T\mathbf L_k=\ell_{i1}\ell_{k1}+\ell_{i2}\ell_{k2}+\cdots+\ell_{im}\ell_{km}\]\[Cov(\mathbf X,\mathbf F)=E(\mathbf X-\boldsymbol\mu)\mathbf F^T=E(\mathbf L\mathbf F+\boldsymbol\epsilon)\mathbf F^T=\mathbf LE(\mathbf F\mathbf F^T)+E(\boldsymbol\epsilon\mathbf F^T)=\mathbf L\] or \[Cov(X_i,F_j)=E(X_i-\mu_i)\mathbf F_j^T=E(\mathbf L_i^T\mathbf F+\epsilon_i)\mathbf F_j^T=\ell_{ij}\]</description>
    </item>
    
    <item>
      <title>Principal Component Analysis</title>
      <link>/2020/10/07/principal-component-analysis/</link>
      <pubDate>Wed, 07 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/10/07/principal-component-analysis/</guid>
      <description>Let the random vector \(\mathbf X^T=[X_1,X_2,\cdots,X_p]\) have the covariance matrix \(\boldsymbol\Sigma\) with eigenvalues \(\lambda_1\ge\lambda_2\ge\cdots\ge\lambda_p\ge0\), the linear combinations \(Y_i=\mathbf a_i^T\mathbf X=a_{i1}X_1+a_{i2}X_2+\cdots+a_{ip}X_p, \quad (i=1,2,\cdots,p)\) has \(Var(Y_i)=Var(\mathbf a_i^T\mathbf X)=\mathbf a_i^TCov(\mathbf X)\mathbf a_i=\mathbf a_i^T\boldsymbol\Sigma\mathbf a_i\) and \(Cov(Y_i,Y_k)=Cov(\mathbf a_i^T\mathbf X, \mathbf a_k^T\mathbf X)=\mathbf a_i^T\boldsymbol\Sigma\mathbf a_k \quad i,k=1,2,\cdots,p\). The principal components are those uncorrelated linear combinations of \([X_1,X_2,\cdots,X_p]\), \(Y_1,Y_2,\cdots,Y_p\) whose variances \(Var(Y_i)=\mathbf a_i^T\boldsymbol\Sigma\mathbf a_i\) are as large as possible, subject to \(\mathbf a_i^T\mathbf a_i=1\). These linear combinations represent the selection of a new coordinate system obtained by rotating the original system with \(Y_1,Y_2,\cdots,Y_p\) as the new coordinate axes.</description>
    </item>
    
    <item>
      <title>Comparisons of several means</title>
      <link>/2020/09/29/comparisons-of-several-means/</link>
      <pubDate>Tue, 29 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/09/29/comparisons-of-several-means/</guid>
      <description>Paired Comparisons:
If there are \(2\) treatments over multivariate \(\mathbf x_p\), the difference between treatment \(1\) and treatment \(2\) is \(\mathbf d_j=\mathbf x_{j1}-\mathbf x_{j2},\quad j=1,2,\cdots,n\) if \(\mathbf d_j\) are independent \(N_p(\boldsymbol\delta, \mathbf\Sigma_d)\) random vectors, inferencesabout the vector of mean differences \(\boldsymbol\delta\) can be based upon a \(T^2\)-statistic: \(T^2=n(\overline{\mathbf d}-\boldsymbol\delta)^T\mathbf S_d^{-1}(\overline{\mathbf d}-\boldsymbol\delta)\) is distributed as an \(\frac{(n-1)p}{n-p}F_{p,n-p}\) random variable, where \(\overline{\mathbf d}=\displaystyle\frac{1}{n}\displaystyle\sum_{j=1}^{n}\mathbf d_j\) and \(\mathbf S_d=\displaystyle\frac{1}{n-1}\displaystyle\sum_{j=1}^{n}(\mathbf d_j-\overline{\mathbf d})(\mathbf d_j-\overline{\mathbf d})^T\), then an \(\alpha\)-level hypothesis test of \(H_0:\boldsymbol\delta=\mathbf 0\) versus \(H_1:\boldsymbol\delta\ne\mathbf 0\), rejects \(H_0\) if the observed \(T^2=n\overline{\mathbf d}^T\mathbf S_d^{-1}\overline{\mathbf d}&amp;gt;\frac{(n-1)p}{n-p}F_{p,n-p}(\alpha)\).</description>
    </item>
    
    <item>
      <title>Inferences about the mean</title>
      <link>/2020/09/25/inferences-about-the-mean/</link>
      <pubDate>Fri, 25 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/09/25/inferences-about-the-mean/</guid>
      <description>The hypothesis testing about the mean is a test of the competing hypotheses: \(H_0:\mu=\mu_0\) and \(H_1:\mu\ne\mu_0\). If \(X_1,X_2,\cdots,X_n\) denote a random sample from a normal population, the appropriate test statistic is \(t=\frac{(\overline X-\mu_0)}{s/\sqrt{n}}\) with \(s^2=\frac{1}{(n-1)}\displaystyle\sum_{i=1}^{n}(X_i-\overline X)^2\). Rejecting \(H_0\) when \(|t|\) is large is equivalent to rejecting \(H_0\) when \(t^2=\frac{(\overline X-\mu_0)^2}{s^2/n}=n(\overline X-\mu_0)(s^2)^{-1}(\overline X-\mu_0)\) is large. Then the test becomes reject \(H_0\) in favor of \(H_1\) at significance level \(\alpha\) if \(n(\overline X-\mu_0)(s^2)^{-1}(\overline X-\mu_0)&amp;gt;t_{n-1}^2(\alpha/2)\), its multivariate analog is \(T^2=(\overline {\mathbf X}-\boldsymbol\mu_0)^T(\frac{1}{n}\mathbf S)^{-1}(\overline {\mathbf X}-\boldsymbol\mu_0)=n(\overline {\mathbf X}-\boldsymbol\mu_0)^T\mathbf S^{-1}(\overline {\mathbf X}-\boldsymbol\mu_0)\), where \(\overline {\mathbf X}=\frac{1}{n}\displaystyle\sum_{j=1}^{n}\mathbf X_j\), \(\underset{(p\times p)}{\mathbf S}=\frac{1}{n-1}\displaystyle\sum_{j=1}^{n}(\underset{(p\times 1)}{\mathbf X_j}-\underset{(p\times 1)}{\overline {\mathbf X}})(\underset{(p\times 1)}{\mathbf X_j}-\underset{(p\times 1)}{\overline {\mathbf X}})^T\)</description>
    </item>
    
  </channel>
</rss>
