<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Inferences on A Hugo website</title>
    <link>/categories/inferences/</link>
    <description>Recent content in Inferences on A Hugo website</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 12 Oct 2020 00:00:00 +0000</lastBuildDate><atom:link href="/categories/inferences/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Random Matrix Theory</title>
      <link>/2020/10/12/random-matrix-theory/</link>
      <pubDate>Mon, 12 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/10/12/random-matrix-theory/</guid>
      <description>If \(\mathbf X_i\) is drawn from a p-variate normal distribution with zero mean \(N_p(\boldsymbol0, \boldsymbol\Sigma)\), let \((p\times n)\) matrix \[\underset{(p\times n)}{\mathbf Z}=\begin{bmatrix}\mathbf X_1&amp;amp;\mathbf X_2&amp;amp;\cdots&amp;amp;\mathbf X_n\end{bmatrix}\], Then the Wishart distribution is the probability distribution of the \(p\times p\) random matrix \[\underset{(p\times p)}{\mathbf W}=\begin{bmatrix}\mathbf X_1&amp;amp;\mathbf X_2&amp;amp;\cdots&amp;amp;\mathbf X_n\end{bmatrix}\]</description>
    </item>
    
    <item>
      <title>Factor analysis</title>
      <link>/2020/10/11/factor-analysis/</link>
      <pubDate>Sun, 11 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/10/11/factor-analysis/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Principal Component Analysis</title>
      <link>/2020/10/07/principal-component-analysis/</link>
      <pubDate>Wed, 07 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/10/07/principal-component-analysis/</guid>
      <description>Let the random vector \(\mathbf X^T=[X_1,X_2,\cdots,X_p]\) have the covariance matrix \(\boldsymbol\Sigma\) with eigenvalues \(\lambda_1\ge\lambda_2\ge\cdots\ge\lambda_p\ge0\), the linear combinations \(Y_i=\mathbf a_i^T\mathbf X=a_{i1}X_1+a_{i2}X_2+\cdots+a_{ip}X_p, \quad (i=1,2,\cdots,p)\) has \(Var(Y_i)=Var(\mathbf a_i^T\mathbf X)=\mathbf a_i^TCov(\mathbf X)\mathbf a_i=\mathbf a_i^T\boldsymbol\Sigma\mathbf a_i\) and \(Cov(Y_i,Y_k)=Cov(\mathbf a_i^T\mathbf X, \mathbf a_k^T\mathbf X)=\mathbf a_i^T\boldsymbol\Sigma\mathbf a_k \quad i,k=1,2,\cdots,p\). The principal components are those uncorrelated linear combinations of \([X_1,X_2,\cdots,X_p]\), \(Y_1,Y_2,\cdots,Y_p\) whose variances \(Var(Y_i)=\mathbf a_i^T\boldsymbol\Sigma\mathbf a_i\) are as large as possible, subject to \(\mathbf a_i^T\mathbf a_i=1\). These linear combinations represent the selection of a new coordinate system obtained by rotating the original system with \(Y_1,Y_2,\cdots,Y_p\) as the new coordinate axes.</description>
    </item>
    
    <item>
      <title>Comparisons of several means</title>
      <link>/2020/09/29/comparisons-of-several-means/</link>
      <pubDate>Tue, 29 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/09/29/comparisons-of-several-means/</guid>
      <description>Paired Comparisons:
If there are \(2\) treatments over multivariate \(\mathbf x_p\), the difference between treatment \(1\) and treatment \(2\) is \(\mathbf d_j=\mathbf x_{j1}-\mathbf x_{j2},\quad j=1,2,\cdots,n\) if \(\mathbf d_j\) are independent \(N_p(\boldsymbol\delta, \mathbf\Sigma_d)\) random vectors, inferencesabout the vector of mean differences \(\boldsymbol\delta\) can be based upon a \(T^2\)-statistic: \(T^2=n(\overline{\mathbf d}-\boldsymbol\delta)^T\mathbf S_d^{-1}(\overline{\mathbf d}-\boldsymbol\delta)\) is distributed as an \(\frac{(n-1)p}{n-p}F_{p,n-p}\) random variable, where \(\overline{\mathbf d}=\displaystyle\frac{1}{n}\displaystyle\sum_{j=1}^{n}\mathbf d_j\) and \(\mathbf S_d=\displaystyle\frac{1}{n-1}\displaystyle\sum_{j=1}^{n}(\mathbf d_j-\overline{\mathbf d})(\mathbf d_j-\overline{\mathbf d})^T\), then an \(\alpha\)-level hypothesis test of \(H_0:\boldsymbol\delta=\mathbf 0\) versus \(H_1:\boldsymbol\delta\ne\mathbf 0\), rejects \(H_0\) if the observed \(T^2=n\overline{\mathbf d}^T\mathbf S_d^{-1}\overline{\mathbf d}&amp;gt;\frac{(n-1)p}{n-p}F_{p,n-p}(\alpha)\).</description>
    </item>
    
    <item>
      <title>Inferences about the mean</title>
      <link>/2020/09/25/inferences-about-the-mean/</link>
      <pubDate>Fri, 25 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/09/25/inferences-about-the-mean/</guid>
      <description>The hypothesis testing about the mean is a test of the competing hypotheses: \(H_0:\mu=\mu_0\) and \(H_1:\mu\ne\mu_0\). If \(X_1,X_2,\cdots,X_n\) denote a random sample from a normal population, the appropriate test statistic is \(t=\frac{(\overline X-\mu_0)}{s/\sqrt{n}}\) with \(s^2=\frac{1}{(n-1)}\displaystyle\sum_{i=1}^{n}(X_i-\overline X)^2\). Rejecting \(H_0\) when \(|t|\) is large is equivalent to rejecting \(H_0\) when \(t^2=\frac{(\overline X-\mu_0)^2}{s^2/n}=n(\overline X-\mu_0)(s^2)^{-1}(\overline X-\mu_0)\) is large. Then the test becomes reject \(H_0\) in favor of \(H_1\) at significance level \(\alpha\) if \(n(\overline X-\mu_0)(s^2)^{-1}(\overline X-\mu_0)&amp;gt;t_{n-1}^2(\alpha/2)\), its multivariate analog is \(T^2=(\overline {\mathbf X}-\boldsymbol\mu_0)^T(\frac{1}{n}\mathbf S)^{-1}(\overline {\mathbf X}-\boldsymbol\mu_0)=n(\overline {\mathbf X}-\boldsymbol\mu_0)^T\mathbf S^{-1}(\overline {\mathbf X}-\boldsymbol\mu_0)\), where \(\overline {\mathbf X}=\frac{1}{n}\displaystyle\sum_{j=1}^{n}\mathbf X_j\), \(\underset{(p\times p)}{\mathbf S}=\frac{1}{n-1}\displaystyle\sum_{j=1}^{n}(\underset{(p\times 1)}{\mathbf X_j}-\underset{(p\times 1)}{\overline {\mathbf X}})(\underset{(p\times 1)}{\mathbf X_j}-\underset{(p\times 1)}{\overline {\mathbf X}})^T\)</description>
    </item>
    
  </channel>
</rss>
