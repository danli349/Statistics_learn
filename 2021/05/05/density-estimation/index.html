<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.80.0" />


<title>Density Estimation - A Hugo website</title>
<meta property="og:title" content="Density Estimation - A Hugo website">


  <link href='../../../../favicon.ico' rel='icon' type='image/x-icon'/>



  







<link rel="stylesheet" href="../../../../css/fonts.css" media="all">
<link rel="stylesheet" href="../../../../css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="../../../../" class="nav-logo">
    <img src="../../../../images/logo.png"
         width="50"
         height="50"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="../../../../about/">about</a></li>
    
    <li><a href="../../../../vitae/">CV</a></li>
    
    <li><a href="https://github.com/danli349">GitHub</a></li>
    
    <li><a href="https://scholar.google.com/citations?user=mNjLK8EAAAAJ&amp;hl=en">Googlr Scholar</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">22 min read</span>
    

    <h1 class="article-title">Density Estimation</h1>

    
    <span class="article-date">2021-05-05</span>
    

    <div class="article-content">
      
<script src="../../../../2021/05/05/density-estimation/index_files/header-attrs/header-attrs.js"></script>

<div id="TOC">
<ul>
<li><a href="#introuction">1. INTROUCTION</a></li>
<li><a href="#survey-of-existing-methods">2. SURVEY OF EXISTING METHODS</a>
<ul>
<li><a href="#histograms">2.2. Histograms</a></li>
<li><a href="#the-naive-estimator">2.3. The naive estimator</a></li>
<li><a href="#references">References</a></li>
</ul></li>
</ul>
</div>

<div id="introuction" class="section level1">
<h1>1. INTROUCTION</h1>
<p>The <strong>probability density function</strong> is a fundamental concept in statistics. Consider any random quantity <span class="math inline">\(X\)</span> that has probability density function <span class="math inline">\(f\)</span>. Specifying the function <span class="math inline">\(f\)</span> gives a natural description of the distribution of <span class="math inline">\(X\)</span>, and allows probabilities associated with <span class="math inline">\(X\)</span> to be found from the relation
<span class="math display">\[P(a&lt;X&lt;b)=\int_{a}^{b}f(d)dx\quad\text{for all } a&lt;b\]</span>
Suppose, now, that we have a set of observed data points assumed to be a sample from an unknown probability density function. <strong>Density estimation</strong>, is the construction of an estimate of the density function from the observed data.
The two main aims are to explain</p>
<ul>
<li>how to estimate a density from a given data set</li>
<li>and to explore how density estimates can be used</li>
</ul>
<p>both in their own right and as an ingredient of other statistical procedures.
One approach to density estimation is <strong>parametric</strong>. Assume that the data are drawn from one of a known parametric family of distributions, for example the normal distribution with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>. The density <span class="math inline">\(f\)</span> underlying the data could then be estimated by finding estimates of <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span> from the data and substituting these estimates into the formula for the normal density.</p>
<p>In <strong>nonparametric estimation</strong>, less rigid assumptions will be made about the distribution of the observed data. Although it will be assumed that the
distribution has a probability density <span class="math inline">\(f\)</span>, the data will be allowed to speak for themselves in determining the estimate of <span class="math inline">\(f\)</span> more than would be the case if <span class="math inline">\(f\)</span> were constrained to fall in a given parametric family.</p>
</div>
<div id="survey-of-existing-methods" class="section level1">
<h1>2. SURVEY OF EXISTING METHODS</h1>
<div id="histograms" class="section level2">
<h2>2.2. Histograms</h2>
<p>The oldest and most widely used density estimator is the histogram. Given an origin <span class="math inline">\(x_0\)</span> and a bin width <span class="math inline">\(h\)</span>,we definethe bins of the histogram to be the intervals <span class="math inline">\([x_0 + mh, x_0 +(m +1)h]\)</span> for positive and negative integers <span class="math inline">\(m\)</span>. The intervals have been chosen closed on the left and open on the right for definiteness.</p>
<p>The histogram is then defined by <span class="math display">\[\hat{f}(x)=\frac{1}{nh}\times(\text{no. of }X_i\text{ in the same bin as }x)\]</span>
Note that, to construct the histogram, we have to choose both an origin and a bin width; it is the choice of bin width which, primarily, controls the amount of smoothing inherent in the procedure.</p>
<p>The histogram can be generalized by allowing the bin widths to vary. Formally, suppose that we have any dissection of the real line into bins; then the estimate will be defined by <span class="math display">\[\hat{f}(x)=\frac{1}{n}\times\frac{(\text{no. of }X_i\text{ in the same bin as }x)}{\text{width of bin containing }x}\]</span>
The dissection into bins can either be carried out a priori or else in some way which depends on the observations themselves.</p>
<p>The discontinuity of histograms causes extreme difficulty if derivatives of the estimates are required. When density estimates are needed as intermediate components of other methods, the case for using alternatives to histograms is quite strong.</p>
<p>For the presentation and exploration of data, histograms are of course an extremely useful class of density estimates, particularly in the univariate case. However, even in one dimension, the choice of origin can have quite an effect.</p>
<p>Though the histogram remains an excellent tool for data presentation, it is worth at least considering the various alternative density estimates that are available.</p>
</div>
<div id="the-naive-estimator" class="section level2">
<h2>2.3. The naive estimator</h2>
<p>From the definition of a probability density, if the random variable <span class="math inline">\(X\)</span> has density <span class="math inline">\(f\)</span>,then <span class="math display">\[f(x)=\lim_{h\to0}\frac{1}{2h}P(x-h&lt;X&lt;x+h)\]</span>
For any given <span class="math inline">\(h\)</span>, we can of course estimate <span class="math inline">\(P(x - h &lt; X&lt; x + h)\)</span> by the proportion of the sample falling in the interval <span class="math inline">\((x - h, x + h)\)</span>. Thus a natural estimator of the density is given by choosing a small number <span class="math inline">\(h\)</span> and setting
<span class="math display">\[\hat f(x)=\lim_{h\to0}\frac{1}{2hn}\times\lbrace\text{no. of }X_i\text{ falling in the same bin as }(x-h,x+h)\rbrace\]</span>
we shall call this the <strong>naive estimator</strong>.</p>
<p>To express the estimator more transparently, define the weight function <span class="math inline">\(w\)</span> by
<span class="math display">\[w(x)=\begin{cases}
1/2,  &amp; \text{if }|x|&lt;1 \\
0, &amp; \text{otherwise}
\end{cases}\]</span> (2.1)</p>
<p>Then it is easy to see that the naive estimator can be written
<span class="math display">\[\hat f(x)=\lim_{h\to0}\frac{1}{2hn}\times\lbrace\text{no. of }X_i\text{ falling in the same bin as }(x-h,x+h)\rbrace\]</span> ()</p>
<p>It follows from (2.1) that the estimate is constructed by placing a <code>box' of width 2h and height (2nh)-1 on each observation and then summing to obtain the estimate. We shall return to this interpretation below, but it is instructive first to consider a connection with histograms. Consider the histogram constructed from the data using bins ofwidth 2h. Assume that no observations lie exactly at the edge of abin. If x happens to be at the centre of one of the histogram bins, it follows at once from (2.1) that the naive estimate (x) will be exactly the ordinate of the histogram at x. Thus the naive estimate can be seen to be an attempt to construct a histogram where every point is the centre of a sampling interval, thus freeing the histogram from a particular choice of bin positions. The choice of bin width still remains and is governed by the parameter h, which controls the amount by which the data are smoothed to produce the estimate. The naive estimator is not wholly satisfactory from the point of view of using density estimates for presentation. It follows from the definition that is not a continuous function, but has jumps at the points Xi ± h and has zero derivative everywhere else. This gives the estimates a somewhat ragged character which is not only aesthetically undesirable, but, more seriously, could provide the untrained observer with a misleading impression. Partly to overcome this difficulty, and partly for other technical reasons given later, it is of interest to consider the generalization of the naive estimator given in the following section. A density estimated using the naive estimator is given in Fig. 2.3. The</code>stepwise’ nature of the estimate is clear. The boxes used
to construct the estimate have the same width as the histogram bins in Fig. 2.1.
8of22
03/15/2002 2:18 PM
﻿Density Estimation for Statistics and Data Analysis - B.W. Silverman
<a href="file:///e%7C/moe/HTML/March02/Silverman/Silver.html" class="uri">file:///e|/moe/HTML/March02/Silverman/Silver.html</a>
Fig. 2.3 Naive estimate constructed from Old Faithful geyser data,
h = 0.25.
2.4. The kernel estimator
It is easy to generalize the naive estimator to overcome some of the difficulties discussed above. Replace the weight function w
by a kernel function Kwhich satisfies the condition
(2.2)
Usually, but not always, K will be a symmetric probability density function, the normal density, for instance, or the weight
function w used in the definition of the naive estimator. By analogy with the definition of the naive estimator, the kernel
estimator with kernel K is defined by
(2.2a)
where h is the window width, also called the smoothing parameter or bandwidth by some authors. We shall consider some
mathematical properties of the kernel estimator later, but first of all an intuitive discussion with some examples may be helpful.
Just as the naive estimator can be considered as a sum of <code>boxes' centred at the observations, the kernel estimator is a sum of</code>bumps’ placed at the observations. The kernel function K determines the shape of the bumps while the window width h
determines their width. An illustration is given in Fig. 2.4, where the individual bumps n-1 h-1 K{(x - Xi)/h}are shown as well as
the estimate constructed by adding them up. It should be stressed that it is not usually appropriate to construct a density
estimate from such a small sample, but that a sample of size 7 has been used here for the sake of clarity.
Fig. 2.4 Kernel estimate showing individual kernels. Window width 0.4.
9of22
03/15/2002 2:18 PM</p>
<p>2.5. The nearest neighbour method
The nearest neighbour class of estimators represents an attempt to adapt the amount of smoothing to the <code>local' density of data. The degree of smoothing is controlled by an integer k, chosen to be considerably smaller than the sample size; typically kn1/2. Define the distance d (x, y) between two points on the line to be |x - y| in the usual way, and for each t define to be the distances, arranged in ascending order, from t to the points of the sample. The kth nearest neighbour density estimate is then defined by (2.3) In order to understand this definition, suppose that the density at t is f(t). Then, of a sample of size n, one would expect about 2r nf(t) observations to fall in the interval [t - r, t + r] for each r &gt; 0; see the discussion of the naive estimator in Section 2.3 above. Since, by definition, exactly k observations fall in the interval [t - dk(t), t + dk(t)], an estimate of the density at t may be obtained by putting this can be rearranged to give the definition of the kth nearest neighbour estimate. While the naive estimator is based on the number of observations falling in a box of fixed width centred at the point of interest, the nearest neighbour estimate is inversely proportional to the size of the box needed to contain a given number of observations. In the tails of the distribution, the distance dk(t) will be larger than in the main part of the distribution, and so the problem of undersmoothing in the tails should be reduced. Like the naive estimator, to which it is related, the nearest choice neighbour estimate as defined in (2.3) is not a smooth curve. The function dk(t) can easily be seen to be continuous, but its derivative will have a discontinuity at every point of the form 1/2(X(j) + X(j+k)), where X(j) are the order statistics of the sample. It follows at once from these remarks and from the definition 13 of22 03/15/2002 2:18 PM ﻿Density Estimation for Statistics and Data Analysis - B.W. Silverman file:///e|/moe/HTML/March02/Silverman/Silver.html that will be positive and continuous everywhere, but will have discontinuous derivative at all the same points as dk.In contrast to the kernel estimate, the nearest neighbour estimate will not itself be a probability density, since it will not integrate to unity. For t less than the smallest data point, we will have dk(t)= X(n-k+1) and for t &gt; X(n) we will have dk(t)= t - X(n-k+1). Substituting into (2.3), it follows that - the nearest neighbour estimate is unlikely to be appropriate if an estimate of the entire density is required. Figure 2.10 gives a nearest neighbour density estimate for the Old Faithful data. The heavy tails and the discontinuities in the derivative are clear. (t)dt is infinite and that the tails of die away at rate t-1, in other words extremely slowly. Thus Fig. 2.10 Nearest neighbour estimate for Old Faithful geyser data, k = 20. It is possible to generalize the nearest neighbour estimate to provide an estimate related to the kernel estimate. As in Section 2.4, let K(x) be a kernel function integrating to one. Then the generalized kth nearest neighbour estimate is defined by (2.4) It can be seen at once that (t) is precisely the kernel estimate evaluated at t with window width dk(t). Thus the overall amount of smoothing is governed by the choice of the integer k, but the window width used at any particular point depends on the density of observations near that point. The ordinary kth nearest neighbour estimate is the special case of (2.4) when K is the uniform kernel w of (2.1); thus (2.4) stands in the to same relation to (2.3) as the kernel estimator does to the naive ed. estimator. However, the derivative of the generalized nearest neighbour estimate will be discontinuous at all the points where the function dk( t) has discontinuous derivative. The precise integrability and tail properties will depend on the exact form of the kernel, and to will not be discussed further here. Further discussion of the nearest neighbour approach will be given in Section 5.2. 2.6. The variable kernel method The variable kernel method is somewhat related to the nearest neighbour approach and is another method which adapts the amount of smoothing to the local density of data. The estimate is constructed similarly to the classical kernel estimate, but the scale parameter of the 'bumps' placed on the data points is allowed to vary from one data point to another. Let K be a kernel function and k a positive integer. Define dj, k to be the distance from Xj to the kth nearest point in the set comprising the other n - 1 data points. Then the variable kernel estimate with smoothing parameter h is defined by (2.5) 14 of22 03/15/2002 2:18 PM ﻿Density Estimation for Statistics and Data Analysis - B.W. Silverman file:///e|/moe/HTML/March02/Silverman/Silver.html The window width of the kernel placed on the point Xj is proportional to dj, k, so that data points in regions where the data are sparse will have flatter kernels associated with them. For any fixed k, the overall degree of smoothing will depend on the parameter h. The choice ofk determines how responsive the window width choice will be to very local detail. Some comparison of the variable kernel estimate with the generalized nearest neighbour estimate (2.4) may be instructive. In (2.4) the window width used to construct the estimate at t depends on the distances from t to the data points; in (2.5) the window widths are independent of the point t at which the density is being estimated, and depend only on the distances between the data points. In contrast with the generalized nearest neighbour estimate, the variable kernel estimate will itself be a probability density function provided the kernel K is; that is an immediate consequence of the definition. Furthermore, as with the ordinary kernel estimator, all the local smoothness properties of the kernel will be inherited by the estimate. In Fig. 2.11 the methodis usedto obtain an estimate for the suicide data. The noise in the tail of the curve has been eliminated, but it is interesting to note that the method exposes some structure in the main part of the distribution which is not really visible even in the undersmoothed curve in Figure 2.9. Fig. 2.11 Variable kernel estimate for suicide study data, k =8, h =5. In Section 5.3, the variable kernel method will be considered in greater detail, and in particular an important generalization, called the adaptive kernel method, will be introduced. 2.7. Orthogonal series estimators Orthogonal series estimators approach the density estimation problem from quite a different point of view. They are best explained by a specific example. Suppose that we are trying to estimate a density fon the unit interval [0, 1]. The idea of the orthogonal series method is then to estimate fby estimating the coefficients of its Fourier expansion. Define the sequence (x)by Then, by standard mathematical analysis, fcan be represented as the Fourier series =0 f , where, for each 0, (2.6) 15 of22 03/15/2002 2:18 PM ﻿ PDFTron PDF2Text: This page is skipped when running in the demo mode. ﻿Density Estimation for Statistics and Data Analysis - B.W. Silverman file:///e|/moe/HTML/March02/Silverman/Silver.html For instance, for data resealed to have zero mean and unit variance, a(x) might be the function e-x2/2 and the multiples of the Hermite polynomials; for details see Kreider et al. (1966). The sample coefficients will then be defined by but otherwise the estimates will be defined as above; possible estimates are (2.9) or (2.10) The properties of estimates obtained by the orthogonal series method depend on the details of the series being used and on the system ofweights. The Fourier series estimates will integrate to unity, provided 0 =1, since and 0 will always be equal to one. However, except for rather special choices of the weights , cannot be guaranteed to be non-negative. The local smoothness properties of the estimates will again depend on the particular case; estimates obtained from (2.8) will have derivatives of all orders. 2.8. Maximum penalized likelihood estimators The methods discussed so far are all derived in an ad hoc way from the definition of a density. It is interesting to ask whether it is possible to apply standard statistical techniques, like maximum likelihood, to density estimation. The likelihood of a curve g as density underlying a set of independent identically distributed observations is given by This likelihood has no finite maximum over the class of all densities. To see this, let window width 1/2 h; then, for each i, h be the naive density estimate with and so Thus the likelihood can be made arbitrarily large by taking densities approaching the sum of delta functions as defined in (2.7) above, and it is not possible to use maximum likelihood directly for density estimation without placing restrictions on the class of densities over which the likelihood is to be maximized. 17 of22 03/15/2002 2:18 PM ﻿Density Estimation for Statistics and Data Analysis - B.W. Silverman file:///e|/moe/HTML/March02/Silverman/Silver.html There are, nevertheless, possible approaches related to maximum likelihood. One method is to incorporate into the likelihood a term which describes the roughness - in some sense - of the curve under consideration. Suppose R(g) is a functional which quantifies the roughness of g. One possible choice of such a functional is (2.11) Define the penalized log likelihood by (2.12) where is a positive smoothing parameter. The penalized log likelihood can be seen as a way of quantifying the conflict between smoothness and goodness-of-fit to the data, since the log likelihood term log g(Xi) measures how well g fits the data. The probability density function is said to be a maximum penalized likelihood density estimate if it maximizes l (g) over the class of all curves g which satisfy - g =1, g(x)0 for all x,and R(g) &lt; . The parameter controls the amount of smoothing since it determines the</code>rate of exchange’
between smoothness and goodness-of-fit; the smaller the value of , the rougher - in terms of R( ) - will be the corresponding
maximum penalized likelihood estimator. Estimates obtained by the maximum penalized likelihood method will, by definition,
be probability densities. Further details of these estimates will be given in Section 5.4.
2.9. General weight function estimators
It is possible to define a general class of density estimators which includes several of the estimators discussed above. Suppose
that w(x, y) is a function of two arguments, which in most cases will satisfy the conditions
(2.13)
and
(2.14)
We should think of w as being defined in such a way that most of the weight of the probability density w(x, .) falls near x.An
estimate of the density underlying the data may be obtained by putting
(2.15)
We shall refer to estimates of the form (2.15) as general weight function estimates. It is clear from (2.15) that the conditions
(2.13) and (2.14) will be sufficient to ensure that is a probability density function, and that the smoothness properties of will
be inherited from those of the functions w(x,
.). This class of estimators can be thought of in two ways. Firstly, it is a unifying
concept which makes it possible, for example, to obtain theoretical results applicable to a whole range of apparently distinct
estimators. On the other hand, it is possible to define useful estimators which do not fall into any of the classes discussed in
previous sections but which are nevertheless of the form (2.15). We shall discuss such an estimator later in this section.
To obtain the histogram as a special case of (2.15), set
18 of22
03/15/2002 2:18 PM
﻿
PDFTron PDF2Text: This page is skipped when running in the demo mode.
﻿
PDFTron PDF2Text: This page is skipped when running in the demo mode.
﻿
PDFTron PDF2Text: This page is skipped when running in the demo mode.
﻿
PDFTron PDF2Text: This page is skipped when running in the demo mode.
﻿Density Estimation for Statistics and Data Analysis - B.W. Silverman
<a href="file:///e%7C/moe/HTML/March02/Silverman/Silver.html" class="uri">file:///e|/moe/HTML/March02/Silverman/Silver.html</a>
2.11. Discussion and bibliography
A brief survey of the kind conducted in this chapter of course asks far more questions than it answers, and some of these
questions will be the subject of discussion in subsequent chapters. The overriding problems are the choice ofwhat method to use
in any given practical context and, given that a particular method is being used, how to choose the various parameters needed by
the method. The remarks already made about the mathematical properties of the estimates obtained by various procedures will of
course be important in making these decisions. To obtain a fuller understanding of the importance and consequences of the
various choices it is essential to investigate the statistical properties of the various methods and also to consider the difficulties
involved in computing the estimates.
This chapter has by no means considered all the methods available for density estimation. Generalizations and other approaches
are considered in later chapters of this book, and in the other books and surveys mentioned in Section 1.3.
The naive estimator was introduced by Fix and Hodges (1951) in an unpublished report; the first published paper to deal
explicitly with probability density estimation was by Rosenblatt (1956), who discussed both the naive estimator and the more
general kernel estimator. Whittle (1958) formulated the general weight function class of estimators, while the orthogonal series
estimator was introduced by Cencov (1962). The nearest neighbour estimate was first considered by Loftsgaarden and
Quesenberry (1965), while the variable kernel method is due to Breiman, Meisel and Purcell (1977), though Wertz (1978, p. 59)
refers to presumably independent but related work by Victor. The maximum penalized likelihood approach was first applied to
density estimation by Good and Gaskins (1971). The reflection and replication techniques of Section 2.10 were introduced and
illustrated by Boneva, Kendall and Stefanov (1971), while the transformation technique is discussed by Copas and Fryer (1980).
22 of22
03/15/2002 2:18 PM
﻿Density Estimation for Statistics and Data Analysis - B.W. Silverman
<a href="file:///e%7C/moe/HTML/March02/Silverman/Silver.html" class="uri">file:///e|/moe/HTML/March02/Silverman/Silver.html</a>
Fig. 1.2 Density estimate constructed from observations of the
height of a steel surface. After Silverman (1980) with the
permission ofAcademic Press, Inc. This version reproduced from
Silverman (1981a) with the permission of John Wiley &amp; Sons
Ltd.
A third example is given in Fig. 1.3. The data used to construct this curve are a standard directional data set and consist of the
directions in which each of 76 turtles was observed to swim when released. It is clear that most of the turtles show a preference
for swimming approximately in the 60° direction, while a small proportion prefer exactly the opposite direction. Although
further statistical modelling of these data is possible (see Mardia 1972) the density estimate really gives all the useful
conclusions tobedrawn from thedataset.
Fig. 1.3 Density estimate constructed from turtle data. After
Silverman (1978a) with the permission of the Biometrika
Trustees. This version reproduced from Silverman (1981a) with
the permission of John Wiley &amp; Sons Ltd.
An important aspect of statistics, often neglected nowadays, is the presentation of data back to the client in order to provide
explanation and illustration of conclusions that may possibly have been obtained by other means. Density estimates are ideal for
this purpose, for the simple reason that they are fairly easily comprehensible to non-mathematicians. Even those statisticians
who are sceptical about estimating densities would no doubt explain a normal distribution by drawing a bell-shaped curve rather
than by one of the other methods illustrated in Fig. 1.4. In all the examples given in this section, the density estimates are as
valuable for explaining conclusions as for drawing these conclusions in the first place. More examples illustrating the use of
density estimates for exploratory and presentational purposes, including the important case of bivariate data, will be given in
later chapters.
3of22
03/15/2002 2:18 PM
﻿Density Estimation for Statistics and Data Analysis - B.W. Silverman
<a href="file:///e%7C/moe/HTML/March02/Silverman/Silver.html" class="uri">file:///e|/moe/HTML/March02/Silverman/Silver.html</a>
Fig. 1.4 Four ways of explaining the normal distribution: a graph of
the density function; a graph of the cumulative distribution function;
a straight line on probability paper, the formula for the density
function.
1.3. Further reading
There is a vast literature on density estimation, much of it concerned with asymptotic results not covered in any detail in this
book.
Prakasa Rao’s (1983) book offers a comprehensive treatment of the theoretical aspects of the subject. Journal papers providing
surveys and bibliography include Rosenblatt (1971), Fryer (1977), Wertz and Schneider (1979), and Bean and Tsokos (1980).
Tapia and Thompson (1978) give an interesting perspective paying particular attention to their own version of the penalized
likelihood approach described in Sections 2.8 and 5.4 below. A thorough treatment, rather technical in nature, of a particular
question and its ramifications is given by Devroye and Gyorfi (1985). Other texts on the subject are Wertz (1978) and Delecroix
(1983). Further references relevant to specific topics will be given, as they arise, later in this book.
2. SURVEY OF EXISTING METHODS
2.1. Introduction
In this chapter a brief summary is given of the main methods available for univariate density estimation. Some of the methods
will be discussed in greater detail in later chapters, but it is helpful to have a general view of the subject before examining any
particular method in detail. Many of the important applications of density estimation are to multivariate data, but since all the
multivariate methods are generalizations of univariate methods, it is worth getting a feel for the univariate case first.
Two data sets will be used to help illustrate some of the methods. The first comprises the lengths of 86 spells of psychiatric
treatment undergone by patients used as controls in a study of suicide risks reported by Copas and Fryer (1980). The data are
given in Table 2.1. The second data set, observations of eruptions ofOld Faithful geyser in Yellowstone National Park, USA, is
taken from Weisberg (1980), and is reproduced in Table 2.2. I am most grateful to John Copas and to Sanford Weisberg for
making these data sets available to me.
4of22
03/15/2002 2:18 PM
﻿Density Estimation for Statistics and Data Analysis - B.W. Silverman
<a href="file:///e%7C/moe/HTML/March02/Silverman/Silver.html" class="uri">file:///e|/moe/HTML/March02/Silverman/Silver.html</a>
Table 2.1 Lengths of treatment spells (in days) of control patients in
suicide study.
1 25
1 27
1 27
5 30
7 30
8 31
8 31
13 32
14 34
14 35
17 36
18 37
21 38
21 39
22 39
40
49
49
54
56
56
62
63
65
65
67
75
76
79
82
83 123
84 126
84 129
84 134
90 144
91 147
92 153
93 163
93 167
103 175
103 228
111 231
112 235
119 242
122 256
256
257
311
314
322
369
415
573
609
640
737
Table 2.2 Eruption lengths (in minutes) of 107 eruptions of Old Faithful geyser.
4.37
4.70
1.68
1.75
4.35
1.77
4.25
4.10
4.05
1.90
4.00
4.42
1.83
1.83
3.95
4.83
3.87
1.73
3.92
3.20
2.33
4.57
3.58
3.70
4.25
3.58
3.67
1.90
4.13
4.53
4.10
4.12
4.00
4.93
3.68
1.85
3.83
1.85
3.80
3.80
3.33
3.73
1.67
4.63
1.83
2.03
2.72
4.03
1.73
3.10
4.62
1.88
3.52
3.77
3.43
2.00
3.73
4.60
2.93
4.65
4.18
4.58
3.50
4.62
4.03
1.97
4.60
4.00
3.75
4.00
4.33
1.82
1.67
3.50
4.20
4.43
1.90
4.08
3.43
1.77
4.50
1.80
3.70
2.50
2.27
2.93
4.63
4.00
1.97
3.93
4.07
4.50
2.25
4.25
4.08
3.92
4.73
3.72
4.50
4.40
4.58
3.50
1.80
4.28
4.33
4.13
1.95
It is convenient to define some standard notation. Except where otherwise stated, it will be assumed that we are given a sample
of n real observations X1, … , Xn whose underlying density is to be estimated. The symbol will be used to denote whatever
density estimator is currently being considered.</p>
</div>
<div id="references" class="section level2 unnumbered">
<h2>References</h2>
<div id="refs" class="references csl-bib-body">
<div id="ref-silverman1986density" class="csl-entry">
1. Silverman BW. Density estimation for statistics and data analysis. CRC press; 1986.
</div>
</div>
</div>
</div>

    </div>
  </article>

  


</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
          </li>
          <li>
          </li>
        </ul>
      </footer>

    </div>
    

    
<script src="../../../../js/math-code.js"></script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
  </body>
</html>

