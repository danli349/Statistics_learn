<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.80.0" />


<title>Density Estimation - A Hugo website</title>
<meta property="og:title" content="Density Estimation - A Hugo website">


  <link href='../../../../favicon.ico' rel='icon' type='image/x-icon'/>



  







<link rel="stylesheet" href="../../../../css/fonts.css" media="all">
<link rel="stylesheet" href="../../../../css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="../../../../" class="nav-logo">
    <img src="../../../../images/logo.png"
         width="50"
         height="50"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="../../../../about/">about</a></li>
    
    <li><a href="../../../../vitae/">CV</a></li>
    
    <li><a href="https://github.com/danli349">GitHub</a></li>
    
    <li><a href="https://scholar.google.com/citations?user=mNjLK8EAAAAJ&amp;hl=en">Googlr Scholar</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">17 min read</span>
    

    <h1 class="article-title">Density Estimation</h1>

    
    <span class="article-date">2021-05-05</span>
    

    <div class="article-content">
      
<script src="../../../../2021/05/05/density-estimation/index_files/header-attrs/header-attrs.js"></script>

<div id="TOC">
<ul>
<li><a href="#introuction">1. INTROUCTION</a></li>
<li><a href="#survey-of-existing-methods">2. SURVEY OF EXISTING METHODS</a>
<ul>
<li><a href="#introduction">2.1 Introduction</a></li>
<li><a href="#histograms">2.2. Histograms</a></li>
<li><a href="#the-naive-estimator">2.3. The naive estimator</a></li>
<li><a href="#the-kernel-estimator">2.4. The kernel estimator</a></li>
<li><a href="#the-nearest-neighbour-method">2.5. The nearest neighbour method</a></li>
<li><a href="#the-variable-kernel-method">2.6. The variable kernel method</a></li>
<li><a href="#orthogonal-series-estimators">2.7. Orthogonal series estimators</a></li>
<li><a href="#maximum-penalized-likelihood-estimators">2.8. Maximum penalized likelihood estimators</a></li>
<li><a href="#general-weight-function-estimators">2.9. General weight function estimators</a></li>
</ul></li>
<li><a href="#introuction-1">1. INTROUCTION</a>
<ul>
<li><a href="#references">References</a></li>
</ul></li>
</ul>
</div>

<div id="introuction" class="section level1">
<h1>1. INTROUCTION</h1>
<p>The <strong>probability density function</strong> is a fundamental concept in statistics. Consider any random quantity <span class="math inline">\(X\)</span> that has probability density function <span class="math inline">\(f\)</span>. Specifying the function <span class="math inline">\(f\)</span> gives a natural description of the distribution of <span class="math inline">\(X\)</span>, and allows probabilities associated with <span class="math inline">\(X\)</span> to be found from the relation
<span class="math display">\[P(a&lt;X&lt;b)=\int_{a}^{b}f(d)dx\quad\text{for all } a&lt;b\]</span>
Suppose, now, that we have a set of observed data points assumed to be a sample from an unknown probability density function. <strong>Density estimation</strong>, is the construction of an estimate of the density function from the observed data.
The two main aims are to explain</p>
<ul>
<li>how to estimate a density from a given data set</li>
<li>and to explore how density estimates can be used</li>
</ul>
<p>both in their own right and as an ingredient of other statistical procedures.
One approach to density estimation is <strong>parametric</strong>. Assume that the data are drawn from one of a known parametric family of distributions, for example the normal distribution with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>. The density <span class="math inline">\(f\)</span> underlying the data could then be estimated by finding estimates of <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span> from the data and substituting these estimates into the formula for the normal density.</p>
<p>In <strong>nonparametric estimation</strong>, less rigid assumptions will be made about the distribution of the observed data. Although it will be assumed that the
distribution has a probability density <span class="math inline">\(f\)</span>, the data will be allowed to speak for themselves in determining the estimate of <span class="math inline">\(f\)</span> more than would be the case if <span class="math inline">\(f\)</span> were constrained to fall in a given parametric family.</p>
</div>
<div id="survey-of-existing-methods" class="section level1">
<h1>2. SURVEY OF EXISTING METHODS</h1>
<div id="introduction" class="section level2">
<h2>2.1 Introduction</h2>
</div>
<div id="histograms" class="section level2">
<h2>2.2. Histograms</h2>
<p>The oldest and most widely used density estimator is the histogram. Given an origin <span class="math inline">\(x_0\)</span> and a bin width <span class="math inline">\(h\)</span>,we definethe bins of the histogram to be the intervals <span class="math inline">\([x_0 + mh, x_0 +(m +1)h]\)</span> for positive and negative integers <span class="math inline">\(m\)</span>. The intervals have been chosen closed on the left and open on the right for definiteness.</p>
<p>The histogram is then defined by <span class="math display">\[\hat{f}(x)=\frac{1}{nh}\times(\text{no. of }X_i\text{ in the same bin as }x)\]</span>
Note that, to construct the histogram, we have to choose both an origin and a bin width; it is the choice of bin width which, primarily, controls the amount of smoothing inherent in the procedure.</p>
<p>The histogram can be generalized by allowing the bin widths to vary. Formally, suppose that we have any dissection of the real line into bins; then the estimate will be defined by <span class="math display">\[\hat{f}(x)=\frac{1}{n}\times\frac{(\text{no. of }X_i\text{ in the same bin as }x)}{\text{width of bin containing }x}\]</span>
The dissection into bins can either be carried out a priori or else in some way which depends on the observations themselves.</p>
<p>The discontinuity of histograms causes extreme difficulty if derivatives of the estimates are required. When density estimates are needed as intermediate components of other methods, the case for using alternatives to histograms is quite strong.</p>
<p>For the presentation and exploration of data, histograms are of course an extremely useful class of density estimates, particularly in the univariate case. However, even in one dimension, the choice of origin can have quite an effect.</p>
<p>Though the histogram remains an excellent tool for data presentation, it is worth at least considering the various alternative density estimates that are available.</p>
</div>
<div id="the-naive-estimator" class="section level2">
<h2>2.3. The naive estimator</h2>
<p>From the definition of a probability density, if the random variable <span class="math inline">\(X\)</span> has density <span class="math inline">\(f\)</span>,then <span class="math display">\[f(x)=\lim_{h\to0}\frac{1}{2h}P(x-h&lt;X&lt;x+h)\]</span>
For any given <span class="math inline">\(h\)</span>, we can of course estimate <span class="math inline">\(P(x - h &lt; X&lt; x + h)\)</span> by the proportion of the sample falling in the interval <span class="math inline">\((x - h, x + h)\)</span>. Thus a natural estimator of the density is given by choosing a small number <span class="math inline">\(h\)</span> and setting
<span class="math display">\[\hat f(x)=\lim_{h\to0}\frac{1}{2hn}\times\lbrace\text{no. of }X_i\text{ falling in the same bin as }(x-h,x+h)\rbrace\]</span>
we shall call this the <strong>naive estimator</strong>.</p>
<p>To express the estimator more transparently, define the weight function <span class="math inline">\(w\)</span> by
<span class="math display">\[w(x)=\begin{cases}
1/2,  &amp; \text{if }|x|&lt;1 \\
0, &amp; \text{otherwise}
\end{cases} \quad (2.1)\]</span></p>
<p>Then it is easy to see that the naive estimator can be written
<span class="math display">\[\hat f(x)=\frac{1}{n}\sum_{i=1}^{n}\frac{1}{h}w\left(\frac{x-X_i}{h}\right)\]</span></p>
<p>It follows from (2.1) that the estimate is constructed by placing a `boxâ€™ of width <span class="math inline">\(2h\)</span> and height <span class="math inline">\(\frac{1}{2nh}\)</span> on each observation and then summing to obtain the estimate. We shall return to this interpretation below, but it is instructive first to consider a connection with histograms.</p>
<p>Consider the histogram constructed from the data using bins of width <span class="math inline">\(2h\)</span>. Assume that no observations lie exactly at the edge of a bin. If <span class="math inline">\(x\)</span> happens to be at the center of one of the histogram bins, it follows at once from (2.1) that the naive estimate <span class="math inline">\(\hat f(x)\)</span> will
be exactly the ordinate of the histogram at <span class="math inline">\(x\)</span>. Thus the naive estimate can be seen to be an attempt to construct a histogram where every point is the center of a sampling interval, thus freeing the histogram from a particular choice of bin positions.</p>
<p>The choice of bin width still remains and is governed by the parameter <span class="math inline">\(h\)</span>, which controls the amount by which the data are smoothed to produce the estimate.</p>
<p>The naive estimator is not wholly satisfactory from the point of view of using density estimates for presentation. It follows from the definition that <span class="math inline">\(\hat f\)</span> is not a continuous function, but has jumps at the points <span class="math inline">\(X_i \pm h\)</span> and has zero derivative everywhere else. This gives the estimates a somewhat ragged character which is not only aesthetically undesirable, but, more seriously, could provide the untrained observer with a misleading impression. Partly to overcome this difficulty, and partly for other technical
reasons given later, it is of interest to consider the generalization of the naive estimator given in the following section.</p>
</div>
<div id="the-kernel-estimator" class="section level2">
<h2>2.4. The kernel estimator</h2>
<p>It is easy to generalize the naive estimator to overcome some of the difficulties discussed above. Replace the weight function <span class="math inline">\(w\)</span> by a kernel function <span class="math inline">\(K\)</span> which satisfies the condition <span class="math display">\[\int_{-\infty}^{\infty}K(x)dx=1 \quad (2.2)\]</span></p>
<p>Usually, but not always, <span class="math inline">\(K\)</span> will be a symmetric probability density function, the normal density, for instance, or the weight function <span class="math inline">\(w\)</span> used in the definition of the naive estimator. By analogy with the definition of the naive estimator, the kernel
estimator with kernel <span class="math inline">\(K\)</span> is defined by
<span class="math display">\[\hat f(x)=\frac{1}{nh}\sum_{i=1}^{n}K\left(\frac{x-X_i}{h}\right)\quad  (2.2a)\]</span>
where <span class="math inline">\(h\)</span> is the window width, also called the smoothing parameter or bandwidth by some authors.</p>
<p>Just as the naive estimator can be considered as a sum of <em>boxes</em> centered at the observations, the kernel estimator is a sum of <em>bumps</em> placed at the observations. The kernel function <span class="math inline">\(K\)</span> determines the shape of the bumps while the window width <span class="math inline">\(h\)</span> determines their width. <img src="2.4.png" alt="Fig. 2.4" /></p>
<p>An illustration is given in Fig. 2.4, where the individual bumps <span class="math display">\[\frac{1}{nh} K\left(\frac{x-X_i}{h}\right)\]</span> are shown as well as the estimate constructed by adding them up. It should be stressed that it is not usually appropriate to construct a density estimate from such a small sample, but that a sample of size 7 has been used here for the sake of clarity.</p>
<div class="figure">
<img src="2.5.png" alt="" />
<p class="caption">Fig. 2.5</p>
</div>
<p>The effect of varying the window width is illustrated in Fig. 2.5. The limit as <span class="math inline">\(h\)</span> tends to zero is (in a sense) a sum of Dirac delta function spikes at the observations, while as <span class="math inline">\(h\)</span> becomes large, all detail, spurious or otherwise, is obscured.</p>
<p>Some elementary properties of kernel estimates follow at once from the definition. Provided the kernel <span class="math inline">\(K\)</span> is everywhere
non-negative and satisfies the condition <span class="math display">\[\int_{-\infty}^{\infty}K(x)dx=1\]</span> in other words is a probability density function - it will follow at once from the
definition that <span class="math inline">\(\hat f\)</span> will itself be a probability density. Furthermore, <span class="math inline">\(\hat f\)</span> will inherit all the continuity and differentiability properties of the kernel <span class="math inline">\(K\)</span>, so that if, for example, <span class="math inline">\(K\)</span> is the normal density function, then <span class="math inline">\(\hat f\)</span> will be a smooth curve having derivatives of all orders.</p>
</div>
<div id="the-nearest-neighbour-method" class="section level2">
<h2>2.5. The nearest neighbour method</h2>
<p>The nearest neighbour class of estimators represents an attempt to adapt the amount of smoothing to the <em>local</em> density of data.
The degree of smoothing is controlled by an integer <span class="math inline">\(k\)</span>, chosen to be considerably smaller than the sample size; typically <span class="math inline">\(k\approx n^{1/2}\)</span>.
Define the distance <span class="math inline">\(d(x, y)\)</span> between two points on the line to be <span class="math inline">\(|x - y|\)</span> in the usual way, and for each <span class="math inline">\(t\)</span> define <span class="math display">\[d_1(t)\le d_2(t)\le\cdots\le d_n(t)\]</span>
to be the distances, arranged in ascending order, from <span class="math inline">\(t\)</span> to the points of the sample.
The <strong><span class="math inline">\(k\)</span>th nearest neighbour density estimate</strong> is then defined by <span class="math display">\[\hat f(t)=\frac{k}{2nd_k(t)}\quad (2.3) \]</span></p>
<p>In order to understand this definition, suppose that the density at <span class="math inline">\(t\)</span> is <span class="math inline">\(f(t)\)</span>. Then, of a sample of size <span class="math inline">\(n\)</span>, one would expect about <span class="math inline">\(2rnf(t)\)</span> observations to fall in the interval <span class="math display">\[[t - r, t + r] \text{ for each } r &gt; 0\]</span> Since, by definition, exactly <span class="math inline">\(k\)</span> observations fall in the interval <span class="math inline">\([t - d_k(t), t + d_k(t)]\)</span>, an estimate of the density at <span class="math inline">\(t\)</span> may be obtained by putting <span class="math display">\[k=2d_k(t)n\hat f(t)\]</span> this can be rearranged to give the definition of the <span class="math inline">\(k\)</span>th nearest neighbour estimate.</p>
<p>While the naive estimator is based on the number of observations falling in a box of fixed width centred at the point of interest,
the nearest neighbour estimate is inversely proportional to the size of the box needed to contain a given number of observations.
In the tails of the distribution, the distance <span class="math inline">\(d_k(t)\)</span> will be larger than in the main part of the distribution, and so the problem of undersmoothing in the tails should be reduced.</p>
<p>Like the naive estimator, to which it is related, the nearest choice neighbour estimate as defined in (2.3) is not a smooth curve. The function <span class="math inline">\(d_k(t)\)</span> can easily be seen to be continuous, but its derivative will have a discontinuity at every point of the form <span class="math display">\[\frac{1}{2}(X_{(j)} + X_{(j+k)})\]</span>, where <span class="math inline">\(X_{(j)}\)</span> are the <strong>order statistics</strong> of the sample.</p>
<blockquote>
<p>In statistics, the <span class="math inline">\(k\)</span>th order statistic of a statistical sample is equal to its kth-smallest value. Together with rank statistics, order statistics are among the most fundamental tools in non-parametric statistics and inference.
Important special cases of the order statistics are the minimum and maximum value of a sample, and the sample median and other sample quantiles.
When using probability theory to analyze order statistics of random samples from a continuous distribution, the cumulative distribution function is used to reduce the analysis to the case of order statistics of the uniform distribution.
<a href="https://www.wikiwand.com/en/Order_statistic">https://www.wikiwand.com/en/Order_statistic</a></p>
</blockquote>
<p>It follows at once from these remarks and from the definition that <span class="math inline">\(\hat f\)</span> will be positive and continuous everywhere, but will have discontinuous derivative at all the same points as <span class="math inline">\(d_k\)</span>. In contrast to the kernel estimate, the nearest neighbour estimate will not itself be a probability density, since it will not integrate to unity. For <span class="math inline">\(t\)</span> less than the smallest data point, we will have <span class="math inline">\(d_k(t)= X_{(n-k+1)}\)</span> and for <span class="math inline">\(t &gt; X_{(n)}\)</span> we will have <span class="math inline">\(d_k(t)= t - X_{(n-k+1)}\)</span>. Substituting into (2.3), it follows that
<span class="math inline">\(\int_{-\infty}^{\infty}(t)dt\)</span> is infinite and that the tails of <span class="math inline">\(\hat f\)</span> die away at rate <span class="math inline">\(t^{-1}\)</span>, in other words extremely slowly.</p>
<p><img src="2.10.png" /></p>
<p>Thus the nearest neighbour estimate is unlikely to be appropriate if an estimate of the entire density is required. Figure 2.10 gives a nearest neighbour density estimate for the Old Faithful data. The heavy tails and the discontinuities in the derivative are clear.</p>
<p>It is possible to generalize the nearest neighbour estimate to provide an estimate related to the kernel estimate. As in Section 2.4, let <span class="math inline">\(K(x)\)</span> be a kernel function integrating to one. Then the <strong>generalized <span class="math inline">\(k\)</span>th nearest neighbour estimate</strong> is defined by <span class="math display">\[\hat f(t)=\frac{1}{nd_k(t)}\sum_{i=1}^{n}K\left(\frac{t-X_i}{d_k(t)}\right)\quad (2.4)\]</span>
It can be seen at once that <span class="math inline">\(\hat f(t)\)</span> is precisely the kernel estimate evaluated at <span class="math inline">\(t\)</span> with window width <span class="math inline">\(d_k(t)\)</span>. Thus the overall amount of smoothing is governed by the choice of the integer <span class="math inline">\(k\)</span>, but the window width used at any particular point depends on the density of observations near that point.</p>
<p>The ordinary <span class="math inline">\(k\)</span>th nearest neighbour estimate is the special case of (2.4) when <span class="math inline">\(K\)</span> is the uniform kernel <span class="math inline">\(w\)</span> of <span class="math display">\[w(x)=\begin{cases}
1/2,  &amp; \text{if }|x|&lt;1 \\
0, &amp; \text{otherwise}
\end{cases} \quad (2.1)\]</span> thus (2.4) stands
in the to same relation to (2.3) as the kernel estimator does to the naive ed.Â estimator. However, the derivative of the generalized nearest neighbour estimate will be discontinuous at all the points where the function <span class="math inline">\(d_k(t)\)</span> has discontinuous derivative. The precise integrability and tail properties will depend on the exact form of the kernel.</p>
</div>
<div id="the-variable-kernel-method" class="section level2">
<h2>2.6. The variable kernel method</h2>
<p>The variable kernel method is somewhat related to the nearest neighbour approach and is another method which adapts the
amount of smoothing to the local density of data. The estimate is constructed similarly to the classical kernel estimate, but the
scale parameter of the <em>bumps</em> placed on the data points is allowed to vary from one data point to another.</p>
<p>Let <span class="math inline">\(K\)</span> be a kernel function and <span class="math inline">\(k\)</span> a positive integer. Define <span class="math inline">\(d_{j, k}\)</span> to be the distance from <span class="math inline">\(X_j\)</span> to the <span class="math inline">\(k\)</span>th nearest point in the set comprising the other <span class="math inline">\(n - 1\)</span> data points. Then the <strong>variable kernel estimate</strong> with smoothing parameter <span class="math inline">\(h\)</span> is defined by <span class="math display">\[\hat f(t)=\frac{1}{n}\sum_{j=1}^{n}\frac{1}{hd_{j,k}}K\left(\frac{t-X_j}{hd_{j,k}}\right)\quad (2.5)\]</span></p>
<p>The window width of the kernel placed on the point <span class="math inline">\(X_j\)</span> is proportional to <span class="math inline">\(d_{j, k}\)</span>, so that data points in regions where the data are sparse will have flatter kernels associated with them. For any fixed <span class="math inline">\(k\)</span>, the overall degree of smoothing will depend on the parameter <span class="math inline">\(h\)</span>. The choice of <span class="math inline">\(k\)</span> determines how responsive the window width choice will be to very local detail.</p>
<p>Some comparison of the variable kernel estimate with the generalized nearest neighbour estimate <span class="math display">\[\hat f(t)=\frac{1}{nd_k(t)}\sum_{i=1}^{n}K\left(\frac{t-X_i}{d_k(t)}\right)\quad (2.4)\]</span> may be instructive. In (2.4) the window width used to construct the estimate at <span class="math inline">\(t\)</span> depends on the distances from <span class="math inline">\(t\)</span> to the data points; in (2.5) the window
widths are independent of the point <span class="math inline">\(t\)</span> at which the density is being estimated, and depend only on the distances between the data
points.</p>
<p>In contrast with the generalized nearest neighbour estimate, the variable kernel estimate will itself be a probability density
function provided the kernel <span class="math inline">\(K\)</span> is; that is an immediate consequence of the definition. Furthermore, as with the ordinary kernel estimator, all the local smoothness properties of the kernel will be inherited by the estimate.</p>
</div>
<div id="orthogonal-series-estimators" class="section level2">
<h2>2.7. Orthogonal series estimators</h2>
<p>Orthogonal series estimators approach the density estimation problem from quite a different point of view. They are best
explained by a specific example. Suppose that we are trying to estimate a density <span class="math inline">\(f\)</span> on the unit interval <span class="math inline">\([0, 1]\)</span>. The idea of the orthogonal series method is then to estimate <span class="math inline">\(f\)</span> by estimating the coefficients of its Fourier expansion.</p>
<p>Define the sequence <span class="math inline">\(\phi_{V}(x)\)</span> by
<span class="math display">\[\begin{align}
\left.
\begin{array}{l}
\phi_0(x)&amp;=1\\
\phi_{2r-1}(x)&amp;=\sqrt{2}\cos2\pi rx\\
\phi_{2r}(x)&amp;=\sqrt{2}\sin2\pi rx\\
\end{array}
\right\}
r=1,2,\cdots
\end{align}\]</span>
Then, by standard mathematical analysis, <span class="math inline">\(f\)</span> can be represented as the Fourier series <span class="math display">\[\sum_{v=0}^{\infty}f_v\phi_V\]</span>, where, for each <span class="math inline">\(V\ge 0\)</span>, <span class="math display">\[f_v=\int_{0}^{1}f(x)\phi_v(x)dx\quad (2.6)\]</span></p>
<p>Suppose <span class="math inline">\(X\)</span> is a random variable with density <span class="math inline">\(f\)</span>. Then <span class="math display">\[f_v=\int_{0}^{1}f(x)\phi_v(x)dx\quad (2.6)\]</span> can be written
<span class="math display">\[f_V=E\phi_V(X)\]</span>
and hence a natural, and unbiased, estimator of <span class="math inline">\(f\)</span> based on a sample <span class="math inline">\(X_1,\cdots, X_n\)</span> from <span class="math inline">\(f\)</span> is <span class="math display">\[\hat f_v=\frac{1}{n}\sum_{i=1}^{n}\phi_v(X_i)\]</span>
Unfortunately, the <span class="math inline">\(sum = 0\)</span> will not be a good estimate of <span class="math inline">\(f\)</span>, but will <em>converge</em> to a sum of delta functions at the
observations; to see this, let <span class="math display">\[w(x)=\frac{1}{n}\sum_{i=1}^{n}\delta(x-X_i)\quad (2.7)\]</span></p>
<p>where <span class="math inline">\(\delta\)</span> is the Dirac delta function. Then, for each <span class="math inline">\(V\)</span>,
<span class="math display">\[\hat f_V=\int_{0}^{1}w(x)\phi_v(x)dx\]</span> and so the <span class="math inline">\(\hat f_V\)</span> are exactly the Fourier coefficients of the function <span class="math inline">\(w\)</span>.</p>
<p>In order to obtain a useful estimate of the density <span class="math inline">\(f\)</span>, it is necessary to smooth <span class="math inline">\(w\)</span> by applying a low-pass filter to the sequence of coefficients <span class="math inline">\(\hat f_v\)</span>. The easiest way to do this is to truncate the expansion <span class="math inline">\(\sum \hat f_v\phi_v\)</span> at some point. Choose an integer <span class="math inline">\(K\)</span> and define the density estimate <span class="math inline">\(\hat f\)</span> by
<span class="math display">\[\hat f(x)=\sum_{v=0}^{K}\hat f_v\phi_v(x)\quad (2.8)\]</span></p>
<p>The choice of the cutoff point <span class="math inline">\(K\)</span> determines the amount of smoothing.</p>
<p>A more general approach is to taper the series by a sequence of weights <span class="math inline">\(\lambda_v\)</span>, which satisfy <span class="math inline">\(\lambda_v\to 0\)</span> as <span class="math inline">\(v\to\infty\)</span>, to obtain the estimate <span class="math display">\[\hat f(x)=\sum_{v=0}^{\infty}\lambda_v\hat f_v\phi_v(x)\]</span>
The rate at which the weights <span class="math inline">\(\lambda_v\)</span> converge to zero will determine the amount of smoothing.</p>
<p>Other orthogonal series estimates, no longer necessarily confined to data lying on a finite interval, can be obtained by using
different orthonormal sequences of functions. Suppose <span class="math inline">\(a(x)\)</span> is a weighting function and <span class="math inline">\((\psi_v)\)</span> is a series satisfying, for <span class="math inline">\(\mu\)</span> and <span class="math inline">\(v\ge 0\)</span>, <span class="math display">\[\int_{-\infty}^{\infty}\psi_{\mu}(x)\psi_{v}(x)a(x)dx=\begin{cases}
1,  &amp; \mu=v \\
0, &amp; \text{otherwise}
\end{cases}\]</span></p>
<p>For instance, for data resealed to have zero mean and unit variance, <span class="math inline">\(a(x)\)</span> might be the function <span class="math inline">\(e^{-x2/2}\)</span> and the <span class="math inline">\(\psi_v\)</span> multiples of the Hermite polynomials.</p>
<p>The sample coefficients will then be defined by
<span class="math display">\[\hat f_v=\frac{1}{n}\sum_{i}\psi_v(X_i)a(X_i)
\]</span> but otherwise the estimates will be defined as above; possible estimates are <span class="math display">\[\hat f(x)=\sum_{v=0}^{K}\hat f_v\psi_v(x)\quad (2.9)\]</span> or <span class="math display">\[\hat f(x)=\sum_{v=0}^{K}\lambda_v\hat f_v\psi_v(x)\quad (2.10)\]</span></p>
<p>The properties of estimates obtained by the orthogonal series method depend on the details of the series being used and on the
system of weights. The Fourier series estimates will integrate to unity, provided <span class="math inline">\(\lambda_0 =1\)</span>, since <span class="math display">\[\int_{0}^{1}\phi_v(x)dx=0\quad\text{for all } v&gt;0\]</span>
and <span class="math inline">\(\hat f_0\)</span> will always be equal to one. However, except for rather special choices of the weights <span class="math inline">\(\lambda_v\)</span>, <span class="math inline">\(\hat f\)</span> cannot be guaranteed to be non-negative. The local smoothness properties of the estimates will again depend on the particular case; estimates obtained from <span class="math display">\[\hat f(x)=\sum_{v=0}^{K}\hat f_v\phi_v(x)\quad (2.8)\]</span> will have derivatives of all orders.</p>
</div>
<div id="maximum-penalized-likelihood-estimators" class="section level2">
<h2>2.8. Maximum penalized likelihood estimators</h2>
<p>The methods discussed so far are all derived in an <em>ad hoc</em> way from the definition of a density. It is interesting to ask whether it is possible to apply standard statistical techniques, like maximum likelihood, to density estimation. The <strong>likelihood</strong> of a curve <span class="math inline">\(g\)</span> as density underlying a set of independent identically distributed observations is given by <span class="math display">\[L(g|X_1,\cdots,X_n)=\prod_{i=1}^{n}g(X_i)\]</span>
This likelihood has no finite maximum over the class of all densities. To see this, let <span class="math inline">\(\hat f\)</span> be the naive density estimate with window width $ h$; then, for each <span class="math inline">\(i\)</span>, <span class="math display">\[\hat f_h(X_i)\ge\frac{1}{nh}\]</span> and so <span class="math display">\[\prod \hat f_h(X_i)\ge n^{-n}h^{-n}\to\infty\quad \text{as }h\to 0\]</span></p>
<p>Thus the likelihood can be made arbitrarily large by taking densities approaching the sum of delta functions <span class="math inline">\(w\)</span> as defined in
<span class="math display">\[w(x)=\frac{1}{n}\sum_{i=1}^{n}\delta(x-X_i)\quad (2.7)\]</span> above, and it is not possible to use maximum likelihood directly for density estimation without placing restrictions on the
class of densities over which the likelihood is to be maximized.</p>
<p>There are, nevertheless, possible approaches related to maximum likelihood. One method is to incorporate into the likelihood a
term which describes the roughness - in some sense - of the curve under consideration. Suppose <span class="math inline">\(R(g)\)</span> is a functional which
quantifies the roughness of <span class="math inline">\(g\)</span>. One possible choice of such a functional is <span class="math display">\[R(g)=\int_{-\infty}^{\infty}(g&#39;&#39;)^2\quad (2.11)\]</span></p>
<p>Define the <strong>penalized log likelihood</strong> by <span class="math display">\[l_{\alpha}(g)=\sum_{i=1}^{n}\log(X_i)-\alpha R(g)\quad (2.12)\]</span> where <span class="math inline">\(\alpha\)</span> is a positive smoothing parameter.</p>
<p>The penalized log likelihood can be seen as a way of quantifying the conflict between smoothness and goodness-of-fit to the
data, since the log likelihood term <span class="math inline">\(\sum\log g(X_i)\)</span> measures how well <span class="math inline">\(g\)</span> fits the data. The probability density function <span class="math inline">\(\hat f\)</span> is said to be a <strong>maximum penalized likelihood density estimate</strong> if it maximizes <span class="math inline">\(l_{\alpha}(g)\)</span> over the class of all curves <span class="math inline">\(g\)</span> which satisfy <span class="math inline">\(\int_{-\infty}^{\infty}g =1\)</span>, <span class="math inline">\(g(x)\ge 0\)</span> for all <span class="math inline">\(x\)</span>,and <span class="math inline">\(R(g) &lt; \infty\)</span>. The parameter <span class="math inline">\(\alpha\)</span> controls the amount of smoothing since it determines the <em>rate of exchange</em>
between smoothness and goodness-of-fit; the smaller the value of <span class="math inline">\(\alpha\)</span>, the rougher - in terms of <span class="math inline">\(R(\hat f)\)</span> - will be the corresponding maximum penalized likelihood estimator. Estimates obtained by the maximum penalized likelihood method will, by definition, be probability densities.</p>
</div>
<div id="general-weight-function-estimators" class="section level2">
<h2>2.9. General weight function estimators</h2>
<p>It is possible to define a general class of density estimators which includes several of the estimators discussed above. Suppose
that <span class="math inline">\(w(x, y)\)</span> is a function of two arguments, which in most cases will satisfy the conditions <span class="math display">\[\int_{-\infty}^{\infty}w(x, y)dy=1\quad (2.13)\]</span>
and <span class="math display">\[w(x, y)\ge0\quad \text{for all }x \text{ and }y\quad (2.14)\]</span>
We should think of <span class="math inline">\(w\)</span> as being defined in such a way that most of the weight of the probability density <span class="math inline">\(w(x, \cdot)\)</span> falls near <span class="math inline">\(x\)</span>. An
estimate of the density underlying the data may be obtained by putting <span class="math display">\[\hat f(t)=\frac{1}{n}\sum_{i=1}^{n}w(X_i,t)\quad (2.15)\]</span></p>
<p>We shall refer to estimates of the form (2.15) as <strong>general weight function estimates</strong>. It is clear from (2.15) that the conditions
(2.13) and (2.14) will be sufficient to ensure that <span class="math inline">\(\hat f\)</span> is a probability density function, and that the smoothness properties of <span class="math inline">\(\hat f\)</span> will be inherited from those of the functions
<span class="math inline">\(w(x,\cdot)\)</span>. This class of estimators can be thought of in two ways. Firstly, it is a unifying concept which makes it possible, for example, to obtain theoretical results applicable to a whole range of apparently distinct estimators. On the other hand, it is possible to define useful estimators which do not fall into any of the classes discussed in previous sections but which are nevertheless of the form (2.15).</p>
</div>
</div>
<div id="introuction-1" class="section level1">
<h1>1. INTROUCTION</h1>
<div id="references" class="section level2 unnumbered">
<h2>References</h2>
<div id="refs" class="references csl-bib-body">
<div id="ref-silverman1986density" class="csl-entry">
1. Silverman BW. Density estimation for statistics and data analysis. CRC press; 1986.
</div>
</div>
</div>
</div>

    </div>
  </article>

  


</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
          </li>
          <li>
          </li>
        </ul>
      </footer>

    </div>
    

    
<script src="../../../../js/math-code.js"></script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
  </body>
</html>

