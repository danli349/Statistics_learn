<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.80.0" />


<title>AOS chapter22 Smoothing Using Orthogonal Functions - A Hugo website</title>
<meta property="og:title" content="AOS chapter22 Smoothing Using Orthogonal Functions - A Hugo website">


  <link href='../../../../favicon.ico' rel='icon' type='image/x-icon'/>



  








<link href='//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css' rel='stylesheet' type='text/css' />



<link rel="stylesheet" href="../../../../css/fonts.css" media="all">
<link rel="stylesheet" href="../../../../css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="../../../../" class="nav-logo">
    <img src="../../../../images/logo.png"
         width="50"
         height="50"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="../../../../about/">about</a></li>
    
    <li><a href="../../../../vitae/">CV</a></li>
    
    <li><a href="https://github.com/danli349">GitHub</a></li>
    
    <li><a href="https://scholar.google.com/citations?user=mNjLK8EAAAAJ&amp;hl=en">Googlr Scholar</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">56 min read</span>
    

    <h1 class="article-title">AOS chapter22 Smoothing Using Orthogonal Functions</h1>

    
    <span class="article-date">2021-05-17</span>
    

    <div class="article-content">
      
<script src="../../../../2021/05/17/aos-chapter22-smoothing-using-orthogonal-functions/index_files/header-attrs/header-attrs.js"></script>

<div id="TOC">
<ul>
<li><a href="#smoothing-using-orthogonal-functions">22. Smoothing Using Orthogonal Functions</a>
<ul>
<li><a href="#orthogonal-functions-and-l_2-spaces">22.1 Orthogonal Functions and <span class="math inline">\(L_2\)</span> Spaces</a></li>
<li><a href="#density-estimation">22.2 Density Estimation</a></li>
<li><a href="#regression">22.3 Regression</a></li>
<li><a href="#wavelets">22.4 Wavelets</a></li>
<li><a href="#exercises">22.6 Exercises</a></li>
</ul></li>
<li><a href="#references">References</a></li>
</ul>
</div>

<div id="smoothing-using-orthogonal-functions" class="section level2">
<h2>22. Smoothing Using Orthogonal Functions</h2>
<p>In this Chapter we study a different approach to nonparametric curve estimation based on <strong>orthogonal functions</strong>. We begin with a brief introduction to the theory of orthogonal functions. Then we turn to density estimation and regression.</p>
<div id="orthogonal-functions-and-l_2-spaces" class="section level3">
<h3>22.1 Orthogonal Functions and <span class="math inline">\(L_2\)</span> Spaces</h3>
<p>Let <span class="math inline">\(v = (v_1, v_2, v_3)\)</span> denote a three dimensional vector. Let <span class="math inline">\(\mathcal{V}\)</span> denote the set of all such vectors.</p>
<ul>
<li>If <span class="math inline">\(a\)</span> is a scalar and <span class="math inline">\(v\)</span> is a vector, we define <span class="math inline">\(av = (av_1, av_2, av_3)\)</span>.</li>
<li>The sum of vectors <span class="math inline">\(v\)</span> and <span class="math inline">\(w\)</span> is defined as <span class="math inline">\(v + w = (v_1 + w_1, v_2 + w_2, v_3 + w_3)\)</span>.</li>
<li>The <strong>inner product</strong> between vectors <span class="math inline">\(v\)</span> and <span class="math inline">\(w\)</span> is defined by <span class="math inline">\(\langle v, w \rangle = \sum_i v_i w_i\)</span>.</li>
<li>The <strong>norm</strong> (or <strong>length</strong>) of a vector is defined by</li>
</ul>
<p><span class="math display">\[ \Vert v \Vert = \sqrt{\langle v, v \rangle} = \sqrt{\sum_i v_i^2 }\]</span></p>
<ul>
<li>Two vectors are <strong>orthogonal</strong> (or <strong>perpendicular</strong>) if <span class="math inline">\(\langle v, w \rangle = 0\)</span>.</li>
<li>A set of vectors are orthogonal if each pair in the set is orthogonal.</li>
<li>A vector is <strong>normal</strong> if <span class="math inline">\(\Vert v \Vert = 1\)</span>.</li>
</ul>
<p>Let <span class="math inline">\(\phi_1 = (1, 0, 0)\)</span>, <span class="math inline">\(\phi_2 = (0, 1, 0)\)</span>, <span class="math inline">\(\phi_3 = (0, 0, 1)\)</span>. These vectors are said to be an <strong>orthonormal basis</strong> for <span class="math inline">\(\mathcal{V}\)</span> since they have the following properties:</p>
<ul>
<li>they are orthogonal</li>
<li>they are normal</li>
<li>they form a basis for <span class="math inline">\(\mathcal{V}\)</span>, that is, if <span class="math inline">\(v \in \mathcal{V}\)</span> then <span class="math inline">\(v\)</span> can be written as a linear combination of <span class="math inline">\(\phi_i\)</span>,</li>
</ul>
<p><span class="math display">\[v = \sum_i \beta_i \phi_i
\quad \text{where } \beta_i = \langle \phi_i, v \rangle\]</span></p>
<p>There are other orthonormal basis for <span class="math inline">\(\mathcal{V}\)</span>, for example</p>
<p><span class="math display">\[ \psi_1 = \left( \frac{1}{\sqrt{3}}, \frac{1}{\sqrt{3}} , \frac{1}{\sqrt{3}} \right),
\quad
\psi_2 = \left( \frac{1}{\sqrt{2}}, -\frac{1}{\sqrt{2}} , 0 \right),
\quad
\psi_3 = \left( \frac{1}{\sqrt{6}}, \frac{1}{\sqrt{6}} , -\frac{2}{\sqrt{6}} \right)
\]</span></p>
<p>Now we make the leap from vectors to functions. Basically, we just replace vectors with functions and sums with integrals.</p>
<p>Let <span class="math inline">\(L_2(a, b)\)</span> denote all functions defined on the interval <span class="math inline">\([a, b]\)</span> such that <span class="math inline">\(\int_a^b f(x)^2 dx &lt; \infty\)</span>:</p>
<p><span class="math display">\[ L_2(a, b) = \left\{ f: [a, b] \rightarrow \mathbb{R} , \int_a^b f(x)^2 &lt; \infty \right\} \]</span></p>
<p>We sometimes write <span class="math inline">\(L_2\)</span> instead of <span class="math inline">\(L_2(a, b)\)</span>.</p>
<ul>
<li>The inner product between two functions <span class="math inline">\(f, g \in L_2\)</span> is <span class="math inline">\(\langle f, g \rangle = \int f(x) g(x) dx\)</span>.</li>
<li>The norm of <span class="math inline">\(f\)</span> is</li>
</ul>
<p><span class="math display">\[ \Vert f \Vert = \sqrt{\int f(x)^2 dx} \]</span></p>
<ul>
<li>Two functions are orthogonal if <span class="math inline">\(\langle f, g \rangle = 0\)</span>.</li>
<li>A function is normal if <span class="math inline">\(\Vert f \Vert = 1\)</span>.</li>
<li>A sequence of functions <span class="math inline">\(\phi_1, \phi_2, \phi_3, \dots\)</span> is <strong>orthonormal</strong> if <span class="math inline">\(|| \phi_i || = 1\)</span> for each <span class="math inline">\(i\)</span> and <span class="math inline">\(\langle \phi_i, \phi_j \rangle = 0\)</span> for <span class="math inline">\(i \neq j\)</span>.</li>
<li>An orthonormal sequence is <strong>complete</strong> if the only function that is orthogonal to each <span class="math inline">\(\phi_i\)</span> is the zero function. In that case, the functions <span class="math inline">\(\phi_1, \phi_2, \phi_3, \dots\)</span> form a basis, meaning that if <span class="math inline">\(f \in L_2\)</span> then <span class="math inline">\(f\)</span> can be written as</li>
</ul>
<p><span class="math display">\[ f(x) = \sum_{j=1}^\infty \beta_j \phi_j(x)
\quad \text{where } \beta_j = \int_a^b f(x) \phi_j(x) dx \]</span></p>
<p><strong>Parseval’s relation</strong> says that</p>
<p><span class="math display">\[ \Vert f \Vert^2 \equiv \int f^2(x) dx = \sum_{j=1}^\infty \beta_j^2 \equiv \Vert \beta \Vert^2\]</span></p>
<p>The <strong>cosine basis</strong> is defined as follows: Let <span class="math inline">\(\phi_1(x) = 1\)</span> and for <span class="math inline">\(j \geq 2\)</span> define</p>
<p><span class="math display">\[\phi_j(x) = \sqrt{2} \cos \left( (j - 1) \pi x\right)\]</span></p>
<p>The first ten functions are plotted below.</p>
<pre class="python"><code>import numpy as np

def cosine_basis(j):
    assert j &gt;= 1
    
    def f(x):
        if j == 1:
            return np.ones_like(x)   
        return np.sqrt(2) * np.cos((j - 1) * np.pi * x)
    
    return f</code></pre>
<pre class="python"><code>import matplotlib.pyplot as plt
%matplotlib inline

step = 1e-3
xx = np.arange(0, 1 + step, step=step)
plt.figure(figsize=(12, 12))
for i in range(1, 11):
    
    # Set up the plot
    ax = plt.subplot(5, 2, i)
    ax.plot(xx, cosine_basis(i)(xx))
    ax.set_title(&#39;j = %i&#39; % i)

plt.tight_layout()
plt.show()</code></pre>
<div class="figure">
<img src="Chapter%2022%20-%20Smoothing%20Using%20Orthogonal%20Functions_files/Chapter%2022%20-%20Smoothing%20Using%20Orthogonal%20Functions_11_0.png" alt="" />
<p class="caption">png</p>
</div>
<p>The <strong>Legendre polynomials</strong> on <span class="math inline">\([-1, 1]\)</span> are defined by</p>
<p><span class="math display">\[ P_j(x) = \frac{1}{2^j j!} \frac{d^j}{dx^j} (x^2 - 1)^j, \quad j = 0, 1, 2, \dots \]</span></p>
<p>It can be shown that these functions are complete and orthogonal, and that</p>
<p><span class="math display">\[ \int_{-1}^1 P_j^2(x) dx = \frac{2}{2j + 1}\]</span></p>
<p>It follows that the functions</p>
<p><span class="math display">\[ \phi_j(x) = \sqrt{\frac{2j + 1}{2}}P_j(x), \quad j = 0, 1, 2, \dots \]</span></p>
<p>form an orthonormal basis for <span class="math inline">\(L_2[-1, 1]\)</span>. The first few Legendre polynomials are</p>
<p><span class="math display">\[
\begin{align}
P_0(x) &amp;= 1 \\
P_1(x) &amp;= x \\
P_2(x) &amp;= \frac{1}{2}\left( 3x^2 - 1 \right) \\
P_3(x) &amp;= \frac{1}{2}\left( 5x^3 - 3x \right)
\end{align}
\]</span></p>
<p>These polynomials may be constructed explicitly using the following recursive relation:</p>
<p><span class="math display">\[ P_{j+1}(x) = \frac{(2j + 1) x P_j(x) - j P_{j - 1}(x)}{j + 1} \]</span></p>
<pre class="python"><code>import sympy
from sympy.abc import x
from functools import lru_cache

@lru_cache(maxsize=None)
def legendre_polynomial(j):
    if j == 0:
        return 1
    if j == 1:
        return x
    
    return sympy.expand(((2*j - 1) * x * legendre_polynomial(j - 1) - (j - 1) * legendre_polynomial(j - 2)) / j)

def legendre_basis(j):
    if j == 0:
        return lambda x: np.sqrt(1/2) * np.ones_like(x)
    
    pj = legendre_polynomial(j)
    return sympy.lambdify(x, sympy.sqrt((2*j + 1) / 2) * pj, &quot;numpy&quot;)</code></pre>
<pre class="python"><code>import matplotlib.pyplot as plt
%matplotlib inline

step = 1e-3
xx = np.arange(-1, 1 + step, step=step)
plt.figure(figsize=(12, 12))
for i in range(0, 10):
    
    # Set up the plot
    ax = plt.subplot(5, 2, i + 1)
    ax.plot(xx, legendre_basis(i)(xx))
    ax.set_title(&#39;j = %i&#39; % i)

plt.tight_layout()
plt.show()</code></pre>
<div class="figure">
<img src="Chapter%2022%20-%20Smoothing%20Using%20Orthogonal%20Functions_files/Chapter%2022%20-%20Smoothing%20Using%20Orthogonal%20Functions_14_0.png" alt="" />
<p class="caption">png</p>
</div>
<p>The coefficients <span class="math inline">\(\beta_1, \beta_2, \dots\)</span> are related to the smoothness of the function <span class="math inline">\(f\)</span>. To see why note that, if <span class="math inline">\(f\)</span> is smooth, then its derivative will be finite. Thus we expect that, for some <span class="math inline">\(k\)</span>, <span class="math inline">\(\int_0^1 (f^{(k)}(x))^2 dx &lt; \infty\)</span>, where <span class="math inline">\(f^{(k)}\)</span> is the <span class="math inline">\(k\)</span>-th derivative of <span class="math inline">\(f\)</span>.</p>
<p>Now consider the cosine basis and let <span class="math display">\[f(x) = \sum_{j=1}^\infty \beta_j \phi_j(x)=\sum_{j=1}^\infty \beta_j\sqrt{2} \cos \left( (j - 1) \pi x\right)\]</span> Then,</p>
<p><span class="math display">\[ \int_0^1 (f^{(k)}(x))^2 dx \le 2 \sum_{j=1}^\infty \beta_j^2 ( \pi (j - 1) ) ^{2k} \]</span></p>
<p>The only way the sum can be finite is if the <span class="math inline">\(\beta_j\)</span>’s get small when <span class="math inline">\(j\)</span> gets large. To summarize:</p>
<p><strong>If the function <span class="math inline">\(f\)</span> is smooth then the coefficients <span class="math inline">\(\beta_j\)</span> will be small when <span class="math inline">\(j\)</span> is large.</strong></p>
<p>For the rest of this chapter, we will assume we are using the cosine basis unless otherwise specified.</p>
</div>
<div id="density-estimation" class="section level3">
<h3>22.2 Density Estimation</h3>
<p>Let <span class="math inline">\(X_1, \dots, X_n\)</span> be IID observations from a distribution on <span class="math inline">\([0, 1]\)</span> with density <span class="math inline">\(f\)</span>. Assuming <span class="math inline">\(f \in L_2\)</span> we can write</p>
<p><span class="math display">\[ f(x) = \sum_{j=1}^\infty \beta_j \phi_j(x) \]</span></p>
<p>where <span class="math inline">\(\phi_i\)</span>s form an orthonormal basis. Define</p>
<p><span class="math display">\[ \hat{\beta}_j = \frac{1}{n} \sum_{i=1}^n \phi_j(X_i)\]</span></p>
<p><strong>Theorem 22.4</strong>. The mean and variance of the <span class="math inline">\(\hat{\beta}_j\)</span> are</p>
<p><span class="math display">\[
\mathbb{E}(\hat{\beta}_j) = \beta_j
\quad \text{and} \quad
\mathbb{V}(\hat{\beta}_j) = \frac{\sigma_j^2}{n}
\]</span></p>
<p>where</p>
<p><span class="math display">\[ \sigma_j^2 = \mathbb{V}(\phi_j(X_i)) = \int \left( \phi_j(x) - \beta_j\right)^2f(x) dx\]</span></p>
<p><strong>Proof</strong>. We have</p>
<p><span class="math display">\[ \mathbb{E}(\hat{\beta}_j) = \frac{1}{n} \sum_{i=1}^n \mathbb{E}(\phi_j(X_i)) = \mathbb{E}(\phi_j(X_1))  = \int \phi_j(x) f(x) dx = \beta_j\]</span></p>
<p>The calculation for variance is similar:</p>
<p><span class="math display">\[ \mathbb{V}(\hat{\beta}_j) = \frac{1}{n^2} \sum_{i=1}^n \mathbb{V}(\phi_j(X_i)) = \frac{1}{n} \mathbb{V}(\phi_j(X_1)) = \frac{1}{n} \int \left( \phi_j(x) - \beta_j\right)^2f(x) dx \]</span></p>
<p>Hence, <span class="math inline">\(\hat{\beta}_j\)</span> is an unbiased estimate of <span class="math inline">\(\beta_j\)</span>. It is tempting to estimate <span class="math inline">\(f\)</span> by <span class="math inline">\(\sum_{i=1}^\infty \hat{\beta}_j \phi_j(x)\)</span> but it turns out to have a very high variance. Instead, consider the estimator</p>
<p><span class="math display">\[ \hat{f}(x) = \sum_{i=1}^J \hat{\beta}_j \phi_j(x) \]</span></p>
<p>The number of terms <span class="math inline">\(J\)</span> is a smoothing parameter. Increasing <span class="math inline">\(J\)</span> will decrease bias while increasing variance. For technical reasons, we restrict <span class="math inline">\(J\)</span> to lie in the range <span class="math inline">\(1 \leq J \leq p\)</span> where <span class="math inline">\(p = p(n) = \sqrt{n}\)</span>. To emphasize the dependence of the risk function on <span class="math inline">\(J\)</span>, we write the risk function as <span class="math inline">\(R(J)\)</span>.</p>
<p><strong>Theorem 22.5</strong>. The risk of <span class="math inline">\(\hat{f}\)</span> is given by</p>
<p><span class="math display">\[ R(J) = \sum_{j=1}^J \frac{\sigma_j^2}{J} + \sum_{j=J+1}^\infty \beta_j^2 \]</span></p>
<p>In kernel estimation, we used cross-validation to estimate the risk. In the orthogonal function approach, we instead use the risk estimator</p>
<p><span class="math display">\[ \hat{R}(J) = \sum_{j=1}^J \frac{\hat{\sigma}_j^2}{n} + \sum_{j=J+1}^p \left( \hat{\beta}_j^2 - \frac{\hat{\sigma}_j^2}{n} \right)_{+}\]</span></p>
<p>where <span class="math inline">\(a_{+} = \max \{ a, 0 \}\)</span> and</p>
<p><span class="math display">\[ \hat{\sigma}_j^2 = \frac{1}{n - 1} \sum_{i=1}^n \left( \phi_j(X_i) - \hat{\beta}_j\right)^2 \]</span></p>
<p>To motivate this estimator, note that <span class="math inline">\(\hat{\sigma}_j^2\)</span> is an unbiased estimator of <span class="math inline">\(\sigma^2\)</span> and <span class="math inline">\(\hat{\beta}_j^2 - \hat{\sigma}_j^2\)</span> is an unbiased estimator of <span class="math inline">\(\beta_j^2\)</span>. We take the positive part of the later term since we know <span class="math inline">\(\beta_j^2\)</span> cannot be negative. We now choose <span class="math inline">\(1 \leq \hat{J} \leq p\)</span> to minimize the risk estimator <span class="math inline">\(\hat{R}(\hat f, f)\)</span>.</p>
<p><strong>Summary of Orthogonal Function Density Estimation</strong></p>
<ol style="list-style-type: decimal">
<li>Let</li>
</ol>
<p><span class="math display">\[ \hat{\beta}_j = \frac{1}{n} \sum_{i=1}^n \phi_j(X_i) \]</span></p>
<ol start="2" style="list-style-type: decimal">
<li>Choose <span class="math inline">\(\hat{J}\)</span> to minimize <span class="math inline">\(\hat{R}(J)\)</span> over <span class="math inline">\(1 \leq J \leq p = \sqrt{n}\)</span> where</li>
</ol>
<p><span class="math display">\[ \hat{R}(J) = \sum_{j=1}^J \frac{\hat{\sigma}_j^2}{n} + \sum_{j=J+1}^p \left( \hat{\beta}_j^2 - \frac{\hat{\sigma}_j^2}{n} \right)_{+}\]</span></p>
<p>and</p>
<p><span class="math display">\[ \hat{\sigma}_j^2 = \frac{1}{n - 1} \sum_{i=1}^n \left( \phi_j(X_i) - \hat{\beta}_j\right)^2 \]</span></p>
<ol start="3" style="list-style-type: decimal">
<li>Let</li>
</ol>
<p><span class="math display">\[ \hat{f}(x) = \sum_{j=1}^\hat{J} \hat{\beta}_j \phi_j(x) \]</span></p>
<p>The estimator <span class="math inline">\(\hat{f}_n\)</span> can be negative. If we are interested in estimating the shape of <span class="math inline">\(f\)</span>, this is not a problem. However, if we need the estimate to be a probability density function, we can truncate the estimate and then normalize it:<br />
<span class="math display">\[\hat{f}^*(x) = \frac{\max \{ \hat{f}_n(x), 0 \}}{\int_0^1 \max \{ \hat{f}_n(u), 0 \} du}\]</span></p>
<p>Now, let us construct a confidence band for <span class="math inline">\(f\)</span>. Suppose we estimate <span class="math inline">\(f\)</span> using <span class="math inline">\(J\)</span> orthogonal functions. We are essentially estimating <span class="math inline">\(\tilde{f}(x) = \sum_{j=1}^J \beta_j \phi_j(x)\)</span> instead of the true density <span class="math inline">\(f(x) = \sum_{j=1}^\infty \beta_j \phi_j(x)\)</span>. Thus the confidence band should be regarded as a band for <span class="math inline">\(\tilde{f}(x)\)</span>.</p>
<p><strong>Theorem 22.6</strong>. An approximate <span class="math inline">\(1 - \alpha\)</span> confidence band for <span class="math inline">\(\tilde{f}\)</span> is <span class="math inline">\((\ell(x), u(x))\)</span> where</p>
<p><span class="math display">\[
\ell(x) = \hat{f}_n(x) - c
\quad \text{and} \quad
u(x) = \hat{f}_n(x) + x
\]</span></p>
<p>where</p>
<p><span class="math display">\[ c = \frac{JK^2}{\sqrt{n}} \sqrt{1 + \frac{\sqrt{2} z_{\alpha}}{\sqrt{J}}} \]</span></p>
<p>and</p>
<p><span class="math display">\[ K = \max_{1 \leq j \leq J} \max_x | \phi_j(x) | \]</span></p>
<p>For the cosine basis, <span class="math inline">\(K = \sqrt{2}\)</span>.</p>
</div>
<div id="regression" class="section level3">
<h3>22.3 Regression</h3>
<p>Consider the regression model</p>
<p><span class="math display">\[ Y_i = r(x_i) + \epsilon_i, \quad i = 1, \dots, n\]</span></p>
<p>where <span class="math inline">\(\epsilon_i\)</span> are independent with mean 0 and variance <span class="math inline">\(\sigma^2\)</span>. We will initially focus on the special case where <span class="math inline">\(x_i = i / n\)</span>. We assume that <span class="math inline">\(r \in L_2[0, 1]\)</span> and hence we can write</p>
<p><span class="math display">\[
r(x) = \sum_{j=1}^\infty \beta_j \phi_j(x)
\quad \text{where } \beta_j = \int_0^1 r(x) \phi_j(x) dx
\]</span></p>
<p>where <span class="math inline">\(\phi_1, \phi_2, \dots\)</span> is an orthonormal basis for <span class="math inline">\([0, 1]\)</span>.</p>
<p>Define</p>
<p><span class="math display">\[ \hat{\beta}_j = \frac{1}{n} \sum_{i=1}^n Y_i \phi_j(x), 
\quad j = 1, 2, \dots\]</span></p>
<p>Since <span class="math inline">\(\hat{\beta}_j\)</span> is an average, the central limit theorem tells us that <span class="math inline">\(\hat{\beta}_j\)</span> will be approximately normally distributed.</p>
<p><strong>Theorem 22.8</strong>.</p>
<p><span class="math display">\[ \hat{\beta}_j \approx N \left( \beta_j, \frac{\sigma^2}{n} \right) \]</span></p>
<p><strong>Proof</strong>. The mean of <span class="math inline">\(\hat{\beta}_j\)</span> is</p>
<p><span class="math display">\[ \mathbb{E}(\hat{\beta}_j) = \frac{1}{n} \sum_{i=1}^n \mathbb{E}(Y_i) \phi_j(x_i) = \frac{1}{n} \sum_{i=1}^n r(x_i) \phi_j(x_i) \approx \int r(x) \phi_j(x) dx = \beta_j\]</span></p>
<p>where the approximate equality follows from the definition of a Riemann integral: <span class="math inline">\(\sum_i h(x_i) / n \rightarrow \int_0^1 h(x) dx\)</span>.</p>
<p>The variance is</p>
<p><span class="math display">\[ \mathbb{V}(\hat{\beta}_j) = \frac{1}{n^2} \sum_{i=1}^n \mathbb{V}(Y_i) \phi_j(x_i) = \frac{\sigma^2}{n} \left( \frac{1}{n} \sum_{i=1}^n \phi_j^2(x_i) \right) \approx \frac{\sigma^2}{n} \left( \int \phi_j^2(x) dx \right) = \frac{\sigma^2}{n}\]</span></p>
<p>since <span class="math inline">\(\phi_j\)</span> has norm 1.</p>
<p>As we did for density estimation, we will estimate <span class="math inline">\(r\)</span> by</p>
<p><span class="math display">\[ \hat{r}(x) = \sum_{j=1}^J \hat{\beta}_j \phi_j(x) \]</span></p>
<p>Let</p>
<p><span class="math display">\[ R(J) = \mathbb{E} \int (r(x) - \hat{r}(x))^2 dx \]</span></p>
<p>be the risk of the estimator.</p>
<p><strong>Theorem 22.9</strong>. The risk <span class="math inline">\(R(J)\)</span> of the estimator $ (x) = _{j=1}^J _j _j(x) $ is</p>
<p><span class="math display">\[ R(J) = \frac{J \sigma^2}{n} + \sum_{j=J+1}^\infty \beta_j^2 \]</span></p>
<p>To motivate the estimator <span class="math inline">\(\sigma^2 = \mathbb{V}(\epsilon_i)\)</span> we use</p>
<p><span class="math display">\[ \hat{\sigma}^2 = \frac{n}{k} \sum_{i=n - k + 1}^n \hat{\beta}_j^2 \]</span></p>
<p>where <span class="math inline">\(k = n / 4\)</span>. To motivate this estimator, recall that if <span class="math inline">\(f\)</span> is smooth then <span class="math inline">\(\beta_j \approx 0\)</span> for large <span class="math inline">\(j\)</span>. So, for <span class="math inline">\(j \geq k\)</span>, <span class="math inline">\(\hat{\beta}_j \approx N(0, \sigma^2 / n)\)</span>. So <span class="math inline">\(\hat{\beta}_j \approx \sigma Z_j / \sqrt{n}\)</span> for <span class="math inline">\(j \geq k\)</span>, where <span class="math inline">\(Z_j \sim N(0, 1)\)</span>. Therefore,</p>
<p><span class="math display">\[ \hat{\sigma}^2 = \frac{n}{k} \sum_{i=n-k+1}^k \hat{\beta}_j^2 \approx \frac{n}{k} \sum_{i=n-k+1}^k \left( \frac{\sigma}{\sqrt{n}} Z_j \right)^2 = \frac{\sigma^2}{k} \sum_{i=n-k+1}^k Z_j^2 = \frac{\sigma^2}{k} \chi_k^2\]</span></p>
<p>since a sum of <span class="math inline">\(k\)</span> squares of independent standard normals has a <span class="math inline">\(\chi_k^2\)</span> distribution. Now <span class="math inline">\(\mathbb{E}(\chi_k^2) = k\)</span>, leading to <span class="math inline">\(\mathbb{E}(\hat{\sigma}^2) \approx \sigma^2\)</span>. Also, <span class="math inline">\(\mathbb{V}(\chi_k^2) = 2k\)</span> and so <span class="math inline">\(\mathbb{V}(\hat{\sigma}^2) \approx (\sigma^4 / k^2) 2k = 2\sigma^4 / k \rightarrow 0\)</span> as <span class="math inline">\(n \rightarrow \infty\)</span>. Thus we expect <span class="math inline">\(\hat{\sigma}^2\)</span> to be a consistent estimator of <span class="math inline">\(\sigma^2\)</span>.</p>
<p>There is nothing special about the choice of <span class="math inline">\(k = n / 4\)</span>; any <span class="math inline">\(k\)</span> that increases with <span class="math inline">\(n\)</span> at an appropriate rate would suffice.</p>
<p>We estimate the risk with</p>
<p><span class="math display">\[ \hat{R}(J) = J \frac{\hat{\sigma}^2}{n} + \sum_{j=J+1}^n \left(\hat{\beta}_j^2 - \frac{\hat{\sigma}^2}{n} \right)_{+} \]</span></p>
<p>We are now ready to give a complete description of the method which Beran (2000) calls REACT (Risk Estimation and Adaptation by Coordinate Transformation).</p>
<p><strong>Orthogonal Series Regression Estimator</strong></p>
<ol style="list-style-type: decimal">
<li>Let</li>
</ol>
<p><span class="math display">\[ \hat{\beta}_j = \frac{1}{n} \sum_{i=1}^n Y_i \phi_i(x_i), \quad j = 1, \dots, n\]</span></p>
<ol start="2" style="list-style-type: decimal">
<li>Let</li>
</ol>
<p><span class="math display">\[ \hat{\sigma}^2 = \frac{n}{k} \sum_{i=n-k+1}^n \hat{\beta}_j^2 \]</span></p>
<p>where <span class="math inline">\(k \approx n / 4\)</span>.</p>
<ol start="3" style="list-style-type: decimal">
<li>For <span class="math inline">\(1 \leq J \leq n\)</span>, compute the risk estimate</li>
</ol>
<p><span class="math display">\[ \hat{R}(J) = J \frac{\hat{\sigma}^2}{n} + \sum_{j=J+1}^n \left(\hat{\beta}_j^2 - \frac{\hat{\sigma}^2}{n} \right)_{+} \]</span></p>
<ol start="4" style="list-style-type: decimal">
<li><p>Choose <span class="math inline">\(\hat{J} \in \{1, \dots, n \}\)</span> to minimize <span class="math inline">\(\hat{R}(J)\)</span>.</p></li>
<li><p>Let</p></li>
</ol>
<p><span class="math display">\[ \hat{f}(x) = \sum_{j=1}^J \hat{\beta}_j \phi_j(x) \]</span></p>
<p>Finally, we turn to confidence bands. As before, these bands are not for the true regression function <span class="math inline">\(r(x)\)</span>, but for the smoothed version of the function <span class="math inline">\(\tilde{r}(x) = \sum_{i=1}^\tilde{J} \beta_j \phi_j(x)\)</span>.</p>
<p><strong>Theorem 22.11</strong>. Suppose the estimate <span class="math inline">\(\hat{r}\)</span> is based on <span class="math inline">\(J\)</span> terms and $^2 =  _{i=n-k+1}^n _j^2 $. Assume that <span class="math inline">\(J &lt; n - k + 1\)</span>. An approximate <span class="math inline">\(1 - \alpha\)</span> confidence band for <span class="math inline">\(\tilde{r}\)</span> is <span class="math inline">\((\ell, u)\)</span>, where</p>
<p><span class="math display">\[ \ell(x) = \hat{r}(x) - K \sqrt{J} \sqrt{\frac{z_{\alpha} \hat{\tau}}{\sqrt{n}} + \frac{J \hat{\sigma}^2}{n}}
\quad \text{and} \quad u(x) = \hat{r}(x) + K \sqrt{J} \sqrt{\frac{z_{\alpha} \hat{\tau}}{\sqrt{n}} + \frac{J \hat{\sigma}^2}{n}} \]</span></p>
<p>where</p>
<p><span class="math display">\[ K = \max_{1 \leq j \leq J} \max_x | \phi_j(x) | \]</span></p>
<p>and</p>
<p><span class="math display">\[ \hat{\tau}^2 = \frac{2 J \hat{\sigma}^4}{n} \left( 1 + \frac{J}{k} \right) \]</span></p>
<p>and <span class="math inline">\(k = n / 4\)</span> as used in the definition of <span class="math inline">\(\hat{\sigma}^2\)</span>. In the cosine basis, <span class="math inline">\(K = \sqrt{2}\)</span>.</p>
<p>So far, we have assumed that the <span class="math inline">\(x_i\)</span> are of the form <span class="math inline">\(\{1/n, 2/n, \dots, 1\}\)</span>. If <span class="math inline">\(x_i \in [a, b]\)</span> then we can rescale them to be in <span class="math inline">\([0, 1]\)</span>. If the <span class="math inline">\(x_i\)</span>’s are not equally spaced the methods discussed here apply as long as they “fill out” the interval in a way as to not be too clumped together. If we want to treat the <span class="math inline">\(x_i\)</span>’s as random instead of fixed, then the method needs significant modifications which will not be treated here.</p>
</div>
<div id="wavelets" class="section level3">
<h3>22.4 Wavelets</h3>
<p>Suppose there is a sharp jump in a regression function <span class="math inline">\(f\)</span> at some point <span class="math inline">\(x\)</span> but that <span class="math inline">\(f\)</span> is otherwise very smooth. Such as function is said to be <strong>spatially inhomogeneous</strong>.</p>
<pre class="python"><code>import numpy as np
from scipy.stats import norm
import matplotlib.pyplot as plt
%matplotlib inline

xx = np.arange(0, 1, step=1e-3)
plt.figure(figsize=(12, 8))
plt.plot(xx, norm.pdf(xx, loc=0.65, scale=0.5) + 1e-2 * norm.pdf(xx, loc=0.45, scale=0.01))
plt.show()</code></pre>
<div class="figure">
<img src="Chapter%2022%20-%20Smoothing%20Using%20Orthogonal%20Functions_files/Chapter%2022%20-%20Smoothing%20Using%20Orthogonal%20Functions_42_0.png" alt="" />
<p class="caption">png</p>
</div>
<p>It’s hard to estimate <span class="math inline">\(f\)</span> using the methods we discussed so far. If we use a cosine basis and only keep the low order terms, we will miss the peak; if we allow higher order terms we will find the peak but we will make the rest of the curve very wiggly. Similar comments apply to kernel regression: if we use a large bandwidth, then we will smooth out the peak; if we use a small bandwidth, then we will find the peak but we will make the rest of the curve very wiggly.</p>
<p>One way to estimate inhomogeneous functions is to use a more carefully chosen basis that allows us to place a “blip” in some small region without adding wiggles elsewhere. In this section, we describe a special class of bases called <strong>wavelets</strong> that are aimed at fixing this problem. Statistical inference using wavelets is a large and active area. We will just discuss a few ideas to get a flavor of this approach.</p>
<p>The <strong>father Haar wavelet</strong> of <strong>Haar scaling function</strong> is defined by</p>
<p><span class="math display">\[ \phi(x) = \begin{cases}
1 &amp; \text{if } 0 \leq x &lt; 1 \\
0 &amp; \text{otherwise}
\end{cases} \]</span></p>
<p>The <strong>mother Haar wavelet</strong> is defined by</p>
<p><span class="math display">\[
\psi(x) = \begin{cases}
-1 &amp; \text{if } 0 \leq x \leq \frac{1}{2} \\
1  &amp; \text{if } \frac{1}{2} &lt; x \leq 1
\end{cases}
\]</span></p>
<p>For any integers <span class="math inline">\(j\)</span> and <span class="math inline">\(k\)</span> define</p>
<p><span class="math display">\[ \phi_{j, k}(x) = 2^{j/2} \phi(2^j x - k) 
\quad \text{and} \quad
\psi_{j, k}(x) = 2^{j/2} \psi(2^j x - k)
\]</span></p>
<p>The function <span class="math inline">\(\psi_{j, k}\)</span> has the same shape as <span class="math inline">\(\psi\)</span> but it has been rescaled by a factor of <span class="math inline">\(2^{j/2}\)</span> and shifted by a factor of <span class="math inline">\(k\)</span>.</p>
<pre class="python"><code>import numpy as np

def haar_father_wavelet(x):
    return np.where((x &gt;= 0) &amp; (x &lt; 1), 1, 0)

def haar_mother_wavelet(x):
    return np.where((x &gt;= 0) &amp; (x &lt; 1),  np.where(x &lt;= 1/2, -1, 1), 0)

def phi_wavelet(j, k):
    def f(x):
        return 2**(j / 2) * haar_father_wavelet((2**j)*x - k)
    return f

def psi_wavelet(j, k):
    def f(x):
        return 2**(j / 2) * haar_mother_wavelet((2**j)*x - k)
    return f</code></pre>
<pre class="python"><code>import matplotlib.pyplot as plt
%matplotlib inline

step = 1e-4
xx = np.arange(-1, 2, step=step)
plt.figure(figsize=(12, 12))

ax = plt.subplot(3, 2, 1)
ax.plot(xx, haar_father_wavelet(xx))
ax.set_title(r&#39;Father wavelet $\phi$&#39;)
    
ax = plt.subplot(3, 2, 2)
ax.plot(xx, haar_mother_wavelet(xx))
ax.set_title(r&#39;Mother wavelet $\psi$&#39;)

ax = plt.subplot(3, 2, 3)
ax.plot(xx, phi_wavelet(2, 2)(xx))
ax.set_title(r&#39;$\phi_{2, 2}$&#39;)

ax = plt.subplot(3, 2, 4)
ax.plot(xx, phi_wavelet(4, 2)(xx))
ax.set_title(r&#39;$\phi_{4, 2}$&#39;)

ax = plt.subplot(3, 2, 5)
ax.plot(xx, psi_wavelet(4, 10)(xx))
ax.set_title(r&#39;$\psi_{4, 10}$&#39;)

ax = plt.subplot(3, 2, 6)
ax.plot(xx, psi_wavelet(6, 10)(xx))
ax.set_title(r&#39;$\psi_{6, 10}$&#39;)

plt.tight_layout()
plt.show()</code></pre>
<div class="figure">
<img src="Chapter%2022%20-%20Smoothing%20Using%20Orthogonal%20Functions_files/Chapter%2022%20-%20Smoothing%20Using%20Orthogonal%20Functions_47_0.png" alt="" />
<p class="caption">png</p>
</div>
<p>Notice that for large <span class="math inline">\(j\)</span>, <span class="math inline">\(\psi_{j, k}\)</span> is a very localized function. This makes it possible to add a blip to a function without adding wiggles elsewhere. In technical terms, we say that wavelets provide a <strong>multiresolution analysis</strong> of <span class="math inline">\(L_2(0, 1)\)</span>.</p>
<p>Let</p>
<p><span class="math display">\[ W_j = \{\psi_{j, k}, \; k = 0, 1, \dots, 2^j - 1\} \]</span>
where <span class="math display">\[\psi_{j, k}(x) = 2^{j/2} \psi(2^j x - k)\]</span></p>
<p>be the set of rescaled and shifted mother wavelets at resolution <span class="math inline">\(j\)</span>.</p>
<p><strong>Theorem 22.13</strong>. The set of functions</p>
<p><span class="math display">\[ \left\{ \phi, W_0, W_1, W_2, \dots \right\} \]</span></p>
<p>is an orthonormal basis for <span class="math inline">\(L_2(0, 1)\)</span>.</p>
<p>It follows from this theorem that we can expand any function <span class="math inline">\(f \in L_2(0, 1)\)</span> in this basis. Because each <span class="math inline">\(W_j\)</span> is itself a set of functions, we write the expansion as a double sum:</p>
<p><span class="math display">\[ f(x) = \alpha \phi(x) + \sum_{j=0}^\infty \sum_{k=0}^{2^j - 1} \beta_{j, k} \psi_{j, k}(x) \]</span></p>
<p>where</p>
<p><span class="math display">\[ \alpha = \int_0^1 f(x) \phi(x) dx, \quad \beta_{j, k} = \int_0^1 f(x) \psi_{j, k}(x) dx\]</span></p>
<p>We call <span class="math inline">\(\alpha\)</span> the <strong>scaling coefficient</strong> and <span class="math inline">\(\beta_{j, k}\)</span> the <strong>detail coefficients</strong>. We call the finite sum</p>
<p><span class="math display">\[ \tilde{f}(x) = \alpha \phi(x) + \sum_{j=0}^{J - 1} \sum_{k=0}^{2^j - 1} \beta_{j, k}\psi_{j, k}(x)\]</span></p>
<p>the <strong>resolution <span class="math inline">\(J\)</span></strong> approximation to <span class="math inline">\(f\)</span>. The total number of terms in this sum is</p>
<p><span class="math display">\[ 1 + \sum_{j=0}^{J - 1}2^j = 2^J\]</span></p>
<p>Haar wavelets are localized, meaning they are zero outside of an interval, but they are not smooth. In 1988, Ingrid Daubechie showed that such wavelets do exist. They can be constructed numerically, but there is no closed form formula for smoother wavelets. To keep things simple, we will continue to use Haar wavelets.</p>
<p>We can now use wavelets to do density estimation and regression. We shall only discuss the regression problem <span class="math inline">\(Y_i = r(x_i) + \sigma \epsilon_i\)</span> where <span class="math inline">\(\epsilon_i \sim N(0, 1)\)</span> and <span class="math inline">\(x_i = i / n\)</span>. To simplify this discussion we assume that <span class="math inline">\(n = 2^J\)</span> for some <span class="math inline">\(J\)</span>.</p>
<p>There is one major difference between estimation using wavelets instead of a cosine (or polynomial) basis. With the cosine basis, we used all terms <span class="math inline">\(1 \leq j \leq J\)</span> for some <span class="math inline">\(J\)</span>. With wavelets, we use a method called <strong>thresholding</strong> where we keep a term in the function approximation only if its coefficient is large. The simplest version is called <strong>hard, universal thresholding</strong>.</p>
<p><strong>Haar Wavelet Regression</strong></p>
<ol style="list-style-type: decimal">
<li>Let <span class="math inline">\(J = \log_2 n\)</span> and define</li>
</ol>
<p><span class="math display">\[ \hat{\alpha} = \frac{1}{n} \sum_i \phi(x_i) Y_i
\quad \text{and} \quad
D_{j, k} = \frac{1}{n} \sum_i \psi_{j, k}(x_i) Y_i
\]</span></p>
<p>for <span class="math inline">\(0 \leq j \leq J - 1\)</span></p>
<ol start="2" style="list-style-type: decimal">
<li>Estimate <span class="math inline">\(\sigma\)</span> by</li>
</ol>
<p><span class="math display">\[ \hat{\sigma} = \sqrt{n} \; \times \; \frac{\text{median} \left( \left| D_{J-1, k} : k = 0, \dots, 2^{J - 1} - 1\right| \right)}{0.6745} \]</span></p>
<ol start="3" style="list-style-type: decimal">
<li>Apply universal thresholding:</li>
</ol>
<p><span class="math display">\[ \hat{\beta}_{j, k} = \begin{cases}
D_{j, k} &amp; \text{if } \left| D_{j, k} \right| &gt; \hat{\sigma} \sqrt{\frac{2 \log n}{n}} \\
0 &amp; \text{otherwise}
\end{cases}\]</span></p>
<ol start="4" style="list-style-type: decimal">
<li>Set</li>
</ol>
<p><span class="math display">\[ \hat{f}(x) = \hat{\alpha} \phi(x) + \sum_{j = j_0}^{J - 1} \sum_{k = 0}^{2^j - 1} \hat{\beta}_{j, k} \psi_{j, k}(x) \]</span></p>
<p>In practice, we do not compute <span class="math inline">\(\hat{\alpha}\)</span> and <span class="math inline">\(D_{j, k}\)</span>. Instead, we use the <strong>discrete wavelet transform (DWT)</strong> which is very fast. For Haar wavelets, the DWT works as follows.</p>
<p><strong>DWT for Haar Wavelets</strong></p>
<ul>
<li>Let <span class="math inline">\(y = (Y_1, \dots, Y_n)\)</span> and let <span class="math inline">\(J = \log_2 n\)</span>.<br />
</li>
<li>Create a list <span class="math inline">\(D\)</span> with elements <span class="math inline">\(D[0], ..., D[J - 1]\)</span></li>
<li>Do:</li>
</ul>
<pre><code>temp &lt;- y / sqrt(n)
for j in (J - 1):0 {
  m &lt;- 2^j
  I &lt;- (1:m)
  D[j] &lt;- (temp[2 * I] - temp[(2 * I) - 1]) / sqrt(2)
  temp &lt;- (temp[2 * I] + temp[(2 * I) - 1]) / sqrt(2)
}</code></pre>
<p>The estimate used for <span class="math inline">\(\sigma\)</span> probably looks strange. It is similar to the estimate used for the cosine basis but it is designed to be insensitive to sharp peaks in the function.</p>
<p>To understand the intuition behind universal thresholding, consider what happens when there is no signal, that is, when <span class="math inline">\(\beta_{j, k} = 0\)</span> for all <span class="math inline">\(j, k\)</span>.</p>
<p><strong>Theorem 22.16</strong>. Suppose that <span class="math inline">\(\beta_{j, k} = 0\)</span> for all <span class="math inline">\(j, k\)</span> and let <span class="math inline">\(\hat{\beta}_{j, k}\)</span> be the universal threshold estimator. Then</p>
<p><span class="math display">\[ \mathbb{P}\left(\hat{\beta}_{j, k} = 0 \; \text{for all } j, k \right) \rightarrow 1 \]</span></p>
<p>as <span class="math inline">\(n \rightarrow \infty\)</span>.</p>
<p><strong>Proof</strong> Assume <span class="math inline">\(\sigma\)</span> is known. Now <span class="math inline">\(D_{j, k} \approx N(0, \sigma^2 / n)\)</span>. We will need <strong>Mill’s inequality</strong>: if <span class="math inline">\(Z \sim N(0, 1)\)</span> then <span class="math inline">\(\mathbb{P}(|Z| &gt; t) \leq (c / t) e^{-t^2 / 2}\)</span> where <span class="math inline">\(c = \sqrt{2 / pi}\)</span> is a constant. Thus,</p>
<p><span class="math display">\[
\begin{align}
\mathbb{P}(\max |D_{j, k}| &gt; \lambda) &amp;\leq \sum_{j, k} \mathbb{P}(|D_{j, k}| &gt; \lambda) \\
&amp; \leq \sum_{j, k} \mathbb{P} \left( \frac{\sqrt{n} |D_{j, k}|}{\sigma} &gt; \frac{\sqrt{n} \lambda}{\sigma} \right) \\
&amp; \leq \sum_{j, k} \frac{c \sigma}{\lambda \sqrt{n}} \exp \left\{ - \frac{1}{2} \frac{n \lambda^2}{\sigma^2} \right\} \\
&amp; = \frac{c}{\sqrt{2 \log n}} \rightarrow 0
\end{align}
\]</span></p>
</div>
<div id="exercises" class="section level3">
<h3>22.6 Exercises</h3>
<p><strong>Exercise 22.6.1</strong>. Prove Theorem 22.5.</p>
<p>The risk of <span class="math inline">\(\hat{f}\)</span> is given by</p>
<p><span class="math display">\[ R(J) = \sum_{j=1}^J \frac{\sigma_j^2}{n} + \sum_{j=J+1}^\infty \beta_j^2 \]</span></p>
<p><strong>Solution</strong>.</p>
<p>The density estimator <span class="math inline">\(\hat{f}\)</span> is defined as</p>
<p><span class="math display">\[ \hat{f}(x) = \sum_{j=1}^J \hat{\beta}_j \phi_j(x) 
\quad \text{where} \quad
\hat{\beta}_j = \frac{1}{n} \sum_{j=1}^n \phi_j(X_j)\]</span></p>
<p>From Theorem 22.4,</p>
<p><span class="math display">\[ \mathbb{E}[\hat{\beta}_j] = \beta_j
\quad \text{and} \quad
\mathbb{V}[\hat{\beta}_j] = \frac{\sigma_j^2}{n}
\]</span></p>
<p>The (unintegrated) variance is</p>
<p><span class="math display">\[ \mathbb{V}[f(x) - \hat{f}(x)] = \mathbb{V}[\hat{f}(x)] = \mathbb{V}\left[\sum_{j=1}^J \hat{\beta}_j \phi_j(x)\right]\\
= \sum_{j=1}^J \mathbb{V}[\hat{\beta}_j]\phi_j(x)^2 + 2 \sum_{i &lt; j} \text{Cov}(\hat{\beta}_i, \hat{\beta}_j) \phi_i(x) \phi_j(x) \]</span></p>
<p>so, integrating,</p>
<p><span class="math display">\[
\begin{align}
\int \mathbb{V}[f(x) - \hat{f}(x)] dx &amp;= \sum_{j=1}^J \mathbb{V}[\hat{\beta}_j] \int \phi_j(x)^2 dx + 2 \sum_{i &lt; j} \text{Cov}(\hat{\beta}_i, \hat{\beta}_j) \int \phi_i(x) \phi_j(x) dx  \\
&amp;= \sum_{j=1}^J \mathbb{V}[\hat{\beta}_j] \langle \phi_j, \phi_j \rangle + 2 \sum_{i &lt; j} \text{Cov}(\hat{\beta}_i, \hat{\beta}_j) \langle \phi_i, \phi_j \rangle \\
&amp;= \sum_{j=1}^J \mathbb{V}[\hat{\beta}_j] = \sum_{j=1}^J \frac{\sigma_j^2}{n}
\end{align}
\]</span></p>
<p>while the integrated expected bias squared is:</p>
<p><span class="math display">\[ 
\begin{align}
\int \mathbb{E}\left[\left(f(x) - \hat{f}(x)\right)^2\right] dx
&amp;= \int \mathbb{E}\left[\left(\sum_{j=1}^\infty \beta_j \phi_j(x) - \sum_{j=1}^J \hat{\beta}_j \phi_j(x)\right)^2\right] dx \\
&amp;= \int \mathbb{E} \left[ \left(\sum_{j=1}^\infty \beta_j \phi_j(x)\right)^2 \right] dx
+ \int \mathbb{E} \left[ \left(\sum_{j=1}^J \hat{\beta}_j \phi_j(x)\right)^2 \right] dx
- 2\int \mathbb{E} \left[ \sum_{i=1}^\infty \sum_{j=1}^J \beta_i \hat{\beta}_j \phi_i(x) \phi_j(x) \right] dx \\
&amp;= \sum_{j=1}^\infty \beta_j^2 \int \phi_j(x)^2 dx 
+ \sum_{j=1}^J \beta_j^2 \int \phi_j(x)^2 dx 
- 2 \sum_{j=1}^J \beta_j^2 \int \phi_j(x)^2 dx \\
&amp;= \sum_{j=J+1}^\infty \beta_j^2
\end{align}
\]</span></p>
<p>since <span class="math inline">\(\langle \phi_i, \phi_j \rangle = \int \phi_i(x) \phi_j(x) dx = 0\)</span> for <span class="math inline">\(i \neq j\)</span>, and since <span class="math inline">\(\langle \phi_j, \phi_j \rangle = \int \phi_j(x)^2 dx = 1\)</span>.</p>
<p>Then, the risk is the bias squared plus the variance,</p>
<p><span class="math display">\[ R(J) = \sum_{j=1}^J \frac{\sigma_j^2}{n} + \sum_{j=J+1}^\infty \beta_j^2\]</span></p>
<p>as desired.</p>
<p><strong>Exercise 22.6.2</strong>. Prove Theorem 22.9.</p>
<p>The risk <span class="math inline">\(R(J)\)</span> of the estimator $ (x) = _{j=1}^J _j _j(x) $ is</p>
<p><span class="math display">\[ R(J) = \frac{J \sigma^2}{n} + \sum_{j=J+1}^\infty \beta_j^2 \]</span></p>
<p><strong>Solution</strong>. Consider the probability distribution function obtained by shifting the true regression function to a minimum of 0, and rescaled to integrate to 1:</p>
<p><span class="math display">\[ f(x) = \frac{r(x) - r_0}{A} \quad \text{where } A = \int r(y) dy - r_0, r_0 = \inf_x r(x) \]</span></p>
<p>But if <span class="math inline">\(r(x) = \sum_{j=1}^\infty \beta_j \phi_j(x)\)</span>, then</p>
<p><span class="math display">\[ f(x) = -\frac{r_0}{A} + \sum_{j=1}^\infty \left( \frac{\beta_j}{A} \right) \phi_j(x) = \sum_{j=1}^\infty \left( \frac{\beta_j}{A} + c_j \right) \phi_j(x) \]</span></p>
<p>where the <span class="math inline">\(c_j\)</span>’s are the decomposition of the constant function <span class="math inline">\(-r_0 / A\)</span> on the basis of the functions <span class="math inline">\(\phi_j\)</span>’s. Assuming a sensible basis, <span class="math inline">\(\phi_0\)</span> is constant and <span class="math inline">\(c_j = 0\)</span> for <span class="math inline">\(j &gt; 1\)</span>.</p>
<p>By theorem 22.5, the risk of the PDF estimation is</p>
<p><span class="math display">\[ \sum_{j=1}^J \frac{\sigma_j^2}{A^2n} + \sum_{j=J+1}^\infty \left( \frac{\beta_j}{A} \right)^2 = \frac{1}{A^2} \left( \frac{J \sigma^2}{n} + \sum_{j=J+1}^\infty \beta_j^2 \right)\]</span></p>
<p>and so the result follows.</p>
<p><strong>Exercise 22.6.3</strong>. Let</p>
<p><span class="math display">\[ \psi_1 = \left( \frac{1}{\sqrt{3}}, \frac{1}{\sqrt{3}} , \frac{1}{\sqrt{3}} \right),
\quad
\psi_2 = \left( \frac{1}{\sqrt{2}}, -\frac{1}{\sqrt{2}} , 0 \right),
\quad
\psi_3 = \left( \frac{1}{\sqrt{6}}, \frac{1}{\sqrt{6}} , -\frac{2}{\sqrt{6}} \right)
\]</span></p>
<p>Show that these vectors have norm 1 and are orthogonal.</p>
<p><strong>Solution</strong>. Results follow from inspection.</p>
<p>Norms:</p>
<p><span class="math display">\[
\begin{align}
\langle \psi_1, \psi_1 \rangle &amp;= \frac{1}{\sqrt{3}} \cdot \frac{1}{\sqrt{3}} + \frac{1}{\sqrt{3}} \cdot \frac{1}{\sqrt{3}} + \frac{1}{\sqrt{3}} \cdot \frac{1}{\sqrt{3}} \\&amp;= \frac{1}{3} + \frac{1}{3} + \frac{1}{3} &amp;= 1 \\
\langle \psi_2, \psi_2 \rangle &amp;= \frac{1}{\sqrt{2}} \cdot \frac{1}{\sqrt{2}} + \frac{-1}{\sqrt{2}} \cdot \frac{-1}{\sqrt{2}} + 0 \cdot 0 \\&amp;= \frac{1}{2} + \frac{1}{2} + 0 &amp;= 1 \\
\langle \psi_3, \psi_3 \rangle &amp;= \frac{1}{\sqrt{6}} \cdot \frac{1}{\sqrt{6}} + \frac{1}{\sqrt{6}} \cdot \frac{1}{\sqrt{6}} + \frac{-2}{\sqrt{6}} \cdot \frac{-2}{\sqrt{6}}\\ &amp;= \frac{1}{6} + \frac{1}{6} + \frac{4}{6} &amp;= 1
\end{align}
\]</span></p>
<p>Orthogonality:
<span class="math display">\[
\begin{align}
\langle \psi_1, \psi_2 \rangle &amp;= \frac{1}{\sqrt{3}} \cdot \frac{1}{\sqrt{2}} + \frac{1}{\sqrt{3}} \cdot \frac{-1}{\sqrt{2}} + \frac{1}{\sqrt{3}} \cdot 0 \\&amp;= \frac{1}{\sqrt{6}} + \frac{-1}{\sqrt{6}} + 0 &amp;= 0 \\
\langle \psi_1, \psi_3 \rangle &amp;= \frac{1}{\sqrt{3}} \cdot \frac{1}{\sqrt{6}} + \frac{1}{\sqrt{3}} \cdot \frac{1}{\sqrt{6}} + \frac{1}{\sqrt{3}} \cdot \frac{-2}{\sqrt{6}} \\&amp;= \frac{1}{3\sqrt{2}} + \frac{1}{3\sqrt{2}} + \frac{-2}{3\sqrt{2}} &amp;= 0 \\
\langle \psi_2, \psi_3 \rangle &amp;= \frac{1}{\sqrt{2}} \cdot \frac{1}{\sqrt{6}} + \frac{-1}{\sqrt{2}} \cdot \frac{1}{\sqrt{6}} + 0 \cdot \frac{-2}{\sqrt{6}} \\&amp;= \frac{1}{2\sqrt{3}} + \frac{-1}{2\sqrt{3}} + 0 &amp;= 0 
\end{align}
\]</span></p>
<p><strong>Exercise 22.6.4</strong>. Prove Parseval’s relation equation.</p>
<p><span class="math display">\[ \Vert f \Vert^2 \equiv \int f^2(x) dx = \sum_{j=1}^\infty \beta_j^2 \equiv \Vert \beta \Vert^2\]</span></p>
<p><strong>Solution</strong>. We have:</p>
<p><span class="math display">\[ 
\begin{align}
\int f^2(x) dx &amp;= \int \left( \sum_{i=1}^\infty \beta_i \phi_i(x) \right)^2 dx \\
&amp;= \int \left( \sum_{i=1}^\infty \beta_i^2 \phi_i^2(x) + \sum_{i=1}^\infty \sum_{j=1, j \neq i}^\infty \beta_i \beta_j \phi_i(x) \phi_j(x) \right) dx \\
&amp;= \sum_{i=1}^\infty \beta_i^2 \int \phi_i^2(x) dx + \sum_{i=1}^\infty \sum_{j=1, j \neq i}^\infty \beta_i \beta_j  \int \phi_i(x) \phi_j(x) dx \\
&amp;= \sum_{i=1}^\infty \beta_i^2 \langle \phi_i, \phi_i \rangle + \sum_{i=1}^\infty \sum_{j=1, j \neq i}^\infty \beta_i \beta_j \langle \phi_i, \phi_j \rangle \\
&amp;= \sum_{i=1}^\infty \beta_i^2
\end{align}
\]</span></p>
<p>since <span class="math inline">\(\langle \phi_i, \phi_i \rangle = 1\)</span> and <span class="math inline">\(\langle \phi_i, \phi_j \rangle = 0\)</span> for <span class="math inline">\(i \neq j\)</span>.</p>
<p><strong>Exercise 22.6.5</strong>. Plot the first five Legendre polynomials. Verify, numerically, that they are orthonormal.</p>
<p>The <strong>Legendre polynomials</strong> on <span class="math inline">\([-1, 1]\)</span> are defined by</p>
<p><span class="math display">\[ P_j(x) = \frac{1}{2^j j!} \frac{d^j}{dx^j} (x^2 - 1)^j, \quad j = 0, 1, 2, \dots \]</span></p>
<p>It can be shown that these functions are complete and orthogonal, and that</p>
<p><span class="math display">\[ \int_{-1}^1 P_j^2(x) dx = \frac{2}{2j + 1}\]</span></p>
<p>It follows that the functions</p>
<p><span class="math display">\[ \phi_j(x) = \sqrt{\frac{2j + 1}{2}}P_j(x), \quad j = 0, 1, 2, \dots \]</span></p>
<p>form an orthonormal basis for <span class="math inline">\(L_2[-1, 1]\)</span>. The first few Legendre polynomials are</p>
<p><span class="math display">\[
\begin{align}
P_0(x) &amp;= 1 \\
P_1(x) &amp;= x \\
P_2(x) &amp;= \frac{1}{2}\left( 3x^2 - 1 \right) \\
P_3(x) &amp;= \frac{1}{2}\left( 5x^3 - 3x \right)
\end{align}
\]</span></p>
<p>These polynomials may be constructed explicitly using the following recursive relation:</p>
<p><span class="math display">\[ P_{j+1}(x) = \frac{(2j + 1) x P_j(x) - j P_{j - 1}(x)}{j + 1} \]</span></p>
<p><strong>Solution</strong>.</p>
<pre class="python"><code>import sympy
from sympy.abc import x
from functools import lru_cache

@lru_cache(maxsize=None)
def legendre_polynomial(j):
    if j == 0:
        return 1
    if j == 1:
        return x
    
    return sympy.expand(((2*j - 1) * x * legendre_polynomial(j - 1) - (j - 1) * legendre_polynomial(j - 2)) / j)

def legendre_basis(j):
    if j == 0:
        return lambda x: np.sqrt(1/2) * np.ones_like(x)
    
    pj = legendre_polynomial(j)
    return sympy.lambdify(x, sympy.sqrt((2*j + 1) / 2) * pj, &quot;numpy&quot;)</code></pre>
<pre class="python"><code>import matplotlib.pyplot as plt
%matplotlib inline

step = 1e-4
xx = np.arange(-1, 1 + step, step=step)
plt.figure(figsize=(12, 12))
for i in range(0, 5):
    
    # Set up the plot
    ax = plt.subplot(5, 1, i + 1)
    ax.plot(xx, legendre_basis(i)(xx))
    ax.set_title(r&#39;Legendre Basis $\phi_%i$&#39; % i)

plt.tight_layout()
plt.show()</code></pre>
<div class="figure">
<img src="Chapter%2022%20-%20Smoothing%20Using%20Orthogonal%20Functions_files/Chapter%2022%20-%20Smoothing%20Using%20Orthogonal%20Functions_73_0.png" alt="" />
<p class="caption">png</p>
</div>
<pre class="python"><code># Verifying orthogonality numerically

for i in range(0, 5):
    for j in range(i,5):
        product = legendre_basis(i)(xx) @ legendre_basis(j)(xx) * step
        print(&quot;&lt;phi_%i, phi_%i&gt;: %.3f&quot; % (i, j, product))</code></pre>
<pre><code>&lt;phi_0, phi_0&gt;: 1.000
&lt;phi_0, phi_1&gt;: -0.000
&lt;phi_0, phi_2&gt;: 0.000
&lt;phi_0, phi_3&gt;: -0.000
&lt;phi_0, phi_4&gt;: 0.000
&lt;phi_1, phi_1&gt;: 1.000
&lt;phi_1, phi_2&gt;: -0.000
&lt;phi_1, phi_3&gt;: 0.000
&lt;phi_1, phi_4&gt;: -0.000
&lt;phi_2, phi_2&gt;: 1.000
&lt;phi_2, phi_3&gt;: -0.000
&lt;phi_2, phi_4&gt;: 0.000
&lt;phi_3, phi_3&gt;: 1.000
&lt;phi_3, phi_4&gt;: -0.000
&lt;phi_4, phi_4&gt;: 1.000</code></pre>
<p><strong>Exercise 22.6.6</strong>. Expand the following functions in the cosine basis on <span class="math inline">\([0, 1]\)</span>. For (a) and (b), find the coefficients <span class="math inline">\(\beta_j\)</span> analytically. For (c) and (d), find the coefficients <span class="math inline">\(\beta_j\)</span> numerically, i.e.</p>
<p><span class="math display">\[ \beta_j = \int_0^1 f(x) \phi_j(x) \approx \frac{1}{N} \sum_{r=1}^N f \left( \frac{r}{N} \right) \phi_j \left( \frac{r}{N} \right) \]</span></p>
<p>for some large integer <span class="math inline">\(N\)</span>. Then plot the partial sum <span class="math inline">\(\sum_{j=1}^n \beta_j \phi_j(x)\)</span> for increasing values of <span class="math inline">\(n\)</span>.</p>
<p><strong>(a)</strong> <span class="math inline">\(f(x) = \sqrt{2} \cos (3 \pi x)\)</span></p>
<p><strong>(b)</strong> <span class="math inline">\(f(x) = \sin(\pi x)\)</span></p>
<p><strong>(c)</strong> <span class="math inline">\(f(x) = \sum_{j=1}^{11} h_j K(x - t_j)\)</span>, where <span class="math inline">\(K(t) = (1 + \text{sign}(t)) / 2\)</span>,</p>
<p><span class="math display">\[ 
(t_j) = (.1, .13, .15, .23, .25, .40, .44, .65, .76, .78, .81), \\
(h_j) = (4, -5, 3, -4, 5, -4.2, 2.1, 4.3, -3.1, 2.1, -4.2)
\]</span></p>
<p><strong>(d)</strong> <span class="math display">\[f(x) = \sqrt{x(1-x)} \sin \left( \frac{2.1 \pi}{(x + .05)} \right) \]</span></p>
<p><strong>Solution</strong>.</p>
<p><strong>(a)</strong> is immediate by inspection:</p>
<p><span class="math display">\[ f(x) = \sqrt{2} \cos(3 \pi x) = \sum_{j=1}^\infty \beta_j \phi_j(x), \quad \text{where } \phi_j(x) = \sqrt{2} \cos((j - 1) \pi x) \]</span></p>
<p>so</p>
<p><span class="math display">\[ \beta_j = \begin{cases}
1 &amp; \text{if } j = 4 \\
0 &amp; \text{otherwise}
\end{cases} \]</span></p>
<pre class="python"><code>import numpy as np

def beta_j(j):
    if j == 4:
        return 1
    return 0

def phi_j(j):
    def f(x):
        if j == 1:
            return np.ones_like(x)
        return np.sqrt(2) * np.cos((j - 1) * np.pi * x)
    return f

def partial_sum(n, x):
    lx = len(x)
    terms = np.empty((n, lx))
    for j in range(1, n + 1):
        terms[j - 1] = beta_j(j) * phi_j(j)(xx)
    return terms.sum(axis = 0)</code></pre>
<pre class="python"><code># Plot in separate boxes for ease of visualization

import matplotlib.pyplot as plt
%matplotlib inline

step = 1e-4
xx = np.arange(0, 1 + step, step=step)
plt.figure(figsize=(12, 12))

def do_subplot(index, xx, yy, label, color):
    ax = plt.subplot(4, 1, index)
    ax.plot(xx, yy, label=label, color=color)
    ax.legend()    

do_subplot(1, xx, partial_sum(3,  xx), label=r&#39;$n \leq 3$&#39;, color=&#39;C0&#39;)
do_subplot(2, xx, partial_sum(4,  xx), label=r&#39;$n \geq 4$&#39;, color=&#39;C2&#39;)
do_subplot(3, xx, partial_sum(4,  xx), label=r&#39;$f(x) = \sqrt{2} \cos (3 \pi x)$&#39;, color=&#39;C3&#39;)
do_subplot(4, xx, partial_sum(5,  xx), label=r&#39;$n = 5$&#39;, color=&#39;C4&#39;)

plt.tight_layout()
plt.legend()
plt.show()</code></pre>
<div class="figure">
<img src="Chapter%2022%20-%20Smoothing%20Using%20Orthogonal%20Functions_files/Chapter%2022%20-%20Smoothing%20Using%20Orthogonal%20Functions_79_0.png" alt="" />
<p class="caption">png</p>
</div>
<p><strong>(b)</strong> can be solved with the definition of <span class="math inline">\(\beta_j\)</span>:</p>
<p>For <span class="math inline">\(j = 1\)</span>,</p>
<p><span class="math display">\[ \beta_j = \langle f, \phi_1 \rangle = \int_0^1 f(x) \phi_1(x) dx = \int_0^1 \sin(\pi x) dx = \frac{2}{\pi} \]</span></p>
<p>For <span class="math inline">\(j \geq 2\)</span>,</p>
<p><span class="math display">\[
\beta_j = \langle f, \phi_j \rangle = \int_0^1 f(x) \phi_j(x) dx = \int_0^1 \sin(\pi x) \left( \sqrt{2} \cos((j-1) \pi x) \right) dx = \frac{\sqrt{2} (\cos(\pi j) - 1)}{\pi j^2 - 2 \pi j }
\]</span></p>
<p>so</p>
<p><span class="math display">\[ \beta_j = \begin{cases}
\frac{2}{\pi} &amp; \text{if } j = 1\\
-\frac{2\sqrt{2}}{\pi j^2 - 2 \pi j} &amp; \text{if } j \text{ is odd}, j &gt; 1 \\
0 &amp; \text{if } j \text{ is even}
\end{cases} \]</span></p>
<pre class="python"><code>import numpy as np

def beta_j(j):
    if j % 2 == 0:
        return 0
    if j == 1:
        return 2 / np.pi
    return -np.sqrt(2)*2 / ((np.pi * j * j) - (2 * np.pi * j))

def phi_j(j):
    def f(x):
        if j == 1:
            return np.ones_like(x)
        return np.sqrt(2) * np.cos((j - 1) * np.pi * x)
    return f

def partial_sum(n, x):
    lx = len(x)
    terms = np.empty((n, lx))
    for j in range(1, n + 1):
        terms[j - 1] = beta_j(j) * phi_j(j)(xx)
    return terms.sum(axis = 0)</code></pre>
<pre class="python"><code># Plot in separate boxes for ease of visualization

import matplotlib.pyplot as plt
%matplotlib inline

step = 1e-4
xx = np.arange(0, 1 + step, step=step)
plt.figure(figsize=(12, 12))

def do_subplot(index, xx, yy, label, color):
    ax = plt.subplot(4, 1, index)
    ax.plot(xx, yy, label=label, color=color)
    ax.legend()    

do_subplot(1, xx, partial_sum(4,  xx), label=r&#39;$n = %i$&#39; % 4, color=&#39;C0&#39;)
do_subplot(2, xx, partial_sum(8,  xx), label=r&#39;$n = %i$&#39; % 8, color=&#39;C1&#39;)
do_subplot(3, xx, partial_sum(16, xx), label=r&#39;$n = %i$&#39; % 16, color=&#39;C2&#39;)
do_subplot(4, xx, np.sin(np.pi * xx), label=r&#39;$f(x) = \sin(\pi x)$&#39;, color=&#39;C3&#39;)
    
plt.tight_layout()
plt.legend()
plt.show()</code></pre>
<div class="figure">
<img src="Chapter%2022%20-%20Smoothing%20Using%20Orthogonal%20Functions_files/Chapter%2022%20-%20Smoothing%20Using%20Orthogonal%20Functions_82_0.png" alt="" />
<p class="caption">png</p>
</div>
<p><strong>(c)</strong></p>
<p><span class="math display">\[f(x) = \sum_{j=1}^{11} h_j K(x - t_j)\]</span> where <span class="math inline">\(K(t) = (1 + \text{sign}(t)) / 2\)</span>,</p>
<p><span class="math display">\[ 
(t_j) = (.1, .13, .15, .23, .25, .40, .44, .65, .76, .78, .81), \\
(h_j) = (4, -5, 3, -4, 5, -4.2, 2.1, 4.3, -3.1, 2.1, -4.2)
\]</span></p>
<pre class="python"><code>import numpy as np

T = np.array([.1, .13, .15, .23, .25, .40, .44, .65, .76, .78, .81])
H = np.array([4, -5, 3, -4, 5, -4.2, 2.1, 4.3, -3.1, 2.1, -4.2])

def phi_j(j):
    def f(x):
        if j == 1:
            return np.ones_like(x)
        return np.sqrt(2) * np.cos((j - 1) * np.pi * x)
    return f

def f(x):
    def K(t):
        return (1 + np.sign(t))/2
    # reshape(1, -1) change to 1 row array, the unspecified value is inferred to be len(x)
    return (H.reshape(1, -1) * K(
        x.reshape(-1, 1).repeat(len(T), axis=1) - T.reshape(-1, 1).repeat(len(x), axis=1).T)
    ).sum(axis = 1)</code></pre>
<pre class="python"><code>N = 10000
step = 1 / N
xx = np.arange(0, 1 + step, step=step)

J = 256
estimated_beta = np.empty(J)
for j in range(1, J + 1):
    estimated_beta[j - 1] = f(xx) @ phi_j(j)(xx) / N
    
def beta_j(j):
    return estimated_beta[j - 1]

def partial_sum(n, x):
    lx = len(x)
    terms = np.empty((n, lx))
    for j in range(1, n + 1):
        terms[j - 1] = beta_j(j) * phi_j(j)(xx)
    return terms.sum(axis = 0)</code></pre>
<pre class="python"><code># Plot in separate boxes for ease of visualization

import matplotlib.pyplot as plt
%matplotlib inline

step = 1e-4
xx = np.arange(0, 1 + step, step=step)
plt.figure(figsize=(12, 12))

def do_subplot(index, xx, yy, label, color):
    ax = plt.subplot(4, 1, index)
    ax.plot(xx, yy, label=label, color=color)
    ax.legend()    

do_subplot(1, xx, partial_sum(16,  xx), label=r&#39;$n = %i$&#39; % 16, color=&#39;C0&#39;)
do_subplot(2, xx, partial_sum(64,  xx), label=r&#39;$n = %i$&#39; % 64, color=&#39;C1&#39;)
do_subplot(3, xx, partial_sum(256, xx), label=r&#39;$n = %i$&#39; % 256, color=&#39;C2&#39;)
do_subplot(4, xx, f(xx), label=r&#39;$f(x) = \sum_j h_j K(x - t_j) $&#39;, color=&#39;C3&#39;)
    
plt.tight_layout()
plt.legend()
plt.show()</code></pre>
<div class="figure">
<img src="Chapter%2022%20-%20Smoothing%20Using%20Orthogonal%20Functions_files/Chapter%2022%20-%20Smoothing%20Using%20Orthogonal%20Functions_86_0.png" alt="" />
<p class="caption">png</p>
</div>
<p><strong>(d)</strong></p>
<p><span class="math display">\[f(x) = \sqrt{x(1-x)} \sin \left( \frac{2.1 \pi}{(x + .05)} \right) \]</span></p>
<pre class="python"><code>import numpy as np

def phi_j(j):
    def f(x):
        if j == 1:
            return np.ones_like(x)
        return np.sqrt(2) * np.cos((j - 1) * np.pi * x)
    return f

def f(x):
    return np.sqrt(x * (1 - x)) * np.sin(2.1 * np.pi / (x + 0.05))</code></pre>
<pre class="python"><code>N = 10000
step = 1 / N
xx = np.arange(0, 1 + step, step=step)

J = 512
estimated_beta = np.empty(J)
for j in range(1, J + 1):
    estimated_beta[j - 1] = f(xx) @ phi_j(j)(xx) / N
    
def beta_j(j):
    return estimated_beta[j - 1]

def partial_sum(n, x):
    lx = len(x)
    terms = np.empty((n, lx))
    for j in range(1, n + 1):
        terms[j - 1] = beta_j(j) * phi_j(j)(xx)
    return terms.sum(axis = 0)</code></pre>
<pre class="python"><code># Plot in separate boxes for ease of visualization

import matplotlib.pyplot as plt
%matplotlib inline

step = 1e-4
xx = np.arange(0, 1 + step, step=step)
plt.figure(figsize=(12, 12))

def do_subplot(index, xx, yy, label, color):
    ax = plt.subplot(4, 1, index)
    ax.plot(xx, yy, label=label, color=color)
    ax.legend()    

do_subplot(1, xx, partial_sum(16,  xx), label=r&#39;$n = %i$&#39; % 16, color=&#39;C0&#39;)
do_subplot(2, xx, partial_sum(64,  xx), label=r&#39;$n = %i$&#39; % 64, color=&#39;C1&#39;)
do_subplot(3, xx, partial_sum(512, xx), label=r&#39;$n = %i$&#39; % 512, color=&#39;C2&#39;)
do_subplot(4, xx, f(xx), label=r&#39;$f(x) = \sqrt{x (1 - x)} \sin \left( \frac{2.1 \pi}{(x + .05)} \right)$&#39;, color=&#39;C3&#39;)
    
plt.tight_layout()
plt.legend()
plt.show()</code></pre>
<div class="figure">
<img src="Chapter%2022%20-%20Smoothing%20Using%20Orthogonal%20Functions_files/Chapter%2022%20-%20Smoothing%20Using%20Orthogonal%20Functions_90_0.png" alt="" />
<p class="caption">png</p>
</div>
<p><strong>Exercise 22.6.7</strong>. Consider the glass fragments data. Let <span class="math inline">\(Y\)</span> be the refractive index and let <span class="math inline">\(X\)</span> be aluminium content (the fourth variable).</p>
<p><strong>(a)</strong> Do a nonparametric regression to fit the model <span class="math inline">\(Y = f(x) + \epsilon\)</span> using the cosine basis method. The data are not on a regular grid. Ignore this when estimating the function. (But do sort the data first.) Provide a function estimate, an estimate of the risk, and a confidence band.</p>
<p><strong>(b)</strong> Use the wavelet method to estimate <span class="math inline">\(f\)</span>.</p>
<p><strong>Solution</strong>.</p>
<pre class="python"><code>import numpy as np
import pandas as pd

data = pd.read_csv(&#39;data/glass.txt&#39;, delim_whitespace=True)
data = data[[&#39;Al&#39;, &#39;RI&#39;]].sort_values(by=&#39;Al&#39;, ignore_index=True)
X, Y = data[&#39;Al&#39;], data[&#39;RI&#39;]
data</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
Al
</th>
<th>
RI
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
0.29
</td>
<td>
1.66
</td>
</tr>
<tr>
<th>
1
</th>
<td>
0.34
</td>
<td>
-6.85
</td>
</tr>
<tr>
<th>
2
</th>
<td>
0.47
</td>
<td>
4.13
</td>
</tr>
<tr>
<th>
3
</th>
<td>
0.47
</td>
<td>
4.13
</td>
</tr>
<tr>
<th>
4
</th>
<td>
0.51
</td>
<td>
5.20
</td>
</tr>
<tr>
<th>
…
</th>
<td>
…
</td>
<td>
…
</td>
</tr>
<tr>
<th>
209
</th>
<td>
2.79
</td>
<td>
-1.77
</td>
</tr>
<tr>
<th>
210
</th>
<td>
2.88
</td>
<td>
-1.77
</td>
</tr>
<tr>
<th>
211
</th>
<td>
3.02
</td>
<td>
-4.79
</td>
</tr>
<tr>
<th>
212
</th>
<td>
3.04
</td>
<td>
-4.84
</td>
</tr>
<tr>
<th>
213
</th>
<td>
3.50
</td>
<td>
-2.86
</td>
</tr>
</tbody>
</table>
<p>
214 rows × 2 columns
</p>
</div>
<p><strong>(a)</strong></p>
<p><strong>Orthogonal Series Regression Estimator</strong></p>
<ol style="list-style-type: decimal">
<li>Let</li>
</ol>
<p><span class="math display">\[ \hat{\beta}_j = \frac{1}{n} \sum_{i=1}^n Y_i \phi_i(x_i), \quad j = 1, \dots, n\]</span></p>
<ol start="2" style="list-style-type: decimal">
<li>Let</li>
</ol>
<p><span class="math display">\[ \hat{\sigma}^2 = \frac{n}{k} \sum_{i=n-k+1}^n \hat{\beta}_j^2 \]</span></p>
<p>where <span class="math inline">\(k \approx n / 4\)</span>.</p>
<ol start="3" style="list-style-type: decimal">
<li>For <span class="math inline">\(1 \leq J \leq n\)</span>, compute the risk estimate</li>
</ol>
<p><span class="math display">\[ \hat{R}(J) = J \frac{\hat{\sigma}^2}{n} + \sum_{j=J+1}^n \left(\hat{\beta}_j^2 - \frac{\hat{\sigma}^2}{n} \right)_{+} \]</span></p>
<ol start="4" style="list-style-type: decimal">
<li><p>Choose <span class="math inline">\(\hat{J} \in \{1, \dots, n \}\)</span> to minimize <span class="math inline">\(\hat{R}(J)\)</span>.</p></li>
<li><p>Let</p></li>
</ol>
<p><span class="math display">\[ \hat{f}(x) = \sum_{j=1}^J \hat{\beta}_j \phi_j(x) \]</span></p>
<p><strong>Theorem 22.11</strong>. Suppose the estimate <span class="math inline">\(\hat{r}\)</span> is based on <span class="math inline">\(J\)</span> terms and $^2 =  _{i=n-k+1}^n _j^2 $. Assume that <span class="math inline">\(J &lt; n - k + 1\)</span>. An approximate <span class="math inline">\(1 - \alpha\)</span> confidence band for <span class="math inline">\(\overline{r}\)</span> is <span class="math inline">\((\ell, u)\)</span>, where</p>
<p><span class="math display">\[ \ell(x) = \hat{r}(x) - K \sqrt{J} \sqrt{\frac{z_{\alpha} \hat{\tau}}{\sqrt{n}} + \frac{J \hat{\sigma}^2}{n}}
\quad \text{and} \quad u(x) = \hat{r}(x) + K \sqrt{J} \sqrt{\frac{z_{\alpha} \hat{\tau}}{\sqrt{n}} + \frac{J \hat{\sigma}^2}{n}} \]</span></p>
<p>where</p>
<p><span class="math display">\[ K = \max_{1 \leq j \leq J} \max_x | \phi_j(x) | \]</span></p>
<p>and</p>
<p><span class="math display">\[ \hat{\tau}^2 = \frac{2 J \hat{\sigma}^4}{n} \left( 1 + \frac{J}{k} \right) \]</span></p>
<p>and <span class="math inline">\(k = n / 4\)</span> as used in the definition of <span class="math inline">\(\hat{\sigma}^2\)</span>. In the cosine basis, <span class="math inline">\(K = \sqrt{2}\)</span>.</p>
<pre class="python"><code># Cosine basis function
def phi_j(j):
    def f(x):
        if j == 1:
            return np.ones_like(x)
        return np.sqrt(2) * np.cos((j - 1) * np.pi * x)
    return f</code></pre>
<pre class="python"><code>from scipy.stats import norm

def estimate_r(X, Y, alpha=0.05):
    n = len(X)

    # Rescale from [X.min(), X.max()] to [0, 1]
    def X_to_L2(t):
        return (t - X.min()) / (X.max() - X.min())
    
    X_scaled = X_to_L2(X)
    beta_hat = np.empty(n)
    for j in range(1, n+1):
        beta_hat[j - 1] = np.sum(Y @ phi_j(j)(X_scaled)) / n
        
    k = int(np.ceil(n / 4))
    sigma2_hat = np.sum(beta_hat[-k:]**2) * (n / k)

    risk_hat = np.zeros(n)
    for J in range(1, n + 1):
        risk_hat[J - 1] = J * sigma2_hat/n + np.sum(np.maximum(beta_hat[J:]**2 - sigma2_hat/n, 0))

    def r_hat(x):
        xx = X_to_L2(x)
        result = np.zeros_like(xx)
        for j in range(1, J_hat):
            result += beta_hat[j - 1] * phi_j(j)(xx)
        return result
        
    J_hat = np.argmin(risk_hat) + 2
    tau_hat = sigma2_hat * np.sqrt((2 * J_hat / n) * (1 + J_hat / k))
    z_alpha = norm.ppf(1 - alpha / 2)
    c = np.sqrt(2 * J * ((z_alpha * tau_hat / np.sqrt(n)) + (J_hat * sigma2_hat / n)))
    
    def r_lower(x):
        return r_hat(x) - c
    
    def r_upper(x):
        return r_hat(x) + c
            
    return r_hat, r_lower, r_upper</code></pre>
<pre class="python"><code>r_hat, r_lower, r_upper = estimate_r(X, Y, alpha=0.05)</code></pre>
<pre class="python"><code>import matplotlib.pyplot as plt
%matplotlib inline

plt.figure(figsize=(12, 8))
plt.scatter(X, Y, color=&#39;C0&#39;, marker=&#39;x&#39;, label=&#39;Data&#39;)

X_min, X_max = X.min(), X.max()
t = np.arange(X_min, X_max, step=(X_max - X_min) / 1000)
plt.plot(t, r_hat(t), color=&#39;purple&#39;, label=&#39;$\hat{r}(x)$&#39;)
plt.plot(t, r_lower(t), color=&#39;red&#39;, alpha=0.5, label=&#39;95% lower bound&#39;)
plt.plot(t, r_upper(t), color=&#39;green&#39;, alpha=0.5, label=&#39;95% upper bound&#39;)

plt.xlabel(&#39;Al content&#39;)
plt.ylabel(&#39;Refractive Index&#39;)
plt.legend()
plt.show()</code></pre>
<div class="figure">
<img src="Chapter%2022%20-%20Smoothing%20Using%20Orthogonal%20Functions_files/Chapter%2022%20-%20Smoothing%20Using%20Orthogonal%20Functions_99_0.png" alt="" />
<p class="caption">png</p>
</div>
<p><strong>(b)</strong></p>
<p><strong>Haar Wavelet Regression</strong></p>
<ol style="list-style-type: decimal">
<li>Let <span class="math inline">\(J = \log_2 n\)</span> and define</li>
</ol>
<p><span class="math display">\[ \hat{\alpha} = \frac{1}{n} \sum_i \phi(x_i) Y_i
\quad \text{and} \quad
D_{j, k} = \frac{1}{n} \sum_i \psi_{j, k}(x_i) Y_i
\]</span></p>
<p>for <span class="math inline">\(0 \leq j \leq J - 1\)</span></p>
<ol start="2" style="list-style-type: decimal">
<li>Estimate <span class="math inline">\(\sigma\)</span> by</li>
</ol>
<p><span class="math display">\[ \hat{\sigma} = \sqrt{n} \; \times \; \frac{\text{median} \left( \left| D_{J-1, k} : k = 0, \dots, 2^{J - 1} - 1\right| \right)}{0.6745} \]</span></p>
<ol start="3" style="list-style-type: decimal">
<li>Apply universal thresholding:</li>
</ol>
<p><span class="math display">\[ \hat{\beta}_{j, k} = \begin{cases}
D_{j, k} &amp; \text{if } \left| D_{j, k} \right| &gt; \hat{\sigma} \sqrt{\frac{2 \log n}{n}} \\
0 &amp; \text{otherwise}
\end{cases}\]</span></p>
<ol start="4" style="list-style-type: decimal">
<li>Set</li>
</ol>
<p><span class="math display">\[ \hat{f}(x) = \hat{\alpha} \phi(x) + \sum_{j = j_0}^{J - 1} \sum_{k = 0}^{2^j - 1} \hat{\beta}_{j, k} \psi_{j, k}(x) \]</span></p>
<pre class="python"><code># Wavelet base functions

import numpy as np

def haar_father_wavelet(x):
    return np.where((x &gt;= 0) &amp; (x &lt; 1), 1, 0)

def haar_mother_wavelet(x):
    return np.where((x &gt;= 0) &amp; (x &lt; 1),  np.where(x &lt;= 1/2, -1, 1), 0)

def psi_wavelet(j, k):
    def f(x):
        return 2**(j / 2) * haar_mother_wavelet((2**j)*x - k)
    return f</code></pre>
<pre class="python"><code>def estimate_r(X, Y):
    n = len(Y)
    J = int(np.ceil(np.log2(n)))

    def X_to_L2(t):
        return (t - X.min()) / (X.max() - X.min())

    xx = X_to_L2(X)
    alpha_hat = np.sum(Y) / n

    D = {}
    for j in range(J):
        D[j] = np.zeros(2**j)
        for k in range(2**j):
            D[j][k] = psi_wavelet(j, k)(xx) @ Y / n

    sigma_hat = np.sqrt(n) * np.median(np.abs(D[J - 1])) / 0.6745
    threshold = sigma_hat * np.sqrt(2 * np.log(n) / n)
    beta_hat = [(j, k, v) for j, values in D.items() for k, v in enumerate(values) if np.abs(v) &gt; threshold]

    def r_hat(X):
        xx = X_to_L2(X)
        return alpha_hat * haar_father_wavelet(xx) \
            + np.sum(np.array([v * psi_wavelet(j, k)(xx) for j, k, v in beta_hat]), axis=0)
    
    return r_hat</code></pre>
<pre class="python"><code>r_hat = estimate_r(X, Y)</code></pre>
<pre class="python"><code>import matplotlib.pyplot as plt
%matplotlib inline

plt.figure(figsize=(12, 8))
plt.scatter(X, Y, color=&#39;C0&#39;, marker=&#39;x&#39;, label=&#39;Data&#39;)

X_min, X_max = X.min(), X.max()
t = np.arange(X_min, X_max, step=(X_max - X_min) / 10000)

plt.plot(t, r_hat(t), color=&#39;purple&#39;, label=&#39;$\hat{r}(x)$&#39;)

plt.xlabel(&#39;Al content&#39;)
plt.ylabel(&#39;Refractive Index&#39;)
plt.legend()
plt.show()</code></pre>
<div class="figure">
<img src="Chapter%2022%20-%20Smoothing%20Using%20Orthogonal%20Functions_files/Chapter%2022%20-%20Smoothing%20Using%20Orthogonal%20Functions_104_0.png" alt="" />
<p class="caption">png</p>
</div>
<p><strong>Exercise 22.6.8</strong>. Show that the Haar wavelets are orthonormal.</p>
<p><strong>Solution</strong>.</p>
<p>Recalling the definitions:</p>
<p>The <strong>father Haar wavelet</strong> of <strong>Haar scaling function</strong> is defined by</p>
<p><span class="math display">\[ \phi(x) = \begin{cases}
1 &amp; \text{if } 0 \leq x &lt; 1 \\
0 &amp; \text{otherwise}
\end{cases} \]</span></p>
<p>The <strong>mother Haar wavelet</strong> is defined by</p>
<p><span class="math display">\[
\psi(x) = \begin{cases}
-1 &amp; \text{if } 0 \leq x \leq \frac{1}{2} \\
1  &amp; \text{if } \frac{1}{2} &lt; x \leq 1
\end{cases}
\]</span></p>
<p>For any integers <span class="math inline">\(j\)</span> and <span class="math inline">\(k\)</span> define</p>
<p><span class="math display">\[ \phi_{j, k}(x) = 2^{j/2} \phi(2^j x - k) 
\quad \text{and} \quad
\psi_{j, k}(x) = 2^{j/2} \psi(2^j x - k)
\]</span></p>
<p>The function <span class="math inline">\(\psi_{j, k}\)</span> has the same shape as <span class="math inline">\(\psi\)</span> but it has been rescaled by a factor of <span class="math inline">\(2^{j/2}\)</span> and shifted by a factor of <span class="math inline">\(k\)</span> and increased frequency by a factor of <span class="math inline">\(2^j\)</span>.</p>
<p>Let</p>
<p><span class="math display">\[ W_j = \{\psi_{j, k}, \; k = 0, 1, \dots, 2^j - 1\} \]</span></p>
<p>be the set of rescaled and shifted mother wavelets at resolution <span class="math inline">\(j\)</span>.</p>
<p><strong>Theorem 22.13</strong>. The set of functions</p>
<p><span class="math display">\[ \left\{ \phi, W_0, W_1, W_2, \dots \right\} \]</span></p>
<p>is an orthonormal basis for <span class="math inline">\(L_2(0, 1)\)</span>.</p>
<p><strong>Proof</strong>.</p>
<p>Normality:</p>
<p><span class="math display">\[
\begin{align}
\langle \phi, \phi \rangle &amp;= \int_0^1 \phi^2(x) dx = \int_0^1 1 \; dx = 1 \\
\langle \psi_{j, k}, \psi_{j, k} \rangle &amp;= \int_0^1 \psi_{j, k}^2(x) dx \\
&amp;= \int_0^1 \left( 2^{j / 2} \psi(2^j x - k) \right)^2 dx \\
&amp;= \int_{-k}^{2^j - k} 2^j  \psi^2(y) \left( 2^{-j} dy \right) \\
&amp;= \int_0^1 1 \; dy = 1
\end{align}
\]</span></p>
<p>Orthogonality:</p>
<p><span class="math display">\[
\begin{align}
\langle \phi, \psi_{j, k} \rangle &amp;= \int_0^1 \phi(x) \psi_{j, k}(x) dx \\
&amp;= \int_0^1  2^{j / 2} \psi(2^j x - k) dx  \\
&amp;= \int_{-k}^{2^j - k} 2^{j / 2}  \psi(y) \left( 2^{-j} dy \right) \\
&amp;= 2^{-j/2} \int_0^1 \psi(y) dy = 0
\end{align}
\]</span></p>
<p>since <span class="math inline">\(\psi(y)\)</span> has values <span class="math inline">\(-1\)</span> for <span class="math inline">\(y \in [0, 1/2]\)</span> and <span class="math inline">\(1\)</span> for <span class="math inline">\([1/2, 1]\)</span>, and 0 otherwise, and <span class="math inline">\([0, 1] \subset [-k, 2^j - k]\)</span>.</p>
<p><span class="math display">\[
\begin{align}
\langle \psi_{a, b}, \psi_{c, d} \rangle &amp;= \int_0^1 \psi_{a, b}(x) \psi_{c, d}(x)
\end{align}
\]</span></p>
<p>For each wavelet <span class="math inline">\(\psi_{j, k}\)</span>, consider the domain of values that produce a non-zero function output,</p>
<p><span class="math display">\[F_{j, k} = \{ x : \psi_{j, k}(x) \neq 0 \} = \left[\frac{k}{2^j}, \frac{k + 1}{2^j} \right]\]</span></p>
<p>Also consider the regions that produce +1 and -1 outputs,</p>
<p><span class="math display">\[F_{j, k}^+ = \{ x : \psi_{j, k}(x) = 1 \} = \left[\frac{k + \frac{1}{2}}{2^j}, \frac{k + 1}{2^j} \right]
\quad \text{and} \quad
F_{j, k}^- = \{ x : \psi_{j, k}(x) = 1 \} = \left[\frac{k}{2^j}, \frac{k + \frac{1}{2}}{2^j} \right]\]</span></p>
<p>If <span class="math inline">\(a = c\)</span>, <span class="math inline">\(b \neq d\)</span>, then the non-zero domain of the wavelets do not overlap – each increment or decrement in the second parameter ‘shifts’ the whole non-zero domain by its size, i.e. the diameter of <span class="math inline">\(F_{j, k}\)</span> which is <span class="math inline">\(2^{-j}\)</span>. Therefore, in this case, the integral is zero.</p>
<p>If <span class="math inline">\(a \neq c\)</span>, assuming without loss of generality <span class="math inline">\(a &lt; c\)</span>, then we may have intersection in the non-zero areas. But in that case, the non-zero domain is fully contained within the plus or minus domain, <span class="math inline">\(F_{c, d} \subset F_{a, b}^+\)</span> or <span class="math inline">\(F_{c, d} \subset F_{a, b}^-\)</span>. Therefore, in this case, the integral must also be zero.</p>
<p>Since we covered all cases, we have shown orthogonality. Since we have orthogonality and normality, the basis is orthonormal, proving the theorem.</p>
<p><strong>Exercise 22.6.9</strong>. Consider again the doppler signal:</p>
<p><span class="math display">\[ f(x) = \sqrt{x (1 - x)} \sin \left( \frac{2.1 \pi}{x + 0.05} \right) \]</span></p>
<p>Let <span class="math inline">\(n = 1024\)</span> and let <span class="math inline">\((x_1, \dots, x_n) = (1/n, \dots, 1)\)</span>. Generate data</p>
<p><span class="math display">\[ Y_i = f(x_i) + \sigma \epsilon_i \]</span></p>
<p>where <span class="math inline">\(\epsilon_i \sim N(0, 1)\)</span>.</p>
<p><strong>(a)</strong> Fit the curve using the cosine basis method. Plot the function estimate and confidence band for <span class="math inline">\(J = 10, 20, \dots, 100\)</span>.</p>
<p><strong>(b)</strong> Use Haar wavelets to fit the curve.</p>
<p><strong>Solution</strong>. Let’s pick <span class="math inline">\(\sigma = 0.1\)</span> when generating the data:</p>
<pre class="python"><code>import numpy as np
from scipy.stats import norm

def f(x):
    return np.sqrt(x * (1 - x)) * np.sin( (2.1 * np.pi) / (x + 0.05) )

n = 1024
sigma = 0.1
X = np.arange(1, n + 1) / n
Y = f(X) + norm.rvs(loc=0, scale=sigma, size=n)</code></pre>
<pre class="python"><code>import matplotlib.pyplot as plt
%matplotlib inline

plt.figure(figsize=(12, 8))
plt.scatter(X, Y, marker=&#39;x&#39;)
plt.show()</code></pre>
<div class="figure">
<img src="Chapter%2022%20-%20Smoothing%20Using%20Orthogonal%20Functions_files/Chapter%2022%20-%20Smoothing%20Using%20Orthogonal%20Functions_110_0.png" alt="" />
<p class="caption">png</p>
</div>
<p><strong>(a)</strong></p>
<p>The <strong>cosine basis</strong> is defined as follows: Let <span class="math inline">\(\phi_1(x) = 1\)</span> and for <span class="math inline">\(j \geq 2\)</span> define</p>
<p><span class="math display">\[\phi_j(x) = \sqrt{2} \cos \left( (j - 1) \pi x\right)\]</span></p>
<p><span class="math display">\[ f(x) = \sum_{j=1}^\infty \beta_j \phi_j(x)
\quad \text{where } \beta_j = \int_a^b f(x) \phi_j(x) dx \]</span></p>
<p><span class="math display">\[ Y_i = f(x_i) + \sigma \epsilon_i \text{   where }\epsilon_i \sim N(0, 1) \]</span></p>
<p><strong>Orthogonal Series Regression Estimator</strong></p>
<ol style="list-style-type: decimal">
<li>Let</li>
</ol>
<p><span class="math display">\[ \hat{\beta}_j = \frac{1}{n} \sum_{i=1}^n Y_i \phi_i(x_i), \quad j = 1, \dots, n\]</span></p>
<ol start="2" style="list-style-type: decimal">
<li>Let</li>
</ol>
<p><span class="math display">\[ \hat{\sigma}^2 = \frac{n}{k} \sum_{i=n-k+1}^n \hat{\beta}_j^2 \]</span></p>
<p>where <span class="math inline">\(k \approx n / 4\)</span>.</p>
<ol start="3" style="list-style-type: decimal">
<li>For <span class="math inline">\(1 \leq J \leq n\)</span>, compute the risk estimate</li>
</ol>
<p><span class="math display">\[ \hat{R}(J) = J \frac{\hat{\sigma}^2}{n} + \sum_{j=J+1}^n \left(\hat{\beta}_j^2 - \frac{\hat{\sigma}^2}{n} \right)_{+} \]</span></p>
<ol start="4" style="list-style-type: decimal">
<li><p>Choose <span class="math inline">\(\hat{J} \in \{1, \dots, n \}\)</span> to minimize <span class="math inline">\(\hat{R}(J)\)</span>.</p></li>
<li><p>Let</p></li>
</ol>
<p><span class="math display">\[ \hat{f}(x) = \sum_{j=1}^J \hat{\beta}_j \phi_j(x) \]</span></p>
<p><strong>Theorem 22.11</strong>. Suppose the estimate <span class="math inline">\(\hat{r}\)</span> is based on <span class="math inline">\(J\)</span> terms and $^2 =  _{i=n-k+1}^n _j^2 $. Assume that <span class="math inline">\(J &lt; n - k + 1\)</span>. An approximate <span class="math inline">\(1 - \alpha\)</span> confidence band for <span class="math inline">\(\overline{r}\)</span> is <span class="math inline">\((\ell, u)\)</span>, where</p>
<p><span class="math display">\[ \ell(x) = \hat{r}(x) - K \sqrt{J} \sqrt{\frac{z_{\alpha} \hat{\tau}}{\sqrt{n}} + \frac{J \hat{\sigma}^2}{n}}
\quad \text{and} \quad u(x) = \hat{r}(x) + K \sqrt{J} \sqrt{\frac{z_{\alpha} \hat{\tau}}{\sqrt{n}} + \frac{J \hat{\sigma}^2}{n}} \]</span></p>
<p>where</p>
<p><span class="math display">\[ K = \max_{1 \leq j \leq J} \max_x | \phi_j(x) | \]</span></p>
<p>and</p>
<p><span class="math display">\[ \hat{\tau}^2 = \frac{2 J \hat{\sigma}^4}{n} \left( 1 + \frac{J}{k} \right) \]</span></p>
<p>and <span class="math inline">\(k = n / 4\)</span> as used in the definition of <span class="math inline">\(\hat{\sigma}^2\)</span>. In the cosine basis, <span class="math inline">\(K = \sqrt{2}\)</span>.</p>
<pre class="python"><code># Cosine basis function
def phi_j(j):
    def f(x):
        if j == 1:
            return np.ones_like(x)
        return np.sqrt(2) * np.cos((j - 1) * np.pi * x)
    return f</code></pre>
<pre class="python"><code>from scipy.stats import norm

def estimate_r(X, Y, J=None, alpha=0.05):
    n = len(X)

    # Rescale from [X.min(), X.max()] to [0, 1]
    def X_to_L2(t):
        return (t - X.min()) / (X.max() - X.min())
    
    X_scaled = X_to_L2(X)
    beta_hat = np.empty(n)
    for j in range(1, n+1):
        beta_hat[j - 1] = np.sum(Y @ phi_j(j)(X_scaled)) / n
        
    k = int(np.ceil(n / 4))
    sigma2_hat = np.sum(beta_hat[-k:]**2) * (n / k)

    if J is None:
        # J not specified, pick J that minimizes risk
        risk_hat = np.zeros(n)
        for J in range(1, n + 1):
            risk_hat[J - 1] = J * sigma2_hat/n + np.sum(np.maximum(beta_hat[J:]**2 - sigma2_hat/n, 0))
        J_hat = np.argmin(risk_hat) + 2
    else:
        # Use pre-specified J
        assert J &lt;= n, &quot;J must be smaller than n&quot;
        J_hat = J

    def r_hat(x):
        xx = X_to_L2(x)
        result = np.zeros_like(xx)
        for j in range(1, J_hat):
            result += beta_hat[j - 1] * phi_j(j)(xx)
        return result
        
    tau_hat = sigma2_hat * np.sqrt((2 * J_hat / n) * (1 + J_hat / k))
    z_alpha = norm.ppf(1 - alpha / 2)
    c = np.sqrt(2 * J * ((z_alpha * tau_hat / np.sqrt(n)) + (J_hat * sigma2_hat / n)))
    
    def r_lower(x):
        return r_hat(x) - c
    
    def r_upper(x):
        return r_hat(x) + c
            
    return r_hat, r_lower, r_upper, J_hat</code></pre>
<pre class="python"><code>import matplotlib.pyplot as plt
%matplotlib inline

def do_subplot(i, J):
    ax = plt.subplot(5, 2, i)
    r_hat, r_lower, r_upper, J_hat = estimate_r(X, Y, J=J)
    ax.scatter(X, Y, color=&#39;C0&#39;, marker=&#39;x&#39;, alpha=0.5, label=&#39;Data&#39;)
    ax.plot(X, r_hat(X), color=&#39;purple&#39;, label=&#39;$r(x)$&#39;)
    ax.plot(X, r_lower(X), color=&#39;red&#39;, alpha=0.5, label=&#39;95% lower bound&#39;)
    ax.plot(X, r_upper(X), color=&#39;green&#39;, alpha=0.5, label=&#39;95% upper bound&#39;)
    ax.set_title(&#39;J = %i&#39; % J)
    ax.legend()
    
plt.figure(figsize=(12, 18))
for i, J in enumerate([10, 20, 30, 40, 50, 60, 70, 80, 90, 100]):
    
    do_subplot(i + 1, J)

plt.tight_layout()
plt.show()

plt.figure(figsize=(14.5, 8))
r_hat, r_lower, r_upper, J_hat = estimate_r(X, Y, J=None)
plt.scatter(X, Y, color=&#39;C0&#39;, marker=&#39;x&#39;, alpha=0.5, label=&#39;Data&#39;)
plt.plot(X, r_hat(X), color=&#39;purple&#39;, label=&#39;$r(x)$&#39;)
plt.plot(X, r_lower(X), color=&#39;red&#39;, alpha=0.5, label=&#39;95% lower bound&#39;)
plt.plot(X, r_upper(X), color=&#39;green&#39;, alpha=0.5, label=&#39;95% upper bound&#39;)
plt.title(&#39;J = %i (min risk)&#39; % J_hat)
plt.legend()
plt.show()</code></pre>
<div class="figure">
<img src="Chapter%2022%20-%20Smoothing%20Using%20Orthogonal%20Functions_files/Chapter%2022%20-%20Smoothing%20Using%20Orthogonal%20Functions_116_0.png" alt="" />
<p class="caption">png</p>
</div>
<div class="figure">
<img src="Chapter%2022%20-%20Smoothing%20Using%20Orthogonal%20Functions_files/Chapter%2022%20-%20Smoothing%20Using%20Orthogonal%20Functions_116_1.png" alt="" />
<p class="caption">png</p>
</div>
<p><strong>(b)</strong></p>
<p>The <strong>father Haar wavelet</strong> of <strong>Haar scaling function</strong> is defined by</p>
<p><span class="math display">\[ \phi(x) = \begin{cases}
1 &amp; \text{if } 0 \leq x &lt; 1 \\
0 &amp; \text{otherwise}
\end{cases} \]</span></p>
<p>The <strong>mother Haar wavelet</strong> is defined by</p>
<p><span class="math display">\[
\psi(x) = \begin{cases}
-1 &amp; \text{if } 0 \leq x \leq \frac{1}{2} \\
1  &amp; \text{if } \frac{1}{2} &lt; x \leq 1
\end{cases}
\]</span></p>
<p>For any integers <span class="math inline">\(j\)</span> and <span class="math inline">\(k\)</span> define</p>
<p><span class="math display">\[ \phi_{j, k}(x) = 2^{j/2} \phi(2^j x - k) 
\quad \text{and} \quad
\psi_{j, k}(x) = 2^{j/2} \psi(2^j x - k)
\]</span></p>
<p>The function <span class="math inline">\(\psi_{j, k}\)</span> has the same shape as <span class="math inline">\(\psi\)</span> but it has been rescaled by a factor of <span class="math inline">\(2^{j/2}\)</span> and shifted by a factor of <span class="math inline">\(k\)</span>.</p>
<p>Let</p>
<p><span class="math display">\[ W_j = \{\psi_{j, k}, \; k = 0, 1, \dots, 2^j - 1\} \]</span>
where <span class="math display">\[\psi_{j, k}(x) = 2^{j/2} \psi(2^j x - k)\]</span></p>
<p>be the set of rescaled and shifted mother wavelets at resolution <span class="math inline">\(j\)</span>.</p>
<p><strong>Theorem 22.13</strong>. The set of functions</p>
<p><span class="math display">\[ \left\{ \phi, W_0, W_1, W_2, \dots \right\} \]</span></p>
<p>is an orthonormal basis for <span class="math inline">\(L_2(0, 1)\)</span>.</p>
<p>It follows from this theorem that we can expand any function <span class="math inline">\(f \in L_2(0, 1)\)</span> in this basis. Because each <span class="math inline">\(W_j\)</span> is itself a set of functions, we write the expansion as a double sum:</p>
<p><span class="math display">\[ f(x) = \alpha \phi(x) + \sum_{j=0}^\infty \sum_{k=0}^{2^j - 1} \beta_{j, k} \psi_{j, k}(x) \]</span></p>
<p>where</p>
<p><span class="math display">\[ \alpha = \int_0^1 f(x) \phi(x) dx, \quad \beta_{j, k} = \int_0^1 f(x) \psi_{j, k}(x) dx\]</span></p>
<p>We call <span class="math inline">\(\alpha\)</span> the <strong>scaling coefficient</strong> and <span class="math inline">\(\beta_{j, k}\)</span> the <strong>detail coefficients</strong>. We call the finite sum</p>
<p><span class="math display">\[ \tilde{f}(x) = \alpha \phi(x) + \sum_{j=0}^{J - 1} \sum_{k=0}^{2^j - 1} \beta_{j, k}\psi_{j, k}(x)\]</span></p>
<p>the <strong>resolution <span class="math inline">\(J\)</span></strong> approximation to <span class="math inline">\(f\)</span>. The total number of terms in this sum is</p>
<p><span class="math display">\[ 1 + \sum_{j=0}^{J - 1}2^j = 2^J\]</span></p>
<p>Haar wavelets are localized, meaning they are zero outside of an interval, but they are not smooth. In 1988, Ingrid Daubechie showed that such wavelets do exist. They can be constructed numerically, but there is no closed form formula for smoother wavelets. To keep things simple, we will continue to use Haar wavelets.</p>
<p>We can now use wavelets to do density estimation and regression. We shall only discuss the regression problem <span class="math inline">\(Y_i = r(x_i) + \sigma \epsilon_i\)</span> where <span class="math inline">\(\epsilon_i \sim N(0, 1)\)</span> and <span class="math inline">\(x_i = i / n\)</span>. To simplify this discussion we assume that <span class="math inline">\(n = 2^J\)</span> for some <span class="math inline">\(J\)</span>.</p>
<p>There is one major difference between estimation using wavelets instead of a cosine (or polynomial) basis. With the cosine basis, we used all terms <span class="math inline">\(1 \leq j \leq J\)</span> for some <span class="math inline">\(J\)</span>. With wavelets, we use a method called <strong>thresholding</strong> where we keep a term in the function approximation only if its coefficient is large. The simplest version is called <strong>hard, universal thresholding</strong>.</p>
<p><strong>Haar Wavelet Regression</strong></p>
<ol style="list-style-type: decimal">
<li>Let <span class="math inline">\(J = \log_2 n\)</span> and define</li>
</ol>
<p><span class="math display">\[ \hat{\alpha} = \frac{1}{n} \sum_i \phi(x_i) Y_i
\quad \text{and} \quad
D_{j, k} = \frac{1}{n} \sum_i \psi_{j, k}(x_i) Y_i
\]</span></p>
<p>for <span class="math inline">\(0 \leq j \leq J - 1\)</span></p>
<ol start="2" style="list-style-type: decimal">
<li>Estimate <span class="math inline">\(\sigma\)</span> by</li>
</ol>
<p><span class="math display">\[ \hat{\sigma} = \sqrt{n} \; \times \; \frac{\text{median} \left( \left| D_{J-1, k} : k = 0, \dots, 2^{J - 1} - 1\right| \right)}{0.6745} \]</span></p>
<ol start="3" style="list-style-type: decimal">
<li>Apply universal thresholding:</li>
</ol>
<p><span class="math display">\[ \hat{\beta}_{j, k} = \begin{cases}
D_{j, k} &amp; \text{if } \left| D_{j, k} \right| &gt; \hat{\sigma} \sqrt{\frac{2 \log n}{n}} \\
0 &amp; \text{otherwise}
\end{cases}\]</span></p>
<ol start="4" style="list-style-type: decimal">
<li>Set</li>
</ol>
<p><span class="math display">\[ \hat{f}(x) = \hat{\alpha} \phi(x) + \sum_{j = j_0}^{J - 1} \sum_{k = 0}^{2^j - 1} \hat{\beta}_{j, k} \psi_{j, k}(x) \]</span></p>
<p>In practice, we do not compute <span class="math inline">\(\hat{\alpha}\)</span> and <span class="math inline">\(D_{j, k}\)</span>. Instead, we use the <strong>discrete wavelet transform (DWT)</strong> which is very fast. For Haar wavelets, the DWT works as follows.</p>
<p><strong>DWT for Haar Wavelets</strong></p>
<ul>
<li>Let <span class="math inline">\(y = (Y_1, \dots, Y_n)\)</span> and let <span class="math inline">\(J = \log_2 n\)</span>.<br />
</li>
<li>Create a list <span class="math inline">\(D\)</span> with elements <span class="math inline">\(D[0], ..., D[J - 1]\)</span></li>
<li>Do:</li>
</ul>
<pre><code>temp &lt;- y / sqrt(n)
for j in (J - 1):0 {
  m &lt;- 2^j
  I &lt;- (1:m)
  D[j] &lt;- (temp[2 * I] - temp[(2 * I) - 1]) / sqrt(2)
  temp &lt;- (temp[2 * I] + temp[(2 * I) - 1]) / sqrt(2)
}</code></pre>
<pre class="python"><code># Wavelet base functions

import numpy as np

def haar_father_wavelet(x):
    return np.where((x &gt;= 0) &amp; (x &lt; 1), 1, 0)

def haar_mother_wavelet(x):
    return np.where((x &gt;= 0) &amp; (x &lt; 1),  np.where(x &lt;= 1/2, -1, 1), 0)

def psi_wavelet(j, k):
    def f(x):
        return 2**(j / 2) * haar_mother_wavelet((2**j)*x - k)
    return f</code></pre>
<pre class="python"><code>def estimate_r(X, Y):
    n = len(Y)
    J = int(np.ceil(np.log2(n)))

    def X_to_L2(t):
        return (t - X.min()) / (X.max() - X.min())

    xx = X_to_L2(X)
    alpha_hat = np.sum(Y) / n

    D = {}
    for j in range(J):
        D[j] = np.zeros(2**j)
        for k in range(2**j):
            D[j][k] = psi_wavelet(j, k)(xx) @ Y / n

    sigma_hat = np.sqrt(n) * np.median(np.abs(D[J - 1])) / 0.6745
    threshold = sigma_hat * np.sqrt(2 * np.log(n) / n)
    beta_hat = [(j, k, v) for j, values in D.items() for k, v in enumerate(values) if np.abs(v) &gt; threshold]

    def r_hat(X):
        xx = X_to_L2(X)
        return alpha_hat * haar_father_wavelet(xx) \
            + np.sum(np.array([v * psi_wavelet(j, k)(xx) for j, k, v in beta_hat]), axis=0)
    
    return r_hat</code></pre>
<pre class="python"><code>r_hat = estimate_r(X, Y)</code></pre>
<pre class="python"><code>import matplotlib.pyplot as plt
%matplotlib inline

plt.figure(figsize=(12, 8))
plt.scatter(X, Y, color=&#39;C0&#39;, alpha=0.5, marker=&#39;x&#39;, label=&#39;Data&#39;)

X_min, X_max = X.min(), X.max()
t = np.arange(X_min, X_max, step=(X_max - X_min) / 10000)

plt.plot(t, r_hat(t), color=&#39;purple&#39;, label=&#39;$\hat{r}(x)$&#39;)
plt.legend()
plt.show()</code></pre>
<div class="figure">
<img src="Chapter%2022%20-%20Smoothing%20Using%20Orthogonal%20Functions_files/Chapter%2022%20-%20Smoothing%20Using%20Orthogonal%20Functions_131_0.png" alt="" />
<p class="caption">png</p>
</div>
<p><strong>Exercise 22.6.10 (Haar density estimation)</strong>. Let <span class="math inline">\(X_1, \dots, X_n \sim f\)</span> for some density <span class="math inline">\(f\)</span> on <span class="math inline">\([0, 1]\)</span>. Let’s consider constructing a wavelet histogram. Let <span class="math inline">\(\phi\)</span> and <span class="math inline">\(\psi\)</span> be the Haar father and mother wavelet. Write</p>
<p><span class="math display">\[ f(x) \approx \alpha\phi(x) + \sum_{j=0}^{J - 1} \sum_{k=0}^{2^j - 1} \beta_{j, k} \psi_{j, k}(x) \]</span></p>
<p>where <span class="math inline">\(J \approx \log_2(n)\)</span> and</p>
<p><span class="math display">\[ \alpha = \int_0^1 f(x) \phi(x) dx, \quad \beta_{j, k} = \int_0^1 f(x) \psi_{j, k}(x) dx\]</span> Let</p>
<p><span class="math display">\[ \hat{\beta}_{j, k} = \frac{1}{n} \sum_{i=1}^n \psi_{j, k}(X_i) \]</span></p>
<p><strong>(a)</strong> Show that <span class="math inline">\(\hat{\beta}_{j, k}\)</span> is an unbiased estimate of <span class="math inline">\(\beta_{j, k}\)</span>.</p>
<p><strong>(b)</strong> Define the Haar histogram</p>
<p><span class="math display">\[ \hat{f}(x) = \phi(x) + \sum_{j=0}^{B} \sum_{k=0}^{2^j - 1} \hat{\beta}_{j, k} \psi_{j, k}(x) \]</span></p>
<p>for <span class="math inline">\(0 \leq B \leq J - 1\)</span>.</p>
<p><strong>(c)</strong> Find an approximate expression for the MSE as a function of <span class="math inline">\(B\)</span>.</p>
<p><strong>(d)</strong> Generate <span class="math inline">\(n = 1000\)</span> observations from a <span class="math inline">\(\text{Beta}(15, 4)\)</span> density. Estimate the density using the Haar histogram. Use leave-one-out cross validation to choose <span class="math inline">\(B\)</span>.</p>
<p><strong>Solution</strong>.</p>
<p><strong>(a)</strong></p>
<p><span class="math display">\[ \mathbb{E}(\hat{\beta}_{j, k}) = \frac{1}{n} \sum_{i=1}^n \mathbb{E}(\psi_{j, k}(X_i))
= \mathbb{E}(\psi_{j, k}(X_1)) = \int_0^1 \psi_{j, k}(x) dx = \beta_{j, k} \]</span></p>
<p><strong>(b)</strong> Let’s interpret “define” as “implement.”</p>
<pre class="python"><code># Wavelet base functions

import numpy as np

def haar_father_wavelet(x):
    return np.where((x &gt;= 0) &amp; (x &lt; 1), 1, 0)

def haar_mother_wavelet(x):
    return np.where((x &gt;= 0) &amp; (x &lt; 1),  np.where(x &lt;= 1/2, -1, 1), 0)

def psi_wavelet(j, k):
    def f(x):
        return 2**(j / 2) * haar_mother_wavelet((2**j)*x - k)
    return f</code></pre>
<pre class="python"><code>import numpy as np

def haar_histogram(X, B): 
    n = len(X)
    
    def X_to_L2(t):
        return (t - X.min()) / (X.max() - X.min())
    
    xx = X_to_L2(X)

    D = {}
    for j in range(B):
        D[j] = np.zeros(2**j)
        for k in range(2**j):
            D[j][k] = np.sum(psi_wavelet(j, k)(xx)) / n
            
    # No thresholding
    beta_hat = [(j, k, v) for j, values in D.items() for k, v in enumerate(values)]
    
    def f_hat(x):
        xx = X_to_L2(x)
        return haar_father_wavelet(xx) \
            + np.sum(np.array([v * psi_wavelet(j, k)(xx) for j, k, v in beta_hat]), axis=0)
    
    return f_hat</code></pre>
<p><strong>(c)</strong> The orthonormal basis of Haar wavelets has its functions ordered as</p>
<p><span class="math display">\[ 
\begin{array}{ccc}
\phi, \\
\psi_{0, 0}, \\
\psi_{1, 0}, &amp; \psi_{1, 1}, \\
\psi_{2, 0}, &amp; \dots, &amp; \psi_{2, 2}, \\
\vdots \\
\psi_{j, 0}, &amp; \dots, &amp; \psi_{j, 2^k - 1}, \\
\vdots
\end{array}
\]</span></p>
<p>where each row of wavelets other than the first goes from <span class="math inline">\(\psi_{j, 0}\)</span> to <span class="math inline">\(\psi_{j, 2^j - 1}\)</span>, containing <span class="math inline">\(2^j\)</span> functions. We can label them in order</p>
<p><span class="math display">\[ q_1, q_2, \dots 
\quad \text{where }q_n = \begin{cases}
\phi &amp; \text{if }  n = 1 \\
\psi_{\lceil \log_2 n \rceil- 1, n + 1 - \lceil \log_2 n \rceil } &amp; \text{otherwise}
\end{cases}\]</span></p>
<p>Now, limiting the Haar histogram estimator to hyperparameter <span class="math inline">\(B\)</span> is equivalent to limiting it to only use the first <span class="math inline">\(2^{B + 1}\)</span> functions.</p>
<p>Then, we can use Theorem 22.5 to calculate the risk on the basis of the <span class="math inline">\(q_j\)</span>’s, where <span class="math inline">\(J = 2^{B + 1}\)</span>:</p>
<p><span class="math display">\[ R = \sum_{j=1}^J \frac{\sigma_j^2}{n} + \sum_{j=J+1}^\infty \beta_j^2 \]</span></p>
<p>with approximation</p>
<p><span class="math display">\[ \hat{R}_J(J) = \sum_{j=1}^J \frac{\hat{\sigma}_j^2}{n} + \sum_{j=J+1}^p \left( \hat{\beta}_j^2 - \frac{\hat{\sigma}_j^2}{n} \right)_{+} \]</span></p>
<p>where</p>
<p><span class="math display">\[ \hat{\sigma}_j^2 = \frac{1}{n - 1} \sum_{i=1}^n \left( q_j(X_i) - \hat{\beta}_j\right)^2 \]</span></p>
<p>Rewriting the approximation on the original Haar wavelet notation basis, we get:</p>
<p><span class="math display">\[ \hat{R}(B) = \frac{\hat{\sigma}_\phi^2}{n} 
+ \sum_{j=0}^B \sum_{k=0}^{2^j - 1} \frac{\hat{\sigma}_{j, k}^2}{n}
+ \sum_{j=B+1}^p \sum_{k=0}^{2^j - 1} \left( \hat{\beta}_{j, k}^2 - \frac{\hat{\sigma}_{j, k}^2}{n} \right)_+
\]</span></p>
<p>where the estimated betas are still</p>
<p><span class="math display">\[ \hat{\beta}_{j, k} = \frac{1}{n} \sum_{i=1}^n \psi_{j, k} (X_i)\]</span></p>
<p>and the estimated variances are</p>
<p><span class="math display">\[ 
\hat{\sigma}_\phi^2 = \frac{1}{n - 1} \sum_{a=0}^B \sum_{b=0}^{2^a - 1} \left( \psi_{a, b}(X_i) - 1\right)^2
\quad \text{and} \quad
\hat{\sigma}_{j, k}^2 = \frac{1}{n - 1} \sum_{a=0}^B \sum_{b=0}^{2^a - 1} \left( \psi_{a, b}(X_i) - \hat{\beta}_{j, k}\right)^2 
\]</span></p>
<p><strong>(d)</strong> Generate <span class="math inline">\(n = 1000\)</span> observations from a <span class="math inline">\(\text{Beta}(15, 4)\)</span> density. Estimate the density using the Haar histogram. Use leave-one-out cross validation to choose <span class="math inline">\(B\)</span>.</p>
<pre class="python"><code># Generate data
from scipy.stats import beta

X = beta.rvs(15, 4, size=1000)</code></pre>
<pre class="python"><code># Plot histograms for various B values

import matplotlib.pyplot as plt
%matplotlib inline

step = 1e-4
xx = np.arange(0, 1 + step, step)

import matplotlib.pyplot as plt
%matplotlib inline

def do_subplot(i, B):
    ax = plt.subplot(5, 2, i)
    f_hat = haar_histogram(X, B=B)
    ax.plot(xx, f_hat(xx), color=&#39;purple&#39;)
    ax.set_title(&#39;B = %i&#39; % B)
    
plt.figure(figsize=(12, 12))
for i, J in enumerate([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]):
    
    do_subplot(i + 1, J)

plt.tight_layout()
plt.show()</code></pre>
<div class="figure">
<img src="Chapter%2022%20-%20Smoothing%20Using%20Orthogonal%20Functions_files/Chapter%2022%20-%20Smoothing%20Using%20Orthogonal%20Functions_143_0.png" alt="" />
<p class="caption">png</p>
</div>
<p>Rather than using the risk expression from (c), we will use the leave-one-out cross-validation score, as in definition 21.15:</p>
<p><span class="math display">\[ \hat{J} = \int \left( \hat{f}(x) \right)^2 dx - \frac{2}{n} \sum_{i=1}^n \hat{f}_{(-i)} (X_i) \]</span></p>
<p>where <span class="math inline">\(\hat{f}\)</span> is the estimator using all data, and <span class="math inline">\(\hat{f}_{(-i)}\)</span> is the estimator dropping the <span class="math inline">\(i\)</span>-th observation.</p>
<pre class="python"><code>def leave_one_out_risk_score(X, B):
    estimator = lambda data: haar_histogram(data, B)
    
    n = len(X)
    f_hat = estimator(X)
    
    step = 1e-4
    X_min, X_max = X.min(), X.max()
    xx = np.arange(X_min, X_max + step, step)

    return (np.sum(f_hat(xx)**2) * step) \
        - (2 / n) * np.sum([estimator(X[np.arange(n) != i])(X[i]) for i in range(n)])</code></pre>
<pre class="python"><code># Explicitly compute risk score for multiple B values
max_B = 8
risk_B = np.empty(max_B)
for B in range(1, max_B + 1):
    risk_B[B - 1] = leave_one_out_risk_score(X, B)

selected_B = np.argmin(risk_B) + 1
selected_B_score = risk_B[selected_B - 1]</code></pre>
<pre class="python"><code># Plot risks and selected B value

plt.figure(figsize=(12, 8))
plt.plot(range(1, max_B + 1), risk_B)
plt.xlabel(&#39;B&#39;)
plt.ylabel(r&#39;Risk score $\hat{J}$&#39;)
plt.show()

print(&#39;Selected B: %i&#39; % selected_B)
print(&#39;Selected risk score: %.3f&#39; % selected_B_score)</code></pre>
<div class="figure">
<img src="Chapter%2022%20-%20Smoothing%20Using%20Orthogonal%20Functions_files/Chapter%2022%20-%20Smoothing%20Using%20Orthogonal%20Functions_147_0.png" alt="" />
<p class="caption">png</p>
</div>
<pre><code>Selected B: 6
Selected risk score: -2.607</code></pre>
<pre class="python"><code>f_hat = haar_histogram(X, B=selected_B)

plt.figure(figsize=(12, 8))
plt.plot(xx, f_hat(xx), color=&#39;purple&#39;)
plt.title(&#39;B = %i (min risk)&#39; % selected_B)
plt.xlabel(&#39;X&#39;)
plt.ylabel(r&#39;Risk score $\hat{J}$&#39;)
plt.show()</code></pre>
<div class="figure">
<img src="Chapter%2022%20-%20Smoothing%20Using%20Orthogonal%20Functions_files/Chapter%2022%20-%20Smoothing%20Using%20Orthogonal%20Functions_148_0.png" alt="" />
<p class="caption">png</p>
</div>
<p><strong>Exercise 22.6.11</strong>. In this exercise, we will explore the motivation for equation (22.38). Let <span class="math inline">\(X_1, \dots, X_n \sim N(0, \sigma^2)\)</span>. Let</p>
<p><span class="math display">\[ \hat{\sigma} = \sqrt{n} \times \frac{\text{median} (| X_1|, \dots, |X_n|)}{0.6745} \]</span></p>
<p><strong>(a)</strong> Show that <span class="math inline">\(\mathbb{E}(\hat{\sigma}) = \sigma\)</span>.</p>
<p><strong>(b)</strong> Simulate <span class="math inline">\(n = 100\)</span> observations from a <span class="math inline">\(N(0, 1)\)</span> distribution. Compute <span class="math inline">\(\hat{\sigma}\)</span> as well as the usual estimate of <span class="math inline">\(\sigma\)</span>. Repeat 1000 times and compare the MSE.</p>
<p><strong>(c)</strong> Repeat (b) but add some outliers to the data. To do this, simulate each observation from a <span class="math inline">\(N(0, 1)\)</span> with probability .95 and simulate each observation from a <span class="math inline">\(N(0, 10)\)</span> with probability 0.05.</p>
<p><strong>Solution</strong>.</p>
<p><strong>(a)</strong> This formula seems incorrect as is – there is no need to rescale by <span class="math inline">\(\sqrt{n}\)</span>.</p>
<p>From the Median Theorem, <span class="math inline">\(\mathbb{E}(\text{median}( \{ |X_i| : i = 1, \dots, n \} )) = \mathbb{E}(\text{median}(|X|))\)</span>, where <span class="math inline">\(X \sim N(0, \sigma^2)\)</span>. But the half normal distribution has CDF <span class="math inline">\(F(x) = \text{erf}\left(\frac{x}{\sigma \sqrt{2}}\right)\)</span>, so the median is <span class="math inline">\(F^{-1}\left(\frac{1}{2}\right) = \frac{\sigma \sqrt{2}}{\sqrt{\pi}} \approx 0.6745 \sigma\)</span>. Therefore, <span class="math inline">\(\mathbb{E}(\hat{\sigma}) = \sigma\)</span>.</p>
<p><strong>(b)</strong> Simulate <span class="math inline">\(n = 100\)</span> observations from a <span class="math inline">\(N(0, 1)\)</span> distribution. Compute <span class="math inline">\(\hat{\sigma}\)</span> as well as the usual estimate of <span class="math inline">\(\sigma\)</span>. Repeat 1000 times and compare the MSE.</p>
<pre class="python"><code>import numpy as np
from scipy.stats import norm

def run_experiment(n=100, B=1000):
    median_sigma_hat = np.empty(B)
    sd_sigma_hat = np.empty(B)
    
    for i in range(B):
        X = norm.rvs(size=n)
        median_sigma_hat[i] = np.median(np.abs(X)) / 0.6745
        sd_sigma_hat[i] = X.std()
    
    return median_sigma_hat, sd_sigma_hat</code></pre>
<pre class="python"><code>median_sigma_hat, sd_sigma_hat = run_experiment(n=100, B=1000)</code></pre>
<pre class="python"><code>print(&#39;E[Median sigma hat]:\t %.3f&#39; % median_sigma_hat.mean())
print(&#39;E[SD sigma hat]:   \t %.3f&#39;  % sd_sigma_hat.mean())

print(&#39;SE[Median sigma hat]:\t %.3f&#39; % median_sigma_hat.std())
print(&#39;SE[SD sigma hat]:\t %.3f&#39;     % sd_sigma_hat.std())</code></pre>
<pre><code>E[Median sigma hat]:     1.004
E[SD sigma hat]:     0.992
SE[Median sigma hat]:    0.115
SE[SD sigma hat]:    0.070</code></pre>
<p>As expected, both methodologies produce similar estimates for <span class="math inline">\(\sigma\)</span>, but the median estimate has a wider variance.</p>
<p><strong>(c)</strong> Repeat (b) but add some outliers to the data. To do this, simulate each observation from a <span class="math inline">\(N(0, 1)\)</span> with probability .95 and simulate each observation from a <span class="math inline">\(N(0, 10)\)</span> with probability 0.05.</p>
<pre class="python"><code>import numpy as np
from scipy.stats import norm

def run_experiment(n=100, B=1000):
    median_sigma_hat = np.empty(B)
    sd_sigma_hat = np.empty(B)
    
    for i in range(B):
        X1 = norm.rvs(size=n, scale=1)
        X2 = norm.rvs(size=n, scale=10)
        X = np.where(np.random.uniform(size=n) &lt; 0.95, X1, X2)
        median_sigma_hat[i] = np.median(np.abs(X)) / 0.6745
        sd_sigma_hat[i] = X.std()
    
    return median_sigma_hat, sd_sigma_hat</code></pre>
<pre class="python"><code>median_sigma_hat, sd_sigma_hat = run_experiment(n=100, B=1000)</code></pre>
<pre class="python"><code>print(&#39;E[Median sigma hat]:\t %.3f&#39; % median_sigma_hat.mean())
print(&#39;E[SD sigma hat]:   \t %.3f&#39;  % sd_sigma_hat.mean())

print(&#39;SE[Median sigma hat]:\t %.3f&#39; % median_sigma_hat.std())
print(&#39;SE[SD sigma hat]:\t %.3f&#39;     % sd_sigma_hat.std())</code></pre>
<pre><code>E[Median sigma hat]:     1.064
E[SD sigma hat]:     2.305
SE[Median sigma hat]:    0.124
SE[SD sigma hat]:    0.740</code></pre>
<p>The presence of 5% of outliers has significantly thrown off the usual estimate for <span class="math inline">\(\sigma\)</span>, while having a smaller effect on the median methodology.</p>
<p><strong>Exercise 22.6.12</strong>. Repeat question 6 using the Haar basis.</p>
<p>Exercise 22.6.6</p>
<p>Expand the following functions in the cosine basis on <span class="math inline">\([0, 1]\)</span>. For (a) and (b), find the coefficients <span class="math inline">\(\beta_j\)</span> analytically. For (c) and (d), find the coefficients <span class="math inline">\(\beta_j\)</span> numerically, i.e.</p>
<p><span class="math display">\[ \beta_j = \int_0^1 f(x) \phi_j(x) \approx \frac{1}{N} \sum_{r=1}^N f \left( \frac{r}{N} \right) \phi_j \left( \frac{r}{N} \right) \]</span></p>
<p>for some large integer <span class="math inline">\(N\)</span>. Then plot the partial sum <span class="math inline">\(\sum_{j=1}^n \beta_j \phi_j(x)\)</span> for increasing values of <span class="math inline">\(n\)</span>.</p>
<p><strong>(a)</strong> <span class="math inline">\(f(x) = \sqrt{2} \cos (3 \pi x)\)</span></p>
<p><strong>(b)</strong> <span class="math inline">\(f(x) = \sin(\pi x)\)</span></p>
<p><strong>(c)</strong> <span class="math inline">\(f(x) = \sum_{j=1}^{11} h_j K(x - t_j)\)</span>, where <span class="math inline">\(K(t) = (1 + \text{sign}(t)) / 2\)</span>,</p>
<p><span class="math display">\[ 
(t_j) = (.1, .13, .15, .23, .25, .40, .44, .65, .76, .78, .81), \\
(h_j) = (4, -5, 3, -4, 5, -4.2, 2.1, 4.3, -3.1, 2.1, -4.2)
\]</span></p>
<p><strong>(d)</strong> <span class="math display">\[f(x) = \sqrt{x(1-x)} \sin \left( \frac{2.1 \pi}{(x + .05)} \right) \]</span></p>
<p><strong>Solution</strong>.</p>
<p>As in question 6, (a) and (b) are to be solved analytically, while (c) and (d) numerically. Plots are to be provided for all scenarios.</p>
<p><strong>(a)</strong> <span class="math inline">\(f(x) = \sqrt{2} \cos (3 \pi x)\)</span></p>
<p>We have:</p>
<p><span class="math display">\[
\begin{align}
\alpha &amp;= \int_0^1 f(x) \phi(x) dx = \int_0^1 \sqrt{2} \cos (3 \pi x) dx = 0 \\
\beta_{j, k} &amp;= \int_0^1 f(x) \psi_{j, k}(x) dx \\
&amp;= 2^{(j + 1)/2} \left( \int_{\frac{k + 1/2}{2^j}}^{\frac{k + 1}{2^j}} \cos(3 \pi x) dx - \int_{\frac{k}{2^j}}^{\frac{k + 1/2}{2^j}} \cos(3 \pi x) dx \right) \\
&amp;= \frac{2^{(j + 1)/2}}{3 \pi} \left(
 \sin\left( \frac{3\pi k}{2^j} \right)
 + \sin\left( \frac{3\pi (k + 1)}{2^j} \right)
 -  2 \sin\left( \frac{3\pi \left(k + \frac{1}{2}\right)}{2^j} \right) 
\right)
\end{align}
\]</span></p>
<pre class="python"><code># Wavelet base functions

import numpy as np

def haar_father_wavelet(x):
    return np.where((x &gt;= 0) &amp; (x &lt; 1), 1, 0)

def haar_mother_wavelet(x):
    return np.where((x &gt;= 0) &amp; (x &lt; 1), np.where(x &lt;= 1/2, -1, 1), 0)

def psi_wavelet(j, k):
    def f(x):
        return 2**(j / 2) * haar_mother_wavelet((2**j)*x - k)
    return f</code></pre>
<pre class="python"><code>alpha = 0

def beta(j, k):
    return (2**((j + 1)/2) / (3 * np.pi)) *  \
            (np.sin(3 * np.pi * k / 2**(j)) + np.sin(3 * np.pi * (k + 1) / 2**(j)) \
             - (2 * np.sin(3 * np.pi * (k + 1/2) / 2**(j)) ))

def partial_sum(B, xx):
    result = alpha * np.ones_like(xx)
    for j in range(B+1):
        for k in range(2**j):
            result += beta(j, k) * psi_wavelet(j, k)(xx)
    return result</code></pre>
<pre class="python"><code># Plot in separate boxes for ease of visualization

import matplotlib.pyplot as plt
%matplotlib inline

step = 1e-4
epsilon = 1e-12  # small shift to avoid spikes
xx = np.arange(0, 1, step) + epsilon
plt.figure(figsize=(12, 12))

def do_subplot(index, xx, yy, label, color):
    ax = plt.subplot(4, 1, index)
    ax.plot(xx, yy, label=label, color=color)
    ax.legend()    

do_subplot(1, xx, partial_sum(2, xx), label=r&#39;$B = %i$&#39; % 2, color=&#39;C0&#39;)
do_subplot(2, xx, partial_sum(4, xx), label=r&#39;$B = %i$&#39; % 4, color=&#39;C1&#39;)
do_subplot(3, xx, partial_sum(6, xx), label=r&#39;$B = %i$&#39; % 6, color=&#39;C2&#39;)
do_subplot(4, xx, np.sqrt(2) * np.cos(3 * np.pi * xx), label=r&#39;$f(x) = \sqrt{2} \cos(3 \pi x)$&#39;, color=&#39;C3&#39;)
    
plt.tight_layout()
plt.legend()
plt.show()</code></pre>
<div class="figure">
<img src="Chapter%2022%20-%20Smoothing%20Using%20Orthogonal%20Functions_files/Chapter%2022%20-%20Smoothing%20Using%20Orthogonal%20Functions_169_0.png" alt="" />
<p class="caption">png</p>
</div>
<p><strong>(b)</strong> <span class="math inline">\(f(x) = \sin(\pi x)\)</span></p>
<p>We have:</p>
<p><span class="math display">\[
\begin{align}
\alpha &amp;= \int_0^1 f(x) \phi(x) dx = \int_0^1 \sin (\pi x) dx = \frac{2}{\pi} \\
\beta_{j, k} &amp;= \int_0^1 f(x) \psi_{j, k}(x) dx \\
&amp;= 2^{j/2} \left( \int_{\frac{k + 1/2}{2^j}}^{\frac{k + 1}{2^j}} \sin(\pi x) dx - \int_{\frac{k}{2^j}}^{\frac{k + 1/2}{2^j}} \sin(\pi x) dx \right) \\
&amp;= \frac{2^{j/2}}{\pi} \left(
 2 \cos\left( \frac{\pi \left(k + \frac{1}{2}\right)}{2^j} \right) 
 - \cos\left( \frac{\pi k}{2^j} \right)
 - \cos\left( \frac{\pi (k + 1)}{2^j} \right) 
\right)
\end{align}
\]</span></p>
<pre class="python"><code># Wavelet base functions

import numpy as np

def haar_father_wavelet(x):
    return np.where((x &gt;= 0) &amp; (x &lt; 1), 1, 0)

def haar_mother_wavelet(x):
    return np.where((x &gt;= 0) &amp; (x &lt; 1), np.where(x &lt;= 1/2, -1, 1), 0)

def psi_wavelet(j, k):
    def f(x):
        return 2**(j / 2) * haar_mother_wavelet((2**j)*x - k)
    return f</code></pre>
<pre class="python"><code>alpha = 2 / np.pi

def beta(j, k):
    return (2**(j/2) / np.pi) *  \
            (2 * np.cos(np.pi * (k + 1/2) / 2**(j)) \
            - np.cos(np.pi * k / 2**(j)) - np.cos(np.pi * (k + 1) / 2**(j)))

def partial_sum(B, xx):
    result = alpha * np.ones_like(xx)
    for j in range(B+1):
        for k in range(2**j):
            result += beta(j, k) * psi_wavelet(j, k)(xx)
    return result</code></pre>
<pre class="python"><code># Plot in separate boxes for ease of visualization

import matplotlib.pyplot as plt
%matplotlib inline

step = 1e-4
epsilon = 1e-12  # small shift to avoid spikes
xx = np.arange(0, 1, step) + epsilon
plt.figure(figsize=(12, 12))

def do_subplot(index, xx, yy, label, color):
    ax = plt.subplot(4, 1, index)
    ax.plot(xx, yy, label=label, color=color)
    ax.legend()    

do_subplot(1, xx, partial_sum(2, xx), label=r&#39;$B = %i$&#39; % 2, color=&#39;C0&#39;)
do_subplot(2, xx, partial_sum(4, xx), label=r&#39;$B = %i$&#39; % 4, color=&#39;C1&#39;)
do_subplot(3, xx, partial_sum(6, xx), label=r&#39;$B = %i$&#39; % 6, color=&#39;C2&#39;)
do_subplot(4, xx, np.sin(np.pi * xx), label=r&#39;$f(x) = \sin(\pi x)$&#39;, color=&#39;C3&#39;)
    
plt.tight_layout()
plt.legend()
plt.show()</code></pre>
<div class="figure">
<img src="Chapter%2022%20-%20Smoothing%20Using%20Orthogonal%20Functions_files/Chapter%2022%20-%20Smoothing%20Using%20Orthogonal%20Functions_173_0.png" alt="" />
<p class="caption">png</p>
</div>
<p><strong>(c)</strong> <span class="math inline">\(f(x) = \sum_{j=1}^{11} h_j K(x - t_j)\)</span>, where <span class="math inline">\(K(t) = (1 + \text{sign}(t)) / 2\)</span>,</p>
<p><span class="math display">\[ 
(t_j) = (.1, .13, .15, .23, .25, .40, .44, .65, .76, .78, .81), \\
(h_j) = (4, -5, 3, -4, 5, -4.2, 2.1, 4.3, -3.1, 2.1, -4.2)
\]</span></p>
<pre class="python"><code># Wavelet base functions

import numpy as np

def haar_father_wavelet(x):
    return np.where((x &gt;= 0) &amp; (x &lt; 1), 1, 0)

def haar_mother_wavelet(x):
    return np.where((x &gt;= 0) &amp; (x &lt; 1), np.where(x &lt;= 1/2, -1, 1), 0)

def psi_wavelet(j, k):
    def f(x):
        return 2**(j / 2) * haar_mother_wavelet((2**j)*x - k)
    return f</code></pre>
<pre class="python"><code>import numpy as np

T = np.array([.1, .13, .15, .23, .25, .40, .44, .65, .76, .78, .81])
H = np.array([4, -5, 3, -4, 5, -4.2, 2.1, 4.3, -3.1, 2.1, -4.2])

def f(x):
    def K(t):
        return (1 + np.sign(t))/2
    return (H.reshape(1, -1) * K(
        x.reshape(-1, 1).repeat(len(T), axis=1) - T.reshape(-1, 1).repeat(len(x), axis=1).T)
    ).sum(axis = 1)</code></pre>
<pre class="python"><code>N = 10000
step = 1 / N
xx = np.arange(0, 1 + step, step=step)

alpha = f(xx) @ haar_father_wavelet(xx) / N

J = 256
estimated_beta = {}
for j in range(0, 7):
    estimated_beta[j] = np.empty(2**j)
    for k in range(2**j):
        estimated_beta[j][k] = f(xx) @ psi_wavelet(j, k)(xx) / N
    
def beta(j, k):
    return estimated_beta[j][k]</code></pre>
<pre class="python"><code>def partial_sum(B, xx):
    result = alpha * np.ones_like(xx)
    for j in range(B+1):
        for k in range(2**j):
            result += beta(j, k) * psi_wavelet(j, k)(xx)
    return result</code></pre>
<pre class="python"><code># Plot in separate boxes for ease of visualization

import matplotlib.pyplot as plt
%matplotlib inline

step = 1e-4
epsilon = 1e-12  # small shift to avoid spikes
xx = np.arange(0, 1, step) + epsilon
plt.figure(figsize=(12, 12))

def do_subplot(index, xx, yy, label, color):
    ax = plt.subplot(4, 1, index)
    ax.plot(xx, yy, label=label, color=color)
    ax.legend()    

do_subplot(1, xx, partial_sum(2, xx), label=r&#39;$B = %i$&#39; % 2, color=&#39;C0&#39;)
do_subplot(2, xx, partial_sum(4, xx), label=r&#39;$B = %i$&#39; % 4, color=&#39;C1&#39;)
do_subplot(3, xx, partial_sum(6, xx), label=r&#39;$B = %i$&#39; % 6, color=&#39;C2&#39;)
do_subplot(4, xx, f(xx), label=r&#39;$f(x) = \sum_j h_j K(x - t_j) $&#39;, color=&#39;C3&#39;)
    
plt.tight_layout()
plt.legend()
plt.show()</code></pre>
<div class="figure">
<img src="Chapter%2022%20-%20Smoothing%20Using%20Orthogonal%20Functions_files/Chapter%2022%20-%20Smoothing%20Using%20Orthogonal%20Functions_179_0.png" alt="" />
<p class="caption">png</p>
</div>
<p><strong>(d)</strong> $f(x) =  (  ) $</p>
<pre class="python"><code># Wavelet base functions

import numpy as np

def haar_father_wavelet(x):
    return np.where((x &gt;= 0) &amp; (x &lt; 1), 1, 0)

def haar_mother_wavelet(x):
    return np.where((x &gt;= 0) &amp; (x &lt; 1), np.where(x &lt;= 1/2, -1, 1), 0)

def psi_wavelet(j, k):
    def f(x):
        return 2**(j / 2) * haar_mother_wavelet((2**j)*x - k)
    return f</code></pre>
<pre class="python"><code>def f(x):
    return np.sqrt(x * (1 - x)) * np.sin(2.1 * np.pi / (x + 0.05))</code></pre>
<pre class="python"><code>N = 10000
step = 1 / N
xx = np.arange(0, 1 + step, step=step)

alpha = f(xx) @ haar_father_wavelet(xx) / N

J = 256
estimated_beta = {}
for j in range(0, 7):
    estimated_beta[j] = np.empty(2**j)
    for k in range(2**j):
        estimated_beta[j][k] = f(xx) @ psi_wavelet(j, k)(xx) / N
    
def beta(j, k):
    return estimated_beta[j][k]</code></pre>
<pre class="python"><code>def partial_sum(B, xx):
    result = alpha * np.ones_like(xx)
    for j in range(B+1):
        for k in range(2**j):
            result += beta(j, k) * psi_wavelet(j, k)(xx)
    return result</code></pre>
<pre class="python"><code># Plot in separate boxes for ease of visualization

import matplotlib.pyplot as plt
%matplotlib inline

step = 1e-4
epsilon = 1e-12  # small shift to avoid spikes
xx = np.arange(0, 1, step) + epsilon
plt.figure(figsize=(12, 12))

def do_subplot(index, xx, yy, label, color):
    ax = plt.subplot(4, 1, index)
    ax.plot(xx, yy, label=label, color=color)
    ax.legend()    

do_subplot(1, xx, partial_sum(2, xx), label=r&#39;$B = %i$&#39; % 2, color=&#39;C0&#39;)
do_subplot(2, xx, partial_sum(4, xx), label=r&#39;$B = %i$&#39; % 4, color=&#39;C1&#39;)
do_subplot(3, xx, partial_sum(6, xx), label=r&#39;$B = %i$&#39; % 6, color=&#39;C2&#39;)
do_subplot(4, xx, f(xx), label=r&#39;$f(x) = \sqrt{x (1 - x)} \sin \left( \frac{2.1 \pi}{(x + .05)} \right)$&#39;, color=&#39;C3&#39;)
    
plt.tight_layout()
plt.legend()
plt.show()</code></pre>
<div class="figure">
<img src="Chapter%2022%20-%20Smoothing%20Using%20Orthogonal%20Functions_files/Chapter%2022%20-%20Smoothing%20Using%20Orthogonal%20Functions_185_0.png" alt="" />
<p class="caption">png</p>
</div>
</div>
</div>
<div id="references" class="section level2 unnumbered">
<h2>References</h2>
<div id="refs" class="references csl-bib-body">
<div id="ref-wasserman2013all" class="csl-entry">
1. Wasserman L. All of statistics: A concise course in statistical inference. Springer Science &amp; Business Media; 2013.
</div>
<div id="ref-telmo-correa/all-of-statistics" class="csl-entry">
2. Https://github.com/telmo-correa/all-of-statistics.
</div>
</div>
</div>

    </div>
  </article>

  


</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
          </li>
          <li>
          </li>
        </ul>
      </footer>

    </div>
    



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/r.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/yaml.min.js"></script>
<script>hljs.configure({languages: []}); hljs.initHighlightingOnLoad();</script>



    
<script src="../../../../js/math-code.js"></script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
  </body>
</html>

