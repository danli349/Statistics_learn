<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.80.0" />


<title>AOS chapter20 Directed Graphs - A Hugo website</title>
<meta property="og:title" content="AOS chapter20 Directed Graphs - A Hugo website">


  <link href='../../../../favicon.ico' rel='icon' type='image/x-icon'/>



  








<link href='//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css' rel='stylesheet' type='text/css' />



<link rel="stylesheet" href="../../../../css/fonts.css" media="all">
<link rel="stylesheet" href="../../../../css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="../../../../" class="nav-logo">
    <img src="../../../../images/logo.png"
         width="50"
         height="50"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="../../../../about/">about</a></li>
    
    <li><a href="../../../../vitae/">CV</a></li>
    
    <li><a href="https://github.com/danli349">GitHub</a></li>
    
    <li><a href="https://scholar.google.com/citations?user=mNjLK8EAAAAJ&amp;hl=en">Googlr Scholar</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">34 min read</span>
    

    <h1 class="article-title">AOS chapter20 Directed Graphs</h1>

    
    <span class="article-date">2021-05-03</span>
    

    <div class="article-content">
      
<script src="../../../../2021/05/03/aos-chapter20-directed-graphs/index_files/header-attrs/header-attrs.js"></script>

<div id="TOC">
<ul>
<li><a href="#directed-graphs">20. Directed Graphs</a>
<ul>
<li><a href="#introduction">20.1 Introduction</a></li>
<li><a href="#dags">20.2 DAG’s</a></li>
<li><a href="#probability-and-dags">20.3 Probability and DAG’s</a></li>
<li><a href="#more-independence-relations">20.4 More Independence Relations</a></li>
<li><a href="#estimation-for-dags">20.5 Estimation for DAG’s</a></li>
<li><a href="#causation-revisited">20.6 Causation Revisited</a></li>
<li><a href="#exercises">20.8 Exercises</a></li>
</ul></li>
</ul>
</div>

<div id="directed-graphs" class="section level2">
<h2>20. Directed Graphs</h2>
<div id="introduction" class="section level3">
<h3>20.1 Introduction</h3>
<p>Directed graphs are similar to undirected graphs, but there are arrows between vertices instead of edges. Like undirected graphs, directed graphs can be used to represent independence relations. They can also be used as an alternative to counterfactuals to represent causal relationships. Some people use the phrase <strong>Bayesian network</strong> to refer to a directed graph endowed with a probability distribution. This is a poor choice of terminology. Statistical inference for directed graphs can be performed using frequentist or Bayesian methods so it is misleading to call them Bayesian networks.</p>
</div>
<div id="dags" class="section level3">
<h3>20.2 DAG’s</h3>
<p>A <strong>directed graph</strong> <span class="math inline">\(\mathcal{G}\)</span> consists of a set of vertices <span class="math inline">\(V\)</span> and an edge set <span class="math inline">\(E\)</span> of ordered pairs of variables. If <span class="math inline">\((X, Y) \in E\)</span> then there is an arrow pointing from <span class="math inline">\(X\)</span> to <span class="math inline">\(Y\)</span>.</p>
<pre class="python"><code>from graphviz import Digraph

d = Digraph()

d.edge(&#39;Y&#39;, &#39;X&#39;)
d.edge(&#39;Y&#39;, &#39;Z&#39;)

d</code></pre>
<div class="figure">
<img src="Chapter%2020%20-%20Directed%20Graphs_files/Chapter%2020%20-%20Directed%20Graphs_5_0.svg" alt="" />
<p class="caption">svg</p>
</div>
<p>If an arrow connects two variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> (in either direction) we say that <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are <strong>adjacent</strong>. If there is an arrow from <span class="math inline">\(X\)</span> to <span class="math inline">\(Y\)</span> then <span class="math inline">\(X\)</span> is a <strong>parent</strong> of <span class="math inline">\(Y\)</span> and <span class="math inline">\(Y\)</span> is a <strong>child</strong> of <span class="math inline">\(X\)</span>. The set of all parents of <span class="math inline">\(X\)</span> is denoted by <span class="math inline">\(\pi_X\)</span> or <span class="math inline">\(\pi(X)\)</span>. A <strong>directed path</strong> from <span class="math inline">\(X\)</span> to <span class="math inline">\(Y\)</span> is a set of vertices beginning with <span class="math inline">\(X\)</span>, ending with <span class="math inline">\(Y\)</span> such that each pair is connected by an arrow and all of the arrows point in the same direction:</p>
<p><span class="math display">\[ X \rightarrow \cdots \rightarrow Y
\quad \text{or} \quad
X \leftarrow \cdots \leftarrow Y \]</span></p>
<p>A sequence of adjacent vertices starting with <span class="math inline">\(X\)</span> and ending with <span class="math inline">\(Y\)</span> but ignoring the directions of the arrows is called an <strong>undirected path</strong>. <span class="math inline">\(X\)</span> is an <strong>ancestor</strong> of <span class="math inline">\(Y\)</span> if there is a directed path from <span class="math inline">\(X\)</span> to <span class="math inline">\(Y\)</span>. We also say that <span class="math inline">\(Y\)</span> is a <strong>descendant</strong> of <span class="math inline">\(X\)</span>.</p>
<p>A configuration of the form:</p>
<p><span class="math display">\[ X \rightarrow Y \leftarrow Z \]</span></p>
<p>is called a <strong>collider</strong>. A configuration not of that form is called a <strong>non-collider</strong>, for example,</p>
<p><span class="math display">\[ X \rightarrow Y \rightarrow Z
\quad \text{or} \quad
X \leftarrow Y \leftarrow Z\]</span></p>
<p>A directed path that starts and ends at the same variable is called a <strong>cycle</strong>. A directed graph is <strong>acyclic</strong> if it has no cycles. In this case we say that the graph is a <strong>directed acyclic graph</strong> or <strong>DAG</strong>. From now on, we will only deal with graphs that are DAG’s.</p>
</div>
<div id="probability-and-dags" class="section level3">
<h3>20.3 Probability and DAG’s</h3>
<p>Let <span class="math inline">\(\mathcal{G}\)</span> be a DAG with vertices <span class="math inline">\(V = (X_1, \dots, X_k)\)</span>.</p>
<p>If <span class="math inline">\(\mathbb{P}\)</span> is a distribution for <span class="math inline">\(V\)</span> with probability function <span class="math inline">\(p\)</span>, we say that <strong><span class="math inline">\(\mathbb{P}\)</span> is Markov to <span class="math inline">\(\mathcal{G}\)</span></strong> or that <strong><span class="math inline">\(\mathcal{G}\)</span> represents <span class="math inline">\(\mathbb{P}\)</span></strong> if</p>
<p><span class="math display">\[ p(v) = \prod_{i=1}^k p(x_i | \pi_i) \]</span></p>
<p>where the <span class="math inline">\(\pi_i\)</span> are the parents of <span class="math inline">\(X_i\)</span>. The set of distributions represented by <span class="math inline">\(\mathcal{G}\)</span> is denoted by <span class="math inline">\(M(\mathcal{G})\)</span>.</p>
<p>The following theorem says that <span class="math inline">\(\mathbb{P} \in M(\mathcal{G})\)</span> if and only if the following <strong>Markov Condition</strong> holds. Roughly speaking, the Markov Condition says that every variable <span class="math inline">\(W\)</span> is independent of the “past” given its parents.</p>
<p><strong>Theorem 20.3</strong>. A distribution <span class="math inline">\(\mathbb{P} \in M(\mathcal{G})\)</span> if and only if the following <strong>Markov Condition</strong> holds: for every variable <span class="math inline">\(W\)</span>,</p>
<p><span class="math display">\[W \text{ ⫫ } \overline{W} | \pi_W \]</span></p>
<p>where <span class="math inline">\(\overline{W}\)</span> denotes all the other variables except parents and descendants of <span class="math inline">\(W\)</span>.</p>
</div>
<div id="more-independence-relations" class="section level3">
<h3>20.4 More Independence Relations</h3>
<p>The Markov Condition allows us to list some independence relations. These relations may logically imply other independence relations. Consider this DAG:</p>
<pre class="python"><code>from graphviz import Digraph

d = Digraph()

d.edge(&#39;X2&#39;, &#39;X3&#39;)
d.edge(&#39;X1&#39;, &#39;X3&#39;)
d.edge(&#39;X1&#39;, &#39;X4&#39;)
d.edge(&#39;X3&#39;, &#39;X5&#39;)
d.edge(&#39;X4&#39;, &#39;X5&#39;)

d</code></pre>
<div class="figure">
<img src="Chapter%2020%20-%20Directed%20Graphs_files/Chapter%2020%20-%20Directed%20Graphs_14_0.svg" alt="" />
<p class="caption">svg</p>
</div>
<p>The Markov Condition implies:</p>
<ul>
<li><span class="math inline">\(X_1 \text{ ⫫ } X_2\)</span></li>
<li><span class="math inline">\(X_2 \text{ ⫫ } \{ X_1, X_4 \}\)</span></li>
<li><span class="math inline">\(X_3 \text{ ⫫ } X_4 | \{ X_1, X_2 \}\)</span></li>
<li><span class="math inline">\(X_4 \text{ ⫫ } \{ X_2, X_3 \} | X_1\)</span></li>
<li><span class="math inline">\(X_5 \text{ ⫫ } \{ X_1, X_2 \} | \{X_3, X_4\}\)</span></li>
</ul>
<p>It turns out that these conditions imply:</p>
<p><span class="math display">\[ \{ X_4, X_5 \} \text{ ⫫ } X_2 | \{ X_1, X_3 \} \]</span></p>
<p>How do we find these extra independence relations? The answer is <strong>d-separation</strong>, which can be summarized by 3 rules.</p>
<p><strong>The rules of d-separation</strong></p>
<ol style="list-style-type: decimal">
<li>In a non-collider <span class="math inline">\((X, Y, Z)\)</span>, <span class="math inline">\(X\)</span> and <span class="math inline">\(Z\)</span> are <strong>d-connected</strong>, but they are <strong>d-separated</strong> given <span class="math inline">\(Y\)</span>.</li>
<li>If <span class="math inline">\(X\)</span> and <span class="math inline">\(Z\)</span> collide at <span class="math inline">\(Y\)</span> then <span class="math inline">\(X\)</span> and <span class="math inline">\(Z\)</span> are <strong>d-separated</strong> but they are <strong>d-connected</strong> given <span class="math inline">\(Y\)</span>.</li>
<li>Conditioning on the descendant of a collider has the same effect as conditioning on the collider. Thus in the figure below, <span class="math inline">\(X\)</span> and <span class="math inline">\(Z\)</span> are <strong>d-separated</strong> but they are <strong>d-connected</strong> given <span class="math inline">\(W\)</span>.</li>
</ol>
<pre class="python"><code>from graphviz import Digraph

d = Digraph()

d.edge(&#39;X&#39;, &#39;Y&#39;)
d.edge(&#39;Z&#39;, &#39;Y&#39;)
d.edge(&#39;Y&#39;, &#39;W&#39;)

d</code></pre>
<div class="figure">
<img src="Chapter%2020%20-%20Directed%20Graphs_files/Chapter%2020%20-%20Directed%20Graphs_19_0.svg" alt="" />
<p class="caption">svg</p>
</div>
<p>Here is a more formal definition of d-separation. Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be distinct vertices and let <span class="math inline">\(W\)</span> be a set of vertices not containing <span class="math inline">\(X\)</span> or <span class="math inline">\(Y\)</span>. Then <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are <strong>d-separated given <span class="math inline">\(W\)</span></strong> if there exists no undirected path <span class="math inline">\(U\)</span> between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> such that (i) every collider on <span class="math inline">\(U\)</span> has a descendant in <span class="math inline">\(W\)</span> and (ii) no other vertex on <span class="math inline">\(U\)</span> is in <span class="math inline">\(W\)</span>. If <span class="math inline">\(U\)</span>, <span class="math inline">\(V\)</span>, <span class="math inline">\(W\)</span> are distinct sets of vertices and <span class="math inline">\(U\)</span> and <span class="math inline">\(V\)</span> are not empty, then <span class="math inline">\(U\)</span> and <span class="math inline">\(V\)</span> are d-separated given <span class="math inline">\(W\)</span> if for every <span class="math inline">\(X \in U\)</span> and <span class="math inline">\(Y \in V\)</span>, <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are d-separated given <span class="math inline">\(W\)</span>. Vertices that are not d-separated are said to be d-connected.</p>
<p><strong>Theorem 20.7 (Spirtes, Glymour and Scheines)</strong>. Let <span class="math inline">\(A\)</span>, <span class="math inline">\(B\)</span> and <span class="math inline">\(C\)</span> be disjoint sets of vertices. Then <span class="math inline">\(A \text{ ⫫ } B | C\)</span> if and only if <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are d-separated by <span class="math inline">\(C\)</span>.</p>
<p>Graphs that look different may imply the same independence relations. If <span class="math inline">\(\mathcal{G}\)</span> is a DAG, we let <span class="math inline">\(\mathcal{I}(\mathcal{G})\)</span> denote all the independence statements implied by <span class="math inline">\(\mathcal{G}\)</span>. Two DAG’s <span class="math inline">\(\mathcal{G}_1\)</span> and <span class="math inline">\(\mathcal{G}_2\)</span> for the same variables <span class="math inline">\(V\)</span> are <strong>Markov equivalent</strong> if <span class="math inline">\(\mathcal{I}(\mathcal{G}_1) = \mathcal{I}(\mathcal{G}_2)\)</span>. Given a DAG <span class="math inline">\(\mathcal{G}\)</span>, let <span class="math inline">\(\text{skeleton}(\mathcal{G})\)</span> denote the undirected graph obtained by replacing the arrows with undirected edges.</p>
<p><strong>Theorem 20.9</strong>. Two DAG’s <span class="math inline">\(\mathcal{G}_1\)</span> and <span class="math inline">\(\mathcal{G}_2\)</span> are Markov equivalent if and only if (i) <span class="math inline">\(\text{skeleton}(\mathcal{G}_1) = \text{skeleton}(\mathcal{G}_2)\)</span> and (ii) <span class="math inline">\(\mathcal{G}_1\)</span> and <span class="math inline">\(\mathcal{G}_2\)</span> have the same colliders.</p>
</div>
<div id="estimation-for-dags" class="section level3">
<h3>20.5 Estimation for DAG’s</h3>
<p>Let <span class="math inline">\(\mathcal{G}\)</span> be a DAG. Assume that all variables <span class="math inline">\(V = \{ X_1, \dots, X_m \}\)</span> are discrete. The probability function can be written</p>
<p><span class="math display">\[ p(v) = \prod_{i=1}^k p(x_i | \pi_i) \]</span></p>
<p>To estimate <span class="math inline">\(p(v)\)</span> we need to estimate <span class="math inline">\(p(x_i | \pi_i)\)</span> for each <span class="math inline">\(i\)</span>. Think of the parents of <span class="math inline">\(X_i\)</span> as one discrete variable <span class="math inline">\(\tilde{X}_i\)</span> with many levels. For example, suppose that <span class="math inline">\(X_3\)</span> has parents <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>, and that <span class="math inline">\(X_1 \in \{ 0, 1, 2 \}\)</span> while <span class="math inline">\(X_2 \in \{ 0, 1 \}\)</span>. We can regard the parents <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> as a single variable <span class="math inline">\(\tilde{X}_3\)</span> defined by</p>
<p><span class="math display">\[ \tilde{X}_3 = \begin{cases}
1 &amp; \text{if } X_1 = 0, X_2 = 0\\
2 &amp; \text{if } X_1 = 0, X_2 = 1\\
3 &amp; \text{if } X_1 = 1, X_2 = 0\\
4 &amp; \text{if } X_1 = 1, X_2 = 1\\
5 &amp; \text{if } X_1 = 2, X_2 = 0\\
6 &amp; \text{if } X_1 = 2, X_2 = 1\\
\end{cases}\]</span></p>
<p>Hence we can write</p>
<p><span class="math display">\[ p(v) = \prod_{i=1}^k p(x_i | \tilde{x}_i) \]</span></p>
<p><strong>Theorem 20.11</strong>. Let <span class="math inline">\(V_1, \dots, V_n\)</span> be IID random vectors from distribution <span class="math inline">\(p\)</span> where</p>
<p><span class="math display">\[ p(v) = \prod_{i=1}^k p(x_i | \tilde{x}_i) \]</span></p>
<p>The maximum likelihood estimator of <span class="math inline">\(p\)</span> is</p>
<p><span class="math display">\[ \hat{p}(v) = \prod_{i=1}^k \hat{p}(x_i | \tilde{x}_i) \]</span></p>
<p>where</p>
<p><span class="math display">\[ \hat{p}(x_i | \tilde{x}_i) = \frac{\# \{i : X_i = x_i \text{ and } \tilde{X}_i = \tilde{x}_i \}}{\# \{i : \tilde{X}_i = \tilde{x}_i\}} \]</span></p>
<p>It is possible to extend these ideas to continuous random variables as well. For example, we might use some parametric model <span class="math inline">\(p(x | \pi_x; \theta_x)\)</span> for each conditional density. The likelihood function is then</p>
<p><span class="math display">\[ \mathcal{L}(\theta) = \prod_{i=1}^n p(V_i; \theta) = \prod_{i=1}^n \prod_{j=1}^m p(X_{ij} | \pi_j; \theta_j) \]</span></p>
<p>where <span class="math inline">\(X_{ij}\)</span> is the value of <span class="math inline">\(X_j\)</span> for the <span class="math inline">\(i\)</span>-th data point and <span class="math inline">\(\theta_j\)</span> are the parameters for the <span class="math inline">\(j\)</span>-th conditional density. We can then proceed using maximum likelihood.</p>
<p>So far, we have assumed that the structure of the DAG is given. One can also try to estimate the structure of the DAG itself from the data. However, there are many possible DAG’s so you would need much data for such a method to be reliable. Producing a valid, accurate confidence set for the DAG structure would require astronomical sample sizes. DAG’s are thus most useful for encoding conditional independence information rather than discovering it.</p>
</div>
<div id="causation-revisited" class="section level3">
<h3>20.6 Causation Revisited</h3>
<p>We discussed causation in Chapter 19 using the idea of counterfactual random variables. A different approach uses DAG’s. The two approaches are mathematically equivalent though they appear to be quite different. The extra element in DAG is the idea of <strong>intervention</strong>.</p>
<pre class="python"><code>from graphviz import Digraph

d = Digraph()

d.edge(&#39;X&#39;, &#39;Y&#39;)
d.edge(&#39;Y&#39;, &#39;Z&#39;)
d.edge(&#39;X&#39;, &#39;Z&#39;)

d</code></pre>
<div class="figure">
<img src="Chapter%2020%20-%20Directed%20Graphs_files/Chapter%2020%20-%20Directed%20Graphs_31_0.svg" alt="" />
<p class="caption">svg</p>
</div>
<p>Consider the DAG above. The probability function for a distribution consistent with it has the form <span class="math inline">\(p(x, y, z) = p(x)p(y | x)p(z| x, y)\)</span>. Here is pseudo-code for generating from this distribution:</p>
<p><span class="math display">\[\begin{align}
\text{for  } &amp;i = 1, ..., n:\\
  &amp;x_i \text{  &lt;-  } p_X(x_i)\\
  &amp;y_i \text{  &lt;-  } p_{Y | X}(y_i | x_i)\\
  &amp;z_i \text{  &lt;-  } p_{Z | X, Y}(z_i | x_i, y_i)\\
\end{align}\]</span></p>
<p>Suppose we repeat this code many times yielding data <span class="math inline">\((x_1, y_1, z_1), \dots, (x_n, y_n, z_n)\)</span>. Among all the times that we observe <span class="math inline">\(Y = y\)</span>, how often is <span class="math inline">\(Z = z\)</span>? The answer to this question is given by the conditional distribution of <span class="math inline">\(Z | Y\)</span>.</p>
<p><span class="math display">\[
\begin{align}
\mathbb{P}(Z = z | Y = y) &amp;= \frac{\mathbb{P}(Y = y, Z = z)}{\mathbb{P}(Y = y)} = \frac{p(y, z)}{p(y)} \\
&amp;= \frac{\sum_x p(x, y, z)}{p(y)} = \frac{\sum_x p(x) p(y | x) p(z | x, y)}{p(y)} \\
&amp;= \sum_x p(z | x, y) \frac{p(y | x) p(x)}{p(y)} = \sum_x p(z | x, y) \frac{p(x, y)}{p(y)} \\
&amp;= \sum_x p(z | x, y) p(x | y)
\end{align}
\]</span></p>
<p>Now suppose we <strong>intervene</strong> by changing the computer code. Specifically, suppose we fix <span class="math inline">\(Y\)</span> at the value <span class="math inline">\(y\)</span>. The code now looks like this:</p>
<p><span class="math display">\[\begin{align}
\text{set  } &amp;Y = y\\
\text{for  } &amp;i = 1, ..., n:\\
  &amp;x_i \text{  &lt;-  } p_X(x_i)\\
  &amp;z_i \text{  &lt;-  } p_{Z | X, Y}(z_i | x_i, y)\\
\end{align}\]</span></p>
<p>Having <strong>set</strong> <span class="math inline">\(Y = y\)</span>, how often was <span class="math inline">\(Z = z\)</span>? To answer, note that the intervention has changed the joint probability to be</p>
<p><span class="math display">\[ p^*(x, z) = p(x) p(z | x, y) \]</span></p>
<p>The answer to our question is given by the marginal distribution</p>
<p><span class="math display">\[ p^*(z) = \sum_x p^*(x, z) = \sum_x p(x) p(z | x, y) \]</span></p>
<p>We shall denote this as <span class="math inline">\(\mathbb{P}(Z = z | Y := y)\)</span> or <span class="math inline">\(p(z | Y := y)\)</span>. We call <span class="math inline">\(\mathbb{P}(Z = z | Y = y)\)</span> <strong>conditioning by observation</strong> or <strong>passive conditioning</strong>. We call <span class="math inline">\(\mathbb{P}(Z = z | Y := y)\)</span> <strong>conditioning by intervention</strong> or <strong>active conditioning</strong>.</p>
<ul>
<li>Passive conditioning is used to answer a predictive question like “Given that Joe smokes, what is the probability that he will get lung cancer?”</li>
<li>Active conditioning is used to answer a predictive question like “If Joe quits smoking, what is the probability that he will get lung cancer?”</li>
</ul>
<p>Consider a pair <span class="math inline">\((\mathcal{G}, \mathbb{P})\)</span> where <span class="math inline">\(\mathcal{G}\)</span> is a DAG and <span class="math inline">\(\mathbb{P}\)</span> is a distribution for the variables <span class="math inline">\(V\)</span> of the DAG. Let <span class="math inline">\(p\)</span> denote the probability function for <span class="math inline">\(p\)</span>. Consider intervening and fixing a variable <span class="math inline">\(X\)</span> to be equal to <span class="math inline">\(x\)</span>. We represent the intervention by doing two things:</p>
<ol style="list-style-type: decimal">
<li>Create a new DAG <span class="math inline">\(\mathcal{G}^*\)</span> by removing all arrows pointing into <span class="math inline">\(X\)</span>;</li>
<li>Create a new distribution <span class="math inline">\(\mathbb{P}^*\)</span> with probability function <span class="math inline">\(p^*(v) = \mathbb{P}(V = v | X := x)\)</span> by removing the term <span class="math inline">\(p(x | \pi_X)\)</span> from <span class="math inline">\(p(v)\)</span>.</li>
</ol>
<p>The new pair <span class="math inline">\((\mathcal{G}^*, \mathbb{P}^*)\)</span> represents the intervention “set <span class="math inline">\(X = x\)</span>.”</p>
<p>We can use DAG’s to represent confounding variables. If <span class="math inline">\(X\)</span> is a treatment and <span class="math inline">\(Y\)</span> is an outcome, a confounding variable <span class="math inline">\(Z\)</span> is a variable with arrows into both <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. It is easy to check, using the formalism of interventions, that the following facts are true.</p>
<ul>
<li>In a randomized study, the arrow between <span class="math inline">\(Z\)</span> and <span class="math inline">\(X\)</span> is broken. In this case, even with <span class="math inline">\(Z\)</span> unobserved, the causal relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is estimable because it can be shown that <span class="math inline">\(\mathbb{E}(Y | X := x) = \mathbb{E}(Y | X = x)\)</span> which does not involved the unobserved <span class="math inline">\(Z\)</span>.<br />
</li>
<li>In an observational study, with all confounders observed, we again get <span class="math inline">\(\mathbb{E}(Y | X := x) = \int \mathbb{E}(Y | X = x, Z = z) d F_Z(z)\)</span> as a formula for the expectation. if <span class="math inline">\(Z\)</span> is unobserved then we cannot estimate the causal effect because this expectation involves the unobserved <span class="math inline">\(Z\)</span>. <span class="math inline">\(\mathbb{P}(Y = y | X = x) \neq \mathbb{P}(Y = y | X := x)\)</span> which is just another way of saying that causation is not association.</li>
</ul>
<p>In fact, we can make a precise connection between DAG’s and counterfactuals as follows. Suppose that <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are binary. Define the confounding variable <span class="math inline">\(Z\)</span> by</p>
<p><span class="math display">\[ Z = \begin{cases}
1 &amp; \text{if } (C_0, C_1) = (0, 0) \\
2 &amp; \text{if } (C_0, C_1) = (0, 1) \\
3 &amp; \text{if } (C_0, C_1) = (1, 0) \\
4 &amp; \text{if } (C_0, C_1) = (1, 1)
\end{cases}\]</span></p>
<p>From this approach, one can make the correspondence between the DAG and counterfactual approaches explicit. This is left as an exercise for the reader.</p>
</div>
<div id="exercises" class="section level3">
<h3>20.8 Exercises</h3>
<p><strong>Exercise 20.8.1</strong>. Consider the three DAG’s below without a collider. Prove that <span class="math inline">\(X \text{ ⫫ } Z | Y\)</span>.</p>
<pre class="python"><code>from graphviz import Digraph

a = Digraph()
a.edge(&#39;X&#39;, &#39;Y&#39;)
a.edge(&#39;Y&#39;, &#39;Z&#39;)

a</code></pre>
<div class="figure">
<img src="Chapter%2020%20-%20Directed%20Graphs_files/Chapter%2020%20-%20Directed%20Graphs_42_0.svg" alt="" />
<p class="caption">svg</p>
</div>
<pre class="python"><code>from graphviz import Digraph

b = Digraph()
b.edge(&#39;Y&#39;, &#39;X&#39;)
b.edge(&#39;Z&#39;, &#39;Y&#39;)

b</code></pre>
<div class="figure">
<img src="Chapter%2020%20-%20Directed%20Graphs_files/Chapter%2020%20-%20Directed%20Graphs_43_0.svg" alt="" />
<p class="caption">svg</p>
</div>
<pre class="python"><code>from graphviz import Digraph

c = Digraph()
c.edge(&#39;Y&#39;, &#39;X&#39;)
c.edge(&#39;Y&#39;, &#39;Z&#39;)

c</code></pre>
<div class="figure">
<img src="Chapter%2020%20-%20Directed%20Graphs_files/Chapter%2020%20-%20Directed%20Graphs_44_0.svg" alt="" />
<p class="caption">svg</p>
</div>
<p><strong>Solution</strong>.</p>
<p>Note that for all three graphs <span class="math inline">\(X\)</span> and <span class="math inline">\(Z\)</span> are d-separated by <span class="math inline">\(Y\)</span> – the only undirected path <span class="math inline">\((X, Y, Z)\)</span> has no colliders, and all its vertices other than <span class="math inline">\(X\)</span> and <span class="math inline">\(Z\)</span> (i.e. <span class="math inline">\(\{ Y \}\)</span>) are listed in the d-separation condition. Thus, conditions (i) and (ii) on the definition of d-separation holds.</p>
<p>Therefore, from Theorem 20.7, <span class="math inline">\(X \text{ ⫫ } Z | Y\)</span>.</p>
<p><strong>Exercise 20.8.2</strong>. Consider the DAG below. Prove that <span class="math inline">\(X \text{ ⫫ } Z\)</span> and that <span class="math inline">\(X\)</span> and <span class="math inline">\(Z\)</span> are dependent given <span class="math inline">\(Y\)</span>.</p>
<pre class="python"><code>from graphviz import Digraph

d = Digraph()
d.edge(&#39;X&#39;, &#39;Y&#39;)
d.edge(&#39;Z&#39;, &#39;Y&#39;)

d</code></pre>
<div class="figure">
<img src="Chapter%2020%20-%20Directed%20Graphs_files/Chapter%2020%20-%20Directed%20Graphs_47_0.svg" alt="" />
<p class="caption">svg</p>
</div>
<p><strong>Solution</strong>.</p>
<p>Note that <span class="math inline">\(X\)</span> and <span class="math inline">\(Z\)</span> are d-connected by <span class="math inline">\(Y\)</span> – the undirected path <span class="math inline">\((X, Y, Z)\)</span> has one collider containing <span class="math inline">\(Y\)</span>, so the definition of d-separation does not hold. Therefore, from Theorem 20.7, <span class="math inline">\(X\)</span> and <span class="math inline">\(Z\)</span> are dependent given <span class="math inline">\(Y\)</span>.</p>
<p>On the other hand, note that <span class="math inline">\(X\)</span> and <span class="math inline">\(Z\)</span> are d-separated given the empty set – the only path from <span class="math inline">\(X\)</span> to <span class="math inline">\(Z\)</span> goes through <span class="math inline">\(Y\)</span>, which is not in the empty set. Thus, from Theorem 20.7, <span class="math inline">\(X \text{ ⫫ } Z\)</span>.</p>
<p><strong>Exercise 20.8.3</strong>. Let <span class="math inline">\(X \in \{0, 1\}\)</span>, <span class="math inline">\(Y \in \{0, 1\}\)</span>, <span class="math inline">\(Z \in \{ 0, 1, 2 \}\)</span>. Suppose the distribution of <span class="math inline">\((X, Y, Z)\)</span> is Markov to:</p>
<p><span class="math display">\[ X \longrightarrow Y \longrightarrow Z \]</span></p>
<p>Create a joint distribution <span class="math inline">\(p(x, y, z)\)</span> that is Markov to this DAG. Generate 1000 random vectors from this distribution. Estimate the distribution from the data using maximum likelihood. Compare the estimated distribution to the true distribution. Let <span class="math inline">\(\theta = (\theta_{000}, \theta_{001}, \dots, \theta_{112})\)</span> where <span class="math inline">\(\theta_{rst} = \mathbb{P}(X = r, Y = s, Z = t)\)</span>. Use the bootstrap to get the standard errors and 95% confidence interval for these 12 parameters.</p>
<p><strong>Solution</strong>. Let’s create the following distributions:</p>
<p><span class="math display">\[ p_X(x) = \begin{cases}
1/2 &amp; \text{if } x \in \{0, 1\} \\
0 &amp; \text{otherwise}
\end{cases}
\quad
p_{Y | X}(y | x) = \begin{cases}
3/4 &amp; \text{if } y = x \\
1/4 &amp; \text{if } x + y = 1 \\
0 &amp; \text{otherwise}
\end{cases}
\quad \\
p_{Z | Y}(z | y) = \begin{cases}
1/2 &amp;\text{if } z = y \text{ and } z \in \{ 0, 1, 2 \}\\
1/4 &amp;\text{if } z \neq y \text{ and } z \in \{ 0, 1, 2 \} \\
0 &amp;\text{otherwise}
\end{cases}
\]</span></p>
<p>and a joint distribution</p>
<p><span class="math display">\[ p(x, y, z) = p_X(x) p_{Y | X}(y | x) p_{Z | Y}(z | y) \]</span></p>
<p>By construction, this distribution is Markov to the given DAG.</p>
<p><span class="math display">\[p(x=0, y=0, z=0)=\frac{1}{2}\times\frac{3}{4}\times\frac{1}{2}\]</span></p>
<p>More explicitly, the joint probability distribution is:</p>
<p><span class="math display">\[
\begin{array}{ccc|c}
X &amp; Y &amp; Z &amp; p \\
\hline
0 &amp; 0 &amp; 0 &amp; .18750 \\
0 &amp; 0 &amp; 1 &amp; .09375 \\
0 &amp; 0 &amp; 2 &amp; .09375 \\
0 &amp; 1 &amp; 0 &amp; .03125 \\
0 &amp; 1 &amp; 1 &amp; .06250 \\
0 &amp; 1 &amp; 2 &amp; .03125 \\
1 &amp; 0 &amp; 0 &amp; .06250 \\
1 &amp; 0 &amp; 1 &amp; .03125 \\
1 &amp; 0 &amp; 2 &amp; .03125 \\
1 &amp; 1 &amp; 0 &amp; .09375 \\
1 &amp; 1 &amp; 1 &amp; .18750 \\
1 &amp; 1 &amp; 2 &amp; .09375 \\
\end{array}
\]</span></p>
<p>The maximum likelihood estimator of <span class="math inline">\(p\)</span> is</p>
<p><span class="math display">\[ \hat{p}(v) = \prod_{i=1}^k \hat{p}(x_i | \tilde{x}_i) \]</span></p>
<p>where</p>
<p><span class="math display">\[ \hat{p}(x_i | \tilde{x}_i) = \frac{\# \{i : X_i = x_i \text{ and } \tilde{X}_i = \tilde{x}_i \}}{\# \{i : \tilde{X}_i = \tilde{x}_i \}} \]</span></p>
<pre class="python"><code>seeds = np.random.uniform(low=0, high=1, size=(10, 3)) # nX3 array
result = np.zeros((10, 3), dtype=int)
result[:, 0] = seeds[:, 0] &lt; 1/2
result[:, 1] = np.where(seeds[:, 1] &lt; 3/4, result[:, 0], 1 - result[:, 0])
result[:, 2] = np.where(seeds[:, 2] &lt; 1/2, result[:, 1], (result[:, 1] + np.where(seeds[:, 2] &lt; 3/4, 1, 2)) % 3)
result</code></pre>
<pre><code>array([[0, 1, 1],
       [1, 1, 2],
       [1, 0, 0],
       [0, 1, 1],
       [0, 0, 1],
       [1, 1, 0],
       [1, 1, 2],
       [0, 1, 0],
       [1, 1, 0],
       [1, 1, 1]])</code></pre>
<pre class="python"><code>import numpy as np

def generate_samples(n):
    seeds = np.random.uniform(low=0, high=1, size=(n, 3)) # nX3 array
    result = np.zeros((n, 3), dtype=int)
    result[:, 0] = seeds[:, 0] &lt; 1/2
    result[:, 1] = np.where(seeds[:, 1] &lt; 3/4, result[:, 0], 1 - result[:, 0])
    result[:, 2] = np.where(seeds[:, 2] &lt; 1/2, result[:, 1], (result[:, 1] + np.where(seeds[:, 2] &lt; 3/4, 1, 2)) % 3)
    return result

def estimate_parameters(X):
    n = X.shape[0]
    
    p_hat_x0 = np.sum(X[:, 0] == 0) / n
    p_hat_x1 = np.sum(X[:, 0] == 1) / n
    
    p_hat_y0_x0 = np.sum((X[:, 0] == 0) &amp; (X[:, 1] == 0)) / np.sum(X[:, 0] == 0)
    p_hat_y1_x0 = np.sum((X[:, 0] == 0) &amp; (X[:, 1] == 1)) / np.sum(X[:, 0] == 0)
    
    p_hat_y0_x1 = np.sum((X[:, 0] == 1) &amp; (X[:, 1] == 0)) / np.sum(X[:, 0] == 1)
    p_hat_y1_x1 = np.sum((X[:, 0] == 1) &amp; (X[:, 1] == 1)) / np.sum(X[:, 0] == 1)
    
    p_hat_z0_y0 = np.sum((X[:, 2] == 0) &amp; (X[:, 1] == 0)) / np.sum(X[:, 1] == 0)
    p_hat_z1_y0 = np.sum((X[:, 2] == 1) &amp; (X[:, 1] == 0)) / np.sum(X[:, 1] == 0)
    p_hat_z2_y0 = np.sum((X[:, 2] == 2) &amp; (X[:, 1] == 0)) / np.sum(X[:, 1] == 0)

    p_hat_z0_y1 = np.sum((X[:, 2] == 0) &amp; (X[:, 1] == 1)) / np.sum(X[:, 1] == 1)
    p_hat_z1_y1 = np.sum((X[:, 2] == 1) &amp; (X[:, 1] == 1)) / np.sum(X[:, 1] == 1)
    p_hat_z2_y1 = np.sum((X[:, 2] == 2) &amp; (X[:, 1] == 1)) / np.sum(X[:, 1] == 1)
    
    theta_hat = np.array([
        p_hat_x0 * p_hat_y0_x0 * p_hat_z0_y0,
        p_hat_x0 * p_hat_y0_x0 * p_hat_z1_y0,
        p_hat_x0 * p_hat_y0_x0 * p_hat_z2_y0,
        
        p_hat_x0 * p_hat_y1_x0 * p_hat_z0_y1,
        p_hat_x0 * p_hat_y1_x0 * p_hat_z1_y1,
        p_hat_x0 * p_hat_y1_x0 * p_hat_z2_y1,
        
        p_hat_x1 * p_hat_y0_x1 * p_hat_z0_y0,
        p_hat_x1 * p_hat_y0_x1 * p_hat_z1_y0,
        p_hat_x1 * p_hat_y0_x1 * p_hat_z2_y0,
        
        p_hat_x1 * p_hat_y1_x1 * p_hat_z0_y1,
        p_hat_x1 * p_hat_y1_x1 * p_hat_z1_y1,
        p_hat_x1 * p_hat_y1_x1 * p_hat_z2_y1
    ])
    
    return theta_hat

true_distribution = np.array([
    0.5 * 0.75 * 0.5,
    0.5 * 0.75 * 0.25,
    0.5 * 0.75 * 0.25,

    0.5 * 0.25 * 0.25,
    0.5 * 0.25 * 0.5,
    0.5 * 0.25 * 0.25,

    0.5 * 0.25 * 0.5,
    0.5 * 0.25 * 0.25,
    0.5 * 0.25 * 0.25,

    0.5 * 0.75 * 0.25,
    0.5 * 0.75 * 0.5,
    0.5 * 0.75 * 0.25
])</code></pre>
<pre class="python"><code>n = 1000
X = generate_samples(n)</code></pre>
<pre class="python"><code>theta_hat = estimate_parameters(X)</code></pre>
<pre class="python"><code>import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline

sns.set()

ind = np.arange(12)
labels = [&#39;000&#39;, &#39;001&#39;, &#39;002&#39;, &#39;010&#39;, &#39;011&#39;, &#39;012&#39;, &#39;100&#39;, &#39;101&#39;, &#39;102&#39;, &#39;110&#39;, &#39;111&#39;, &#39;112&#39;]

x = np.arange(len(labels))
fig, ax = plt.subplots(figsize=(12, 8))
ax.plot(x, theta_hat, label=&#39;Estimated&#39;, linestyle=&#39;&#39;, marker=&#39;+&#39;, ms=10)
ax.plot(x, true_distribution, label=&#39;Actual&#39;, linestyle=&#39;&#39;, marker=&#39;x&#39;, ms=10)

ax.set_title(&#39;Estimated vs actual parameters&#39;)
ax.set_xticks(x)
ax.set_xticklabels(labels)
ax.legend()

fig.tight_layout()

plt.show();</code></pre>
<div class="figure">
<img src="Chapter%2020%20-%20Directed%20Graphs_files/Chapter%2020%20-%20Directed%20Graphs_55_0.png" alt="" />
<p class="caption">png</p>
</div>
<pre class="python"><code># Bootstrapping:
from tqdm import notebook

B = 10000
parameters_bootstrap = np.empty((B, 12))
for i in notebook.tqdm(range(B)):
    XX = generate_samples(n)
    parameters_bootstrap[i] = estimate_parameters(XX)
    
se_theta_hat = parameters_bootstrap.std(axis=0)
theta_p05 = np.quantile(parameters_bootstrap, 0.05, axis=0)
theta_p95 = np.quantile(parameters_bootstrap, 0.95, axis=0)</code></pre>
<pre><code>  0%|          | 0/10000 [00:00&lt;?, ?it/s]</code></pre>
<pre class="python"><code>import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline

sns.set()

ind = np.arange(12)
labels = [&#39;000&#39;, &#39;001&#39;, &#39;002&#39;, &#39;010&#39;, &#39;011&#39;, &#39;012&#39;, &#39;100&#39;, &#39;101&#39;, &#39;102&#39;, &#39;110&#39;, &#39;111&#39;, &#39;112&#39;]

x = np.arange(len(labels))
fig, ax = plt.subplots(figsize=(12, 8))
ax.plot(x, theta_hat, label=&#39;Estimated&#39;, linestyle=&#39;&#39;, marker=&#39;+&#39;, ms=10)
ax.plot(x, true_distribution, label=&#39;Actual&#39;, linestyle=&#39;&#39;, marker=&#39;x&#39;, ms=10)
ax.plot(x, theta_p95, label=&#39;95% percentile&#39;, linestyle=&#39;&#39;, marker=&#39;_&#39;, ms=10)
ax.plot(x, theta_p05, label=&#39;5% percentile&#39;, linestyle=&#39;&#39;, marker=&#39;_&#39;, ms=10)
ax.plot(x, theta_hat - 2*se_theta_hat, label=&#39;Estimated - 2 SE&#39;, linestyle=&#39;&#39;, marker=&#39;1&#39;, ms=10)
ax.plot(x, theta_hat + 2*se_theta_hat, label=&#39;Estimated + 2 SE&#39;, linestyle=&#39;&#39;, marker=&#39;2&#39;, ms=10)

ax.set_title(&#39;Estimated vs actual parameters (with confidence + SE bounds)&#39;)
ax.set_xticks(x)
ax.set_xticklabels(labels)
ax.legend()

fig.tight_layout()

plt.show();</code></pre>
<div class="figure">
<img src="Chapter%2020%20-%20Directed%20Graphs_files/Chapter%2020%20-%20Directed%20Graphs_57_0.png" alt="" />
<p class="caption">png</p>
</div>
<p><strong>Exercise 20.8.4</strong>. Let <span class="math inline">\(V = (X, Y, Z)\)</span> have the joint distribution</p>
<p><span class="math display">\[
\begin{align}
X &amp;\sim \text{Bernoulli}\left(\frac{1}{2}\right) \\
Y | X = x &amp;\sim \text{Bernoulli}\left(\frac{e^{4x - 2}}{1 + e^{4x - 2}}\right) \\
Z | X = x, Y = y &amp;\sim \text{Bernoulli}\left(\frac{e^{2(x+y)-2}}{1 + e^{2(x+y)-2}}\right)
\end{align}
\]</span></p>
<p><strong>(a)</strong> Find an expression for <span class="math inline">\(\mathbb{P}(Z = z | Y = y)\)</span>. In particular, find <span class="math inline">\(\mathbb{P}(Z = 1 | Y = 1)\)</span>.</p>
<p><strong>(b)</strong> Write a program to simulate the model. Conduct a simulation and compute <span class="math inline">\(\mathbb{P}(Z = 1 | Y = 1)\)</span> empirically. Plot this as a function of the simulation size <span class="math inline">\(N\)</span>. It should converge to the theoretical value you computed in (a).</p>
<p><strong>(c)</strong> Write down an expression for <span class="math inline">\(\mathbb{P}(Z = 1 | Y := y)\)</span>. In particular, find <span class="math inline">\(\mathbb{P}(Z = 1 | Y := 1)\)</span>.</p>
<p><strong>(d)</strong> Modify your program to simulate the intervention “set <span class="math inline">\(Y = 1\)</span>.” Conduct a simulation and compute <span class="math inline">\(\mathbb{P}(Z = 1 | Y := 1)\)</span> empirically. Plot this as a function of the simulation size <span class="math inline">\(N\)</span>. It should converge to the theoretical value you computed in (c).</p>
<p><strong>Solution</strong>.</p>
<p><strong>(a)</strong></p>
<p>We have:</p>
<p><span class="math display">\[
\begin{align}
\mathbb{P}(Z = z | Y = y) &amp;= \frac{\mathbb{P}(Y = y, Z = z)}{\mathbb{P}(Y = y)} = \frac{p(y, z)}{p(y)} \\
&amp;= \frac{\sum_x p(x, y, z)}{p(y)} = \frac{\sum_x p(x) p(y | x) p(z | x, y)}{p(y)} \\
&amp;= \sum_x p(z | x, y) \frac{p(y | x) p(x)}{p(y)} = \sum_x p(z | x, y) \frac{p(x, y)}{p(y)} \\
&amp;= \sum_x p(z | x, y) p(x | y)
\end{align}
\]</span></p>
<p>Let <span class="math inline">\(\alpha = \frac{e^{-2}}{1 + e^{-2}}\)</span>. Note that <span class="math inline">\(\frac{e^{-2}}{1 + e^{-2}} + \frac{e^{2}}{1 + e^{2}} = 1\)</span>, so <span class="math inline">\(1 - \alpha = \frac{e^{2}}{1 + e^{2}}\)</span>.</p>
<p>Looking explicitly at each potential outcome for the joint distribution of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, we get:</p>
<p><span class="math display">\[
\begin{array}{c | cc | c}
p(x, y) &amp; Y = 0 &amp; Y = 1 &amp; \\
\hline
X = 0 &amp; \alpha / 2 &amp; (1 - \alpha) / 2 &amp; 1/2 \\
X = 1 &amp; \alpha / 2 &amp; (1 - \alpha) / 2 &amp; 1/2 \\
\hline
      &amp; \alpha &amp; (1 - \alpha) &amp; 1
\end{array}
\]</span></p>
<p>Therefore,</p>
<p><span class="math display">\[ X | Y = 0 \sim \text{Bernoulli}\left( \alpha \right) 
\quad \text{and} \quad
X | Y = 1 \sim \text{Bernoulli}\left( 1 - \alpha \right) 
\]</span></p>
<p>or, more generally,</p>
<p><span class="math display">\[
X | Y = y \sim \text{Bernoulli}\left( \frac{e^{4y - 2}}{1 + e^{4y - 2}} \right) 
\]</span></p>
<p>Then, for <span class="math inline">\(Z = 1\)</span> and <span class="math inline">\(y \in \{ 0, 1 \}\)</span>,</p>
<p><span class="math display">\[
\begin{align}
\mathbb{P}(Z = 1 | Y = y) &amp;= \sum_x p(z = 1 | x, y) p(x | y) \\
&amp;= p(z = 1 | x = 0, y) p(x = 0 | y) + p(z = 1 | x = 1, y) p(x = 1 | y) \\
&amp;= \frac{e^{2y - 2}}{1 + e^{2y - 2}} \left( 1 - \frac{e^{4y - 2}}{1 + e^{4y - 2}} \right) + \frac{e^{2y}}{1 + e^{2y}} \frac{e^{4y - 2}}{1 + e^{4y - 2}}
\end{align}
\]</span></p>
<p>More explicitly,</p>
<p><span class="math display">\[ 
\begin{align}
\mathbb{P}(Z = 1 | Y = 0) &amp;= \alpha (1 - \alpha) + \alpha/2 &amp;\approx 0.164595 \\
\mathbb{P}(Z = 0 | Y = 0) &amp;= 1 - \mathbb{P}(Z = 1 | Y = 0) \\
&amp;= 1 - \alpha (1 - \alpha) - \alpha/2 &amp;\approx 0.835405\\
\mathbb{P}(Z = 1 | Y = 1) &amp;= \frac{1}{2} \alpha + (1 - \alpha)^2 &amp;\approx 0.8354\\
\mathbb{P}(Z = 0 | Y = 1) &amp;= 1 - \mathbb{P}(Z = 1 | Y = 1) \\
&amp;= 1 - \left(\frac{1}{2} \alpha + (1 - \alpha)^2 \right) &amp;\approx 0.1646\\
\end{align}
\]</span></p>
<p><strong>(b)</strong></p>
<pre class="python"><code>import numpy as np

def generate_samples(n):
    seeds = np.random.uniform(low=0, high=1, size=(n, 3))
    
    x = np.where(seeds[:, 0] &lt; 1/2, 1, 0)
    y = np.where(seeds[:, 1] &lt; np.exp(4*x - 2) / (1 + np.exp(4*x - 2)), 1, 0)
    z = np.where(seeds[:, 2] &lt; np.exp(2*(x + y) - 2) / (1 + np.exp(2*(x + y) - 2)), 1, 0)
    
    result = np.zeros((n, 3), dtype=int)
    result[:, 0] = x
    result[:, 1] = y
    result[:, 2] = z
    
    return result

def estimate_z_given_y(X, z, y):
    # Just estimate 0.5 if there are no valid samples
    if np.sum((X[:, 1] == y)) == 0:
        return 0.5
    return np.sum((X[:, 1] == y) &amp; (X[:, 2] == z)) / np.sum((X[:, 1] == y))</code></pre>
<pre class="python"><code># Generate data
X = generate_samples(10**6)
X</code></pre>
<pre><code>array([[0, 0, 0],
       [1, 1, 0],
       [1, 1, 0],
       ...,
       [0, 0, 0],
       [1, 1, 0],
       [1, 1, 1]])</code></pre>
<pre class="python"><code>X[:5]</code></pre>
<pre><code>array([[0, 0, 0],
       [1, 1, 0],
       [1, 1, 0],
       [1, 1, 0],
       [1, 1, 1]])</code></pre>
<pre class="python"><code># Estimate with various segments of the data
n_values = np.floor(np.logspace(0, 6, base=10)).astype(int)
z_estimates = [estimate_z_given_y(X[:k], 1, 1) for k in n_values]
n_values</code></pre>
<pre><code>array([      1,       1,       1,       2,       3,       4,       5,
             7,       9,      12,      16,      22,      29,      39,
            51,      68,      91,     120,     159,     212,     281,
           372,     494,     655,     868,    1151,    1526,    2023,
          2682,    3556,    4714,    6250,    8286,   10985,   14563,
         19306,   25595,   33932,   44984,   59636,   79060,  104811,
        138949,  184206,  244205,  323745,  429193,  568986,  754312,
       1000000])</code></pre>
<pre class="python"><code>z_estimates</code></pre>
<pre><code>[0.5,
 0.5,
 0.5,
 0.0,
 0.0,
 0.0,
 0.25,
 0.3333333333333333,
 0.42857142857142855,
 0.5,
 0.5555555555555556,
 0.6666666666666666,
 0.7333333333333333,
 0.7,
 0.72,
 0.7837837837837838,
 0.7659574468085106,
 0.8225806451612904,
 0.8292682926829268,
 0.8504672897196262,
 0.8478260869565217,
 0.8191489361702128,
 0.8298755186721992,
 0.8258258258258259,
 0.8256880733944955,
 0.8193202146690519,
 0.8236074270557029,
 0.8341759352881699,
 0.8301886792452831,
 0.8296041308089501,
 0.8311688311688312,
 0.8301208755308722,
 0.8314082084050135,
 0.8321561338289963,
 0.8307092099763133,
 0.8317307692307693,
 0.8337398693839012,
 0.832927191679049,
 0.8323650339649624,
 0.8345917885795187,
 0.8345071317790038,
 0.83506598127973,
 0.8354984610493764,
 0.8349496618756659,
 0.8348298405728236,
 0.8349905024718323,
 0.8350089892362651,
 0.8348522886296351,
 0.835314172700207,
 0.835202742021601]</code></pre>
<pre class="python"><code># Plot estimates

import matplotlib.pyplot as plt
%matplotlib inline

plt.figure(figsize=(12, 8))
plt.scatter(n_values, z_estimates)
plt.xscale(&#39;log&#39;)
plt.xlabel(&#39;N&#39;)
plt.ylabel(&#39;Z | Y = 1&#39;)
plt.title(&#39;Empirical estimates of Z | Y = 1&#39;)
plt.show()</code></pre>
<div class="figure">
<img src="Chapter%2020%20-%20Directed%20Graphs_files/Chapter%2020%20-%20Directed%20Graphs_67_0.png" alt="" />
<p class="caption">png</p>
</div>
<p><strong>(c)</strong></p>
<p>Fixing <span class="math inline">\(Y := y \in \{0, 1\}\)</span>, we get:</p>
<p><span class="math display">\[ 
\begin{align}
\mathbb{P}(Z = 1 | Y := y) &amp;= p^*(z = 1) = \sum_x p^*(x, z = 1) = \sum_x p(x) p(z = 1 | x, y) \\
&amp;= \frac{e^{2y - 2}}{1 + e^{2y - 2}} \left( 1 - \frac{e^{4y - 2}}{1 + e^{4y - 2}} \right) + \frac{e^{2y}}{1 + e^{2y}} \frac{e^{4y - 2}}{1 + e^{4y - 2}}\\
&amp;= \frac{1}{2} \frac{e^{2y-2}}{1 + e^{2y-2}} + \frac{1}{2} \frac{e^{2y}}{1 + e^{2y}}
\end{align}
\]</span></p>
<p>More explicitly,</p>
<p><span class="math display">\[ 
\begin{align}
\mathbb{P}(Z = 1 | Y := 0) &amp;= \frac{1}{2} \alpha + \frac{1}{2}\frac{1}{2} \\
&amp;= \frac{1}{4} + \frac{\alpha}{2} &amp;\approx 0.3096 \\
\mathbb{P}(Z = 0 | Y := 0) &amp;= 1 - \mathbb{P}(Z = 1 | Y := 0) \\
&amp;= \frac{3}{4} - \frac{\alpha}{2} &amp;\approx 0.6904 \\
\mathbb{P}(Z = 1 | Y := 1) &amp;= \frac{1}{2} \frac{1}{2} + \frac{1}{2} \left( 1 - \alpha \right) \\
&amp;= \frac{3}{4} - \frac{\alpha}{2} &amp;\approx 0.6904 \\
\mathbb{P}(Z = 0 | Y := 1) &amp;= 1 - \mathbb{P}(Z = 1 | Y := 1) \\
&amp;= \frac{1}{4} + \frac{\alpha}{2} &amp;\approx 0.3096
\end{align}
\]</span></p>
<p><strong>(d)</strong></p>
<pre class="python"><code>import numpy as np

def generate_samples_modified(n, y0):
    seeds = np.random.uniform(low=0, high=1, size=(n, 3))
    
    x = np.where(seeds[:, 0] &lt; 1/2, 1, 0)
    y = y0 * np.ones(n, dtype=int) # y=1
    z = np.where(seeds[:, 2] &lt; np.exp(2*(x + y) - 2) / (1 + np.exp(2*(x + y) - 2)), 1, 0)
    
    result = np.zeros((n, 3), dtype=int)
    result[:, 0] = x
    result[:, 1] = y
    result[:, 2] = z
    
    return result</code></pre>
<pre class="python"><code># Generate data
Xd = generate_samples_modified(n=10**6, y0=1)
Xd</code></pre>
<pre><code>array([[1, 1, 0],
       [1, 1, 1],
       [0, 1, 1],
       ...,
       [0, 1, 1],
       [0, 1, 1],
       [0, 1, 1]])</code></pre>
<pre class="python"><code>sum(Xd[:,1])</code></pre>
<pre><code>1000000</code></pre>
<pre class="python"><code># Estimate with various segments of the data
n_values = np.floor(np.logspace(0, 6, base=10)).astype(int)
zd_estimates = [estimate_z_given_y(Xd[:k], 1, 1) for k in n_values]</code></pre>
<pre class="python"><code># Plot estimates

import matplotlib.pyplot as plt
%matplotlib inline

plt.figure(figsize=(12, 8))
plt.scatter(n_values, zd_estimates)
plt.xscale(&#39;log&#39;)
plt.xlabel(&#39;N&#39;)
plt.ylabel(&#39;Z | Y := 1&#39;)
plt.title(&#39;Empirical estimates of Z | Y := 1&#39;)
plt.show()</code></pre>
<div class="figure">
<img src="Chapter%2020%20-%20Directed%20Graphs_files/Chapter%2020%20-%20Directed%20Graphs_74_0.png" alt="" />
<p class="caption">png</p>
</div>
<p><strong>Exercise 20.8.5</strong>. This is a continuous, Gaussian version of the last question. Let <span class="math inline">\(V = (X, Y, Z)\)</span> have the following joint distribution:</p>
<p><span class="math display">\[
\begin{align}
X &amp;\sim \text{Normal}(0, 1) \\
Y | X = x &amp;\sim \text{Normal}(\alpha x, 1) \\
Z | X = x, Y = y &amp;\sim \text{Normal}(\beta y + \gamma x, 1)
\end{align}
\]</span></p>
<p>Here, <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\gamma\)</span> are fixed parameters. Economists refer to models like this as <em>structural equation models</em>.</p>
<p><strong>(a)</strong> Find an explicit expression for <span class="math inline">\(f(z | y)\)</span> and <span class="math inline">\(\mathbb{E}(Z | Y = y) = \int z f(z | y) dz\)</span>.</p>
<p><strong>(b)</strong> Find an explicit expression for <span class="math inline">\(f(z | Y := y)\)</span> and then find <span class="math inline">\(\mathbb{E}(Z | Y := y) \equiv \int z f(z | Y := y) dy\)</span>. Compare to (b).</p>
<p><strong>(c)</strong> Find the joint distribution of <span class="math inline">\((Y, Z)\)</span>. Find the correlation <span class="math inline">\(\rho\)</span> between <span class="math inline">\(Y\)</span> and <span class="math inline">\(Z\)</span>.</p>
<p><strong>(d)</strong> Suppose that <span class="math inline">\(X\)</span> is not observed and we try to make causal conclusions from the marginal distribution of <span class="math inline">\((Y, Z)\)</span>. (Think of <span class="math inline">\(X\)</span> as unobserved confounding variables.) In particular, suppose we declare that <span class="math inline">\(Y\)</span> causes <span class="math inline">\(Z\)</span> if <span class="math inline">\(\rho \neq 0\)</span> and we declare that <span class="math inline">\(Y\)</span> does not cause <span class="math inline">\(Z\)</span> if <span class="math inline">\(\rho = 0\)</span>. Show that this will lead to erroneous conclusions.</p>
<p><strong>(e)</strong> Suppose we conduct a randomized experiment in which <span class="math inline">\(Y\)</span> is randomly assigned. To be concrete, suppose that</p>
<p><span class="math display">\[
\begin{align}
X &amp;\sim \text{Normal}(0, 1) \\
Y | X = x &amp;\sim \text{Normal}(\alpha, 1) \\
Z | X = x, Y = y &amp;\sim \text{Normal}(\beta y + \gamma x, 1)
\end{align}
\]</span></p>
<p>Show that the method in (d) now yields correct conclusions i.e. <span class="math inline">\(\rho = 0\)</span> if and only if <span class="math inline">\(f(z | Y := y)\)</span> does not depend on <span class="math inline">\(y\)</span>.</p>
<p><strong>Solution</strong>.</p>
<p><strong>(a)</strong></p>
<p>The joint distribution of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is:</p>
<p><span class="math display">\[ 
\begin{align}
f_{X, Y}(x, y) &amp;= f_X(x) f_{Y | X}(y | x) \\
&amp;= \left(\frac{1}{\sigma_X \sqrt{2\pi}} \exp \left\{-\frac{1}{2} \left(\frac{x - \mu_X}{\sigma_X}\right)^2 \right\} \right)
\left(\frac{1}{\sigma_{Y | X} \sqrt{2\pi}} \exp \left\{-\frac{1}{2} \left(\frac{y - \mu_{Y | X}}{\sigma_{Y | X}}\right)^2 \right\}\right) \\
&amp;= \left(\frac{1}{\sqrt{2\pi}}\right)^2 \frac{1}{\sigma_X}\frac{1}{\sigma_{Y | X}} \exp \left\{ -\frac{1}{2} \left( x^2 + (y - \alpha x)^2\right) \right\} \\
&amp;= \left(\frac{1}{\sqrt{2\pi}}\right)^2 \exp \left\{ -\frac{1}{2} \left( x^2 + (y - \alpha x)^2\right) \right\} \\
&amp;= (2 \pi)^{-2 / 2} \text{det} (\Sigma_{X, Y})^{-1/2} \exp \left\{ -\frac{1}{2} (v - \mu_{X, Y})^T \Sigma_{X, Y}^{-1} (v - \mu_{X, Y})\right\}
\end{align}
\]</span></p>
<p>where</p>
<p><span class="math display">\[
v = \begin{bmatrix}x \\ y\end{bmatrix},
\quad \mu_{X, Y} = \begin{bmatrix}0 \\ 0\end{bmatrix}
\quad \text{and} \quad
\Sigma_{X, Y} = \begin{bmatrix}
1 &amp; \alpha \\
\alpha &amp; 1 + \alpha^2
\end{bmatrix} \quad \text{and} \quad
\text{det} (\Sigma_{X, Y})^{-1/2} =1\\
\quad \text{and} \quad (\Sigma_{X, Y})^{-1} =\begin{bmatrix}
1 + \alpha^2 &amp; -\alpha \\
-\alpha &amp; 1\\
\end{bmatrix} \\
\quad \text{and} \quad
(v - \mu_{X, Y})^T \Sigma_{X, Y}^{-1} (v - \mu_{X, Y})=\begin{bmatrix}x &amp; y\end{bmatrix}\begin{bmatrix}
1 + \alpha^2 &amp; -\alpha \\
-\alpha &amp; 1\\
\end{bmatrix}\begin{bmatrix}x \\ y\end{bmatrix}\\
=(1+a^2)x^2-2axy+y^2\\
=x^2 + (y - \alpha x)^2
\]</span></p>
<p>or, in words, the joint distribution of <span class="math inline">\(X, Y\)</span> is a multivariate normal with mean <span class="math inline">\(\mu_{X, Y} = 0\)</span> and variance <span class="math inline">\(\Sigma_{X, Y}\)</span>.</p>
<p><span class="math display">\[ (X, Y) \sim \text{Normal}( 0, \Sigma_{X, Y}) \]</span></p>
<p>This can be obtained by inspection, by defining the inverse variance matrix</p>
<p><span class="math display">\[
\Sigma_{X, Y}^{-1} = \begin{bmatrix}
1 + \alpha^2 &amp; -\alpha \\
-\alpha      &amp; 1
\end{bmatrix}
\]</span></p>
<p>from expanding <span class="math inline">\(x^2 + (y - \alpha x)^2\)</span> and <span class="math inline">\((v - \mu_{X, Y})^T \Sigma_{X, Y}^{-1} (v - \mu_{X, Y})\)</span>, taking mean <span class="math inline">\(\mu_{X, Y} = 0\)</span> assuming the inverse matrix is symmetric, and equating the coefficients for the monomials <span class="math inline">\(x^2, y^2, xy\)</span>.</p>
<p>The joint distribution of <span class="math inline">\(X, Y, Z\)</span> then is:</p>
<p><span class="math display">\[ 
\begin{align}
f_{X, Y, Z} (x, y, z) &amp;= f_{X, Y}(x, y) f_{Z | X, Y}(z | x, y) \\
&amp;= \left(\frac{1}{\sqrt{2\pi}}\right)^2 \exp \left\{ -\frac{1}{2} \left( x^2 + (y - \alpha x)^2\right) \right\} 
\left(\frac{1}{\sqrt{2\pi}} \right) \exp \left\{ -\frac{1}{2} (z - (\beta y + \gamma x))^2\right\} \\
&amp;= \left(\frac{1}{\sqrt{2\pi}}\right)^3 \exp \left\{ -\frac{1}{2} \left( x^2 + (y - \alpha x)^2 + (z - (\beta y + \gamma x))^2\ \right) \right\} \\
&amp;= (2\pi)^{-3/2} \text{det}(\Sigma_{X, Y, Z})^{-1/2} \exp \left\{ -\frac{1}{2} (v - \mu_{X, Y, Z})^T \Sigma_{X, Y, Z}^{-1} (v - \mu_{X, Y, Z})\right\}
\end{align}
\]</span></p>
<p>where</p>
<p><span class="math display">\[
v = \begin{bmatrix}x \\ y \\ z\end{bmatrix},
\quad \mu_{X, Y, Z} = \begin{bmatrix}0 \\ 0 \\ 0\end{bmatrix}
\quad \text{and} \quad
\Sigma_{X, Y, Z} = \begin{bmatrix}
1 &amp; \alpha &amp; \alpha \beta + \gamma\\
\alpha &amp; 1 + \alpha^2 &amp; \alpha(\alpha \beta + \gamma) + \beta \\
\alpha \beta + \gamma &amp; \alpha(\alpha \beta + \gamma) + \beta &amp; -(\alpha - \beta\gamma)^2 + (\beta^2 + 1)(\alpha^2 + \gamma^2 + 1)
\end{bmatrix}
\]</span></p>
<p>or, in words, the joint distribution of <span class="math inline">\(X, Y, Z\)</span> is a multivariate normal with mean <span class="math inline">\(\mu_{X, Y, Z} = 0\)</span> and variance <span class="math inline">\(\Sigma_{X, Y, Z}\)</span>.</p>
<p><span class="math display">\[ (X, Y, Z) \sim \text{Normal}(0, \Sigma_{X, Y, Z}) \]</span></p>
<p>This can be obtained by inspection, by defining the inverse variance matrix</p>
<p><span class="math display">\[
\Sigma_{X, Y, Z}^{-1} = \begin{bmatrix}
1 + \alpha^2 + \gamma^2 &amp; -\alpha + \beta \gamma &amp; -\gamma \\
-\alpha + \beta \gamma  &amp; 1 + \beta^2            &amp; -\beta  \\
-\gamma                 &amp; -\beta                 &amp; 1
\end{bmatrix}
\]</span></p>
<p>from expanding <span class="math inline">\(x^2 + (y - \alpha x)^2 + (z - (\beta y + \gamma x))^2\)</span> and <span class="math inline">\((v - \mu_{X, Y, Z})^T \Sigma_{X, Y, Z}^{-1} (v - \mu_{X, Y, Z})\)</span>, taking mean <span class="math inline">\(\mu_{X, Y, Z} = 0\)</span>, assuming the inverse matrix is symmetric, and equating the coefficients for the monomials <span class="math inline">\(x^2, y^2, z^2, xy, xz, yz\)</span>. A symbolic mathematics library was then used to calculate the inverse and simplify it as above.</p>
<p><strong>Theorem 15.5</strong>. Let <span class="math inline">\(X \sim N(\mu, \Sigma)\)</span>. Then:</p>
<ol style="list-style-type: decimal">
<li>The marginal distribution of <span class="math inline">\(X_a\)</span> is <span class="math inline">\(X_a \sim N(\mu_a, \Sigma_{aa})\)</span>.</li>
<li>The conditional distribution of <span class="math inline">\(X_b\)</span> given <span class="math inline">\(X_a = x_a\)</span> is</li>
</ol>
<p><span class="math display">\[ X_b | X_a = x_a \sim N(\mu(x_a), \Sigma(x_a))\]</span></p>
<p>where</p>
<p><span class="math display">\[
\begin{align}
\mu(x_a) &amp;= \mu_b + \Sigma_{ba} \Sigma_{aa}^{-1} (x_a - \mu_a) \\
\Sigma(x_a) &amp;= \Sigma_{bb} - \Sigma_{ba}\Sigma_{aa}^{-1}\Sigma_{ab}
\end{align}
\]</span></p>
<p>The marginal distribution result is immediate: for any given sample drawn from the distribution, collect only the first <span class="math inline">\(k\)</span> dimensions of the sample vector, where <span class="math inline">\(k\)</span> is the number of dimensions of <span class="math inline">\(X_a\)</span>. The resulting distribution will be multivariate normal, with mean and variance given by getting the first <span class="math inline">\(k\)</span> dimensions of <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\Sigma\)</span>.</p>
<p>For the conditional distribution result, let <span class="math inline">\(A = -\Sigma_{ba} \Sigma_{aa}^{-1}\)</span> and <span class="math inline">\(z = x_b + A x_a\)</span>. We have:</p>
<p><span class="math display">\[\begin{align}
\text{Cov}(z, x_a) &amp;= \text{Cov}(x_b, x_a) + \text{Cov}(A x_a, x_a) \\
&amp;= \Sigma_{ba} + A \mathbb{V}(x_a) \\
&amp;= \Sigma_{ba} - \Sigma_{ba} \Sigma_{aa}^{-1} \Sigma_{aa} \\
&amp;= 0
\end{align}\]</span></p>
<p>so <span class="math inline">\(z\)</span> and <span class="math inline">\(x_a\)</span> are uncorrelated (and since they are jointly normal, they are also independent). We then have:</p>
<p><span class="math display">\[\begin{align}
\mathbb{E}(x_b | x_a) &amp;= \mathbb{E}(z - A x_a | x_a) \\
&amp;= \mathbb{E}(z | x_a) - \mathbb{E}(A x_a | x_a) \\
&amp;= \mathbb{E}(z) - A x_a \\
&amp;= \mu_b + A \mu_a - A x_a \\
&amp;= \mu_b + \Sigma_{ba} \Sigma_{aa}^{-1} (x_a - \mu_a)
\end{align}\]</span></p>
<p>For the covariance matrix,</p>
<p><span class="math display">\[\begin{align}
\mathbb{V}(x_b | x_a) &amp;= \mathbb{V}(z - A x_a | x_a) \\
&amp;= \mathbb{V}(z | x_a) - \mathbb{V}(A x_a | x_a) - A \text{Cov}(z, -x_a) - \text{Cov}(z, -x_a) A^T \\
&amp;= \mathbb{V}(z | x_a) - 0 - A \cdot 0 - 0 \cdot A \\
&amp;= \mathbb{V}(z) \\
&amp;= \mathbb{V}(x_b + A x_a) \\
&amp;= \mathbb{V}(x_b) + A \mathbb{V}(x_a) A^T + A \text{Cov}(x_a, x_b) + \text{Cov}(x_b, x_a) A^T \\
&amp;= \Sigma_{bb} + (- \Sigma_{ba} \Sigma_{aa}^{-1}) \Sigma_{aa} (- \Sigma_{ba} \Sigma_{aa}^{-1})^T + (- \Sigma_{ba} \Sigma_{aa}^{-1}) \Sigma_{ab} + \Sigma_{ba} (- \Sigma_{ba} \Sigma_{aa}^{-1})^T \\
&amp;= \Sigma_{bb} + \Sigma_{ba} \Sigma_{aa}^{-1} \Sigma_{aa} \Sigma_{aa}^{-1} \Sigma_{ab} - 2 \Sigma_{ba}\Sigma_{aa}^{-1}\Sigma_{ab} \\
&amp;= \Sigma_{bb} - \Sigma_{ba}\Sigma_{aa}^{-1}\Sigma_{ab}
\end{align}\]</span></p>
<p>Reference: Macro (<a href="https://stats.stackexchange.com/users/4856/macro" class="uri">https://stats.stackexchange.com/users/4856/macro</a>), Deriving the conditional distributions of a multivariate normal distribution, URL (version: 2015-06-18): <a href="https://stats.stackexchange.com/q/30600" class="uri">https://stats.stackexchange.com/q/30600</a></p>
<p>Now that we have characterized the joint distribution of <span class="math inline">\(X, Y, Z\)</span>, we can use theorem 15.5 to first compute the marginal distribution of <span class="math inline">\(Y, Z\)</span>, and then use it again to compute the conditional distribution of <span class="math inline">\(Z | Y = y\)</span>.</p>
<p>The marginal distribution of <span class="math inline">\(Y, Z\)</span> is</p>
<p><span class="math display">\[ (Y, Z) \sim \text{Normal}(0, \Sigma_{Y, Z})\]</span></p>
<p>where</p>
<p><span class="math display">\[ \Sigma_{Y, Z} = \begin{bmatrix}
1 + \alpha^2 &amp; \alpha(\alpha \beta + \gamma) + \beta \\
\alpha(\alpha \beta + \gamma) + \beta &amp; -(\alpha - \beta\gamma)^2 + (\beta^2 + 1)(\alpha^2 + \gamma^2 + 1)
\end{bmatrix} \]</span></p>
<p>Now, the conditional distribution of <span class="math inline">\(Z | Y = y\)</span> is:</p>
<p><span class="math display">\[ Z | Y = y \sim \text{Normal}(\mu_{Z | Y} (y), \Sigma_{Z | Y}(y)) \]</span></p>
<p>where</p>
<p><span class="math display">\[ \mu_{Z | Y}(y) = \mu_Z + \Sigma_{zy} \Sigma_{yy}^{-1} (y - \mu_y) = \frac{\alpha(\alpha \beta + \gamma) + \beta}{\alpha^2 + 1} y\]</span></p>
<p><span class="math display">\[ \Sigma_{Z | Y}(y) = \Sigma_{zz} - \Sigma_{zy} \Sigma_{yy}^{-1} \Sigma_{yz} = \frac{\alpha^2 + \gamma^2 + 1}{\alpha^2 + 1}\]</span></p>
<p>The explicit PDF is</p>
<p><span class="math display">\[ f_{Z | Y}(z | y) = \frac{1}{\sqrt{\Sigma_{Z | Y}(y)} \sqrt{2 \pi}} \exp \left\{ -\frac{1}{2} \frac{\left( z - \mu_{Z | Y}(y) \right)^2}{\Sigma_{Z | Y}(y)} \right\} \]</span></p>
<p>Finally, the expectation is the mean,</p>
<p><span class="math display">\[\mathbb{E}(Z | Y = y) = \mu_{Z | Y} (y) = \frac{\alpha(\alpha \beta + \gamma) + \beta}{\alpha^2 + 1} y\]</span></p>
<p><strong>(b)</strong> The joint distribution of <span class="math inline">\(X, Z\)</span> once we set <span class="math inline">\(Y := y\)</span> is:</p>
<p><span class="math display">\[ 
\begin{align}
f_{X, Z | Y := y}(x, z) &amp;= f_X(x) f_{Z | X = x, Y := y}(z | x) \\
&amp;= \left(\frac{1}{\sigma_X \sqrt{2\pi}} \exp \left\{-\frac{1}{2} \left(\frac{x - \mu_X}{\sigma_X}\right)^2 \right\} \right)
\left(\frac{1}{\sigma_{Z | X} \sqrt{2\pi}} \exp \left\{-\frac{1}{2} \left(\frac{z - \mu_{Z | X}}{\sigma_{Z | X}}\right)^2 \right\}\right) \\
&amp;= \left( \frac{1}{\sqrt{2 \pi}} \right)^2 \exp \left\{ -\frac{1}{2} \left( x^2 + (z - (\beta y + \gamma x))^2\right) \right\} \\
&amp;= \left( \frac{1}{\sqrt{2 \pi}} \right)^2 \exp \left\{ -\frac{1}{2} \left( x^2 + ((z - \beta y) - \gamma x))^2\right) \right\} \\
&amp;= (2 \pi)^{-2 / 2} \text{det} (\Sigma_{X, Z})^{-1/2} \exp \left\{ -\frac{1}{2} (v - \mu_{X, Z})^T \Sigma_{X, Z}^{-1} (v - \mu_{X, Z})\right\}
\end{align}
\]</span></p>
<p>where</p>
<p><span class="math display">\[
v = \begin{bmatrix}x \\ z\end{bmatrix},
\quad \mu_{X, Z} = \begin{bmatrix}0 \\ \beta y\end{bmatrix}
\quad \text{and} \quad
\Sigma_{X, Z} = \begin{bmatrix}
1  &amp; \gamma \\
\gamma &amp; 1 + \gamma^2
\end{bmatrix}
\]</span></p>
<p>or, in words, the joint distribution of <span class="math inline">\(X, Z\)</span> is a multivariate normal with mean <span class="math inline">\(\mu_{X, Z}\)</span> and variance <span class="math inline">\(\Sigma_{X, Z}\)</span> (a result analogous to (a), but with a non-zero mean).</p>
<p><span class="math display">\[ (X, Z) \sim \text{Normal}(\mu_{X, Z}, \Sigma_{X, Z})\]</span></p>
<p>Therefore, from theorem 15.5, the marginal distribution of <span class="math inline">\(Z\)</span> is:</p>
<p><span class="math display">\[ Z | Y := y \sim \text{Normal}\left( \beta y, 1 + \gamma^2\right) \]</span></p>
<p>with a explicit PDF</p>
<p><span class="math display">\[ f(z | Y := y) = \frac{1}{\sqrt{1 + \gamma^2} \sqrt{2 \pi}} \exp \left\{ -\frac{1}{2} \frac{\left( z - \beta y \right)^2}{1 + \gamma^2} \right\}\]</span></p>
<p>and the expectation is the mean of the distribution,</p>
<p><span class="math display">\[ \mathbb{E}(Z | Y := y) = \beta y \]</span></p>
<p>Note that this is distinct from the result in (a) – the relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is gone – no <span class="math inline">\(\alpha\)</span> is present in the definition of this distribution, since setting <span class="math inline">\(Y\)</span> “breaks” the arrow from <span class="math inline">\(X\)</span> into <span class="math inline">\(Y\)</span>. Accordingly, the distribution is the same as in (a) when setting <span class="math inline">\(\alpha = 0\)</span>, which also breaks that relation.</p>
<p><strong>(c)</strong></p>
<p>We already found the joint distribution between <span class="math inline">\(Y\)</span> and <span class="math inline">\(Z\)</span> during part (a); it is</p>
<p><span class="math display">\[ (Y, Z) \sim \text{Normal}(0, \Sigma_{Y, Z}) \]</span></p>
<p>where</p>
<p><span class="math display">\[ \Sigma_{Y, Z} = \begin{bmatrix}
1 + \alpha^2 &amp; \alpha(\alpha \beta + \gamma) + \beta \\
\alpha(\alpha \beta + \gamma) + \beta &amp; -(\alpha - \beta\gamma)^2 + (\beta^2 + 1)(\alpha^2 + \gamma^2 + 1)
\end{bmatrix} \]</span></p>
<p>From the variance matrix, we can extract the correlation <span class="math inline">\(\rho\)</span>,</p>
<p><span class="math display">\[ \rho = \frac{\text{Cov}(Y, Z)}{\sigma_Y \sigma_Z} = \frac{\alpha(\alpha \beta + \gamma) + \beta}{\sqrt{(1 + \alpha^2)(-(\alpha - \beta\gamma)^2 + (\beta^2 + 1)(\alpha^2 + \gamma^2 + 1))}}\]</span></p>
<p><strong>(d)</strong></p>
<p>Note that we can have <span class="math inline">\(\beta = 0\)</span> and <span class="math inline">\(\rho \neq 0\)</span>. In this situation, <span class="math inline">\(Y\)</span> and <span class="math inline">\(Z\)</span> are correlated but there is no causation between <span class="math inline">\(Y\)</span> and <span class="math inline">\(Z\)</span>; we would still have <span class="math inline">\(\mathbb{E}(Z | Y := y) = \beta y = 0\)</span>, so the conclusion would be erroneous.</p>
<p><strong>(e)</strong></p>
<p>In this new experiment, there is no relation between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> (given the empty set), as <span class="math inline">\(Y\)</span> is drawn from a distribution that does not depend on <span class="math inline">\(X\)</span>. <span class="math inline">\(Z\)</span> is then drawn from a distribution that depends on <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.</p>
<p>Let’s determine the joint distribution of <span class="math inline">\(X, Y, Z\)</span> in this scenario:</p>
<p><span class="math display">\[ 
\begin{align}
f_{X, Y, Z}(x, y, z) &amp;= f_X(x) f_Y(y) f_{Z | X, Y}(z | x, y) \\
&amp;= \left(\frac{1}{\sigma_X \sqrt{2\pi}} \exp \left\{-\frac{1}{2} \left(\frac{x - \mu_X}{\sigma_X}\right)^2 \right\} \right)
\left(\frac{1}{\sigma_Y \sqrt{2\pi}} \exp \left\{-\frac{1}{2} \left(\frac{y - \mu_Y}{\sigma_Y}\right)^2 \right\} \right)
\left(\frac{1}{\sigma_{Z | X, Y} \sqrt{2\pi}} \exp \left\{-\frac{1}{2} \left(\frac{z - \mu_{Z | X, Y}}{\sigma_{Z | X, Y}}\right)^2 \right\} \right) \\
&amp;= (2 \pi)^{-3/2} \exp \left\{ -\frac{1}{2} \left(x^2 + (y - \alpha)^2 + (z - (\beta y + \gamma x))^2 \right)\right\} \\
&amp;= (2 \pi)^{-3/2} \text{det} (\Sigma_{X, Y, Z})^{-1/2} \exp \left\{ -\frac{1}{2} (v - \mu_{X, Y, Z})^T \Sigma_{X, Y, Z}^{-1} (v - \mu_{X, Y, Z})\right\}
\end{align}
\]</span></p>
<p>where</p>
<p><span class="math display">\[
v = \begin{bmatrix}x \\ y \\ z\end{bmatrix},
\quad \mu_{X, Y, Z} = \begin{bmatrix}0 \\ \alpha \\ \alpha \beta \end{bmatrix}
\quad \text{and} \quad
\Sigma_{X, Y, Z} = \begin{bmatrix}
1 &amp; 0 &amp; \gamma \\
0 &amp; 1 &amp; \beta \\
\gamma &amp; \beta &amp; 1 + \beta^2 + \gamma^2
\end{bmatrix}
\]</span></p>
<p>which can be constructed by:
- getting the mean vector <span class="math inline">\(\mu_{X, Y, Z}\)</span> from the expectations of the distributions (<span class="math inline">\(\mathbb{E}(X) = 0, \mathbb{E}(Y) = \alpha, \mathbb{E}(Z) = \mathbb{E}(\mathbb{E}(Z | X, Y)) = \alpha \beta\)</span>)
- constructing a symmetric inverse matrix <span class="math inline">\(\Sigma_{X, Y, Z}^{-1}\)</span>, expanding <span class="math inline">\((v - \mu_{X, Y, Z})^T \Sigma_{X, Y, Z}^{-1} (v - \mu_{X, Y, Z})\)</span>, expanding <span class="math inline">\(x^2 + (y - \alpha)^2 + (z - (\beta y + \gamma x))^2\)</span> and equating the monomial coefficients;
- obtaining</p>
<p><span class="math display">\[ \Sigma_{X, Y, Z}^{-1} = \begin{bmatrix}
1 + \gamma^2 &amp; \beta \gamma &amp; -\gamma \\
\beta \gamma &amp; 1 + \beta^2 &amp; -\beta \\
-\gamma &amp; -\beta &amp; 1
\end{bmatrix} \]</span></p>
<p>and inverting it using a symbolic mathematics library.</p>
<p>We then have,</p>
<p><span class="math display">\[ (X, Y, Z) \sim \text{Normal}(\mu_{X, Y, Z}, \Sigma_{X, Y, Z}) \]</span></p>
<p>The correlation between <span class="math inline">\(Y\)</span> and <span class="math inline">\(Z\)</span> becomes</p>
<p><span class="math display">\[\rho = \frac{\text{Cov}(Y, Z)}{\sigma_Y \sigma_Z} = \frac{\beta}{\sqrt{1 + \beta^2 + \gamma^2}}\]</span></p>
<p>so the correlation is 0 if and only if <span class="math inline">\(\beta = 0\)</span>.</p>
<p>On the other hand, if we set <span class="math inline">\(Y := y\)</span>, the joint distribution becomes equivalent to the one in (b), where</p>
<p><span class="math display">\[ Z | Y := y \sim \text{Normal}\left( \beta y, 1 + \gamma^2\right) \]</span></p>
<p>This does not depend in <span class="math inline">\(y\)</span> if and only if <span class="math inline">\(\beta = 0\)</span>.</p>
<p>Therefore, for this concrete example, the method in (b) yields correct conclusions if and only if <span class="math inline">\(\rho = 0\)</span>.</p>
<div id="refs" class="references csl-bib-body">
<div id="ref-wasserman2013all" class="csl-entry">
1. Wasserman L. All of statistics: A concise course in statistical inference. Springer Science &amp; Business Media; 2013.
</div>
<div id="ref-telmo-correa/all-of-statistics" class="csl-entry">
2. Https://github.com/telmo-correa/all-of-statistics.
</div>
</div>
</div>
</div>

    </div>
  </article>

  


</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
          </li>
          <li>
          </li>
        </ul>
      </footer>

    </div>
    



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/r.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/yaml.min.js"></script>
<script>hljs.configure({languages: []}); hljs.initHighlightingOnLoad();</script>



    
<script src="../../../../js/math-code.js"></script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
  </body>
</html>

