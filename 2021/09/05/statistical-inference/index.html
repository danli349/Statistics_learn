<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.80.0" />


<title>Statistical Inference - A Hugo website</title>
<meta property="og:title" content="Statistical Inference - A Hugo website">


  <link href='../../../../favicon.ico' rel='icon' type='image/x-icon'/>



  







<link rel="stylesheet" href="../../../../css/fonts.css" media="all">
<link rel="stylesheet" href="../../../../css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="../../../../" class="nav-logo">
    <img src="../../../../images/logo.png"
         width="50"
         height="50"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="../../../../about/">about</a></li>
    
    <li><a href="../../../../vitae/">CV</a></li>
    
    <li><a href="https://github.com/danli349">GitHub</a></li>
    
    <li><a href="https://scholar.google.com/citations?user=mNjLK8EAAAAJ&amp;hl=en">Googlr Scholar</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">16 min read</span>
    

    <h1 class="article-title">Statistical Inference</h1>

    
    <span class="article-date">2021-09-05</span>
    

    <div class="article-content">
      
<script src="../../../../2021/09/05/statistical-inference/index_files/header-attrs/header-attrs.js"></script>

<div id="TOC">
<ul>
<li><a href="#common-families-of-distributions">3. Common Families of Distributions</a>
<ul>
<li><a href="#some-distributions">Some Distributions</a>
<ul>
<li><a href="#chi-squared-distribution">Chi-Squared Distribution</a></li>
<li><a href="#students-t-distribution">Student’s <span class="math inline">\(t\)</span>-Distribution</a></li>
<li><a href="#snedcors-f-distribution">Snedcor’s <span class="math inline">\(F\)</span>-Distribution</a></li>
<li><a href="#multinomial-distribution">Multinomial Distribution</a></li>
</ul></li>
<li><a href="#exponential-families">Exponential Families</a></li>
<li><a href="#location-and-scale-families">Location and Scale Families</a></li>
<li><a href="#inequalities-and-identities">Inequalities and Identities</a></li>
</ul></li>
<li><a href="#multiple-random-variables">4. Multiple Random Variables</a>
<ul>
<li><a href="#facts">Facts</a></li>
<li><a href="#bivariate-relations">Bivariate Relations</a></li>
<li><a href="#inequalities">Inequalities</a>
<ul>
<li><a href="#numerical-inequalities">Numerical Inequalities</a></li>
<li><a href="#functional-inequalities">Functional Inequalities</a></li>
</ul></li>
</ul></li>
<li><a href="#properties-of-a-random-sample">5. Properties of a Random Sample</a>
<ul>
<li><a href="#sampling-from-the-normal-distribution">Sampling from the Normal Distribution</a></li>
<li><a href="#convergence-concepts">Convergence Concepts</a>
<ul>
<li><a href="#convergence-in-probability">Convergence in Probability</a></li>
<li><a href="#almost-sure-convergence">Almost Sure Convergence</a></li>
<li><a href="#convergence-in-distribution">Convergence in Distribution</a></li>
<li><a href="#the-delta-method">The Delta Method</a></li>
</ul></li>
<li><a href="#generating-a-random-sample">Generating A Random Sample</a></li>
</ul></li>
<li><a href="#references">References</a></li>
</ul>
</div>

<div id="common-families-of-distributions" class="section level1">
<h1>3. Common Families of Distributions</h1>
<div id="some-distributions" class="section level2">
<h2>Some Distributions</h2>
<p>[Normal Distribution] If <span class="math inline">\(\vec{X} \sim \text{n}(\vec{\mu}, \boldsymbol{\Sigma})\)</span> then
<span class="math inline">\(\vec{X}\)</span> has pdf
<span class="math display">\[f_{\vec{X}}(x_1, \dots, x_k \vert{} \vec{\mu}, \boldsymbol{\Sigma})  = \frac{1}{\sqrt{(2\pi)^k \det\boldsymbol{\Sigma}}} \exp\left(-\frac12 (\vec{ x}-\vec{\mu})^\top \boldsymbol{\Sigma}^{-1}(\vec{x}-\vec{\mu})\right)\]</span>.</p>
<div id="chi-squared-distribution" class="section level3">
<h3>Chi-Squared Distribution</h3>
<p>The <em>chi-squared distribution with <span class="math inline">\(p\)</span> degrees of freedom</em> has pdf
<span class="math display">\[\chi_p^2 \sim \frac{1}{\Gamma(p/2)2^{p/2}} x^{(p/2)-1} e^{-x/2}, \quad 0&lt; x&lt; \infty.\]</span></p>
<p>[Some facts]</p>
<ol style="list-style-type: lower-alpha">
<li><p>If <span class="math inline">\(Z \sim \text{n}(0, 1)\)</span> then <span class="math inline">\(Z^2 \sim \chi_1^2\)</span></p></li>
<li><p>If <span class="math inline">\(X_1, \dots, X_n\)</span> are independent <span class="math inline">\(X_i \sim \chi_{p_i}^2\)</span> then
<span class="math inline">\(X_1 + \cdots + X_n \sim \chi_{p_1 + \cdots + p_n}^2\)</span>.</p></li>
</ol>
</div>
<div id="students-t-distribution" class="section level3">
<h3>Student’s <span class="math inline">\(t\)</span>-Distribution</h3>
<p><a href="#students-t-distribution">Student’s <span class="math inline">\(t\)</span>-distribution</a> <span class="math inline">\(T \sim t_p\)</span>, a <em><span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(p\)</span>
degrees of freedom</em> if it has pdf
<span class="math display">\[f_T(t) = \frac{\Gamma\left(\frac{p+1}{2}\right)}{\Gamma\left(\frac{p}{2}\right)} \frac{1}{\sqrt{p\pi}} \frac{1}{(1 + t^2/p)^{(p+1)/2}}, \quad t \in \mathbb R{}\]</span></p>
<p>If <span class="math inline">\(p = 1\)</span> then this is the Cauchy distribution.</p>
<p>If <span class="math inline">\(X_1, \dots, X_n\)</span> are a random sample from <span class="math inline">\(\text{n}(\mu, \sigma^2)\)</span> then
<span class="math display">\[\frac{\bar{X} - \mu}{S/\sqrt{n}} \sim t_{n-1}.\]</span> This is often taken
as the definition. Note that the denominator is independent of the
numerator.</p>
<p>[Moments and mgf of <span class="math inline">\(t\)</span>-distribution] Student’s <span class="math inline">\(t\)</span> has no mgf because
it does not have moments of all orders: <span class="math inline">\(t_p\)</span> has only <span class="math inline">\(p-1\)</span> moments. If
<span class="math inline">\(T_p \sim t_p\)</span> then</p>
<p><span class="math display">\[\begin{aligned}
        \mathbb E{}[T_p] &amp;= 0 \quad p &gt; 1 \\
        \mathbb V{}[T_p] &amp;= \frac{p}{p-2} \quad p &gt; 2
    \end{aligned}\]</span></p>
</div>
<div id="snedcors-f-distribution" class="section level3">
<h3>Snedcor’s <span class="math inline">\(F\)</span>-Distribution</h3>
<p><a href="#snedcors-f-distribution">Snedcor’s <span class="math inline">\(F\)</span>-distribution</a> A random variable <span class="math inline">\(X \sim F_{p,q}\)</span> has
<em><span class="math inline">\(F\)</span>-distribution with p and q degrees of freedom</em> if its pdf is
<span class="math display">\[f_X(x) = \frac{\Gamma\left(\frac{p+q}{2}\right)}{\Gamma\left(\frac p2 \right)\Gamma\left(\frac q2 \right)} (p/q)^{p/2} \frac{x^{(p/2) - 1}}{(1 + px/q)^{(p+q)/2}}, \quad 0 &lt; x &lt; \infty.\]</span></p>
<p>If <span class="math inline">\(X_1, \dots, X_n\)</span> is a random sample from <span class="math inline">\(\text{n}(\mu_X, \sigma_X^2)\)</span> and
<span class="math inline">\(Y_1, \dots, Y_m\)</span> is an independent random sample from
<span class="math inline">\(\text{n}(\mu_Y, \sigma_Y^2)\)</span>, then
<span class="math display">\[\frac{S^2_X/\sigma^2_X}{S_Y^2/\sigma_Y^2} \sim F_{n-1, m-1}.\]</span> This is
often taken as the definition.</p>
<p>[Some facts]</p>
<ol style="list-style-type: lower-alpha">
<li><p><span class="math inline">\(X \sim F_{p,q} \,\, \implies \,\, 1/X \sim F_{q,p}\)</span></p></li>
<li><p><span class="math inline">\(X \sim t_q \,\, \implies \,\, X^2 \sim F_{1,q}\)</span></p></li>
<li><p><span class="math inline">\(X \sim F_{p,q} \,\, \implies \,\, \frac{(p/q)X}{1 + (p/q)X} \sim \text{beta}(p/2, q/2)\)</span></p></li>
</ol>
</div>
<div id="multinomial-distribution" class="section level3">
<h3>Multinomial Distribution</h3>
<p><a href="#multinomial-distribution">Multinomial Distribution</a> Let <span class="math inline">\(m\)</span> and <span class="math inline">\(n\)</span> be positive integers and let
<span class="math inline">\(p_1, \dots, p_n \in [0, 1]\)</span> satisfy <span class="math inline">\(\sum_{i=1}^n p_i = 1\)</span>. Then the
random vector <span class="math inline">\((X_1, \dots, X_n)\)</span> has <em>multinomial distribution with m
trials and cell probabilities <span class="math inline">\(p_1, \dots, p_n\)</span></em> if the joint pmf of
<span class="math inline">\((X_1, \dots, X_n)\)</span> is
<span class="math display">\[f(x_1, \dots , x_n) = \frac{m!}{x_1! \cdots x_n!} p_1^{x_1} \cdots
p_n^{x_n} = m! \prod_{i=1}^n \frac{p_i^{x_i}}{x_i!}\]</span> on the set of
<span class="math inline">\((x_1, \dots, x_n)\)</span> such that each <span class="math inline">\(x_i\)</span> is a nonnegative integer and
<span class="math inline">\(\sum_{i=1}^n x_i = m\)</span>.</p>
<p>The marginal distributions have <span class="math inline">\(X_i \sim \text{binomial}(m, p_i)\)</span>.</p>
<p>[Multinomial Theorem] Let <span class="math inline">\(m\)</span> and <span class="math inline">\(n\)</span> be positive integers and let
<span class="math inline">\(\mathcal{A}\)</span> be the set of vectors <span class="math inline">\(\vec{x} = (x_1, \dots, x_n)\)</span> such
that each <span class="math inline">\(x_i\)</span> is a nonnegative integer and <span class="math inline">\(\sum_{i=1}^n x_i = m\)</span>.
Then for any real numbers <span class="math inline">\(p_1, \dots, p_n\)</span>,
<span class="math display">\[(p_1 + \cdots + p_n)^m = \sum_{\vec{x} \in \mathcal{A}} \frac{m!}{x_1! \cdots x_n!} p_1^{x_1}\cdots p_n^{x_n}.\]</span></p>
</div>
</div>
<div id="exponential-families" class="section level2">
<h2>Exponential Families</h2>
<p>[Exponential family 1] A family of pmfs/pdfs is called an <em>exponential
family</em> if it can be expressed
<span class="math display">\[f(x|\vec{\theta}) = h(x)c(\vec{\theta})\exp\left( \sum_{i=1}^k w_i(\vec{\theta}) t_i(x) \right)\]</span>
where <span class="math inline">\(h(x) \geq 0\)</span>, the <span class="math inline">\(t_i\)</span> are real valued functions of the
observation <span class="math inline">\(x\)</span> that do not depend on <span class="math inline">\(\vec{\theta}\)</span> and
<span class="math inline">\(c(\theta) \geq 0\)</span> and the <span class="math inline">\(w_i(\vec{\theta})\)</span> are real valued functions
of <span class="math inline">\(\vec{\theta}\)</span> that do not depend on <span class="math inline">\(x\)</span>.</p>
<p>If <span class="math inline">\(X\)</span> is a random variable from an exponential family distribution then
<span class="math display">\[\mathbb E{}\left[ \sum_{i=1}^k \frac{\partial w_i(\vec{\theta})}{\partial \theta_j} t_i(X) \right] = - \frac{\partial}{\partial \theta_j} \log c(\vec{\theta})\]</span>
and
<span class="math display">\[\mathbb V\left[ \frac{\partial w_i(\vec{\theta})}{\partial \theta_j} t_i(X) \right] = - \frac{\partial^2}{\partial \theta_j^2} \log c(\vec{\theta}) - \mathbb E{}\left[ \sum_{i=1}^{k} \frac{\partial^2 w_i(\vec{\theta})}{\partial \theta_j^2} t_i(X) \right]\]</span></p>
<p>[Exponential family 2] We can write another parameterisation of the
exponential family
<span class="math display">\[f(x | \vec{\eta}) = h(x) c^{*}(\vec{\eta}) \exp(\vec{\eta} \cdot \vec{t}(x))\]</span>
where <span class="math inline">\(\vec{\eta}\)</span> is called the <em>natural parameter</em> and the set
<span class="math inline">\(\mathcal{H} = \{\vec{\eta} : \int_\mathbb R{} f(x|\eta) \text{d}x &lt; \infty \}\)</span> is called
the <em>natural parameter space</em> and is convex.</p>
<p><span class="math inline">\(\{\vec{\eta}: \vec{\eta} = \vec{w}(\vec{\theta}), \,\, \vec{\theta} \in \Theta\} \subseteq \mathcal{H}\)</span>.
So there may be more parameterisations here than previously.<br />
</p>
<p>The natural parameter provides a convenient mathematical formulation,
but sometimes lacks simple interpretation.</p>
<p>[Curved exponential family] A <em>curved exponential family</em> distribution
is one for which the dimension of <span class="math inline">\(\vec{\theta}\)</span> is <span class="math inline">\(d &lt; k\)</span>. If <span class="math inline">\(d = k\)</span>
then we have a <em>full exponential family</em>.</p>
</div>
<div id="location-and-scale-families" class="section level2">
<h2>Location and Scale Families</h2>
<p>[Location family] Let <span class="math inline">\(f(x)\)</span> be any pdf. The family of pdfs <span class="math inline">\(f(x - \mu)\)</span>
for <span class="math inline">\(\mu \in \mathbb R{}\)</span> is called the <em>location family with standard pdf
<span class="math inline">\(f(x)\)</span></em> and <span class="math inline">\(\mu\)</span> is the <em>location parameter</em> of the family.</p>
<p>[Scale family] Let <span class="math inline">\(f(x)\)</span> be any pdf. For any <span class="math inline">\(\sigma &gt; 0\)</span> the family of
pdfs <span class="math inline">\(\frac{1}{\sigma} f(x/\sigma)\)</span> is called the <em>scale family with
standard pdf <span class="math inline">\(f(x)\)</span></em> and <span class="math inline">\(\sigma\)</span> is the <em>scale parameter</em> of the
family.</p>
<p>[Location-Scale family] Let <span class="math inline">\(f(x)\)</span> be any pdf. For <span class="math inline">\(\mu \in \mathbb R{}\)</span> and
<span class="math inline">\(\sigma &gt; 0\)</span> the family of pdfs
<span class="math inline">\(\frac{1}{\sigma} f(\frac{x - \mu}{\sigma})\)</span> is called the
<em>location-scale family with standard pdf <span class="math inline">\(f(x)\)</span></em>; <span class="math inline">\(\mu\)</span> is the <em>location
parameter</em> and <span class="math inline">\(\sigma\)</span> is the <em>scale parameter</em>.</p>
<p>[Standardisation] Let <span class="math inline">\(f\)</span> be any pdf, <span class="math inline">\(\mu \in \mathbb R{}\)</span> and
<span class="math inline">\(\sigma \in \mathbb R{}_{&gt;0}\)</span>. Then <span class="math inline">\(X\)</span> is a random variable with pdf
<span class="math inline">\(\frac{1}{\sigma}f(\frac{x - \mu}{\sigma})\)</span> if and only if there exists
a random variable <span class="math inline">\(Z\)</span> with pdf <span class="math inline">\(f(z)\)</span> and <span class="math inline">\(X = \sigma Z + \mu\)</span>.</p>
<p>Probabilities of location-scale families can be computed in terms of
their standard variables <span class="math inline">\(Z\)</span>
<span class="math display">\[\text{P}(X \leq x) = \text{P}\left(Z \leq \frac{x - \mu}{\sigma} \right)\]</span></p>
</div>
<div id="inequalities-and-identities" class="section level2">
<h2>Inequalities and Identities</h2>
<p>[Chebychev’s inequality] Let <span class="math inline">\(X\)</span> be a random variable and let <span class="math inline">\(g(x)\)</span> be
a nonnegative function. Then, for any <span class="math inline">\(r &gt; 0\)</span>,
<span class="math display">\[\text{P}(g(X) \geq r) \leq \frac{\mathbb E{}[g(X)]}{r}.\]</span></p>
<p>This bound is conservative and almost never attained.</p>
<p>[Markov inequality] The Markov inequality is the special case with
<span class="math inline">\(g = \mathbb{I}\)</span>.</p>
<p>Let <span class="math inline">\(X_{\alpha, \beta}\)</span> denote a gamma<span class="math inline">\((\alpha, \beta)\)</span> random variable
with pdf <span class="math inline">\(f(x \vert{} \alpha, \beta)\)</span>, where <span class="math inline">\(\alpha &gt; 1\)</span>. Then for any
constants <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>:
<span class="math display">\[\text{P}(a &lt; X_{\alpha, \beta} &lt; b) = \beta (f(a \vert{} \alpha, \beta) - f(b \vert{} \alpha, \beta)) + \text{P}(a &lt; X_{\alpha - 1, \beta} &lt; b)\]</span></p>
<p>[Stein’s Lemma] Let <span class="math inline">\(X \sim \text{n}(\theta, \sigma^2)\)</span> and let <span class="math inline">\(g\)</span> be a
differentiable function with <span class="math inline">\(\mathbb E{}[g&#39;(x)] &lt; \infty\)</span>. Then
<span class="math display">\[\mathbb E{}[g(X)(X - \theta)] = \sigma^2 \mathbb E{}[g&#39;(X)]\]</span></p>
<p>The proof is just integration by parts.</p>
<p>Stein’s lemma is useful for moment calculations</p>
<p>Let <span class="math inline">\(\chi^2_p\)</span> denote a chi squared distribution with <span class="math inline">\(p\)</span> degrees of
freedom. For any function <span class="math inline">\(h(x)\)</span>,
<span class="math display">\[\mathbb E{}[h(\chi^2_p)] = p \mathbb E{}\left[\frac{h(\chi_{p+2}^2)}{\chi_{p+2}^2}\right]\]</span>
provided the expressions exist.</p>
<p>Let <span class="math inline">\(g(x)\)</span> be a function that is bounded at <span class="math inline">\(-1\)</span> and has finite
expectation, then</p>
<ol style="list-style-type: lower-alpha">
<li><p>If <span class="math inline">\(X \sim \text{Poisson}(\lambda)\)</span>,
<span class="math display">\[\mathbb E{}[\lambda g(X)] = \mathbb E{}[Xg(X-1)].\]</span></p></li>
<li><p>If <span class="math inline">\(X\sim \text{negative-binomial}(r, p)\)</span>,
<span class="math display">\[\mathbb E{}[(1-p)g(X)] = \mathbb E{}\left[ \frac{X}{r + X - 1}g(X) \right].\]</span></p></li>
</ol>
</div>
</div>
<div id="multiple-random-variables" class="section level1">
<h1>4. Multiple Random Variables</h1>
<div id="facts" class="section level2">
<h2>Facts</h2>
<ul>
<li><p>RVs are independent if and only if their pdfs factorise</p></li>
<li><p>Functions of independent RVs are independent</p></li>
<li><p>Expectations of products (and hence mgfs, etc. of sums) of
independent RVs factor</p></li>
<li><p>Variance of sum of independent RVs is sum of variances.</p></li>
<li><p>Independent RVs have vanishing covariance/correlation, but the
converse is not true in general.</p></li>
</ul>
</div>
<div id="bivariate-relations" class="section level2">
<h2>Bivariate Relations</h2>
<p>[Conditional Expectation] <span class="math display">\[\mathbb E{}[\mathbb E{}[X|Y]] = \mathbb E{}[X]\]</span> provided the
expectations exist.</p>
<p>[Conditional variance]
<span class="math display">\[\mathbb V[X] = \mathbb E{}[\mathbb V[X \vert{} Y]] + \mathbb V[\mathbb E{}[X \vert{} Y]]\]</span>
provided the expectations exist.</p>
<p>[Covariance] <span class="math display">\[\text{ Cov }[X, Y] = \mathbb E{}[(X - \mu_X)(Y - \mu_Y)]\]</span></p>
<p><span class="math display">\[\text{ Cov }[X, Y] = \mathbb E{}[XY] - \mu_X\mu_Y\]</span></p>
<p><span class="math display">\[\mathbb V[aX + bY] = a^2\mathbb V[X] + b^2\mathbb V[Y] + 2ab\text{Cov }[X, Y]\]</span></p>
<p>[Correlation] <span class="math display">\[\rho_XY = \frac{\text{Cov }[X, Y]}{\sigma_X \sigma_Y}\]</span></p>
<p>The correlation measures the strength of <em>linear</em> relation between two
RVs. It is possible to have strong non-linear relationships but with
<span class="math inline">\(\rho = 0\)</span>.</p>
<p>We can use an argument similar to the standard proof of Cauchy-Schwarz
to show the following</p>
<p>Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be any RVs, then</p>
<ol style="list-style-type: lower-alpha">
<li><p><span class="math inline">\(-1 \leq \rho_{XY} \leq 1\)</span>,</p></li>
<li><p><span class="math inline">\(\lvert{\rho_{XY}}\lvert = 1\)</span> if and only if there are constants
<span class="math inline">\(a \text{n}eq 0, b\)</span> such that <span class="math inline">\(\text{P}{}(Y = aX + b) = 1\)</span>. If
<span class="math inline">\(\lvert{\rho_{XY}}\lvert = 1\)</span> then <span class="math inline">\(\text{sign}(\rho) = \text{sign}(a)\)</span>.</p></li>
</ol>
<p>Let <span class="math inline">\(X_1, \cdots , X_n\)</span> be independent random vectors. Let <span class="math inline">\(g_i(x_i)\)</span> be a function only of <span class="math inline">\(x_i, i = 1 , \cdots, n\)</span>. Then the random variables <span class="math inline">\(U_i = g_i(X_i), i = 1,\cdots, n\)</span>, are mutually independent.</p>
</div>
<div id="inequalities" class="section level2">
<h2>Inequalities</h2>
<div id="numerical-inequalities" class="section level3">
<h3>Numerical Inequalities</h3>
<p>Let <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> be any positive numbers and let <span class="math inline">\(p, q &gt; 1\)</span> satisfy
<span class="math inline">\(1/p + 1/q = 1\)</span>, then <span class="math display">\[p+q=pq\]</span>
<span class="math display">\[q(p-1)=p\]</span>
<span class="math display">\[1-1/q=1/p\]</span>
<span class="math display">\[\frac1p a^p + \frac1q b^q \geq ab\]</span> with
equality if and only if <span class="math inline">\(a^p = b^q\)</span>.</p>
<p><strong>H<span class="math inline">\(\ddot{o}\)</span>lder’s Inequality</strong> Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be any random
variables and let <span class="math inline">\(p, q &gt; 1\)</span> satisfy <span class="math inline">\(1/p + 1/q = 1\)</span>, then
<span class="math display">\[\lvert{\mathbb E{}[XY]}\lvert \leq \mathbb E{}[\lvert{XY}\lvert] \leq \mathbb E{}[\lvert{X}\lvert^p]^{1/p} \mathbb E{}[\lvert{Y}\lvert^q]^{1/q}\]</span></p>
<ul>
<li><p><strong>Cauchy-Schwarz Inequality</strong> is the special case <span class="math inline">\(p = q = 2\)</span></p></li>
<li><p><span class="math inline">\(\text{Cov }[X, Y]^2 \leq \sigma_X^2 \sigma_Y^2\)</span></p></li>
<li><p><span class="math inline">\(\mathbb E{}[\lvert{X}\lvert] \leq \mathbb E{}[\lvert{X}\lvert^p]^{1/p}\)</span></p></li>
<li><p><strong>Liapounov’s Inequality</strong>
<span class="math inline">\(\mathbb E{}[\lvert{X}\lvert^r]^{1/r} \leq \mathbb E{}[\lvert{X}\lvert^s]^{1/s}\)</span> where
<span class="math inline">\(1 &lt; r &lt; s &lt; \infty\)</span>.</p></li>
<li><p>For numbers <span class="math inline">\(a_i,b_i,i=1,\cdots,n\)</span>, <span class="math display">\[\sum_{i=1}^{n}|a_ib_i|\leq\left(\sum_{i=1}^{n}a_i^p\right)^{1/p}\left(\sum_{i=1}^{n}b_i^q\right)^{1/q}\]</span> where <span class="math inline">\(\frac{1}{p}+\frac{1}{q}=1\)</span></p></li>
<li><p><span class="math display">\[\frac{1}{n}\left(\sum_{i=1}^{n}|a_i|\right)^2\leq\sum_{i=1}^{n}a_i^2\]</span></p></li>
</ul>
<p><strong>Minkowski’s Inequality</strong><br />
Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be any two random variables. Then for <span class="math inline">\(1\le p&lt;\infty\)</span>
<span class="math display">\[(\mathbb E|X+Y|^p)^{1/p}\le (\mathbb E|X|^p)^{1/p}+(\mathbb E|Y|^p)^{1/p}\]</span></p>
</div>
<div id="functional-inequalities" class="section level3">
<h3>Functional Inequalities</h3>
<p><strong>Convex Function</strong> A function <span class="math inline">\(g(x)\)</span> is <em>convex</em> on a set <span class="math inline">\(S\)</span> if for all
<span class="math inline">\(x, y \in S\)</span> and <span class="math inline">\(0&lt; \lambda &lt; 1\)</span>
<span class="math display">\[g(\lambda x + (1 - \lambda)y) \leq \lambda g(x) + (1 - \lambda)g(y).\]</span>
<strong>Strictly convex</strong> is when the inequality is strict. <span class="math inline">\(g\)</span> is <strong>concave</strong> if
<span class="math inline">\(-g\)</span> is convex.</p>
<p><span class="math inline">\(g(x)\)</span> is convex on <span class="math inline">\(S\)</span> if <span class="math inline">\(g&#39;&#39;(x) \geq 0\)</span> <span class="math inline">\(\forall x \in S\)</span>.</p>
<p><strong>Jensen’s Inequality</strong> If <span class="math inline">\(g(x)\)</span> is convex, then for any random variable
<span class="math inline">\(X\)</span> <span class="math display">\[\mathbb E{}[g(X)] \leq g(\mathbb E{}[X]).\]</span> Equality holds if and only if, for
every line <span class="math inline">\(a + bx\)</span> that is tangent to <span class="math inline">\(g(x)\)</span> at <span class="math inline">\(x = \mathbb E{}[X]\)</span>,
<span class="math inline">\(\text{P}{}\{g(X) = a + bX\} = 1\)</span>. (So if and only if <span class="math inline">\(g\)</span> is affine with
probability 1.)</p>
<ul>
<li><p><span class="math inline">\(x^2\)</span> is convex, <span class="math inline">\(\mathbb E[X^2] \geq \mathbb E{}[X]^2\)</span></p></li>
<li><p><span class="math inline">\(1/x\)</span> is convex, <span class="math inline">\(\mathbb E[1/X] \geq 1/ \mathbb E{}[X]\)</span></p></li>
<li><p><span class="math inline">\(\log x\)</span> is concave, <span class="math inline">\(\mathbb E(\log X) \leq \log (\mathbb E X)\)</span></p></li>
<li><p>Arithmetic mean: <span class="math display">\[a_A=\frac{1}{n}(a_1+a_2+\cdots+a_n)\]</span>
Geometric mean: <span class="math display">\[a_G=(a_1a_2\cdots a_n)^{1/n}\]</span>
Harmonic mean: <span class="math display">\[a_H=\frac{1}{\frac{1}{n}\left(\frac{1}{a_1}+\frac{1}{a_2}+\cdots+\frac{1}{a_n}\right)}\]</span>
<span class="math display">\[a_H\leq a_G\leq a_A\]</span></p></li>
</ul>
<p><strong>Covariance Inequality</strong> Let <span class="math inline">\(X\)</span> be any random variable and <span class="math inline">\(g(x)\)</span> and <span class="math inline">\(h(x)\)</span> any functions such that <span class="math inline">\(\mathbb E g(X), \mathbb E h(X)\)</span> and <span class="math inline">\(\mathbb E (g(X)h(X))\)</span> exist.<br />
- If <span class="math inline">\(g(x)\)</span> is a nondecreasing function and <span class="math inline">\(h(x)\)</span> is a nonincreasing function, then
<span class="math display">\[\mathbb E \bigg(g(X)h(X)\bigg)\leq\bigg(\mathbb E g(X)\bigg)\bigg(\mathbb E h(X)\bigg)\]</span></p>
<ul>
<li>If <span class="math inline">\(g(x)\)</span> and <span class="math inline">\(h(x)\)</span> are both nondecreasing or nonincreasing functions, then
<span class="math display">\[\mathbb E \bigg(g(X)h(X)\bigg)\geq\bigg(\mathbb E g(X)\bigg)\bigg(\mathbb E h(X)\bigg)\]</span></li>
</ul>
</div>
</div>
</div>
<div id="properties-of-a-random-sample" class="section level1">
<h1>5. Properties of a Random Sample</h1>
<div class="definition">
<p><span id="def:unlabeled-div-1" class="definition"><strong>Definition 1  </strong></span>A collection of random variables <span class="math inline">\(X_1, \dots, X_n\)</span> is a <em>random sample
of size n from population <span class="math inline">\(f(x)\)</span></em> if they are iid with pdf/pmf f(x).</p>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-2" class="definition"><strong>Definition 2  </strong></span><span class="math inline">\(Y = T(X_1, \dots, X_n)\)</span> is a <em>statistic</em> if the domain of <span class="math inline">\(T\)</span> contains
the sample space of <span class="math inline">\((X_1, \dots, X_n)\)</span>. The distribution of <span class="math inline">\(Y\)</span> is the
<em>sampling distribution of <span class="math inline">\(Y\)</span></em>.</p>
</div>
<div class="remark">
<p><span id="unlabeled-div-3" class="remark"><em>Remark</em>. </span>A statistic is any function of the data. The only restriction is that
the statistic is not also a function of some other parameters.</p>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-4" class="definition"><strong>Definition 3  </strong></span>The <em>sample mean</em> <span class="math inline">\(\bar{X}\)</span> and <em>sample variance</em> <span class="math inline">\(S^2\)</span> of a random
sample <span class="math inline">\(X_1, \dots, X_n\)</span> are, respectively,</p>
<ul>
<li><p><span class="math inline">\(\bar{X} = \frac1n \sum_{i=1}^n X_i\)</span></p></li>
<li><p><span class="math inline">\(S^2 = \frac{1}{n-1}\sum_{i=1}^n (X_i - \bar{X})^2\)</span>.</p></li>
</ul>
<p>The <em>sample standard deviation</em> is <span class="math inline">\(S = \sqrt{S^2}\)</span>.</p>
</div>
<div class="theorem">
<p><span id="thm:unlabeled-div-5" class="theorem"><strong>Theorem 1  </strong></span>Let <span class="math inline">\(X_1, \dots, X_n\)</span> be a random sample from a population with mean
<span class="math inline">\(\mu\)</span> and finite variance <span class="math inline">\(\sigma^2\)</span>, then</p>
<ol style="list-style-type: lower-alpha">
<li><p><span class="math inline">\(\mathbb E[\bar{X}] = \mu\)</span></p></li>
<li><p><span class="math inline">\(\mathbb V[\bar{X}] = \frac{\sigma^2}{n}\)</span></p></li>
<li><p><span class="math inline">\(\mathbb E[S^2] = \sigma ^2\)</span>.</p></li>
</ol>
</div>
<div class="theorem">
<p><span id="thm:unlabeled-div-6" class="theorem"><strong>Theorem 2  </strong></span>Take <span class="math inline">\(x_1, \dots, x_n \in \mathbb R\)</span> and let <span class="math inline">\(\bar{x}\)</span> be their mean. Then,</p>
<ol style="list-style-type: lower-alpha">
<li><p><span class="math inline">\(\min_a \sum_{i=1}^n (x_i - a)^2 = \sum_{i=1}^n (x_i - \bar{x})^2\)</span></p></li>
<li><p><span class="math inline">\((n-1)s^2 = \sum_{i=1}^n(x_i - \bar{x})^2 = \sum_{i=1}^n x_i^2 - n\bar{x}^2\)</span>.</p></li>
<li><p>The mgf <span class="math inline">\(M_{\bar{X}}(t)=\mathbb E e^{t\bar{X}}=\mathbb E e^{t(X_1+\cdots+X_n)/n}=\mathbb E e^{(t/n)(X_1+\cdots+X_n)}=M_{(X_1+\cdots+X_n)}(t/n)=[M_{X}(t/n)]^n\)</span>. The last equal sign is caused by <span class="math inline">\(X_1,\cdots,X_n\)</span> are identically distributed, <span class="math inline">\(M_{X_i}(t)\)</span> is the same function for each <span class="math inline">\(i\)</span>.</p></li>
<li><p>Let <span class="math inline">\(X_1, \dots, X_n\)</span> be a random sample from a <span class="math inline">\(\text{n}(\mu, \sigma^2)\)</span>
distribution. Then the mgf of the sample mean is <span class="math display">\[\begin{align}
M_{\bar{X}}(t)&amp;=\left[\exp\left(\mu\frac{t}{n}+\frac{\sigma^2(t/n)^2}{2}\right)\right]^n\\
&amp;=\exp\left(n\left(\mu\frac{t}{n}+\frac{\sigma^2(t/n)^2}{2}\right)\right)\\
&amp;=\exp\left(\mu t+\frac{(\sigma^2/n)t^2}{2}\right)
\end{align}\]</span> Thus <span class="math inline">\(\bar{X}\)</span> has a <span class="math inline">\(\text{n}(\mu, \sigma^2/n)\)</span> distribution.</p></li>
</ol>
</div>
<div class="theorem">
<p><span id="thm:unlabeled-div-7" class="theorem"><strong>Theorem 3  </strong></span>Let <span class="math inline">\(X_1, \dots, X_n\)</span> be a random sample from population
<span class="math inline">\(f(x\vert{}\vec{\theta})\)</span> belonging to an exponential family
<span class="math display">\[f(x \vert{} \vec{\theta}) = h(x)c(\vec{\theta}) \exp\left(\sum_{i=1}^k w_i(\vec{\theta})t_i(x)\right).\]</span>
Define the statistics
<span class="math display">\[T_i(X_1, \dots, X_n) = \sum_{j=1}^n t_i(X_j), \quad i=1, \dots, k.\]</span>
Then if the set
<span class="math inline">\(\{(w_1(\vec{\theta}), \dots, w_n(\vec{\theta}), \, \theta \in \Theta\}\)</span>
contains and open subset of <span class="math inline">\(\mathbb R^k\)</span> then the distribution of
<span class="math inline">\((X_1, \dots, X_n)\)</span> is an exponential family of the form
<span class="math display">\[f(u_1, \dots, u_n \vert{} \vec{\theta}) = H(u_1, \dots, u_n) c(\vec{\theta})^n \exp\left(\sum_{i=1}^kw_i(\vec{\theta})u_i\right).\]</span></p>
</div>
<div class="remark">
<p><span id="unlabeled-div-8" class="remark"><em>Remark</em>. </span>The open condition eliminates curved exponential families from this
result.</p>
</div>
<div id="sampling-from-the-normal-distribution" class="section level2">
<h2>Sampling from the Normal Distribution</h2>
<div class="theorem">
<p><span id="thm:unlabeled-div-9" class="theorem"><strong>Theorem 4  </strong></span>Let <span class="math inline">\(X_1, \dots, X_n\)</span> be a random sample from a <span class="math inline">\(\text{n}(\mu, \sigma^2)\)</span>
distribution. Then,</p>
<ul>
<li><p><span class="math inline">\(\bar{X}\)</span> and <span class="math inline">\(S^2\)</span> are independent</p></li>
<li><p><span class="math inline">\(\bar{X} \sim \text{n}(\mu, \sigma^2/n)\)</span></p></li>
<li><p><span class="math inline">\((n-1)S^2/\sigma^2 \sim \chi^2_{n-1}\)</span>.</p></li>
</ul>
</div>
<div class="lemma">
<p><span id="lem:unlabeled-div-10" class="lemma"><strong>Lemma 1  </strong></span>In the case of samples from a multivariate normal</p>
<ul>
<li><p>Independence <span class="math inline">\(\iff\)</span> vanishing covariance</p></li>
<li><p>Pairwise independence <span class="math inline">\(\iff\)</span> independence</p></li>
</ul>
</div>
</div>
<div id="convergence-concepts" class="section level2">
<h2>Convergence Concepts</h2>
<div id="convergence-in-probability" class="section level3">
<h3>Convergence in Probability</h3>
<div class="definition">
<p><span id="def:unlabeled-div-11" class="definition"><strong>Definition 4  </strong></span>A sequence of random variables <span class="math inline">\(\{X_i: i \in \mathbb N \}\)</span> <em>converges in
probability</em> to a random variable <span class="math inline">\(X\)</span> if <span class="math inline">\(\forall \epsilon &gt; 0\)</span>
<span class="math display">\[\lim_{n\to\infty}\text{P }(|{X_n - X}| \geq \epsilon) = 0\]</span> (or
equivalently if <span class="math inline">\(\lim_{n\to\infty}\text{P }(|{X_n - X}| &lt; \epsilon) = 1\)</span>).</p>
</div>
<div class="theorem">
<p><span id="thm:unlabeled-div-12" class="theorem"><strong>Theorem 5  </strong></span><strong>Weak Law of Large Numbers, WLLN</strong>
Let <span class="math inline">\(X_1, X_2, \dots\)</span> be iid RVs with mean <span class="math inline">\(\mu\)</span> and
<span class="math inline">\(\mathbb V X_i = \sigma^2 &lt; \infty\)</span>. Define <span class="math display">\[\bar{X}_n = \frac1n \sum_{i=1}^n X_i\]</span>
Then the sequence <span class="math inline">\(\bar{X}_n\)</span> converges in probability to <span class="math inline">\(\mu\)</span>. That
is, <span class="math inline">\(\forall \epsilon &gt; 0\)</span>
<span class="math display">\[\lim_{n\to\infty} \text{P }(|{\bar X_n - \mu}| &lt; \epsilon) = 1\]</span></p>
</div>
<p>Proof is using Chebychev’s inequality.</p>
<p>We have for every <span class="math inline">\(\epsilon&gt;0\)</span>, <span class="math display">\[P(|\bar X_n - \mu|\ge\epsilon)=P((\bar X_n - \mu)^2\ge\epsilon^2)\leq\frac{\mathbb E(\bar X_n - \mu)^2}{\epsilon^2}=\frac{\mathbb V\bar X_n}{\epsilon^2}=\frac{\sigma^2}{n\epsilon^2}\]</span>
Hence, <span class="math display">\[P(|\bar X_n - \mu|&lt;\epsilon)=1-P(|\bar X_n - \mu|\ge\epsilon)\ge 1-\frac{\sigma^2}{n\epsilon^2}\to 1 \text{ as } n\to \infty\]</span></p>
<p>The property summarized by the WLLN, that a sequence of the “same” sample quantity approaches a constant as <span class="math inline">\(n \to \infty\)</span>, is known as <strong>consistency</strong>.</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-13" class="theorem"><strong>Theorem 6  </strong></span>If <span class="math inline">\(X_1, X_2, \dots\)</span> converges in probability to <span class="math inline">\(X\)</span> and <span class="math inline">\(h\)</span> is a
continuous function, then <span class="math inline">\(h(X_1), h(X_2), \dots\)</span> converges in
probability to <span class="math inline">\(h(X)\)</span>.</p>
</div>
</div>
<div id="almost-sure-convergence" class="section level3">
<h3>Almost Sure Convergence</h3>
<div class="definition">
<p><span id="def:unlabeled-div-14" class="definition"><strong>Definition 5  </strong></span>A sequence of random variables <span class="math inline">\(X_1, X_2, \dots\)</span> <em>converges almost surely</em> to a random variable <span class="math inline">\(X\)</span> if <span class="math inline">\(\forall \epsilon &gt; 0\)</span>
<span class="math display">\[\text{P }\left(\lim_{n\to\infty} |{\bar{X}_n - X}| &lt; \epsilon\right) = 1.\]</span></p>
</div>
<div class="comments">
<ul>
<li><p>Almost sure convergence is much stronger than convergence in probability. Convergence in probability states that the sequence of measures of the sets on which the sequence has finite difference from its the limit converges to <span class="math inline">\(0\)</span>. Almost sure convergence states that any place where the sequence has finite difference from its limit must have measure <span class="math inline">\(0\)</span>. It’s like a sequence of integrals converging vs. whether the integrands converge.</p></li>
<li><p>Almost sure convergence implies convergence in probability but not the other way around.</p></li>
</ul>
</div>
<div class="theorem">
<p><span id="thm:unlabeled-div-15" class="theorem"><strong>Theorem 7  </strong></span>If a sequence converges in probability then it is possible to find a subsequence that converges almost surely.</p>
</div>
<div class="theorem">
<p><span id="thm:unlabeled-div-16" class="theorem"><strong>Theorem 8  </strong></span><strong>Strong Law of Large Numbers (SLLN)</strong>
Let <span class="math inline">\(X_1, X_2, \dots\)</span> be iid RVs with mean <span class="math inline">\(\mathbb E {X_i} = \mu\)</span> and
<span class="math inline">\(\mathbb V{X_i} = \sigma^2 &lt; \infty\)</span>. Define <span class="math display">\[\bar{X}_n = \frac{1}{n} \sum_{i=1}^n X_i\]</span>.
Then the sequence <span class="math inline">\(\bar{X}_n\)</span> converges almost surely to <span class="math inline">\(\mu\)</span>. That is,
<span class="math inline">\(\forall \epsilon &gt; 0\)</span>
<span class="math display">\[\text{P }\left(\lim_{n\to\infty}|{\bar{X}_n - \mu}| &lt; \epsilon\right) = 1.\]</span></p>
</div>
</div>
<div id="convergence-in-distribution" class="section level3">
<h3>Convergence in Distribution</h3>
<div class="definition">
<p><span id="def:unlabeled-div-17" class="definition"><strong>Definition 6  </strong></span>A sequence of random variables <span class="math inline">\(X_1, X_2, \dots\)</span> <em>converges in
distribution</em> to a random variable <span class="math inline">\(X\)</span> if
<span class="math display">\[\lim_{n \to \infty} F_{X_n}(x) = F_X(x)\]</span> at all points where <span class="math inline">\(F_X\)</span> is
continuous.</p>
</div>
<div class="remark">
<p><span id="unlabeled-div-18" class="remark"><em>Remark</em>. </span>Here it is really the cdfs that converge, rather than the random
variables. In this way convergence in distribution differs from the
previous two concepts.</p>
</div>
<div class="theorem">
<p><span id="thm:unlabeled-div-19" class="theorem"><strong>Theorem 9  </strong></span>Convergence in probability implies convergence in distribution</p>
</div>
<div class="theorem">
<p><span id="thm:unlabeled-div-20" class="theorem"><strong>Theorem 10  </strong></span><strong>Central Limit Theorem</strong>
Let <span class="math inline">\(X_1, X_2, \dots\)</span> be a sequence of iid random variables with
<span class="math inline">\(\mathbb E[X_i] = \mu\)</span> and finite variance <span class="math inline">\(\mathbb V[X_i] = \sigma^2 &lt; \infty\)</span>.
Define <span class="math inline">\(\bar{X}_n = \frac1n \sum_{i=1}^n X_i\)</span>. Let <span class="math inline">\(G_n(x)\)</span> denote the
cdf of <span class="math inline">\(\sqrt{n}(\bar{X}_n - \mu)/\sigma\)</span>. Then <span class="math inline">\(\forall x \in \mathbb R\)</span>,
<span class="math display">\[\lim_{n\to\infty} G_n(x) = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^x e^{-y^2/2}dy.\]</span>
That is, <span class="math inline">\(\sqrt{n}(\bar{X}_n - \mu)/\sigma\)</span> converges in distribution to
the standard normal.</p>
</div>
<div class="theorem">
<p><span id="thm:unlabeled-div-21" class="theorem"><strong>Theorem 11  </strong></span>If <span class="math inline">\(X_n \to X\)</span> in distribution and <span class="math inline">\(Y_n \to a\)</span> in probability with <span class="math inline">\(a\)</span>
constant, then</p>
<ol style="list-style-type: lower-alpha">
<li><p><span class="math inline">\(Y_nX_n \to aX\)</span> in distribution</p></li>
<li><p><span class="math inline">\(X_n + Y_n \to X + a\)</span> in distribution</p></li>
</ol>
</div>
<div class="remark">
<p><span id="unlabeled-div-22" class="remark"><em>Remark</em>. </span>This tells us, for instance, that
<span class="math display">\[\frac{\sqrt{n}(\bar{X}_n - \mu)}{S_n} \to \text{n}(0, 1)\]</span> in distribution,
since we know that <span class="math inline">\(S_n \to \sigma\)</span> in probability.</p>
</div>
</div>
<div id="the-delta-method" class="section level3">
<h3>The Delta Method</h3>
<p>If we are interested in the convergence of some function of a sequence
of RVs, rather than the RVs themselves, then we can use the Delta Method
(follows from an application of Taylor’s theorem and Slutsky’s theorem).</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-23" class="theorem"><strong>Theorem 12  </strong></span>Let <span class="math inline">\(Y_n\)</span> be a sequence of random variables that satisfies
<span class="math inline">\(\sqrt{n}(Y_n - \theta) \to \text{n}(0, \sigma^2)\)</span> in distribution. For a
given function <span class="math inline">\(g\)</span> and a specific value of <span class="math inline">\(\theta\)</span>, suppose that
<span class="math inline">\(g&#39;(\theta)\)</span> exists and is non-zero. Then
<span class="math display">\[\sqrt{n}[g(Y_n) - g(\theta)] \to \text{n}(0, \sigma^2g&#39;(\theta)^2)\]</span> in
distribution.</p>
</div>
<div class="remark">
<p><span id="unlabeled-div-24" class="remark"><em>Remark</em>. </span>There exists a corresponding multivariate result.</p>
</div>
<p>If <span class="math inline">\(g&#39;(\theta) = 0\)</span> then we take the next term in the Taylor series.</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-25" class="theorem"><strong>Theorem 13  </strong></span>Let <span class="math inline">\(Y_n\)</span> be a sequence of random variables that satisfies
<span class="math inline">\(\sqrt{n}(Y_n - \theta) \to \text{n}(0, \sigma^2)\)</span> in distribution. For a
given function <span class="math inline">\(g\)</span> and a specific value of <span class="math inline">\(\theta\)</span>, suppose that
<span class="math inline">\(g&#39;(\theta) = 0\)</span> and <span class="math inline">\(g&#39;&#39;(\theta)\)</span> exists and is non-zero. Then
<span class="math display">\[\sqrt{n}[g(Y_n) - g(\theta)] \to \text{n}(0, \sigma^2[g&#39;(\theta)]^2)\]</span> in
distribution.</p>
</div>
</div>
</div>
<div id="generating-a-random-sample" class="section level2">
<h2>Generating A Random Sample</h2>
<div class="definition">
<p><span id="def:unlabeled-div-26" class="definition"><strong>Definition 7  </strong></span>A <em>Direct Method</em> of generating a random sample uses the probability
integral transform to map draws from a <span class="math inline">\(\text{uniform}(0, 1)\)</span> random
variable to draws from the distribution of interest.<br />
The Probability Integral Transform states that if <span class="math inline">\(X\)</span> has continuous cdf
<span class="math inline">\(F_X(x)\)</span> then <span class="math display">\[F_X(X) \sim \text{uniform}(0, 1).\]</span></p>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-27" class="definition"><strong>Definition 8  </strong></span>Let <span class="math inline">\(Y \sim f_Y(y)\)</span> and <span class="math inline">\(V \sim f_V(v)\)</span> where <span class="math inline">\(f_Y\)</span> and <span class="math inline">\(f_V\)</span> have
common support with <span class="math display">\[M = \sup_y f_Y(y) / f_V(y) &lt; \infty.\]</span> To generate
a random variable <span class="math inline">\(Y \sim f_Y\)</span>:</p>
<ol style="list-style-type: lower-alpha">
<li><p>Generate <span class="math inline">\(U \sim \text{uniform}(0, 1)\)</span>, <span class="math inline">\(V \sim f_V\)</span> independent.</p></li>
<li><p>If <span class="math inline">\(U &lt; \frac 1M f_Y(V)/f_V(V)\)</span>, return <span class="math inline">\(V\)</span> as a sample of <span class="math inline">\(Y\)</span>;
otherwise go back to (a.).</p></li>
</ol>
</div>
<ul>
<li><p>It is typical to call <span class="math inline">\(V\)</span> the <em>candidate density</em> and <span class="math inline">\(Y\)</span> the
<em>target density</em>.</p></li>
<li><p>One would normally try to choose a candidate density with heavier
tails than the target density (e.g. Cauchy and normal) to ensure
that the tails of the target are well represented. If the target has
heavy tails, however, it can be hard to find a candidate that
results in finite <span class="math inline">\(M\)</span>. In this case people turn to MCMC methods.</p></li>
<li><p>Note that <span class="math inline">\(\text{P }(\texttt{terminate}) = 1/M\)</span>. The number of trials to
generate one sample of <span class="math inline">\(Y\)</span> is therefore <span class="math inline">\(\text{geometric}(1/M)\)</span>,
with <span class="math inline">\(M\)</span> the expected number of trials.</p></li>
<li><p>The intuition behind this algorithm is that if we consider placing
the density of a random variable <span class="math inline">\(Y\)</span> in a box (2d for simplicity)
with coordinates <span class="math inline">\((v,u)\)</span>, we express the cdf of <span class="math inline">\(Y\)</span> using
<span class="math inline">\(V, U \sim \text{uniform}(0, 1)\)</span>
<span class="math display">\[\text{P }(Y \leq y) = \text{P }(V \leq y \vert{} U \leq \frac1c f_Y(V))\]</span>
where <span class="math inline">\(c = \sup_y f_Y(y)\)</span>. In the actual algorithm we take
<span class="math inline">\(U \sim \text{uniform}(0,1)\)</span> and <span class="math inline">\(V\)</span> to be an RV that has common
support with <span class="math inline">\(Y\)</span>.</p></li>
</ul>
</div>
</div>
<div id="references" class="section level1 unnumbered">
<h1>References</h1>
<div id="refs" class="references csl-bib-body">
<div id="ref-casella2021statistical" class="csl-entry">
1. Casella G, Berger RL. Statistical inference. Cengage Learning; 2021.
</div>
</div>
</div>

    </div>
  </article>

  


</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
          </li>
          <li>
          </li>
        </ul>
      </footer>

    </div>
    

    
<script src="../../../../js/math-code.js"></script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
  </body>
</html>

