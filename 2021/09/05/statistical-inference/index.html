<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.80.0" />


<title>Statistical Inference - A Hugo website</title>
<meta property="og:title" content="Statistical Inference - A Hugo website">


  <link href='../../../../favicon.ico' rel='icon' type='image/x-icon'/>



  







<link rel="stylesheet" href="../../../../css/fonts.css" media="all">
<link rel="stylesheet" href="../../../../css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="../../../../" class="nav-logo">
    <img src="../../../../images/logo.png"
         width="50"
         height="50"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="../../../../about/">about</a></li>
    
    <li><a href="../../../../vitae/">CV</a></li>
    
    <li><a href="https://github.com/danli349">GitHub</a></li>
    
    <li><a href="https://scholar.google.com/citations?user=mNjLK8EAAAAJ&amp;hl=en">Googlr Scholar</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">74 min read</span>
    

    <h1 class="article-title">Statistical Inference</h1>

    
    <span class="article-date">2021-09-05</span>
    

    <div class="article-content">
      
<script src="../../../../2021/09/05/statistical-inference/index_files/header-attrs/header-attrs.js"></script>

<div id="TOC">
<ul>
<li><a href="#probability-theory">1. Probability Theory</a>
<ul>
<li><a href="#set-theory">Set Theory</a></li>
<li><a href="#basics-of-probability-theory">Basics of Probability Theory</a></li>
<li><a href="#conditional-probability-and-independence">Conditional Probability and Independence</a></li>
<li><a href="#random-variables">Random Variables</a></li>
<li><a href="#distribution-functions">Distribution Functions</a></li>
<li><a href="#density-and-mass-functions">Density and Mass Functions</a></li>
</ul></li>
<li><a href="#transformations-and-expectations">2. Transformations and Expectations</a>
<ul>
<li><a href="#distributions-of-functions-of-a-random-variable-theorem">Distributions of Functions of a Random Variable Theorem</a></li>
</ul></li>
<li><a href="#common-families-of-distributions">3. Common Families of Distributions</a>
<ul>
<li><a href="#continuous-distributions">ContinuouS Distributions</a>
<ul>
<li><a href="#gamma-distribution">Gamma Distribution</a></li>
<li><a href="#normal-distribution">Normal Distribution</a></li>
<li><a href="#chi-squared-distribution">Chi-Squared Distribution</a></li>
<li><a href="#students-t-distribution">Student’s <span class="math inline">\(t\)</span>-Distribution</a></li>
<li><a href="#snedcors-f-distribution">Snedcor’s <span class="math inline">\(F\)</span>-Distribution</a></li>
<li><a href="#multinomial-distribution">Multinomial Distribution</a></li>
</ul></li>
<li><a href="#exponential-families">Exponential Families</a></li>
<li><a href="#location-and-scale-families">Location and Scale Families</a></li>
<li><a href="#inequalities-and-identities">Inequalities and Identities</a></li>
</ul></li>
<li><a href="#multiple-random-variables">4. Multiple Random Variables</a>
<ul>
<li><a href="#facts">Facts</a></li>
<li><a href="#bivariate-relations">Bivariate Relations</a></li>
<li><a href="#inequalities">Inequalities</a>
<ul>
<li><a href="#numerical-inequalities">Numerical Inequalities</a></li>
<li><a href="#functional-inequalities">Functional Inequalities</a></li>
</ul></li>
</ul></li>
<li><a href="#properties-of-a-random-sample">5. Properties of a Random Sample</a>
<ul>
<li><a href="#sampling-from-the-normal-distribution">Sampling from the Normal Distribution</a></li>
<li><a href="#convergence-concepts">Convergence Concepts</a>
<ul>
<li><a href="#convergence-in-probability">Convergence in Probability</a></li>
<li><a href="#almost-sure-convergence">Almost Sure Convergence</a></li>
<li><a href="#convergence-in-distribution">Convergence in Distribution</a></li>
<li><a href="#the-delta-method">The Delta Method</a></li>
<li><a href="#the-second-order-delta-method">The second-order Delta Method</a></li>
<li><a href="#multivariate-delta-method">Multivariate Delta Method</a></li>
</ul></li>
<li><a href="#generating-a-random-sample">Generating A Random Sample</a>
<ul>
<li><a href="#direct-method">Direct Method</a></li>
<li><a href="#indirect-methods">Indirect Methods</a></li>
<li><a href="#the-acceptreject-algorithm">The Accept/Reject Algorithm</a></li>
</ul></li>
</ul></li>
<li><a href="#principles-of-data-reduction">6. Principles of Data Reduction</a>
<ul>
<li><a href="#the-sufficiency-principle">The Sufficiency Principle</a></li>
<li><a href="#ancillary-statistic">Ancillary Statistic</a></li>
<li><a href="#complete-statistic">Complete Statistic</a></li>
<li><a href="#the-likelihood-principle">The Likelihood Principle</a></li>
<li><a href="#a-slightly-more-formal-construction">A Slightly More Formal Construction</a></li>
<li><a href="#the-equivariance-principle">The Equivariance Principle</a></li>
</ul></li>
<li><a href="#point-estimation">7. Point Estimation</a>
<ul>
<li><a href="#methods-of-finding-estimators">Methods of Finding Estimators</a>
<ul>
<li><a href="#method-of-moments">Method of Moments</a></li>
<li><a href="#maximum-likelihood-estimators">Maximum Likelihood Estimators</a></li>
<li><a href="#bayes-estimators">Bayes Estimators</a></li>
<li><a href="#the-em-expectation-maximization-algorithm">The EM (Expectation-Maximization) Algorithm</a></li>
</ul></li>
<li><a href="#methods-of-evaluating-estimators">Methods of Evaluating Estimators</a>
<ul>
<li><a href="#mean-squared-error">Mean Squared Error</a></li>
<li><a href="#best-unbiased-estimators">Best Unbiased Estimators</a></li>
<li><a href="#fisher-information">Fisher Information</a></li>
<li><a href="#sufficiency-and-unbiasedness">Sufficiency and Unbiasedness</a></li>
<li><a href="#uniqueness-of-best-unbiased-estimator">Uniqueness of best unbiased estimator</a></li>
<li><a href="#completeness-and-best-unbiasedness">completeness and best unbiasedness</a></li>
<li><a href="#loss-function-optimality">Loss Function Optimality</a></li>
</ul></li>
</ul></li>
<li><a href="#hypothesis-testing">8. Hypothesis Testing</a>
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#methods-of-finding-tests">Methods of Finding Tests</a>
<ul>
<li><a href="#likelihood-ratio-tests">Likelihood Ratio Tests</a></li>
<li><a href="#likelihood-ratio-tests-using-the-sufficiency-statistic">Likelihood Ratio Tests using the sufficiency statistic</a></li>
<li><a href="#bayesian-tests">Bayesian tests</a></li>
<li><a href="#union-intersection-and-intersection-union-tests">Union-intersection and intersection-union tests</a></li>
</ul></li>
<li><a href="#methods-of-evaluating-tests">Methods of Evaluating Tests</a>
<ul>
<li><a href="#error-probabilities-and-the-power-function">Error Probabilities and the Power Function</a></li>
<li><a href="#most-powerful-tests-ump">Most Powerful Tests (UMP)</a></li>
<li><a href="#sizes-of-union-intersection-and-intersection-union-tests">Sizes of Union-Intersection and Intersection-Union Tests</a></li>
<li><a href="#p-values"><span class="math inline">\(p\)</span>-Values</a></li>
<li><a href="#loss-function-optimality-1">Loss Function Optimality</a></li>
</ul></li>
</ul></li>
<li><a href="#interval-estimation">9. Interval Estimation</a>
<ul>
<li><a href="#introduction-1">Introduction</a></li>
<li><a href="#methods-of-finding-interval-estimators">Methods of Finding Interval Estimators</a>
<ul>
<li><a href="#inverting-a-test-statistic">Inverting a Test Statistic</a></li>
<li><a href="#pivotal-quantities">Pivotal Quantities</a></li>
<li><a href="#pivoting-the-cdf">Pivoting the CDF</a></li>
</ul></li>
<li><a href="#methods-of-evaluating-interval-estimators">Methods of Evaluating Interval Estimators</a>
<ul>
<li><a href="#size-and-coverage-probability">Size and Coverage Probability</a></li>
<li><a href="#test-related-optimality">Test-Related Optimality</a></li>
</ul></li>
</ul></li>
<li><a href="#asymptotic-evaluations">10. Asymptotic Evaluations</a>
<ul>
<li><a href="#point-estimation-1">Point Estimation</a>
<ul>
<li><a href="#consistency">Consistency</a></li>
</ul></li>
</ul></li>
<li><a href="#analysis-of-variance-and-regression">11. Analysis of Variance and Regression</a>
<ul>
<li><a href="#simple-linear-regression">Simple Linear Regression</a></li>
</ul></li>
<li><a href="#regression-models">12. Regression Models</a></li>
<li><a href="#references">References</a></li>
</ul>
</div>

<div id="probability-theory" class="section level1">
<h1>1. Probability Theory</h1>
<div id="set-theory" class="section level2">
<h2>Set Theory</h2>
</div>
<div id="basics-of-probability-theory" class="section level2">
<h2>Basics of Probability Theory</h2>
<div id="theorem-1.2.9-and-bonferroni-s-inequality" class="section level4">
<h4>Theorem 1.2.9 and Bonferroni ’s Inequality</h4>
<p>If <span class="math inline">\(P\)</span> is a probability function and <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are any sets in <span class="math inline">\(B\)</span>, then<br />
- <strong>a.</strong> <span class="math inline">\(P(B\cap A^c)=P(B)-P(A\cap B)\)</span>;<br />
- <strong>b.</strong> <span class="math inline">\(P(A\cup B)=P(A)+P(B)-P(A\cap B)\)</span>;<br />
- <strong>c.</strong> iF <span class="math inline">\(A\subset B\)</span> then <span class="math inline">\(P(A)\le P(B)\)</span>;</p>
<p><strong>Proof:</strong> To establish (a) note that for any sets <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> we have <span class="math display">\[B=\{B\cap A\}\cup\{B\cap A^c\}\]</span> and therefor
<span class="math display">\[P(B)=P(\{B\cap A\}\cup\{B\cap A^c\})=P(B\cap A)+P(B\cap A^c)\]</span> since <span class="math inline">\(B\cap A\)</span> and <span class="math inline">\(B\cap A^c\)</span> are disjoint.</p>
<p>To establish (b), we use the identity <span class="math display">\[A\cup B=A\cup \{B\cap A^c\}\]</span> since <span class="math inline">\(A\)</span> and <span class="math inline">\(B\cap A^c\)</span> are disjoint, we have <span class="math display">\[P(A\cup B)=P(A)+P(B\cap A^c)=P(A)+P(B)-P(A\cap B)\]</span> from (a).</p>
<p>If <span class="math inline">\(A\subset B\)</span>, then <span class="math inline">\(A\cap B=A\)</span>. Therefore, using (a) we have <span class="math display">\[0\le P(B\cap A^c)=P(B)-P(A)\]</span> establishing (c).</p>
<p>Formula (b) of Theorem 1.2.9 gives a useful inequality for the probability of an intersection. Since <span class="math inline">\(P(A\cup B)\le 1\)</span>, we have from <span class="math display">\[P(A\cup B)=P(A)+P(B\cap A^c)=P(A)+P(B)-P(A\cap B)\le 1\]</span>, after some rearranging, <span class="math display">\[P(A\cap B)\ge P(A)+P(B)-1\]</span> This inequality is a special case of what is known as <strong>Bonferroni ’s Inequality</strong>.</p>
</div>
<div id="booles-inequality" class="section level4">
<h4>Boole’s Inequality</h4>
<p>If <span class="math inline">\(P\)</span> is a probability junction, then</p>
<ul>
<li><strong>a.</strong> <span class="math inline">\(P(A)=\sum_{i=1}^{\infty}P(A\cap C_i)\)</span> for any partition <span class="math inline">\(C_1,C_2,\cdots\)</span>;<br />
</li>
<li><strong>b.</strong> <span class="math inline">\(P(\bigcup_{i=1}^{\infty}A_i)\le\sum_{i=1}^{\infty}P(A_i)\)</span> for any sets <span class="math inline">\(A_1,A_2,\cdots\)</span>.</li>
</ul>
<p><strong>Proof:</strong> Since <span class="math inline">\(C_1,C_2,\cdots\)</span> form a partition, we have that <span class="math inline">\(C_i\cap C_j=\emptyset\)</span> for all <span class="math inline">\(i \ne j\)</span>, and <span class="math inline">\(S=\bigcup_{i=1}^{\infty}C_i\)</span>. Hence, <span class="math display">\[A=A\cap S=A\cap\left(\bigcup_{i=1}^{\infty}C_i\right)=\bigcup_{i=1}^{\infty}(A\cap C_i)\]</span> where the last equality follows from the Distributive Law. We therefore have <span class="math display">\[P(A)=P\left(\bigcup_{i=1}^{\infty}(A\cap C_i)\right)\]</span> Now, since the <span class="math inline">\(C_i\)</span> are disjoint, the sets <span class="math inline">\(A\cap C_i\)</span> are also disjoint, and from the properties of a probability function we have <span class="math display">\[P\left(\bigcup_{i=1}^{\infty}(A\cap C_i)\right)=\sum_{i=1}^{\infty}P(A\cap C_i),\]</span> establishing (a).</p>
<p>To establish (b), construct a disjoint collection <span class="math inline">\(A_1^*,A_2^*,\cdots\)</span>, with the property that <span class="math inline">\(\cup_{i=1}^{\infty}A_i^*=\cup_{i=1}^{\infty}A_i\)</span>, we define <span class="math inline">\(A_i^*\)</span> by <span class="math display">\[A_1^*=A_1, A_i^*=A_i\setminus\left(\bigcup_{j=1}^{i-1}A_j\right),\quad i=2,3,\cdots,\]</span> where <span class="math inline">\(A\setminus B=A\cap B^c\)</span>. It should be easy to see that <span class="math inline">\(\cup_{i=1}^{\infty}A_i^*=\cup_{i=1}^{\infty}A_i\)</span> and we have <span class="math display">\[P\left(\bigcup_{i=1}^{\infty}A_i\right)=P\left(\bigcup_{i=1}^{\infty}A_i^*\right)=\sum_{i=1}^{\infty}P(A_i^*)\le\sum_{i=1}^{\infty} P(A_i)\]</span> Since, by construction <span class="math inline">\(A_i^*\subset A_i\)</span>. establishing (b).</p>
</div>
<div id="bonferroni-s-inequality" class="section level4">
<h4>Bonferroni ’s Inequality</h4>
<p>If we apply Boole’s Inequality to <span class="math inline">\(A^c\)</span> of Bonferroni ’s Inequality, we have <span class="math display">\[P\left(\bigcup_{i=1}^{n}A_i^c\right)\le\sum_{i=1}^{n}P(A_i^c)\]</span> and using the fact that <span class="math inline">\(\bigcup_{i=1}^{n}A_i^c=\left(\bigcap_{i=1}^{n} A_i\right)^c\)</span> and <span class="math inline">\(P(A_i^c)=1-P(A_i)\)</span> so we obtain <span class="math display">\[1-P\left(\bigcap_{i=1}^{n} A_i\right)=P\left(\bigcup_{i=1}^{n}A_i^c\right)\le\sum_{i=1}^{n}P(A_i^c)= n-\sum_{i=1}^{n}P(A_i)\]</span> This becomes <span class="math display">\[P\left(\bigcap_{i=1}^{n} A_i\right)\ge\sum_{i=1}^{n}P(A_i)-(n-1)\]</span>
which is a more general version of the Bonferroni Inequality.</p>
</div>
</div>
<div id="conditional-probability-and-independence" class="section level2">
<h2>Conditional Probability and Independence</h2>
</div>
<div id="random-variables" class="section level2">
<h2>Random Variables</h2>
</div>
<div id="distribution-functions" class="section level2">
<h2>Distribution Functions</h2>
</div>
<div id="density-and-mass-functions" class="section level2">
<h2>Density and Mass Functions</h2>
</div>
</div>
<div id="transformations-and-expectations" class="section level1">
<h1>2. Transformations and Expectations</h1>
<div id="distributions-of-functions-of-a-random-variable-theorem" class="section level2">
<h2>Distributions of Functions of a Random Variable Theorem</h2>
<div id="probability-integral-transformation" class="section level4">
<h4>Probability integral transformation</h4>
<p>Let <span class="math inline">\(X\)</span> have continuous cdf <span class="math inline">\(F_{X}(x)\)</span> and define the random variable <span class="math inline">\(Y\)</span> as <span class="math inline">\(Y = F_{X}(X)\)</span>. Then <span class="math inline">\(Y\)</span> is uniformly distributed on <span class="math inline">\((0, 1)\)</span>, that is, <span class="math inline">\(P(Y \le y) = y, 0 &lt; y &lt; 1\)</span>.</p>
<p>Let <span class="math inline">\(F_{X}^{-1}\)</span> be the inverse of the cdf <span class="math inline">\(F_{X}\)</span>. If <span class="math inline">\(F_{X}\)</span> is strictly increasing, then <span class="math inline">\(F_{X}^{-1}\)</span> is well defined by
<span class="math display">\[F_{X}^{-1}(y)=x\iff F_{X}(x)=y.\]</span>
However, if <span class="math inline">\(F_{X}\)</span> is constant on some interval <span class="math inline">\(x_1\le x \le x_2\)</span>, then <span class="math inline">\(F_{X}^{-1}\)</span> is not well defined by <span class="math inline">\(F_{X}^{-1}(y)=x\iff F_{X}(x)=y.\)</span> Any <span class="math inline">\(x\)</span> satisfying <span class="math inline">\(x_1\le x \le x_2\)</span> satisfies <span class="math inline">\(F_{X}(x) = y\)</span>.
This problem is avoided by defining <span class="math inline">\(F_{X}^{-1}(y)\)</span> for <span class="math inline">\(0 &lt; y &lt; 1\)</span> by <span class="math display">\[F_{X}^{-1}(y) = \inf\; \{x: F_{X}(x) \ge y\},\]</span>
Using this definition, we have <span class="math inline">\(F_{X}^{-1}(y)=x_1\)</span>. At the endpoints of the range of <span class="math inline">\(y\)</span>, <span class="math inline">\(F_{X}^{-1}(y)\)</span> can also be defined. <span class="math inline">\(F_{X}^{-1}(1)=\infty\)</span> if <span class="math inline">\(F_X(x) &lt; 1\)</span> for all <span class="math inline">\(x\)</span> and, for any <span class="math inline">\(F_X\)</span>, <span class="math inline">\(F_{X}^{-1}(0) = -\infty.\)</span></p>
<p><strong>Proof: </strong> For <span class="math inline">\(Y = F_X (X)\)</span> we have, for <span class="math inline">\(0 &lt; y &lt; 1\)</span>,
<span class="math display">\[\begin{align}
P(Y\le y)&amp;=P(F_X (X)\le y)\\
&amp;=P(F_X^{-1}[F_X (X)]\le F_X^{-1}(y))\quad\quad(F_X^{-1} \text{ is increasing})\\
&amp;=P(X\le F_X^{-1}(y))\\
&amp;=F_X(F_X^{-1}(y))\\
&amp;=y\\
\end{align}\]</span> At the endpoints we have <span class="math inline">\(P(Y\le y)=1\)</span> for <span class="math inline">\(y \ge 1\)</span> and <span class="math inline">\(P (Y\le y) = 0\)</span> for <span class="math inline">\(y \le 0\)</span>, showing that <span class="math inline">\(Y\)</span> has a uniform distribution.</p>
</div>
</div>
</div>
<div id="common-families-of-distributions" class="section level1">
<h1>3. Common Families of Distributions</h1>
<div id="continuous-distributions" class="section level2">
<h2>ContinuouS Distributions</h2>
<div id="gamma-distribution" class="section level3">
<h3>Gamma Distribution</h3>
<p><span class="math display">\[f(x|\alpha,\beta)=\frac{1}{\Gamma(\alpha)\beta^{\alpha}}x^{\alpha-1}e^{-x/\beta}\quad 0&lt;x&lt;\infty;\quad\alpha,\beta&gt;0\]</span></p>
<div id="gamma-poisson-relationship" class="section level4">
<h4>Gamma-Poisson relationship</h4>
<p>If <span class="math inline">\(X\)</span> is a <span class="math inline">\(gamma(\alpha,\beta)\)</span> random variable, where <span class="math inline">\(\alpha\)</span> is an integer, then for any <span class="math inline">\(x\)</span>, <span class="math display">\[P(X\le x)=P(Y\ge\alpha),\]</span> where <span class="math inline">\(Y\sim Poisson(x/\beta).\)</span> <span class="math display">\[\begin{align}
P_{Gamma}(X\le x|\alpha,\beta)&amp;=\frac{1}{\Gamma(\alpha)\beta^{\alpha}}\int_{0}^{x}t^{\alpha-1}e^{-t/\beta}dt\\
&amp;=\frac{1}{(\alpha-1)!\beta^{\alpha}}\left[-t^{\alpha-1}\beta e^{-t/\beta}\bigg|_{0}^{x}+\int_{0}^{x}(\alpha-1)t^{\alpha-2}\beta e^{-t/\beta}dt\right]\\
&amp;=\frac{-1}{(\alpha-1)!\beta^{\alpha-1}}x^{\alpha-1}e^{-x/\beta}+\frac{1}{(\alpha-2)!\beta^{\alpha-1}}\int_{0}^{x}t^{\alpha-2} e^{-t/\beta}dt\\
&amp;=\frac{1}{(\alpha-2)!\beta^{\alpha-1}}\int_{0}^{x}t^{\alpha-2} e^{-t/\beta}dt-\frac{1}{(\alpha-1)!}\left(\frac{x}{\beta}\right)^{\alpha-1}e^{-x/\beta}\\
&amp;=\frac{1}{(\alpha-2)!\beta^{\alpha-1}}\int_{0}^{x}t^{\alpha-2} e^{-t/\beta}dt-Poisson_{x/\beta}(Y=\alpha-1)\\
&amp;=P_{Poisson}(Y\ge\alpha|\frac{x}{\beta})\\
\end{align}\]</span> where <span class="math inline">\(Y\sim Poisson(x/\beta)\)</span>.</p>
</div>
</div>
<div id="normal-distribution" class="section level3">
<h3>Normal Distribution</h3>
<p>If <span class="math inline">\(\mathbf{X} \sim \text{n}(\mathbf{\mu}, \boldsymbol{\Sigma})\)</span> then <span class="math inline">\(\mathbf{X}\)</span> has pdf
<span class="math display">\[f_{\mathbf{X}}(x_1, \dots, x_k \vert{} \mathbf{\mu}, \boldsymbol{\Sigma})  = \frac{1}{\sqrt{(2\pi)^k \det\boldsymbol{\Sigma}}} \exp\left(-\frac12 (\mathbf{ x}-\mathbf{\mu})^\top \boldsymbol{\Sigma}^{-1}(\mathbf{x}-\mathbf{\mu})\right).\]</span></p>
</div>
<div id="chi-squared-distribution" class="section level3">
<h3>Chi-Squared Distribution</h3>
<p>The <em>chi-squared distribution with <span class="math inline">\(p\)</span> degrees of freedom</em> has pdf
<span class="math display">\[\chi_p^2 \sim \frac{1}{\Gamma(p/2)2^{p/2}} x^{(p/2)-1} e^{-x/2}, \quad 0&lt; x&lt; \infty.\]</span></p>
<p>[Some facts]</p>
<ol style="list-style-type: lower-alpha">
<li><p>If <span class="math inline">\(Z \sim \text{n}(0, 1)\)</span> then <span class="math inline">\(Z^2 \sim \chi_1^2\)</span></p></li>
<li><p>If <span class="math inline">\(X_1, \dots, X_n\)</span> are independent <span class="math inline">\(X_i \sim \chi_{p_i}^2\)</span> then
<span class="math inline">\(X_1 + \cdots + X_n \sim \chi_{p_1 + \cdots + p_n}^2\)</span>.</p></li>
</ol>
</div>
<div id="students-t-distribution" class="section level3">
<h3>Student’s <span class="math inline">\(t\)</span>-Distribution</h3>
<p><a href="#students-t-distribution">Student’s <span class="math inline">\(t\)</span>-distribution</a> <span class="math inline">\(T \sim t_p\)</span>, a <em><span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(p\)</span>
degrees of freedom</em> if it has pdf
<span class="math display">\[f_T(t) = \frac{\Gamma\left(\frac{p+1}{2}\right)}{\Gamma\left(\frac{p}{2}\right)} \frac{1}{\sqrt{p\pi}} \frac{1}{(1 + t^2/p)^{(p+1)/2}}, \quad t \in \mathbb R{}\]</span></p>
<p>If <span class="math inline">\(p = 1\)</span> then this is the Cauchy distribution.</p>
<p>If <span class="math inline">\(X_1, \dots, X_n\)</span> are a random sample from <span class="math inline">\(\text{n}(\mu, \sigma^2)\)</span> then
<span class="math display">\[\frac{\bar{X} - \mu}{S/\sqrt{n}} \sim t_{n-1}.\]</span> This is often taken
as the definition. Note that the denominator is independent of the
numerator.</p>
<p>[Moments and mgf of <span class="math inline">\(t\)</span>-distribution] Student’s <span class="math inline">\(t\)</span> has no mgf because
it does not have moments of all orders: <span class="math inline">\(t_p\)</span> has only <span class="math inline">\(p-1\)</span> moments. If
<span class="math inline">\(T_p \sim t_p\)</span> then</p>
<p><span class="math display">\[\begin{aligned}
        \mathbb E{}[T_p] &amp;= 0 \quad p &gt; 1 \\
        \mathbb V{}[T_p] &amp;= \frac{p}{p-2} \quad p &gt; 2
    \end{aligned}\]</span></p>
</div>
<div id="snedcors-f-distribution" class="section level3">
<h3>Snedcor’s <span class="math inline">\(F\)</span>-Distribution</h3>
<p><a href="#snedcors-f-distribution">Snedcor’s <span class="math inline">\(F\)</span>-distribution</a> A random variable <span class="math inline">\(X \sim F_{p,q}\)</span> has
<em><span class="math inline">\(F\)</span>-distribution with p and q degrees of freedom</em> if its pdf is
<span class="math display">\[f_X(x) = \frac{\Gamma\left(\frac{p+q}{2}\right)}{\Gamma\left(\frac p2 \right)\Gamma\left(\frac q2 \right)} (p/q)^{p/2} \frac{x^{(p/2) - 1}}{(1 + px/q)^{(p+q)/2}}, \quad 0 &lt; x &lt; \infty.\]</span></p>
<p>If <span class="math inline">\(X_1, \dots, X_n\)</span> is a random sample from <span class="math inline">\(\text{n}(\mu_X, \sigma_X^2)\)</span> and
<span class="math inline">\(Y_1, \dots, Y_m\)</span> is an independent random sample from
<span class="math inline">\(\text{n}(\mu_Y, \sigma_Y^2)\)</span>, then
<span class="math display">\[\frac{S^2_X/\sigma^2_X}{S_Y^2/\sigma_Y^2} \sim F_{n-1, m-1}.\]</span> This is
often taken as the definition.</p>
<p>[Some facts]</p>
<ol style="list-style-type: lower-alpha">
<li><p><span class="math inline">\(X \sim F_{p,q} \,\, \implies \,\, 1/X \sim F_{q,p}\)</span></p></li>
<li><p><span class="math inline">\(X \sim t_q \,\, \implies \,\, X^2 \sim F_{1,q}\)</span></p></li>
<li><p><span class="math inline">\(X \sim F_{p,q} \,\, \implies \,\, \frac{(p/q)X}{1 + (p/q)X} \sim \text{beta}(p/2, q/2)\)</span></p></li>
</ol>
</div>
<div id="multinomial-distribution" class="section level3">
<h3>Multinomial Distribution</h3>
<p><a href="#multinomial-distribution">Multinomial Distribution</a> Let <span class="math inline">\(m\)</span> and <span class="math inline">\(n\)</span> be positive integers and let
<span class="math inline">\(p_1, \dots, p_n \in [0, 1]\)</span> satisfy <span class="math inline">\(\sum_{i=1}^n p_i = 1\)</span>. Then the
random vector <span class="math inline">\((X_1, \dots, X_n)\)</span> has <em>multinomial distribution with m
trials and cell probabilities <span class="math inline">\(p_1, \dots, p_n\)</span></em> if the joint pmf of
<span class="math inline">\((X_1, \dots, X_n)\)</span> is
<span class="math display">\[f(x_1, \dots , x_n) = \frac{m!}{x_1! \cdots x_n!} p_1^{x_1} \cdots
p_n^{x_n} = m! \prod_{i=1}^n \frac{p_i^{x_i}}{x_i!}\]</span> on the set of
<span class="math inline">\((x_1, \dots, x_n)\)</span> such that each <span class="math inline">\(x_i\)</span> is a nonnegative integer and
<span class="math inline">\(\sum_{i=1}^n x_i = m\)</span>.</p>
<p>The marginal distributions have <span class="math inline">\(X_i \sim \text{binomial}(m, p_i)\)</span>.</p>
<p>[Multinomial Theorem] Let <span class="math inline">\(m\)</span> and <span class="math inline">\(n\)</span> be positive integers and let
<span class="math inline">\(\mathcal{A}\)</span> be the set of vectors <span class="math inline">\(\mathbf{x} = (x_1, \dots, x_n)\)</span> such
that each <span class="math inline">\(x_i\)</span> is a nonnegative integer and <span class="math inline">\(\sum_{i=1}^n x_i = m\)</span>.
Then for any real numbers <span class="math inline">\(p_1, \dots, p_n\)</span>,
<span class="math display">\[(p_1 + \cdots + p_n)^m = \sum_{\mathbf{x} \in \mathcal{A}} \frac{m!}{x_1! \cdots x_n!} p_1^{x_1}\cdots p_n^{x_n}.\]</span></p>
</div>
</div>
<div id="exponential-families" class="section level2">
<h2>Exponential Families</h2>
<p>[Exponential family 1] A family of pmfs/pdfs is called an <em>exponential
family</em> if it can be expressed
<span class="math display">\[f(x|\mathbf{\theta}) = h(x)c(\mathbf{\theta})\exp\left( \sum_{i=1}^k w_i(\mathbf{\theta}) t_i(x) \right)\]</span>
where <span class="math inline">\(h(x) \geq 0\)</span>, the <span class="math inline">\(t_i\)</span> are real valued functions of the
observation <span class="math inline">\(x\)</span> that do not depend on <span class="math inline">\(\mathbf{\theta}\)</span> and
<span class="math inline">\(c(\theta) \geq 0\)</span> and the <span class="math inline">\(w_i(\mathbf{\theta})\)</span> are real valued functions
of <span class="math inline">\(\mathbf{\theta}\)</span> that do not depend on <span class="math inline">\(x\)</span>.</p>
<p>If <span class="math inline">\(X\)</span> is a random variable from an exponential family distribution then
<span class="math display">\[\mathbb E{}\left[ \sum_{i=1}^k \frac{\partial w_i(\mathbf{\theta})}{\partial \theta_j} t_i(X) \right] = - \frac{\partial}{\partial \theta_j} \log c(\mathbf{\theta})\]</span>
and
<span class="math display">\[\mathbb V\left[ \frac{\partial w_i(\mathbf{\theta})}{\partial \theta_j} t_i(X) \right] = - \frac{\partial^2}{\partial \theta_j^2} \log c(\mathbf{\theta}) - \mathbb E{}\left[ \sum_{i=1}^{k} \frac{\partial^2 w_i(\mathbf{\theta})}{\partial \theta_j^2} t_i(X) \right]\]</span></p>
<p>[Exponential family 2] We can write another parameterisation of the
exponential family
<span class="math display">\[f(x | \mathbf{\eta}) = h(x) c^{*}(\mathbf{\eta}) \exp(\mathbf{\eta} \cdot \mathbf{t}(x))\]</span>
where <span class="math inline">\(\mathbf{\eta}\)</span> is called the <em>natural parameter</em> and the set
<span class="math inline">\(\mathcal{H} = \{\mathbf{\eta} : \int_\mathbb R{} f(x|\eta) \text{d}x &lt; \infty \}\)</span> is called
the <em>natural parameter space</em> and is convex.</p>
<p><span class="math inline">\(\{\mathbf{\eta}: \mathbf{\eta} = \mathbf{w}(\mathbf{\theta}), \,\, \mathbf{\theta} \in \Theta\} \subseteq \mathcal{H}\)</span>.
So there may be more parameterisations here than previously.<br />
</p>
<p>The natural parameter provides a convenient mathematical formulation,
but sometimes lacks simple interpretation.</p>
<p>[Curved exponential family] A <em>curved exponential family</em> distribution
is one for which the dimension of <span class="math inline">\(\mathbf{\theta}\)</span> is <span class="math inline">\(d &lt; k\)</span>. If <span class="math inline">\(d = k\)</span>
then we have a <em>full exponential family</em>.</p>
</div>
<div id="location-and-scale-families" class="section level2">
<h2>Location and Scale Families</h2>
<p>[Location family] Let <span class="math inline">\(f(x)\)</span> be any pdf. The family of pdfs <span class="math inline">\(f(x - \mu)\)</span>
for <span class="math inline">\(\mu \in \mathbb R{}\)</span> is called the <em>location family with standard pdf
<span class="math inline">\(f(x)\)</span></em> and <span class="math inline">\(\mu\)</span> is the <em>location parameter</em> of the family.</p>
<p>[Scale family] Let <span class="math inline">\(f(x)\)</span> be any pdf. For any <span class="math inline">\(\sigma &gt; 0\)</span> the family of
pdfs <span class="math inline">\(\frac{1}{\sigma} f(x/\sigma)\)</span> is called the <em>scale family with
standard pdf <span class="math inline">\(f(x)\)</span></em> and <span class="math inline">\(\sigma\)</span> is the <em>scale parameter</em> of the
family.</p>
<p>[Location-Scale family] Let <span class="math inline">\(f(x)\)</span> be any pdf. For <span class="math inline">\(\mu \in \mathbb R{}\)</span> and
<span class="math inline">\(\sigma &gt; 0\)</span> the family of pdfs
<span class="math inline">\(\frac{1}{\sigma} f(\frac{x - \mu}{\sigma})\)</span> is called the
<em>location-scale family with standard pdf <span class="math inline">\(f(x)\)</span></em>; <span class="math inline">\(\mu\)</span> is the <em>location
parameter</em> and <span class="math inline">\(\sigma\)</span> is the <em>scale parameter</em>.</p>
<p>[Standardisation] Let <span class="math inline">\(f\)</span> be any pdf, <span class="math inline">\(\mu \in \mathbb R{}\)</span> and
<span class="math inline">\(\sigma \in \mathbb R{}_{&gt;0}\)</span>. Then <span class="math inline">\(X\)</span> is a random variable with pdf
<span class="math inline">\(\frac{1}{\sigma}f(\frac{x - \mu}{\sigma})\)</span> if and only if there exists
a random variable <span class="math inline">\(Z\)</span> with pdf <span class="math inline">\(f(z)\)</span> and <span class="math inline">\(X = \sigma Z + \mu\)</span>.</p>
<p>Probabilities of location-scale families can be computed in terms of
their standard variables <span class="math inline">\(Z\)</span>
<span class="math display">\[\text{P}(X \leq x) = \text{P}\left(Z \leq \frac{x - \mu}{\sigma} \right)\]</span></p>
</div>
<div id="inequalities-and-identities" class="section level2">
<h2>Inequalities and Identities</h2>
<div id="chebyshevs-inequality" class="section level4">
<h4>Chebyshev’s inequality</h4>
<p>Let <span class="math inline">\(X\)</span> be a random variable and let <span class="math inline">\(g(x)\)</span> be
a nonnegative function. Then, for any <span class="math inline">\(r &gt; 0\)</span>,
<span class="math display">\[\text{P}(g(X) \geq r) \leq \frac{\mathbb E{}[g(X)]}{r}.\]</span></p>
<p><strong>Proof:</strong> <span class="math display">\[\begin{align}
\mathbb E g(X)&amp;=\int_{-\infty}^{\infty}g(x)f_{X}(x)dx\\
&amp;\ge\int_{x:g(x)\ge r}g(x)f_{X}(x)dx\\
&amp;\ge r\int_{x:g(x)\ge r}f_{X}(x)dx\\
&amp;=r P(g(x)\ge r)
\end{align}\]</span></p>
<p>This bound is conservative and almost never attained.</p>
</div>
<div id="chebyshevs-inequality-variance-version" class="section level4">
<h4>Chebyshev’s inequality (variance version)</h4>
<p>The Markov inequality is the special case with
<span class="math inline">\(g = \frac{(X-\mu)^2}{\sigma^2}\)</span>, where <span class="math inline">\(\mu=\mathbb E X\)</span> and <span class="math inline">\(\sigma^2=\mathbb V X\)</span>. For convenience write <span class="math inline">\(r=t^2\)</span>.</p>
<p><span class="math display">\[\text{P}\left(\frac{(X-\mu)^2}{\sigma^2} \geq t^2\right) \leq \frac{1}{t^2}.\]</span></p>
<p>Let <span class="math inline">\(X_{\alpha, \beta}\)</span> denote a gamma<span class="math inline">\((\alpha, \beta)\)</span> random variable
with pdf <span class="math inline">\(f(x \vert{} \alpha, \beta)\)</span>, where <span class="math inline">\(\alpha &gt; 1\)</span>. Then for any
constants <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>:
<span class="math display">\[\text{P}(a &lt; X_{\alpha, \beta} &lt; b) = \beta (f(a \vert{} \alpha, \beta) - f(b \vert{} \alpha, \beta)) + \text{P}(a &lt; X_{\alpha - 1, \beta} &lt; b)\]</span></p>
<p>[Stein’s Lemma] Let <span class="math inline">\(X \sim \text{n}(\theta, \sigma^2)\)</span> and let <span class="math inline">\(g\)</span> be a
differentiable function with <span class="math inline">\(\mathbb E{}[g&#39;(x)] &lt; \infty\)</span>. Then
<span class="math display">\[\mathbb E{}[g(X)(X - \theta)] = \sigma^2 \mathbb E{}[g&#39;(X)]\]</span></p>
<p>The proof is just integration by parts.</p>
<p>Stein’s lemma is useful for moment calculations</p>
<p>Let <span class="math inline">\(\chi^2_p\)</span> denote a chi squared distribution with <span class="math inline">\(p\)</span> degrees of
freedom. For any function <span class="math inline">\(h(x)\)</span>,
<span class="math display">\[\mathbb E{}[h(\chi^2_p)] = p \mathbb E{}\left[\frac{h(\chi_{p+2}^2)}{\chi_{p+2}^2}\right]\]</span>
provided the expressions exist.</p>
<p>Let <span class="math inline">\(g(x)\)</span> be a function that is bounded at <span class="math inline">\(-1\)</span> and has finite
expectation, then</p>
<ol style="list-style-type: lower-alpha">
<li><p>If <span class="math inline">\(X \sim \text{Poisson}(\lambda)\)</span>,
<span class="math display">\[\mathbb E{}[\lambda g(X)] = \mathbb E{}[Xg(X-1)].\]</span></p></li>
<li><p>If <span class="math inline">\(X\sim \text{negative-binomial}(r, p)\)</span>,
<span class="math display">\[\mathbb E{}[(1-p)g(X)] = \mathbb E{}\left[ \frac{X}{r + X - 1}g(X) \right].\]</span></p></li>
</ol>
</div>
</div>
</div>
<div id="multiple-random-variables" class="section level1">
<h1>4. Multiple Random Variables</h1>
<div id="facts" class="section level2">
<h2>Facts</h2>
<ul>
<li><p>RVs are independent if and only if their pdfs factorise</p></li>
<li><p>Functions of independent RVs are independent</p></li>
<li><p>Expectations of products (and hence mgfs, etc. of sums) of
independent RVs factor</p></li>
<li><p>Variance of sum of independent RVs is sum of variances.</p></li>
<li><p>Independent RVs have vanishing covariance/correlation, but the
converse is not true in general.</p></li>
</ul>
</div>
<div id="bivariate-relations" class="section level2">
<h2>Bivariate Relations</h2>
<p>[Conditional Expectation] <span class="math display">\[\mathbb E{}[\mathbb E{}[X|Y]] = \mathbb E{}[X]\]</span> provided the
expectations exist.</p>
<p><strong>Proof:</strong> Let <span class="math inline">\(f(x, y)\)</span> denote the joint pdf of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. By definition, we have <span class="math display">\[\mathbb E X=\int\int xf(x,y)dxdy=\int\left[\int xf(x|y)dx\right]f_Y(y)dy\\
=\int\mathbb E(X|y)f_Y(y)dy\\
=\mathbb E(\mathbb E(X|Y))\]</span></p>
<p>[Conditional variance]
<span class="math display">\[\mathbb V[X] = \mathbb E{}[\mathbb V[X \vert{} Y]] + \mathbb V[\mathbb E{}[X \vert{} Y]]\]</span>
provided the expectations exist.</p>
<p><strong>Proof:</strong> By definition, we have
<span class="math display">\[\begin{align}
\mathbb V X&amp;=\mathbb E\left((X-\mathbb E X)^2\right)\\
&amp;=\mathbb E\left((X-\mathbb E(X|Y)+\mathbb E(X|Y)-\mathbb E X)^2\right)\\
&amp;=\mathbb E\left[\left(X-\mathbb E(X|Y)\right)^2\right]+\mathbb E\left[\left(\mathbb E(X|Y)-\mathbb E X\right)^2\right]+2\mathbb E\bigg[\left(X-\mathbb E(X|Y)\right)\left(\mathbb E(X|Y)-\mathbb E X\right)\bigg]\\
&amp;=\mathbb E\left[\left(X-\mathbb E(X|Y)\right)^2\right]+\mathbb E\left[\left(\mathbb E(X|Y)-\mathbb E X\right)^2\right]+2\mathbb E\bigg[\mathbb E\bigg(\left(X-\mathbb E(X|Y)\right)\left(\mathbb E(X|Y)-\mathbb E X\right)|Y\bigg)\bigg]\\
&amp;=\mathbb E\left[\left(X-\mathbb E(X|Y)\right)^2\right]+\mathbb E\left[\left(\mathbb E(X|Y)-\mathbb E X\right)^2\right]+2\mathbb E\bigg[\left(\mathbb E(X|Y)-\mathbb E X\right)\mathbb E\bigg(\left(X-\mathbb E(X|Y)\right)|Y\bigg)\bigg]\\
&amp;=\mathbb E\left[\left(X-\mathbb E(X|Y)\right)^2\right]+\mathbb E\left[\left(\mathbb E(X|Y)-\mathbb E X\right)^2\right]+2\mathbb E\bigg[\left(\mathbb E(X|Y)-\mathbb E X\right)\cdot 0 \bigg]\\
&amp;=\mathbb E\left[\left(X-\mathbb E(X|Y)\right)^2\right]+\mathbb E\left[\left(\mathbb E(X|Y)-\mathbb E X\right)^2\right]\\
&amp;=\mathbb E\left[\mathbb E\{\left(X-\mathbb E(X|Y)\right)^2\}|Y\right]+\mathbb E\left[\left(\mathbb E(X|Y)-\mathbb E X\right)^2\right]\\
&amp;=\mathbb E\left[\mathbb V(X|Y)\right]+\mathbb V[\mathbb E(X|Y)]\\
\end{align}\]</span></p>
<p>[Covariance] <span class="math display">\[\text{ Cov }[X, Y] = \mathbb E{}[(X - \mu_X)(Y - \mu_Y)]\]</span></p>
<p><span class="math display">\[\text{ Cov }[X, Y] = \mathbb E{}[XY] - \mu_X\mu_Y\]</span></p>
<p><span class="math display">\[\mathbb V[aX + bY] = a^2\mathbb V[X] + b^2\mathbb V[Y] + 2ab\text{Cov }[X, Y]\]</span></p>
<p>[Correlation] <span class="math display">\[\rho_{XY} = \frac{\text{Cov }[X, Y]}{\sigma_X \sigma_Y}\]</span></p>
<p>The correlation measures the strength of <em>linear</em> relation between two
RVs. It is possible to have strong non-linear relationships but with
<span class="math inline">\(\rho = 0\)</span>.</p>
<p>We can use an argument similar to the standard proof of Cauchy-Schwarz
to show the following</p>
<p>Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be any RVs, then</p>
<ol style="list-style-type: lower-alpha">
<li><p><span class="math inline">\(-1 \leq \rho_{XY} \leq 1\)</span>,</p></li>
<li><p><span class="math inline">\(\lvert{\rho_{XY}}\lvert = 1\)</span> if and only if there are constants
<span class="math inline">\(a \text{n}eq 0, b\)</span> such that <span class="math inline">\(\text{P}{}(Y = aX + b) = 1\)</span>. If
<span class="math inline">\(\lvert{\rho_{XY}}\lvert = 1\)</span> then <span class="math inline">\(\text{sign}(\rho) = \text{sign}(a)\)</span>.</p></li>
</ol>
<p>Let <span class="math inline">\(X_1, \cdots , X_n\)</span> be independent random vectors. Let <span class="math inline">\(g_i(x_i)\)</span> be a function only of <span class="math inline">\(x_i, i = 1 , \cdots, n\)</span>. Then the random variables <span class="math inline">\(U_i = g_i(X_i), i = 1,\cdots, n\)</span>, are mutually independent.</p>
</div>
<div id="inequalities" class="section level2">
<h2>Inequalities</h2>
<div id="numerical-inequalities" class="section level3">
<h3>Numerical Inequalities</h3>
<p>Let <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> be any positive numbers and let <span class="math inline">\(p, q &gt; 1\)</span> satisfy
<span class="math inline">\(1/p + 1/q = 1\)</span>, then <span class="math display">\[p+q=pq\]</span>
<span class="math display">\[q(p-1)=p\]</span>
<span class="math display">\[1-1/q=1/p\]</span>
<span class="math display">\[\frac1p a^p + \frac1q b^q \geq ab\]</span> with
equality if and only if <span class="math inline">\(a^p = b^q\)</span>.</p>
<p><strong>H<span class="math inline">\(\ddot{o}\)</span>lder’s Inequality</strong> Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be any random
variables and let <span class="math inline">\(p, q &gt; 1\)</span> satisfy <span class="math inline">\(1/p + 1/q = 1\)</span>, then
<span class="math display">\[\lvert{\mathbb E{}[XY]}\lvert \leq \mathbb E{}[\lvert{XY}\lvert] \le \mathbb E{}[\lvert{X}\lvert^p]^{1/p} \mathbb E{}[\lvert{Y}\lvert^q]^{1/q}\]</span>
<strong>Proof:</strong> The first inequality follows from <span class="math inline">\(- |XY| \leq XY \leq |XY|\)</span>. To prove the second inequality, define <span class="math display">\[a=\frac{|X|}{(\mathbb E|X|^p)^{1/p}}\quad\text{and}\quad b=\frac{|Y|}{(\mathbb E|Y|^q)^{1/q}}\]</span> then <span class="math display">\[\frac1p a^p+ \frac1q b^q=\frac1p\frac{|X|^p}{(\mathbb E|X|^p)}+\frac1q\frac{|Y|^q}{(\mathbb E|Y|^q)}\ge ab=\frac{|XY|}{(\mathbb E|X|^p)^{1/p}(\mathbb E|Y|^q)^{1/q}}\]</span> Now take expectations of both sides. The expectation of the left-hand side is <span class="math inline">\(1\)</span>, then <span class="math display">\[1\ge \frac{\mathbb E|XY|}{(\mathbb E|X|^p)^{1/p}(\mathbb E|Y|^q)^{1/q}}\]</span>.</p>
<ul>
<li><strong>Cauchy-Schwarz Inequality</strong> is the special case <span class="math inline">\(p = q = 2\)</span> <span class="math display">\[\mathbb E|XY|\le (\mathbb E|X|^2)^{1/2}(\mathbb E|Y|^2)^{1/2}\]</span><br />
Also let <span class="math display">\[u(t) = E[(tX - Y)^2]\]</span>
Then:<span class="math display">\[t^2E[X^2] - 2tE[XY] + E[Y^2] \ge 0\]</span></li>
</ul>
<p>This is a quadratic in <span class="math inline">\(t\)</span>. Thus the discriminant must be non-positive. Therefore: <span class="math display">\[(\mathbb E[XY])^2 - \mathbb E[X^2]\mathbb E[Y^2] \le 0\]</span> or <span class="math display">\[(\mathbb E[XY])^2 \le \mathbb E[X^2]\mathbb E[Y^2]\]</span> Then <span class="math display">\[|(\mathbb E[XY])|\le(\mathbb E|X|^2)^{1/2}(\mathbb E|Y|^2)^{1/2}\]</span></p>
<ul>
<li><p><strong>Covariance inequality</strong> <span class="math display">\[(\text{Cov}[X, Y])^2 \leq \sigma_X^2 \sigma_Y^2=\mathbb V(X)\mathbb V(Y)\]</span> If <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> have means <span class="math inline">\(\mu_X\)</span> and <span class="math inline">\(\mu_Y\)</span> and
variances <span class="math inline">\(\sigma_X^2\)</span> and <span class="math inline">\(\sigma_Y^2\)</span>, respectively, we can apply the Cauchy-Schwarz Inequality to get <span class="math display">\[\mathbb E\lvert(X-\mu_X)(Y-\mu_Y)\rvert\le \{\mathbb E(X-\mu_X)^2\}^{1/2}\{\mathbb E(Y-\mu_Y)^2\}^{1/2}\]</span>
Squaring both sides and using statistical notation, we have
<span class="math display">\[(\text{Cov}(X, Y))^2 \leq \sigma_X^2\sigma_Y^2.\]</span></p></li>
<li><p><span class="math display">\[\mathbb E{}[\lvert{X}\lvert] \leq \mathbb E{}[\lvert{X}\lvert^p]^{1/p}\]</span></p></li>
<li><p><strong>Liapounov’s Inequality</strong>
<span class="math display">\[\mathbb E{}[\lvert{X}\lvert^r]^{1/r} \leq \mathbb E{}[\lvert{X}\lvert^s]^{1/s}\]</span> where
<span class="math inline">\(1 &lt; r &lt; s &lt; \infty\)</span>.</p></li>
<li><p>For numbers <span class="math inline">\(a_i,b_i,i=1,\cdots,n\)</span>, <span class="math display">\[\sum_{i=1}^{n}|a_ib_i|\leq\left(\sum_{i=1}^{n}a_i^p\right)^{1/p}\left(\sum_{i=1}^{n}b_i^q\right)^{1/q}\]</span> where <span class="math inline">\(\frac{1}{p}+\frac{1}{q}=1\)</span></p></li>
<li><p><span class="math display">\[\frac{1}{n}\left(\sum_{i=1}^{n}|a_i|\right)^2\leq\sum_{i=1}^{n}a_i^2\]</span></p></li>
</ul>
<p><strong>Minkowski’s Inequality</strong><br />
Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be any two random variables. Then for <span class="math inline">\(1\le p&lt;\infty\)</span>
<span class="math display">\[(\mathbb E|X+Y|^p)^{1/p}\le (\mathbb E|X|^p)^{1/p}+(\mathbb E|Y|^p)^{1/p}\]</span>
<strong>Proof:</strong> Write
<span class="math display">\[\mathbb E(|X + Y|^p) = \mathbb E \left(|X + Y||X + Y|^{p-1}\right)\le \mathbb E\left(|X| |X + Y|^{p-1}\right) + \mathbb E\left(|Y||X + Y|^{p-1}\right),\]</span>
where we have used the fact that <span class="math inline">\(|X + Y| \le |X| + |Y|\)</span> (the triangle inequality. Now apply HOlder’s Inequality to each expectation on the right-hand side to get
<span class="math display">\[\begin{align}
\mathbb E(|X + Y|^p) &amp;\leq \mathbb E\left(|X| |X + Y|^{p-1}\right) + \mathbb E\left(|Y||X + Y|^{p-1}\right)\\
&amp;=\left(\mathbb E|X|^p\right)^{1/p} \left(\mathbb E|X + Y|^{q(p-1)}\right)^{1/q} + \left(\mathbb E|Y|^p\right)^{1/p}\left(\mathbb E|X + Y|^{q(p-1)}\right)^{1/q}\\
\end{align}\]</span> where <span class="math inline">\(q\)</span> satisfies <span class="math inline">\(1/p + 1/q = 1\)</span>. Now divide through by <span class="math inline">\(\left(\mathbb E|X + Y|^{q(p-1)}\right)^{1/q}\)</span>. Then <span class="math display">\[\frac{\mathbb E(|X + Y|^p)}{\left(\mathbb E|X + Y|^{q(p-1)}\right)^{1/q}}=\left(\mathbb E(|X + Y|^p\right)^{1/p}\le \left(\mathbb E|X|^p\right)^{1/p} + \left(\mathbb E|Y|^p\right)^{1/p}\]</span></p>
</div>
<div id="functional-inequalities" class="section level3">
<h3>Functional Inequalities</h3>
<p><strong>Convex Function</strong> A function <span class="math inline">\(g(x)\)</span> is <em>convex</em> on a set <span class="math inline">\(S\)</span> if for all
<span class="math inline">\(x, y \in S\)</span> and <span class="math inline">\(0&lt; \lambda &lt; 1\)</span>
<span class="math display">\[g(\lambda x + (1 - \lambda)y) \leq \lambda g(x) + (1 - \lambda)g(y).\]</span>
<strong>Strictly convex</strong> is when the inequality is strict. <span class="math inline">\(g\)</span> is <strong>concave</strong> if
<span class="math inline">\(-g\)</span> is convex.</p>
<p><span class="math inline">\(g(x)\)</span> is convex on <span class="math inline">\(S\)</span> if <span class="math inline">\(g&#39;&#39;(x) \geq 0\)</span> <span class="math inline">\(\forall x \in S\)</span>.</p>
<p><strong>Jensen’s Inequality</strong> If <span class="math inline">\(g(x)\)</span> is convex, then for any random variable
<span class="math inline">\(X\)</span> <span class="math display">\[\mathbb E{}[g(X)] \leq g(\mathbb E{}[X]).\]</span> Equality holds if and only if, for
every line <span class="math inline">\(a + bx\)</span> that is tangent to <span class="math inline">\(g(x)\)</span> at <span class="math inline">\(x = \mathbb E{}[X]\)</span>,
<span class="math inline">\(\text{P}{}\{g(X) = a + bX\} = 1\)</span>. (So if and only if <span class="math inline">\(g\)</span> is affine with
probability 1.)</p>
<ul>
<li><p><span class="math inline">\(x^2\)</span> is convex, <span class="math inline">\(\mathbb E[X^2] \geq \mathbb E{}[X]^2\)</span></p></li>
<li><p><span class="math inline">\(1/x\)</span> is convex, <span class="math inline">\(\mathbb E[1/X] \geq 1/ \mathbb E{}[X]\)</span></p></li>
<li><p><span class="math inline">\(\log x\)</span> is concave, <span class="math inline">\(\mathbb E(\log X) \leq \log (\mathbb E X)\)</span></p></li>
<li><p>Arithmetic mean: <span class="math display">\[a_A=\frac{1}{n}(a_1+a_2+\cdots+a_n)\]</span>
Geometric mean: <span class="math display">\[a_G=(a_1a_2\cdots a_n)^{1/n}\]</span>
Harmonic mean: <span class="math display">\[a_H=\frac{1}{\frac{1}{n}\left(\frac{1}{a_1}+\frac{1}{a_2}+\cdots+\frac{1}{a_n}\right)}\]</span>
<span class="math display">\[a_H\leq a_G\leq a_A\]</span></p></li>
</ul>
<p><strong>Covariance Inequality</strong> Let <span class="math inline">\(X\)</span> be any random variable and <span class="math inline">\(g(x)\)</span> and <span class="math inline">\(h(x)\)</span> any functions such that <span class="math inline">\(\mathbb E g(X), \mathbb E h(X)\)</span> and <span class="math inline">\(\mathbb E (g(X)h(X))\)</span> exist.<br />
- If <span class="math inline">\(g(x)\)</span> is a nondecreasing function and <span class="math inline">\(h(x)\)</span> is a nonincreasing function, then
<span class="math display">\[\mathbb E \bigg(g(X)h(X)\bigg)\leq\bigg(\mathbb E g(X)\bigg)\bigg(\mathbb E h(X)\bigg)\]</span></p>
<ul>
<li>If <span class="math inline">\(g(x)\)</span> and <span class="math inline">\(h(x)\)</span> are both nondecreasing or nonincreasing functions, then
<span class="math display">\[\mathbb E \bigg(g(X)h(X)\bigg)\geq\bigg(\mathbb E g(X)\bigg)\bigg(\mathbb E h(X)\bigg)\]</span></li>
</ul>
</div>
</div>
</div>
<div id="properties-of-a-random-sample" class="section level1">
<h1>5. Properties of a Random Sample</h1>
<div class="definition">
<p><span id="def:unlabeled-div-1" class="definition"><strong>Definition 1  </strong></span>A collection of random variables <span class="math inline">\(X_1, \dots, X_n\)</span> is a <em>random sample
of size n from population <span class="math inline">\(f(x)\)</span></em> if they are iid with pdf/pmf f(x).</p>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-2" class="definition"><strong>Definition 2  </strong></span><span class="math inline">\(Y = T(X_1, \dots, X_n)\)</span> is a <em>statistic</em> if the domain of <span class="math inline">\(T\)</span> contains
the sample space of <span class="math inline">\((X_1, \dots, X_n)\)</span>. The distribution of <span class="math inline">\(Y\)</span> is the
<em>sampling distribution of <span class="math inline">\(Y\)</span></em>.</p>
</div>
<div class="remark">
<p><span id="unlabeled-div-3" class="remark"><em>Remark</em>. </span>A statistic is any function of the data. The only restriction is that
the statistic is not also a function of some other parameters.</p>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-4" class="definition"><strong>Definition 3  </strong></span>The <em>sample mean</em> <span class="math inline">\(\bar{X}\)</span> and <em>sample variance</em> <span class="math inline">\(S^2\)</span> of a random
sample <span class="math inline">\(X_1, \dots, X_n\)</span> are, respectively,</p>
<ul>
<li><p><span class="math inline">\(\bar{X} = \frac1n \sum_{i=1}^n X_i\)</span></p></li>
<li><p><span class="math inline">\(S^2 = \frac{1}{n-1}\sum_{i=1}^n (X_i - \bar{X})^2\)</span>.</p></li>
</ul>
<p>The <em>sample standard deviation</em> is <span class="math inline">\(S = \sqrt{S^2}\)</span>.</p>
</div>
<div class="theorem">
<p><span id="thm:unlabeled-div-5" class="theorem"><strong>Theorem 1  </strong></span>Let <span class="math inline">\(X_1, \dots, X_n\)</span> be a random sample from a population with mean
<span class="math inline">\(\mu\)</span> and finite variance <span class="math inline">\(\sigma^2\)</span>, then</p>
<ol style="list-style-type: lower-alpha">
<li><p><span class="math inline">\(\mathbb E[\bar{X}] = \mu\)</span></p></li>
<li><p><span class="math inline">\(\mathbb V[\bar{X}] = \frac{\sigma^2}{n}\)</span></p></li>
<li><p><span class="math inline">\(\mathbb E[S^2] = \sigma ^2\)</span>.</p></li>
</ol>
</div>
<div class="theorem">
<p><span id="thm:unlabeled-div-6" class="theorem"><strong>Theorem 2  </strong></span>Take <span class="math inline">\(x_1, \dots, x_n \in \mathbb R\)</span> and let <span class="math inline">\(\bar{x}\)</span> be their mean. Then,</p>
<ol style="list-style-type: lower-alpha">
<li><p><span class="math inline">\(\min_a \sum_{i=1}^n (x_i - a)^2 = \sum_{i=1}^n (x_i - \bar{x})^2\)</span></p></li>
<li><p><span class="math inline">\((n-1)s^2 = \sum_{i=1}^n(x_i - \bar{x})^2 = \sum_{i=1}^n x_i^2 - n\bar{x}^2\)</span>.</p></li>
<li><p>The mgf <span class="math inline">\(M_{\bar{X}}(t)=\mathbb E e^{t\bar{X}}=\mathbb E e^{t(X_1+\cdots+X_n)/n}=\mathbb E e^{(t/n)(X_1+\cdots+X_n)}=M_{(X_1+\cdots+X_n)}(t/n)=[M_{X}(t/n)]^n\)</span>. The last equal sign is caused by <span class="math inline">\(X_1,\cdots,X_n\)</span> are identically distributed, <span class="math inline">\(M_{X_i}(t)\)</span> is the same function for each <span class="math inline">\(i\)</span>.</p></li>
<li><p>Let <span class="math inline">\(X_1, \dots, X_n\)</span> be a random sample from a <span class="math inline">\(\text{n}(\mu, \sigma^2)\)</span>
distribution. Then the mgf of the sample mean is <span class="math display">\[\begin{align}
M_{\bar{X}}(t)&amp;=\left[\exp\left(\mu\frac{t}{n}+\frac{\sigma^2(t/n)^2}{2}\right)\right]^n\\
&amp;=\exp\left(n\left(\mu\frac{t}{n}+\frac{\sigma^2(t/n)^2}{2}\right)\right)\\
&amp;=\exp\left(\mu t+\frac{(\sigma^2/n)t^2}{2}\right)
\end{align}\]</span> Thus <span class="math inline">\(\bar{X}\)</span> has a <span class="math inline">\(\text{n}(\mu, \sigma^2/n)\)</span> distribution.</p></li>
</ol>
</div>
<div class="theorem">
<p><span id="thm:unlabeled-div-7" class="theorem"><strong>Theorem 3  </strong></span>Let <span class="math inline">\(X_1, \dots, X_n\)</span> be a random sample from population
<span class="math inline">\(f(x\vert{}\mathbf{\theta})\)</span> belonging to an exponential family
<span class="math display">\[f(x \vert{} \mathbf{\theta}) = h(x)c(\mathbf{\theta}) \exp\left(\sum_{i=1}^k w_i(\mathbf{\theta})t_i(x)\right).\]</span>
Define the statistics
<span class="math display">\[T_i(X_1, \dots, X_n) = \sum_{j=1}^n t_i(X_j), \quad i=1, \dots, k.\]</span>
Then if the set
<span class="math inline">\(\{(w_1(\mathbf{\theta}), \dots, w_n(\mathbf{\theta}), \, \theta \in \Theta\}\)</span>
contains and open subset of <span class="math inline">\(\mathbb R^k\)</span> then the distribution of
<span class="math inline">\((X_1, \dots, X_n)\)</span> is an exponential family of the form
<span class="math display">\[f(u_1, \dots, u_n \vert{} \mathbf{\theta}) = H(u_1, \dots, u_n) c(\mathbf{\theta})^n \exp\left(\sum_{i=1}^kw_i(\mathbf{\theta})u_i\right).\]</span></p>
</div>
<div class="remark">
<p><span id="unlabeled-div-8" class="remark"><em>Remark</em>. </span>The open condition eliminates curved exponential families from this
result.</p>
</div>
<div id="sampling-from-the-normal-distribution" class="section level2">
<h2>Sampling from the Normal Distribution</h2>
<div class="theorem">
<p><span id="thm:unlabeled-div-9" class="theorem"><strong>Theorem 4  </strong></span>Let <span class="math inline">\(X_1, \dots, X_n\)</span> be a random sample from a <span class="math inline">\(\text{n}(\mu, \sigma^2)\)</span>
distribution. Then,</p>
<ul>
<li><p><span class="math inline">\(\bar{X}\)</span> and <span class="math inline">\(S^2\)</span> are independent</p></li>
<li><p><span class="math inline">\(\bar{X} \sim \text{n}(\mu, \sigma^2/n)\)</span></p></li>
<li><p><span class="math inline">\((n-1)S^2/\sigma^2 \sim \chi^2_{n-1}\)</span>.</p></li>
</ul>
</div>
<div class="lemma">
<p><span id="lem:unlabeled-div-10" class="lemma"><strong>Lemma 1  </strong></span>In the case of samples from a multivariate normal</p>
<ul>
<li><p>Independence <span class="math inline">\(\iff\)</span> vanishing covariance</p></li>
<li><p>Pairwise independence <span class="math inline">\(\iff\)</span> independence</p></li>
</ul>
</div>
</div>
<div id="convergence-concepts" class="section level2">
<h2>Convergence Concepts</h2>
<div id="convergence-in-probability" class="section level3">
<h3>Convergence in Probability</h3>
<div class="definition">
<p><span id="def:unlabeled-div-11" class="definition"><strong>Definition 4  </strong></span>A sequence of random variables <span class="math inline">\(\{X_i: i \in \mathbb N \}\)</span> <em>converges in
probability</em> to a random variable <span class="math inline">\(X\)</span> if <span class="math inline">\(\forall \epsilon &gt; 0\)</span>
<span class="math display">\[\lim_{n\to\infty}\text{P }(|{X_n - X}| \geq \epsilon) = 0\]</span> (or
equivalently if <span class="math inline">\(\lim_{n\to\infty}\text{P }(|{X_n - X}| &lt; \epsilon) = 1\)</span>).</p>
</div>
<div class="theorem">
<p><span id="thm:unlabeled-div-12" class="theorem"><strong>Theorem 5  </strong></span><strong>Weak Law of Large Numbers, WLLN</strong>
Let <span class="math inline">\(X_1, X_2, \dots\)</span> be iid RVs with mean <span class="math inline">\(\mu\)</span> and
<span class="math inline">\(\mathbb V X_i = \sigma^2 &lt; \infty\)</span>. Define <span class="math display">\[\bar{X}_n = \frac1n \sum_{i=1}^n X_i\]</span>
Then the sequence <span class="math inline">\(\bar{X}_n\)</span> converges in probability to <span class="math inline">\(\mu\)</span>. That
is, <span class="math inline">\(\forall \epsilon &gt; 0\)</span>
<span class="math display">\[\lim_{n\to\infty} \text{P }(|{\bar X_n - \mu}| &lt; \epsilon) = 1\]</span></p>
</div>
<p>Proof is using Chebychev’s inequality.</p>
<p>We have for every <span class="math inline">\(\epsilon&gt;0\)</span>, <span class="math display">\[P(|\bar X_n - \mu|\ge\epsilon)=P((\bar X_n - \mu)^2\ge\epsilon^2)\leq\frac{\mathbb E(\bar X_n - \mu)^2}{\epsilon^2}=\frac{\mathbb V\bar X_n}{\epsilon^2}=\frac{\sigma^2}{n\epsilon^2}\]</span>
Hence, <span class="math display">\[P(|\bar X_n - \mu|&lt;\epsilon)=1-P(|\bar X_n - \mu|\ge\epsilon)\ge 1-\frac{\sigma^2}{n\epsilon^2}\to 1 \text{ as } n\to \infty\]</span></p>
<p>The property summarized by the WLLN, that a sequence of the “same” sample quantity approaches a constant as <span class="math inline">\(n \to \infty\)</span>, is known as <strong>consistency</strong>.</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-13" class="theorem"><strong>Theorem 6  </strong></span>If <span class="math inline">\(X_1, X_2, \dots\)</span> converges in probability to <span class="math inline">\(X\)</span> and <span class="math inline">\(h\)</span> is a
continuous function, then <span class="math inline">\(h(X_1), h(X_2), \dots\)</span> converges in
probability to <span class="math inline">\(h(X)\)</span>.</p>
</div>
</div>
<div id="almost-sure-convergence" class="section level3">
<h3>Almost Sure Convergence</h3>
<div class="definition">
<p><span id="def:unlabeled-div-14" class="definition"><strong>Definition 5  </strong></span>A sequence of random variables <span class="math inline">\(X_1, X_2, \dots\)</span> <em>converges almost surely</em> to a random variable <span class="math inline">\(X\)</span> if <span class="math inline">\(\forall \epsilon &gt; 0\)</span>
<span class="math display">\[\text{P }\left(\lim_{n\to\infty} |{\bar{X}_n - X}| &lt; \epsilon\right) = 1.\]</span></p>
</div>
<div class="comments">
<ul>
<li><p>Almost sure convergence is much stronger than convergence in probability. Convergence in probability states that the sequence of measures of the sets on which the sequence has finite difference from its the limit converges to <span class="math inline">\(0\)</span>. Almost sure convergence states that any place where the sequence has finite difference from its limit must have measure <span class="math inline">\(0\)</span>. It’s like a sequence of integrals converging vs. whether the integrands converge.</p></li>
<li><p>Almost sure convergence implies convergence in probability but not the other way around.</p></li>
</ul>
</div>
<div class="theorem">
<p><span id="thm:unlabeled-div-15" class="theorem"><strong>Theorem 7  </strong></span>If a sequence converges in probability then it is possible to find a subsequence that converges almost surely.</p>
</div>
<div class="theorem">
<p><span id="thm:unlabeled-div-16" class="theorem"><strong>Theorem 8  </strong></span><strong>Strong Law of Large Numbers (SLLN)</strong>
Let <span class="math inline">\(X_1, X_2, \dots\)</span> be iid RVs with mean <span class="math inline">\(\mathbb E {X_i} = \mu\)</span> and
<span class="math inline">\(\mathbb V{X_i} = \sigma^2 &lt; \infty\)</span>. Define <span class="math display">\[\bar{X}_n = \frac{1}{n} \sum_{i=1}^n X_i\]</span>.
Then the sequence <span class="math inline">\(\bar{X}_n\)</span> converges almost surely to <span class="math inline">\(\mu\)</span>. That is,
<span class="math inline">\(\forall \epsilon &gt; 0\)</span>
<span class="math display">\[\text{P }\left(\lim_{n\to\infty}|{\bar{X}_n - \mu}| &lt; \epsilon\right) = 1.\]</span></p>
</div>
</div>
<div id="convergence-in-distribution" class="section level3">
<h3>Convergence in Distribution</h3>
<div class="definition">
<p><span id="def:unlabeled-div-17" class="definition"><strong>Definition 6  </strong></span>A sequence of random variables <span class="math inline">\(X_1, X_2, \dots\)</span> <em>converges in
distribution</em> to a random variable <span class="math inline">\(X\)</span> if
<span class="math display">\[\lim_{n \to \infty} F_{X_n}(x) = F_X(x)\]</span> at all points where <span class="math inline">\(F_X\)</span> is
continuous.</p>
</div>
<div class="remark">
<p><span id="unlabeled-div-18" class="remark"><em>Remark</em>. </span>Here it is really the cdfs that converge, rather than the random
variables. In this way convergence in distribution differs from the
previous two concepts.</p>
</div>
<div class="theorem">
<p><span id="thm:unlabeled-div-19" class="theorem"><strong>Theorem 9  </strong></span>Convergence in probability implies convergence in distribution</p>
</div>
<div class="theorem">
<p><span id="thm:unlabeled-div-20" class="theorem"><strong>Theorem 10  </strong></span><strong>Central Limit Theorem</strong>
Let <span class="math inline">\(X_1, X_2, \dots\)</span> be a sequence of iid random variables with
<span class="math inline">\(\mathbb E[X_i] = \mu\)</span> and finite variance <span class="math inline">\(\mathbb V[X_i] = \sigma^2 &lt; \infty\)</span>.
Define <span class="math inline">\(\bar{X}_n = \frac1n \sum_{i=1}^n X_i\)</span>. Let <span class="math inline">\(G_n(x)\)</span> denote the
cdf of <span class="math inline">\(\sqrt{n}(\bar{X}_n - \mu)/\sigma\)</span>. Then <span class="math inline">\(\forall x \in \mathbb R\)</span>,
<span class="math display">\[\lim_{n\to\infty} G_n(x) = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^x e^{-y^2/2}dy.\]</span>
That is, <span class="math inline">\(\sqrt{n}(\bar{X}_n - \mu)/\sigma\)</span> converges in distribution to
the standard normal.</p>
</div>
<div class="theorem">
<p><span id="thm:unlabeled-div-21" class="theorem"><strong>Theorem 11  </strong></span><strong>Slutsky’s Theorem</strong>
If <span class="math inline">\(X_n \to X\)</span> in distribution and <span class="math inline">\(Y_n \to a\)</span> in probability with <span class="math inline">\(a\)</span>
constant, then</p>
<ol style="list-style-type: lower-alpha">
<li><p><span class="math inline">\(Y_nX_n \to aX\)</span> in distribution</p></li>
<li><p><span class="math inline">\(X_n + Y_n \to X + a\)</span> in distribution</p></li>
</ol>
</div>
<div class="remark">
<p><span id="unlabeled-div-22" class="remark"><em>Remark</em>. </span>This tells us, for instance, that
<span class="math display">\[\frac{\sqrt{n}(\bar{X}_n - \mu)}{S_n} \to \text{n}(0, 1)\]</span> in distribution,
since we know that <span class="math inline">\(S_n \to \sigma\)</span> in probability.</p>
</div>
</div>
<div id="the-delta-method" class="section level3">
<h3>The Delta Method</h3>
<p>If we are interested in the convergence of some function of a sequence
of RVs, rather than the RVs themselves, then we can use the Delta Method
(follows from an application of Taylor’s theorem and Slutsky’s theorem).</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-23" class="theorem"><strong>Theorem 12  </strong></span>Let <span class="math inline">\(Y_n\)</span> be a sequence of random variables that satisfies
<span class="math inline">\(\sqrt{n}(Y_n - \theta) \to \text{n}(0, \sigma^2)\)</span> in distribution. For a
given function <span class="math inline">\(g\)</span> and a specific value of <span class="math inline">\(\theta\)</span>, suppose that
<span class="math inline">\(g&#39;(\theta)\)</span> exists and is non-zero. Then
<span class="math display">\[\sqrt{n}[g(Y_n) - g(\theta)] \to \text{n}(0, \sigma^2g&#39;(\theta)^2)\]</span> in
distribution.</p>
</div>
<div class="remark">
<p><span id="unlabeled-div-24" class="remark"><em>Remark</em>. </span>There exists a corresponding multivariate result.</p>
</div>
<p>If <span class="math inline">\(g&#39;(\theta) = 0\)</span> then we take the next term in the Taylor series.</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-25" class="theorem"><strong>Theorem 13  </strong></span>Let <span class="math inline">\(Y_n\)</span> be a sequence of random variables that satisfies
<span class="math inline">\(\sqrt{n}(Y_n - \theta) \to \text{n}(0, \sigma^2)\)</span> in distribution. For a
given function <span class="math inline">\(g\)</span> and a specific value of <span class="math inline">\(\theta\)</span>, suppose that
<span class="math inline">\(g&#39;(\theta) = 0\)</span> and <span class="math inline">\(g&#39;&#39;(\theta)\)</span> exists and is non-zero. Then
<span class="math display">\[\sqrt{n}[g(Y_n) - g(\theta)] \to \text{n}(0, \sigma^2[g&#39;(\theta)]^2)\]</span> in
distribution.</p>
<p>Furthermore, as both <span class="math inline">\(\bar Y\)</span> and <span class="math inline">\(\sigma^2\)</span> are consistent estimators, we can again apply Slutsky’s Theorem to conclude that
<span class="math display">\[\sqrt{n}[g(Y_n) - g(\bar Y)] \to \text{n}(0, S^2[g&#39;(\bar Y)]^2)\]</span></p>
</div>
<p><strong>Proof</strong> The Taylor expansion of <span class="math inline">\(g(Y_n)\)</span> around <span class="math inline">\(Y_n = 0\)</span> is
<span class="math display">\[g(Y_n) = g(\theta) + g&#39;(\theta) (Y_n - \theta) + \text{Remainder},\]</span>
where the remainder <span class="math inline">\(\to 0\)</span> as <span class="math inline">\(Y_n \to \theta\)</span>. Since <span class="math inline">\(Y_n \to \theta\)</span> in probability it follows that the remainder <span class="math inline">\(\to 0\)</span> in probability. By applying <strong>Slutsky’s Theorem</strong> to
<span class="math display">\[\sqrt{n}[g(Y_n) - g(\theta)] = g&#39;(\theta)\sqrt{n}(Y_n-\theta),\]</span>
the result now follows.</p>
</div>
<div id="the-second-order-delta-method" class="section level3">
<h3>The second-order Delta Method</h3>
<p>If <span class="math inline">\(g&#39;(\theta) = 0\)</span>, we take one more term in the Taylor expansion to get
<span class="math display">\[g(Y_n) = g(\theta) + g&#39;(\theta)(Y_n -\theta) +\frac{g&#39;&#39;(\theta)}{2} (Y_n -\theta)^2 + \text{Remainder}.\]</span>
If we do some rearranging (setting <span class="math inline">\(g&#39; = 0\)</span>), we have
<span class="math display">\[ g(Y_n) -g(\theta) = \frac{g&#39;&#39;(\theta)}{2} (Y_n -\theta)^2 + \text{Remainder}.\]</span>
Now recall that the square of a <span class="math inline">\(n(0, 1)\)</span> is a <span class="math inline">\(\chi_1^2\)</span> , which implies that
<span class="math display">\[\frac{n(Y_n-\theta)^2}{\sigma^2}\to\chi_1^2\]</span>
in distribution. Therefore,</p>
<p>(<strong>Second-order Delta Method</strong>) Let <span class="math inline">\(Y_n\)</span> be a sequence of random variables that satisfies <span class="math inline">\(\sqrt{n}(Y_n -\theta) \to n(0, \sigma^2)\)</span> in distribution. For a given function <span class="math inline">\(g\)</span>
and a specific value of <span class="math inline">\(\theta\)</span>, suppose that <span class="math inline">\(g&#39;(\theta)=0\)</span> and <span class="math inline">\(g&#39;&#39;(\theta)\)</span> exists and is not <span class="math inline">\(0\)</span>. Then
<span class="math display">\[n[g(Y_n) -g(\theta)]\approx n\frac{g&#39;&#39;(\theta)}{2} (Y_n -\theta)^2\to \sigma^2\frac{g&#39;&#39;(\theta)}{2}\chi_1^2\]</span> in distribution.</p>
</div>
<div id="multivariate-delta-method" class="section level3">
<h3>Multivariate Delta Method</h3>
<p>Note that we must deal with multiple random variables although the ultimate CLT is a univariate one. Suppose the vector-valued random variable <span class="math inline">\(\mathbf X=(X_1,\cdots, X_p)\)</span> has mean <span class="math inline">\(\boldsymbol\mu = (\mu_1, \cdots , \mu_p)\)</span> and covariances <span class="math inline">\(Cov(X_i, X_j) = \sigma_{ij}\)</span>, and we observe an independent
random sample <span class="math inline">\(\mathbf X_1, \cdots, \mathbf X_n\)</span> and calculate the means <span class="math display">\[\bar X_i = \frac{1}{n}\sum_{k=1}^{n}X_{ik},\quad i=1,\cdots,p\]</span>
For a function <span class="math inline">\(g(\mathbf x) = g(x_1, \cdots, x_p)\)</span> we can write
<span class="math display">\[g(\bar x_1, \cdots, \bar x_p) = g(\mu_1, \cdots, \mu_p) + \sum_{k=1}^{p}g&#39;_k(\mathbf x)(\bar x_k - \mu_k),\]</span>
and we then have the following theorem:</p>
<p><strong>Multivariate Delta Method</strong> Let <span class="math inline">\(\mathbf X_1, \cdots, \mathbf X_n\)</span> be a random sample with <span class="math inline">\(E(X_{ij})=\mu_i\)</span> and <span class="math inline">\(Cov(X_{ik}, X_{jk}) = \sigma_{ij}\)</span> . For a given function <span class="math inline">\(g\)</span> with continuous first partial derivatives and a specific value of <span class="math inline">\(\boldsymbol\mu = (\mu_1, \cdots , \mu_p)\)</span> for which
<span class="math display">\[\tau^2=\sum\sum\sigma_{ij}\frac{\partial g(\boldsymbol\mu)}{\partial{\mu_i}}\frac{\partial g(\boldsymbol\mu)}{\partial{\mu_j}}&gt;0,\]</span>
<span class="math display">\[\sqrt{n}\big[g(\bar x_1,\cdots,\bar x_p)-g(\mu_1,\cdots,\mu_p)\big]\to n(0,\tau^2)\;\;\text {in distribution}\]</span></p>
</div>
</div>
<div id="generating-a-random-sample" class="section level2">
<h2>Generating A Random Sample</h2>
<div id="direct-method" class="section level3">
<h3>Direct Method</h3>
<p><strong>Probability integral transformation</strong> Let <span class="math inline">\(X\)</span> have continuous
cdf <span class="math inline">\(F_X(x)\)</span> and define the random variable <span class="math inline">\(Y\)</span> as <span class="math inline">\(Y = F_X (X)\)</span>. Then <span class="math inline">\(Y\)</span> is uniformly distributed on <span class="math inline">\((0, 1)\)</span>, that is, <span class="math inline">\(P(Y \leq y) = y, 0 &lt; y &lt; 1\)</span>.</p>
<p><strong>Proof</strong> For <span class="math inline">\(Y = F_X (X)\)</span> we have, for <span class="math inline">\(0 &lt; y &lt; 1\)</span>,
<span class="math display">\[\begin{align} P(Y\leq y)&amp;=P(F_X (X)\leq y)\\
&amp;=P(F_X^{-1} [F_X (X)]\leq F_X^{-1}(y))\quad (F_X^{-1} \text{ is increasing})\\
&amp;=P(X\leq F_X^{-1}(y))\\
&amp;=F_X(F_X^{-1}(y))\\
&amp;=y
\end{align}\]</span>
At the endpoints we have <span class="math inline">\(P (Y \leq y) = 1\)</span> for <span class="math inline">\(y \ge 1\)</span> and <span class="math inline">\(P (Y \leq y)=0\)</span> for <span class="math inline">\(y \leq 0\)</span>, showing that <span class="math inline">\(Y\)</span> has a uniform distribution.</p>
<p>Let <span class="math inline">\(X\)</span> have cdf <span class="math inline">\(F_X(x)\)</span>, let <span class="math inline">\(Y=g(X)\)</span>, and let <span class="math inline">\(\mathcal X\)</span> and <span class="math inline">\(\mathcal Y\)</span> be defined as <span class="math inline">\(\mathcal X \{x: f_X(x) &gt; 0\}\)</span> and <span class="math inline">\(\mathcal Y = \{y: y = g(x)\},\text{ for some }x \in \mathcal X\}\)</span>.</p>
<ul>
<li><ol style="list-style-type: lower-alpha">
<li>If <span class="math inline">\(g\)</span> is an increasing function on <span class="math inline">\(\mathcal X\)</span>, <span class="math display">\[F_Y (y) =\int_{x\in \mathcal X:x\leq g^{-1}(y)}f_X(x)dx=\int_{-\infty}^{g^{-1}(y)}f_X(x)dx= F_X (g^{-1}(y)) \text{ for }y \in \mathcal Y\]</span></li>
</ol></li>
<li><ol start="2" style="list-style-type: lower-alpha">
<li>If <span class="math inline">\(g\)</span> is a decreasing function on <span class="math inline">\(\mathcal X\)</span> and <span class="math inline">\(X\)</span> is a continuous random variable, <span class="math display">\[F_Y (y)=\int_{g^{-1}(y)}^{\infty}f_X(x)dx=1 - F_X(g^{-1} (y)) \text{ for } y \in \mathcal Y\]</span></li>
</ol></li>
</ul>
<div class="definition">
<p><span id="def:unlabeled-div-26" class="definition"><strong>Definition 7  </strong></span>A <strong>Direct Method</strong> of generating a random sample uses the probability
integral transform to map draws from a <span class="math inline">\(\text{uniform}(0, 1)\)</span> random
variable to draws from the distribution of interest.<br />
The Probability Integral Transform states that if <span class="math inline">\(X\)</span> has continuous cdf
<span class="math inline">\(F_X(x)\)</span> then <span class="math display">\[F_X(X) \sim \text{uniform}(0, 1).\]</span></p>
</div>
</div>
<div id="indirect-methods" class="section level3">
<h3>Indirect Methods</h3>
</div>
<div id="the-acceptreject-algorithm" class="section level3">
<h3>The Accept/Reject Algorithm</h3>
<div class="definition">
<p><span id="def:unlabeled-div-27" class="definition"><strong>Definition 8  </strong></span>Let <span class="math inline">\(Y \sim f_Y(y)\)</span> and <span class="math inline">\(V \sim f_V(v)\)</span> where <span class="math inline">\(f_Y\)</span> and <span class="math inline">\(f_V\)</span> have
common support with <span class="math display">\[M = \sup_y f_Y(y) / f_V(y) &lt; \infty.\]</span> To generate
a random variable <span class="math inline">\(Y \sim f_Y\)</span>:</p>
<ol style="list-style-type: lower-alpha">
<li><p>Generate <span class="math inline">\(U \sim \text{uniform}(0, 1)\)</span>, <span class="math inline">\(V \sim f_V\)</span> independent.</p></li>
<li><p>If <span class="math inline">\(U &lt; \frac 1M f_Y(V)/f_V(V)\)</span>, return <span class="math inline">\(V\)</span> as a sample of <span class="math inline">\(Y\)</span>;
otherwise go back to (a.).</p></li>
</ol>
</div>
<ul>
<li><p>It is typical to call <span class="math inline">\(V\)</span> the <em>candidate density</em> and <span class="math inline">\(Y\)</span> the
<em>target density</em>.</p></li>
<li><p>One would normally try to choose a candidate density with heavier
tails than the target density (e.g. Cauchy and normal) to ensure
that the tails of the target are well represented. If the target has
heavy tails, however, it can be hard to find a candidate that
results in finite <span class="math inline">\(M\)</span>. In this case people turn to MCMC methods.</p></li>
<li><p>Note that <span class="math inline">\(\text{P }(\texttt{terminate}) = 1/M\)</span>. The number of trials to
generate one sample of <span class="math inline">\(Y\)</span> is therefore <span class="math inline">\(\text{geometric}(1/M)\)</span>,
with <span class="math inline">\(M\)</span> the expected number of trials.</p></li>
<li><p>The intuition behind this algorithm is that if we consider placing
the density of a random variable <span class="math inline">\(Y\)</span> in a box (2d for simplicity)
with coordinates <span class="math inline">\((v,u)\)</span>, we express the cdf of <span class="math inline">\(Y\)</span> using
<span class="math inline">\(V, U \sim \text{uniform}(0, 1)\)</span>
<span class="math display">\[\text{P }(Y \leq y) = \text{P }(V \leq y \vert{} U \leq \frac1c f_Y(V))\]</span>
where <span class="math inline">\(c = \sup_y f_Y(y)\)</span>. In the actual algorithm we take
<span class="math inline">\(U \sim \text{uniform}(0,1)\)</span> and <span class="math inline">\(V\)</span> to be an RV that has common
support with <span class="math inline">\(Y\)</span>.</p></li>
</ul>
</div>
</div>
</div>
<div id="principles-of-data-reduction" class="section level1">
<h1>6. Principles of Data Reduction</h1>
<p>In this chapter, we explore how we can use functions of a sample
<span class="math inline">\(\mathbf{X}\)</span> to make inferences about an unknown parameter (of the
distribution of the sample) <span class="math inline">\(\theta\)</span>.</p>
<p>[Statistic] A statistic is any function of the data.</p>
<p>A statistic <span class="math inline">\(T(X)\)</span> forms a partition of the sample space <span class="math inline">\(\mathcal X\)</span> according to
its image,
<span class="math inline">\(\mathcal{T} = \{t: \exists \mathbf{x} \in \mathcal X{} \text{  so that  } t = T(\mathbf{x})\}\)</span>.
In this way a statistic provides a method of data reduction. An
experimenter who observes only <span class="math inline">\(T\)</span> will treat as equal two samples
<span class="math inline">\(\mathbf{x}, \mathbf{y}\)</span> for which <span class="math inline">\(T(\mathbf{x}) = T(\mathbf{y})\)</span>.</p>
<div id="the-sufficiency-principle" class="section level3">
<h3>The Sufficiency Principle</h3>
<p><strong>Sufficient Statistic</strong> A statistic <span class="math inline">\(T(\mathbf{X})\)</span> is a <strong>sufficient
statistic</strong> for <span class="math inline">\(\theta\)</span> if the conditional distribution of the sample
<span class="math inline">\(\mathbf{X}\)</span> given <span class="math inline">\(T(\mathbf{X})\)</span> does not depend on <span class="math inline">\(\theta\)</span>.</p>
<p>We ignore the fact that all points have <span class="math inline">\(0\)</span> probability for continuous
distributions.</p>
<p><strong>The Sufficiency Principle</strong> If <span class="math inline">\(T(\mathbf{X})\)</span> is a sufficient statistic
for <span class="math inline">\(\theta\)</span>, then any inference about <span class="math inline">\(\theta\)</span> should depend on the
sample <span class="math inline">\(\mathbf{X}\)</span> only through <span class="math inline">\(T(\mathbf{X})\)</span>.</p>
<p>If <span class="math inline">\(p(\mathbf{x}\vert{}\theta)\)</span> is the pmf/pdf of the sample <span class="math inline">\(\mathbf{X}\)</span> and
<span class="math inline">\(q(t\vert{} \theta)\)</span> is the pmf/pdf of <span class="math inline">\(T(\mathbf{X})\)</span>, then <span class="math inline">\(T(\mathbf{X})\)</span>
is a sufficient statistic for <span class="math inline">\(\theta\)</span> if <span class="math inline">\(\forall \mathbf{x} \in \mathcal X{}\)</span>,
<span class="math display">\[f(\mathbf{x}\vert{}\theta) / q(T(\mathbf{X})\lvert\theta)\]</span> is constant as a function
of <span class="math inline">\(\theta\)</span>.</p>
<p><strong>Niceness of the exponential family</strong> It turns out that outside of the
exponential family, it is rare to have a sufficient statistic that is of
smaller dimension than the size of the sample.</p>
<div id="theorem1" class="section level4">
<h4>Factorization Theorem</h4>
<p>Let <span class="math inline">\(f(\mathbf{x} \vert{} \theta)\)</span> denote the
pmf/pdf of a sample <span class="math inline">\(\mathbf{X}\)</span>. A statistic <span class="math inline">\(T(\mathbf{X})\)</span> is a sufficient
statistic for <span class="math inline">\(\theta\)</span> if and only if there exists functions
<span class="math inline">\(g(T(\mathbf{X})\vert{} \theta)\)</span> and <span class="math inline">\(h(\mathbf{x})\)</span> such that
<span class="math inline">\(\forall \mathbf{x} \in \mathcal X{}\)</span>, <span class="math inline">\(\forall \theta \in \Theta\)</span>
<span class="math display">\[\begin{equation}
f(\mathbf{x} \vert{} \theta) = g(T(\mathbf{x}) \vert{} \theta)h(\mathbf{x}).
\end{equation}\]</span></p>
<p>This theorem shows that the identity is a sufficient statistic. It is
straightforward to show from this that any bijection of a sufficient
statistic is a sufficient statistic.</p>
<p><strong>Proof</strong> Suppose <span class="math inline">\(T(\mathbf X)\)</span> is a sufficient statistic. Choose <span class="math inline">\(g(t\lvert \theta) = P_{\theta}(T(\mathbf X) = t)\)</span> and <span class="math inline">\(h(\mathbf x) =P\big(\mathbf X = \mathbf x\lvert T(\mathbf X) = T(\mathbf x)\big)\)</span>. Because <span class="math inline">\(T(\mathbf X)\)</span> is sufficient, the conditional probability
defining <span class="math inline">\(h(\mathbf x)\)</span> does not depend on <span class="math inline">\(\theta\)</span>. Thus this choice of <span class="math inline">\(h(\mathbf x)\)</span> and <span class="math inline">\(g(t\lvert \theta)\)</span> is legitimate,
and for this choice we have
<span class="math display">\[\begin{align}
f(\mathbf x\lvert \theta) &amp;= P_{\theta}(\mathbf X = \mathbf x)\\
&amp;= P_{\theta}\bigg(\mathbf X = \mathbf x \text{ and } T(\mathbf X) = T(\mathbf x)\bigg)\\
&amp;= P_{\theta}(T(\mathbf X) = T(\mathbf x)) P(\mathbf X=\mathbf x\lvert T(\mathbf X) = T(\mathbf x))\quad \text{(sufficiency)}\\
&amp;= g(T(\mathbf x)\lvert \theta)h(\mathbf x).
\end{align}\]</span></p>
<p>So factorization has been exhibited. We also see from the last two lines above that
<span class="math display">\[P_{\theta}\big(T(\mathbf X) = T(\mathbf x)\big) =g(T(\mathbf x)\lvert \theta),\]</span> so <span class="math inline">\(g(T(\mathbf x)\lvert \theta)\)</span> is the pmf of <span class="math inline">\(T(\mathbf X).\)</span></p>
<p>Now assume the factorization exists. Let <span class="math inline">\(q(t\lvert \theta)\)</span> be the pmf of <span class="math inline">\(T(\mathbf X)\)</span>. To show that <span class="math inline">\(T(\mathbf X)\)</span> is sufficient we examine the ratio <span class="math inline">\(f(\mathbf x\lvert \theta)/q(T(\mathbf x)\lvert \theta)\)</span>. Define <span class="math inline">\(A_{T(\mathbf x)} =\{\mathbf y: T(\mathbf y) = T(\mathbf x)\}\)</span>. Then
<span class="math display">\[\begin{align}
\frac{f(\mathbf x\lvert \theta)}{q(T(\mathbf x)\lvert \theta)}&amp;=\frac{g(T(\mathbf x)\lvert \theta)h(\mathbf x)}{q(T(\mathbf x)\lvert \theta)}\\
&amp;=\frac{g(T(\mathbf x)\lvert \theta)h(\mathbf x)}{\sum_{A_{T(\mathbf x)}}g(T(\mathbf y)\lvert \theta)h(\mathbf y)}\quad \text{(definition of the pmf of T)}\\
&amp;=\frac{g(T(\mathbf x)\lvert \theta)h(\mathbf x)}{g(T(\mathbf y)\lvert \theta)\sum_{A_{T(\mathbf x)}}h(\mathbf y)}\quad \text{(since T is constant on} \;A_{T(\mathbf x)})\\
&amp;=\frac{g(T(\mathbf x)\lvert \theta)h(\mathbf x)}{g(T(\mathbf x)\lvert \theta)\sum_{A_{T(\mathbf x)}}h(\mathbf y)}\\
&amp;=\frac{h(\mathbf x)}{\sum_{A_{T(\mathbf x)}}h(\mathbf y)}\\
\end{align}\]</span></p>
<p>Since the ratio does not depend on <span class="math inline">\(\theta\)</span>, <span class="math inline">\(T(\mathbf X)\)</span> is a sufficient statistic for <span class="math inline">\(\theta\)</span>.</p>
<p>Let <span class="math inline">\(X_1, \dots, X_n\)</span> be iid observations from a pmf/pdf
<span class="math inline">\(f(x \vert{} \boldsymbol\theta)\)</span> from an exponential family
<span class="math display">\[f(x|\boldsymbol{\theta}) = h(x)c(\boldsymbol{\theta})\exp\left( \sum_{i=1}^k w_i(\boldsymbol{\theta}) t_i(x) \right),\]</span>
where <span class="math inline">\(\boldsymbol{\theta} = (\theta_1, \dots, \theta_d)\)</span>, <span class="math inline">\(d \leq k\)</span>. Then
<span class="math inline">\(T(\mathbf{X})\)</span> defined by
<span class="math display">\[T(\mathbf{X}) = \left(\sum_{j=1}^n t_1(\mathbf{X}_j),\cdots,\sum_{j=1}^n t_k(\mathbf{X}_j)\right)\]</span> is a sufficient statistic for <span class="math inline">\(\boldsymbol{\theta}\)</span>.</p>
<p><strong>Minimal sufficient statistic</strong> A sufficient statistic <span class="math inline">\(T(\mathbf{X})\)</span> is called a <strong>minimal sufficient statistic</strong> if, for any other sufficient statistic <span class="math inline">\(T&#39;(\mathbf{X})\)</span>, <span class="math inline">\(T(\mathbf{x})\)</span> is a function of <span class="math inline">\(T&#39;(\mathbf{x})\)</span>.</p>
<p>By ‘function of’ we mean that if <span class="math inline">\(T&#39;(\mathbf{x}) = T&#39;(\mathbf{y})\)</span> then
<span class="math inline">\(T(\mathbf{x}) = T(\mathbf{y})\)</span>, <span class="math inline">\(T\)</span> varies with respect to <span class="math inline">\(X\)</span> only as it varies with <span class="math inline">\(T&#39;\)</span>. This means that each tile of the partition <span class="math inline">\(\{B_{t&#39;}:t&#39;\in \mathcal T&#39;\}\)</span> of the sample space according to the image of <span class="math inline">\(T&#39;\)</span> is a subset of some tile in the partition <span class="math inline">\(\{A_{t}:t\in \mathcal T\}\)</span> according to <span class="math inline">\(T\)</span>. This means that minimal sufficient statistics provide the <em>coarsest</em> possible tiling of the sample space and thus are the sufficient statistics that provide the greatest data reduction.</p>
<p><strong>Theorem</strong> Let <span class="math inline">\(f(\mathbf{x} \vert{} \theta)\)</span> be the pmf/pdf of a sample <span class="math inline">\(\mathbf{X}\)</span>.
Suppose there exists a function <span class="math inline">\(T(\mathbf{X})\)</span> such that
<span class="math inline">\(\forall \mathbf{x}, \mathbf{y} \in \mathcal X{}\)</span> the ratio
<span class="math inline">\(f(\mathbf{x}\vert{}\theta)/f(\mathbf{y}\vert{}\theta)\)</span> is constant as a function of <span class="math inline">\(\theta\)</span> if an only if <span class="math inline">\(T(\mathbf{x}) = T(\mathbf{y})\)</span>. Then <span class="math inline">\(T(\mathbf{X})\)</span> is a minimal sufficient statistic for <span class="math inline">\(\theta\)</span>.</p>
<p><strong>Proof</strong> To simplify the proof, we assume <span class="math inline">\(f(\mathbf x\lvert \theta) &gt; 0\)</span> for all <span class="math inline">\(\mathbf x \in \mathcal X\)</span> and <span class="math inline">\(\theta\)</span>.</p>
<p>First we show that <span class="math inline">\(T(\mathbf X)\)</span> is a sufficient statistic. Let <span class="math inline">\(\mathcal T = \{t : t=T(\mathbf x)\; \text{for some }\; \mathbf x \in \mathcal X\}\)</span> be the image of <span class="math inline">\(\mathcal X\)</span> under <span class="math inline">\(T(\mathbf x)\)</span>. Define the partition sets induced by <span class="math inline">\(T(\mathbf x)\)</span> as <span class="math inline">\(A_t = \{\mathbf x: T(\mathbf x) = t\}\)</span>. For each <span class="math inline">\(A_t\)</span>, choose and fix one element <span class="math inline">\(\mathbf x_t \in A_t\)</span>. For any <span class="math inline">\(\mathbf x \in \mathcal X\)</span>,
<span class="math inline">\(\mathbf x_{T(\mathbf x)}\)</span> is the fixed element that is in the same set, <span class="math inline">\(A_t\)</span> , as <span class="math inline">\(\mathbf x\)</span>. Since <span class="math inline">\(\mathbf x\)</span> and <span class="math inline">\(\mathbf x_{T(\mathbf x)}\)</span> are in the same set <span class="math inline">\(A_t\)</span> , <span class="math inline">\(T(\mathbf x) = T(\mathbf x_{T(\mathbf x)})\)</span> and, hence, <span class="math inline">\(f(\mathbf x\lvert\theta)/ f(\mathbf x_{T(\mathbf x)}\lvert\theta)\)</span> is constant as a
function of <span class="math inline">\(\theta\)</span>. Thus, we can define a function on <span class="math inline">\(\mathcal X\)</span> by <span class="math inline">\(h(\mathbf x) = f(\mathbf x\lvert\theta)/ f(\mathbf x_{T(\mathbf x)}\lvert\theta)\)</span> and <span class="math inline">\(h\)</span> does not depend on <span class="math inline">\(\theta\)</span>. Define a function on <span class="math inline">\(\mathcal T\)</span> by <span class="math inline">\(g(t\lvert\theta) = f(\mathbf x_t\lvert\theta)\)</span>. Then it can be seen that
<span class="math display">\[f(\mathbf x_t\lvert\theta)=\frac{f(\mathbf x_{T(\mathbf x)}\lvert\theta)f(\mathbf x\lvert\theta)}{f(\mathbf x_{T(\mathbf x)}\lvert\theta)} = g(T(\mathbf x)\lvert\theta)h(\mathbf x)\]</span><br />
and, by the Factorization Theorem, <span class="math inline">\(T(\mathbf X)\)</span> is a sufficient statistic for <span class="math inline">\(\theta\)</span>.</p>
<p>Now to show that <span class="math inline">\(T(\mathbf X)\)</span> is minimal, let <span class="math inline">\(T&#39;(\mathbf X)\)</span> be any other sufficient statistic. By the Factorization Theorem, there exist functions <span class="math inline">\(g&#39;\)</span> and <span class="math inline">\(h&#39;\)</span> such that <span class="math inline">\(f(\mathbf x\lvert\theta) =g&#39;(T&#39;(\mathbf x)\lvert\theta)h&#39;(\mathbf x)\)</span>. Let <span class="math inline">\(\mathbf x\)</span> and <span class="math inline">\(\mathbf y\)</span> be any two sample points with <span class="math inline">\(T&#39;(\mathbf x) = T&#39;(\mathbf y)\)</span>. Then <span class="math display">\[\frac{f(\mathbf x\lvert\theta)}{f(\mathbf y\lvert\theta)}=\frac{g&#39;(T&#39;(\mathbf x)\lvert\theta)h&#39;(\mathbf x)}{g&#39;(T&#39;(\mathbf y)\lvert\theta)h&#39;(\mathbf y)}=\frac{h&#39;(\mathbf x)}{h&#39;(\mathbf y)}\]</span> Since this ratio does not depend on <span class="math inline">\(\theta\)</span>, the assumptions of the theorem imply that <span class="math inline">\(T(\mathbf x) = T(\mathbf y)\)</span>. Thus, <span class="math inline">\(T(\mathbf x)\)</span> is a function of <span class="math inline">\(T&#39;(\mathbf x)\)</span> and <span class="math inline">\(T(\mathbf x)\)</span> is minimal.</p>
<p><strong>Necessary Statistic</strong> A statistic is <em>necessary</em> if it can be written as
a function of every sufficient statistic.</p>
<p>A statistic is minimal sufficient if and only if it is a necessary and
sufficient statistic.</p>
</div>
</div>
<div id="ancillary-statistic" class="section level3">
<h3>Ancillary Statistic</h3>
<p><strong>Ancillary Statistic</strong> A statistic <span class="math inline">\(S(\mathbf{X})\)</span> whose distribution does not depend on the parameter <span class="math inline">\(\theta\)</span> is called an <strong>ancillary statistic</strong>.</p>
<p><strong>First Order Ancillary</strong> A statistic <span class="math inline">\(V(\mathbf{X})\)</span> is <strong>first order ancillary</strong> if <span class="math inline">\(\mathbb E[V(\mathbf{X})]\)</span> is independent of <span class="math inline">\(\theta\)</span>.</p>
</div>
<div id="complete-statistic" class="section level3">
<h3>Complete Statistic</h3>
<p><strong>Complete Statistic</strong> Let <span class="math inline">\(f(t \vert{} \theta)\)</span> be a family of pdfs or
pmfs for a statistic <span class="math inline">\(T(\mathbf{X})\)</span>. The family of probability
distributions is called <strong>complete</strong> if for every (measurable) function
<span class="math inline">\(g\)</span>
<span class="math display">\[\mathbb E_{\theta}[g(T)] = 0  \,\, \forall \theta \implies P_{\theta}(g(T) = 0) = 1 \,\, \forall \theta.\]</span>
Equivalently, <span class="math inline">\(T(\mathbf{X})\)</span> is called a <strong>complete statistic</strong>.</p>
<p>(It is left unsaid that the function <span class="math inline">\(g\)</span> must be independent of
<span class="math inline">\(\theta\)</span>.)</p>
<p><strong>Basu’s Theorem</strong> If <span class="math inline">\(T(\mathbf{X})\)</span> is a complete and minimal sufficient
statistic, then <span class="math inline">\(T(\mathbf{X})\)</span> is independent of every ancillary statistic.</p>
<p><strong>Proof</strong></p>
<ul>
<li>For discrete distributions:</li>
</ul>
<p>Let <span class="math inline">\(S(\mathbf X)\)</span> be any ancillary statistic. Then <span class="math inline">\(P(S(\mathbf X) = s)\)</span> does not depend on <span class="math inline">\(\theta\)</span> since <span class="math inline">\(S(\mathbf X)\)</span> is ancillary. Also the conditional probability,
<span class="math display">\[P\big(S(\mathbf X) = s\lvert T(\mathbf X)=t\big) = P\big(\mathbf X \in \{\mathbf x: S(\mathbf x) = s\}\lvert T(\mathbf X)=t\big),\]</span>
does not depend on <span class="math inline">\(\theta\)</span> because <span class="math inline">\(T(\mathbf X)\)</span> is a sufficient statistic (the definition!). Thus, to show that <span class="math inline">\(S(\mathbf X)\)</span> and <span class="math inline">\(T(\mathbf X)\)</span> are independent, it suffices to show that
<span class="math display">\[P(S(\mathbf X) = s\lvert T(\mathbf X) = t) = P(S(\mathbf X) = s)\]</span>
for all possible values <span class="math inline">\(t \in \mathcal T\)</span>. Now,
<span class="math display">\[P(S(\mathbf X)=s)= \sum_{t\in\mathcal T}^{} P\big(S(\mathbf X) =s\lvert T(\mathbf X) = t\big)P_{\theta}\big(T(\mathbf X)= t\big).\]</span>
Furthermore, since <span class="math inline">\(\sum_{t\in\mathcal T}P_{\theta}(T(\mathbf X) = t) = 1\)</span>, we can write
<span class="math display">\[P(S(\mathbf X) = s) = \sum_{t \in \mathcal T}^{} P(S(\mathbf X)=s)P_{\theta}(T(\mathbf X)=t).\]</span></p>
<p>Therefore, if we define the statistic
<span class="math display">\[g(t) = P\big(S(\mathbf X) = s\lvert T(\mathbf X) = t\big) - P\big(S(\mathbf X)=s\big),\]</span>
the above two equations show that
<span class="math display">\[\mathbb E_{\theta}g(T) = \sum_{t \in \mathcal T} g(t)P_{\theta}(T(\mathbf X) = t) = 0\quad  \forall  \theta.\]</span>
Since <span class="math inline">\(T(\mathbf X)\)</span> is a complete statistic, this implies that <span class="math inline">\(g(t)=0\)</span> for all possible values <span class="math inline">\(t\in\mathcal T\)</span>. Hence <span class="math display">\[P(S(\mathbf X) = s\lvert T(\mathbf X) = t) = P(S(\mathbf X) = s)\]</span> is verified.</p>
<ul>
<li>For continuous distributions:</li>
</ul>
<p>Let <span class="math inline">\(T(\mathbf X)\)</span> be a complete sufficient statistic. Let <span class="math inline">\(S(\mathbf X)\)</span> be an ancillary statistic. Let <span class="math inline">\(A\)</span> be an event in the sample space.</p>
<p>Basu’s theorem states that <span class="math inline">\(S\)</span> and <span class="math inline">\(T\)</span> are independent. We need to show:</p>
<p><span class="math display">\[\begin{align}
P( S \in A  |  T=t )&amp;=P(S \in A)\\
&amp;=\mathbb{E}[I(S \in A)]\\
&amp;=\mathbb{E}_{\theta}\mathbb{E}[I(S \in A)|T=t]\\
\end{align}\]</span></p>
<p>Define the function <span class="math inline">\(g(t)\)</span> by <span class="math display">\[g(t)=\mathbb E[I(V∈A)|T=t]\]</span>
then we conclude <span class="math display">\[\mathbb{E}_{\theta}[g(t)-P(S \in A)]=\mathbb{E}_{\theta}[\mathbb E[I(V∈A)|T=t]-P(S \in A)]=0\]</span> for all <span class="math inline">\(\theta\)</span> in the sample space.</p>
<p>This is using the definition of a <a href="https://www.wikiwand.com/en/Completeness_(statistics)#/Definition">complete statistic</a>:</p>
<blockquote>
<p>If you have a function <span class="math inline">\(h(T)\)</span> that</p>
<ul>
<li>does not depend on the parameter <span class="math inline">\(\theta\)</span> directly, but only on <span class="math inline">\(T\)</span> (as it is written, “<span class="math inline">\(h(T)\)</span>,” and</li>
<li>for which <span class="math inline">\(\mathbb E_{\theta}(h(T)) = 0\)</span> for whatever <span class="math inline">\(\theta\)</span> you pick,</li>
</ul>
</blockquote>
<blockquote>
<p>then <span class="math inline">\(h(t)\)</span> is itself zero almost everywhere (or:
<span class="math inline">\(P_{\theta}(h(T) = 0) = 1\)</span>), again for any value of <span class="math inline">\(\theta\)</span>.</p>
</blockquote>
<p><span class="math inline">\(T\)</span> is complete, and the function <span class="math inline">\(h\)</span> of <span class="math inline">\(T\)</span> is <span class="math display">\[h(T) = g(T) − P(S \in A) = g(T) - c\]</span>. We need that <span class="math inline">\(S\)</span> is ancillary because else, <span class="math inline">\(h\)</span> would not be purely a function of <span class="math inline">\(T\)</span>, but some <span class="math inline">\(h(\theta, T)\)</span>.</p>
<p>Since <span class="math display">\[\mathbb E_{\theta}(I_{V \in A}) = \mathbb E_{\theta}[g(T)],\]</span> so <span class="math inline">\(\mathbb E_{\theta}[h(T)]\)</span> is zero, so the second point is fulfilled, too, so the conclusion follows.</p>
<p>Thus <span class="math display">\[\mathbb{E}_{\theta}[I(S \in A)|T=t]=P(S \in A)|T=t)=P(S \in A)\]</span></p>
<p><strong>Complete statistics in the exponential family</strong> Let <span class="math inline">\(X_1, \dots, X_n\)</span> be
iid observations from a pmf/pdf <span class="math inline">\(f(x \vert{} \theta)\)</span> from an
exponential family
<span class="math display">\[f(x|\mathbf{\theta}) = h(x)c(\mathbf{\theta})\exp\left( \sum_{i=1}^k w_i(\mathbf{\theta}) t_i(x) \right),\]</span>
where <span class="math inline">\(\mathbf{\theta} = (\theta_1, \dots, \theta_d)\)</span>, <span class="math inline">\(d \leq k\)</span>. Then
<span class="math inline">\(\mathbf{T}(\mathbf{X})\)</span> defined by
<span class="math display">\[T_i(\mathbf{X}) = \sum_{j=1}^k t_i(\mathbf{X}_j)\]</span> is complete if
<span class="math inline">\(\{(w_1(\mathbf{\theta}), \dots, w_n(\mathbf{\theta}))\}\)</span> contains an open set
in <span class="math inline">\(\mathbb R^k\)</span>.</p>
<p>The open set criteria excludes curved exponential families.</p>
<p><strong>If a minimal sufficient statistic exists, then every complete statistic is minimal sufficient.</strong></p>
</div>
<div id="the-likelihood-principle" class="section level3">
<h3>The Likelihood Principle</h3>
<p><strong>Likelihood</strong> Let <span class="math inline">\(f(\mathbf{x}\vert{} \theta)\)</span> denote the pmf/pdf of the sample <span class="math inline">\(\mathbf{X}\)</span>. Then the <em>likelihood function</em>, given an observation
<span class="math inline">\(\mathbf{X} = \mathbf{x}\)</span>, is
<span class="math display">\[L(\theta \vert{} \mathbf{x}) = f(\mathbf{x} \vert{} \theta)\]</span> as a function
of <span class="math inline">\(\theta\)</span>.</p>
<p><strong>Likelihood Principle</strong> If <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(\mathbf{y}\)</span> are two sample points
such that <span class="math inline">\(L(\theta \vert{} \mathbf{x})\)</span> is proportional to
<span class="math inline">\(L(\theta \vert{} \mathbf{y})\)</span>, that is, there exists a constant
<span class="math inline">\(C(\mathbf{x}, \mathbf{y})\)</span>, which does not depend on <span class="math inline">\(\theta\)</span>, such that
<span class="math display">\[L(\theta \vert{} \mathbf{x}) = C(\mathbf{x}, \mathbf{y})L(\theta \vert{} \mathbf{y}) \quad \forall \theta,\]</span>
then the conclusions drawn from <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(\mathbf{y}\)</span> should be identical.</p>
</div>
<div id="a-slightly-more-formal-construction" class="section level3">
<h3>A Slightly More Formal Construction</h3>
<p>We define an <strong>experiment</strong> <span class="math inline">\(E\)</span> to be a triple
<span class="math inline">\((\mathbf{X}, \theta, f(\mathbf{x}\vert{}\theta))\)</span>, where <span class="math inline">\(\mathbf{X}\)</span> is a random vector with pmf/pdf <span class="math inline">\(f(\mathbf{x}\vert{}\theta)\)</span> for some <span class="math inline">\(\theta\)</span> in the parameter space <span class="math inline">\(\Theta\)</span>. As an experimenter, knowing what experiment <span class="math inline">\(E\)</span> was performed and having observed a particular sample <span class="math inline">\(\mathbf{X}=\mathbf{x}\)</span>, will make some inference or draw some conclusion about <span class="math inline">\(\theta\)</span>. This conclusion we denote by <span class="math inline">\(E_v(E,\mathbf{x})\)</span>, which stands for the <strong>evidence about <span class="math inline">\(\theta\)</span> arising from <span class="math inline">\(E\)</span> and <span class="math inline">\(\mathbf{x}\)</span>.</strong></p>
<p><strong>Formal Sufficiency Principle</strong> Consider an experiment
<span class="math inline">\(E = (\mathbf{X}, \theta, f(\mathbf{x}\vert{}\theta))\)</span> and suppose that
<span class="math inline">\(T(\mathbf{X})\)</span> is a sufficient statistic for <span class="math inline">\(\theta\)</span>. If <span class="math inline">\(\mathbf{x}\)</span> and
<span class="math inline">\(\mathbf{y}\)</span> are sample points satisfying <span class="math inline">\(T(\mathbf{x}) = T(\mathbf{y})\)</span>, then
<span class="math inline">\(\text{Ev}(E, \mathbf{x}) = \text{Ev}(E, \mathbf{y})\)</span>.</p>
<p><strong>Conditionality Principle</strong> Suppose that
<span class="math inline">\(E_1 = \{X_1, \theta, f_1(x_1\vert{} \theta)\}\)</span> and
<span class="math inline">\(E_2 = \{X_2, \theta, f_2(x_2\vert{} \theta)\}\)</span> are two experiments,
where only the unknown parameter <span class="math inline">\(\theta\)</span> need be common between the two
experiments. Consider the mixed experiment in which the random variable <span class="math inline">\(J\)</span> is observed, where <span class="math inline">\(P(J = 1) = P(J = 2) = \frac12\)</span> (independent of <span class="math inline">\(\mathbf{X}_1, \mathbf{X}_2, \theta\)</span>), and then the experiment <span class="math inline">\(E_J\)</span> is performed. Formally, the experiment performed is <span class="math inline">\(E^* = (\mathbf{X}^*, \theta, f(\mathbf{x}^* \vert{} \theta))\)</span>, where
<span class="math inline">\(\mathbf{X}^* = (j, \mathbf{X}_j)\)</span> and
<span class="math display">\[f^*(\mathbf{x}^* \vert{} \theta) = f^*((j, \mathbf{x}_j)\vert{} \theta) = \frac12 f_j(\mathbf{x}_j \vert{}\theta).\]</span>
Then <span class="math display">\[\text{Ev}(E^*, (j, \mathbf{x}_j)) = \text{Ev}(E_j, \mathbf{x_j}).\]</span>
That is, information about <span class="math inline">\(\theta\)</span> depends only on the experiment run (not on the fact that the particular experiment was chosen).</p>
<p><strong>Formal Likelihood Principle</strong> Suppose that we have two experiments, <span class="math inline">\(E_1 = (\mathbf{X}_1, \theta, f_1(\mathbf{x}_1 \vert{} \theta))\)</span>
and <span class="math inline">\(E_2 = (\mathbf{X}_2, \theta, f_2(\mathbf{x}_2 \vert{} \theta))\)</span> where the
unknown parameter <span class="math inline">\(\theta\)</span> is the same in both experiments. Suppose that
<span class="math inline">\(\mathbf{x}_1^*\)</span> and <span class="math inline">\(\mathbf{x}_2^*\)</span> are sample points from <span class="math inline">\(E_1\)</span> and <span class="math inline">\(E_2\)</span>
respectively, such that
<span class="math display">\[L(\theta \vert{} \mathbf{x}_2^*) = CL(\theta \vert{} \mathbf{x}_1^*)\]</span> for
all <span class="math inline">\(\theta\)</span> and for some constant <span class="math inline">\(C(\mathbf{x}_1^*, \mathbf{x}_2^*)\)</span> that is
independent of <span class="math inline">\(\theta\)</span>. Then
<span class="math display">\[\text{Ev}(E_1, \mathbf{x}_1^*) = \text{Ev}(E_2, \mathbf{x}_2^*).\]</span></p>
<p>Note that this is more general from the other likelihood principle since
it concerns two experiments (that we can of course set to be equal).</p>
<p><strong>Likelihood Principle Corollary</strong> If
<span class="math inline">\(E = (\mathbf{X}, \theta, f(\mathbf{x} \vert{} \theta))\)</span> is an experiment then
<span class="math inline">\(\text{Ev}(E, \mathbf{x})\)</span> should depend on <span class="math inline">\(E\)</span> and <span class="math inline">\(\mathbf{x}\)</span> only through
<span class="math inline">\(L(\theta \vert{} \mathbf{x})\)</span>.</p>
<p><strong>Birnbaum’s Theorem</strong> The Formal Likelihood Principle follows from the
Formal Sufficiency Principle and the Conditionality Principle. The
converse is also true.</p>
<p>Many common statistical procedures violate the Formal Likelihood
Principle – the topic of the applicability of these principles is not
settled. For instance, checking the residuals of a model (to grade the
model) violates the Sufficiency Principle. These notions are model
dependent, so may not be applicable until <em>after</em> we have decided on a
model.</p>
</div>
<div id="the-equivariance-principle" class="section level3">
<h3>The Equivariance Principle</h3>
<p><strong>Measurement Equivariance</strong> Inferences should not depend on the measurement scale used.</p>
<p><strong>Formal Invariance</strong> If two inference problems have the same formal structure, in terms of the mathematical model used, then the same inference procedure should be used, regardless of the physical realisation.</p>
<p><strong>Equivariance Principle</strong> If <span class="math inline">\(\mathbf{Y} = g(\mathbf{X})\)</span> is a change of
measurement scale such that the model <span class="math inline">\(\mathbf{Y}\)</span> has the same formal structure as the model <span class="math inline">\(\mathbf{X}\)</span>, then an inference procedure should be both measurement equivariant and formally equivariant.</p>
<p>A set of functions <span class="math inline">\(\{g(\mathbf{x}) : g \in \mathcal G\}\)</span> from the sample space <span class="math inline">\(\mathcal X\)</span> onto
<span class="math inline">\(\mathcal X\)</span> is called a <strong>group of transformations</strong> of <span class="math inline">\(\mathcal X\)</span> if:</p>
<ul>
<li><p>(i). (<strong>Inverse</strong>) For every <span class="math inline">\(g \in \mathcal G\)</span> there is a <span class="math inline">\(g&#39; \in \mathcal G\)</span> such that <span class="math inline">\(g&#39;(g(\mathbf{x})) = \mathbf{x}\)</span> for all <span class="math inline">\(\mathbf{x} \in \mathcal X\)</span>.</p></li>
<li><p>(ii). (<strong>Composition</strong>) For every <span class="math inline">\(g \in \mathcal G\)</span> and <span class="math inline">\(g&#39; \in \mathcal G\)</span> there exists <span class="math inline">\(g&#39;&#39; \in \mathcal G\)</span> such that <span class="math inline">\(g&#39;(g(\mathbf{x})) = g&#39;&#39;(\mathbf{x})\)</span> for all <span class="math inline">\(\mathbf x \in \mathcal X\)</span>.</p></li>
<li><p>(iii). (<strong>Identity</strong>) The identity, <span class="math inline">\(e(\mathbf x)\)</span>, defined by <span class="math inline">\(e(\mathbf x) =\mathbf x\)</span> is an element of <span class="math inline">\(\mathcal G\)</span>, which is a consequence of (i) and (ii)
and need not be verified separately.</p></li>
</ul>
<p>Let <span class="math inline">\(\mathcal{F} = \{f(\mathbf{x} \vert{} \theta): \theta \in \Theta\}\)</span> be
a set of pdfs or pmfs for <span class="math inline">\(\mathbf{X}\)</span>, and let <span class="math inline">\(\mathcal{G}\)</span> be group of
transformations on the sample space <span class="math inline">\(\mathcal X{}\)</span>. Then <span class="math inline">\(\mathcal{F}\)</span> is
<strong>invariant under the group</strong> <span class="math inline">\(\mathcal{G}\)</span> if for every <span class="math inline">\(\theta \in \Theta\)</span> and <span class="math inline">\(g \in \mathcal{G}\)</span> there exists a unique <span class="math inline">\(\theta&#39; \in \Theta\)</span> such that <span class="math inline">\(\mathbf{Y} = g(\mathbf{X})\)</span> has the
distribution <span class="math inline">\(f(\mathbf{y} \vert{} \theta&#39;)\)</span> if <span class="math inline">\(\mathbf{X}\)</span> has the
distribution <span class="math inline">\(f(\mathbf{x} \vert{} \theta)\)</span>.</p>
</div>
</div>
<div id="point-estimation" class="section level1">
<h1>7. Point Estimation</h1>
<p><strong>Point Estimator</strong> A <em>point estimator</em> is any function <span class="math inline">\(W(X_1, \dots, X_n)\)</span> of a sample; that is, any statistic is a point estimator. An <em>estimate</em> is a realised value <span class="math inline">\(w(x_1, \dots, x_n)\)</span>.</p>
<p>Note that there is the implicit restriction that the estimator is not a function of the parameter you are trying to estimate.</p>
<div id="methods-of-finding-estimators" class="section level2">
<h2>Methods of Finding Estimators</h2>
<div id="method-of-moments" class="section level3">
<h3>Method of Moments</h3>
<p>The method of moments is performed using a random sample <span class="math inline">\(X_1, \dots, X_n\)</span> from a population with unknown parameters <span class="math inline">\(\theta_1, \dots, \theta_k\)</span> by computing the first <span class="math inline">\(k\)</span> empirical moments
<span class="math inline">\(m_k = \frac{1}{n}\sum_{i=1}^nX_i^k\)</span> and matching them with the corresponding moments of the population <span class="math inline">\(\mu_k&#39; = \mathbb E[X^t]\)</span>.</p>
<p><span class="math display">\[\begin{aligned}
    m_1 &amp;= \mu_1&#39;(\theta_1, \dots, \theta_k)\\
    m_2 &amp;= \mu_2&#39;(\theta_1, \dots, \theta_k)\\
        &amp;\vdots  \\
    m_k &amp;= \mu_k&#39; (\theta_1, \dots, \theta_k)\\
\end{aligned}\]</span></p>
<p>From this you get <span class="math inline">\(k\)</span> simultaneous equations that you can use to solve for the parameters of the population.</p>
</div>
<div id="maximum-likelihood-estimators" class="section level3">
<h3>Maximum Likelihood Estimators</h3>
<p><strong>Maximum Likelihood Estimator</strong> For each sample point <span class="math inline">\(\mathbf x\)</span>, let <span class="math inline">\(\hat{\boldsymbol\theta}(\mathbf x)\)</span> be a parameter value at which the likelihood function <span class="math display">\[L(\boldsymbol\theta \vert{} \mathbf x)=\prod_{i=1}^{n}f(x_i\lvert \theta_1,\cdots,\theta_k)\]</span> attains its maximum as a function of <span class="math inline">\(\boldsymbol\theta\)</span>, with <span class="math inline">\(\mathbf x\)</span> held fixed. A <strong>maximum likelihood estimator (MLE)</strong> of the parameter <span class="math inline">\(\boldsymbol\theta\)</span> based on a sample <span class="math inline">\(\mathbf X\)</span> is <span class="math inline">\(\hat{\boldsymbol\theta}(\mathbf X)\)</span>.</p>
<p>Suppose we want to find the MLE for some function of the parameter
<span class="math inline">\(\tau(\theta)\)</span>.</p>
<p><strong>Induced Likelihood</strong> Given some function of the parameter
<span class="math inline">\(\tau(\theta)\)</span>, we define the <strong>induced likelihood function</strong> <span class="math inline">\(L^*\)</span> by
<span class="math display">\[L^*(\eta \vert{} \mathbf x) = \sup_{\theta : \tau(\theta) = \eta} L(\theta \vert{} \mathbf x).\]</span>
The value <span class="math inline">\(\hat{\eta}\)</span> that maximises <span class="math inline">\(L^*(\eta \vert{} \mathbf x)\)</span> will
be called the MLE of <span class="math inline">\(\eta = \tau(\theta)\)</span>. It can be seen that the
maxima of <span class="math inline">\(L^*\)</span> and <span class="math inline">\(L\)</span> coincide.</p>
<p><strong>Invariance Property of MLEs</strong> IF <span class="math inline">\(\hat{\theta}\)</span> is the MLE of <span class="math inline">\(\theta\)</span>, then for any function <span class="math inline">\(\tau(\theta)\)</span>, the MLE of <span class="math inline">\(\tau(\theta)\)</span> is <span class="math inline">\(\tau(\hat{\theta})\)</span>.</p>
<ol style="list-style-type: decimal">
<li><p>The MLE can be an unstable function of the data.</p></li>
<li><p>When verifying maxima for multi-dimensional problems try to avoid going down the hessian route, which can be tedious.</p></li>
</ol>
<p>To use two-variate calculus to verify that a function <span class="math inline">\(H(\theta_1,\theta_2)\)</span> has a local maximum at <span class="math inline">\((\hat\theta_1,\hat\theta_2)\)</span>, it must be shown that the following three conditions hold.</p>
<ul>
<li><ol style="list-style-type: lower-alpha">
<li>The first-order partial derivatives are <span class="math inline">\(0\)</span>, <span class="math display">\[\frac{\partial}{\partial\theta_1}H(\theta_1,\theta_2)|_{\theta_1=\hat\theta_1,\theta_2=\hat\theta_2}=0\]</span> and <span class="math display">\[\frac{\partial}{\partial\theta_2}H(\theta_1,\theta_2)|_{\theta_1=\hat\theta_1,\theta_2=\hat\theta_2}=0\]</span></li>
</ol></li>
<li><ol start="2" style="list-style-type: lower-alpha">
<li>At least one second-order partial derivative is negative, <span class="math display">\[\frac{\partial^2}{\partial\theta_1^2}H(\theta_1,\theta_2)|_{\theta_1=\hat\theta_1,\theta_2=\hat\theta_2}&lt;0\]</span> or <span class="math display">\[\frac{\partial^2}{\partial\theta_2^2}H(\theta_1,\theta_2)|_{\theta_1=\hat\theta_1,\theta_2=\hat\theta_2}&lt;0\]</span></li>
</ol></li>
<li><ol start="3" style="list-style-type: lower-alpha">
<li>The Jacobian of the second-order partial derivatives is positive, <span class="math display">\[\begin{vmatrix}
\frac{\partial^2}{\partial\theta_1^2}H(\theta_1,\theta_2)&amp;\frac{\partial^2}{\partial\theta_1\partial\theta_2}H(\theta_1,\theta_2)\\
\frac{\partial^2}{\partial\theta_1\partial\theta_2}H(\theta_1,\theta_2)&amp;\frac{\partial^2}{\partial\theta_2^2}H(\theta_1,\theta_2)
\end{vmatrix}_{\theta_1=\hat\theta_1,\theta_2=\hat\theta_2}\\
=\frac{\partial^2}{\partial\theta_1^2}H(\theta_1,\theta_2)\frac{\partial^2}{\partial\theta_2^2}H(\theta_1,\theta_2)-\left(\frac{\partial^2}{\partial\theta_1\partial\theta_2}H(\theta_1,\theta_2)\right)^2 &gt;0\]</span><br />
Since <span class="math display">\[f(\theta) \approx f(\theta_0) + \nabla f(\theta_0)^T (\theta - \theta_0) + \frac12 (\theta - \theta_0)^T \nabla^2 f(\theta_0) (\theta - \theta_0).\]</span> If <span class="math inline">\(\theta_0\)</span> is a local extremum for <span class="math inline">\(f\)</span>, then <span class="math inline">\(\nabla f(\theta_0) = 0\)</span>, so if <span class="math inline">\(\nabla^2 f(\theta_0)\)</span> is negative definite, then <span class="math inline">\((\theta - \theta_0)^T \nabla^2 f(\theta_0) (\theta - \theta_0) \leq 0\)</span> for all <span class="math inline">\(\theta\)</span>, so <span class="math inline">\(\theta_0\)</span> is a local maximum. Then the eigenvalues of <span class="math inline">\(\nabla^2 f(\theta_0)\)</span> should be both negative and the determinant should be positive.</li>
</ol></li>
</ul>
</div>
<div id="bayes-estimators" class="section level3">
<h3>Bayes Estimators</h3>
<p>If we denote the prior distribution by <span class="math inline">\(\pi(\theta)\)</span> and the sampling
distribution by <span class="math inline">\(f(\mathbf x \vert{} \theta)\)</span>, then the posterior
distribution of <span class="math inline">\(\theta\)</span> given the sample <span class="math inline">\(\mathbf x\)</span> is
<span class="math display">\[\pi(\theta \vert{} \mathbf x) = \frac{f(\mathbf x\vert{}\theta)\pi(\theta)}{m(\mathbf x)},\]</span>
where <span class="math inline">\(m(\mathbf x)\)</span> is the marginal distribution of the sample <span class="math inline">\(\mathbf x\)</span>
<span class="math display">\[m(\mathbf x) = \int f(\mathbf x\vert{}\theta) \pi(\theta) d\theta.\]</span></p>
<p><strong>Conjugate Priors</strong> Let <span class="math inline">\(\mathcal{F}\)</span> denote the class of pdfs/pmfs
<span class="math inline">\(f(\mathbf x \vert{} \theta)\)</span> (indexed by <span class="math inline">\(\theta\)</span>). A class <span class="math inline">\(\Pi\)</span> of
prior distribution is a <strong>conjugate family</strong> for <span class="math inline">\(\mathcal{F}\)</span> if,
<span class="math inline">\(\forall f \in \mathcal{F}\)</span>, <span class="math inline">\(\forall \, \text{priors} \in \mathcal{F}\)</span>
and <span class="math inline">\(\forall \mathbf x \in \mathcal X\)</span>, the posterior distribution is in <span class="math inline">\(\Pi\)</span>.</p>
<p>Note that this relation is not said to be symmetric.</p>
<p>[Some Examples]</p>
<ul>
<li><p>Normal is self-conjugate as a family.</p></li>
<li><p>Beta distribution is conjugate to binomial.</p></li>
<li><p>Gamma distribution is conjugate to Poisson.<br />
Let <span class="math inline">\(X_1,\cdots, X_n\)</span> be iid <span class="math inline">\(Poisson(\lambda)\)</span>, and let <span class="math inline">\(\lambda\)</span> have a <span class="math inline">\(gamma(\alpha,\beta)\)</span> distribution, the conjugate family for the Poisson.</p></li>
</ul>
<p>For <span class="math inline">\(n\)</span> observations, <span class="math inline">\(Y =\sum_i X_i\sim Poisson(n\lambda)\)</span>.
- a. The marginal pmf of <span class="math inline">\(Y\)</span> is <span class="math display">\[\begin{align}
m(y)&amp;=\int_{-\infty}^{\infty}f(Y|\lambda)f(\lambda)d\lambda\\
&amp;=\int_{-\infty}^{\infty}\frac{(n\lambda)^ye^{-n\lambda}}{y!}\frac{1}{\Gamma(\alpha)\beta^{\alpha}}\lambda^{\alpha-1}e^{-\lambda/\beta}d\lambda\\
&amp;=\frac{n^y}{y!\Gamma(\alpha)\beta^{\alpha}}\int_{-\infty}^{\infty}\lambda^{(y+\alpha)-1}e^{-n\lambda-\lambda/\beta}d\lambda\\
&amp;=\frac{n^y}{y!\Gamma(\alpha)\beta^{\alpha}}\int_{-\infty}^{\infty}\lambda^{(y+\alpha)-1}e^{-\frac{\lambda}{\beta/(n\beta+1)}}d\lambda\\
&amp;=\frac{n^y}{y!\Gamma(\alpha)\beta^{\alpha}}\Gamma(y+\alpha)\left(\frac{\beta}{n\beta+1}\right)^{y+\alpha}\\
\end{align}\]</span>
Thus <span class="math display">\[\pi(\lambda|y)=\frac{f(y|\lambda)\pi(\lambda)}{m(y)}=\frac{\lambda^{(y+\alpha)-1}e^{-\frac{\lambda}{\beta/(n\beta+1)}}}{\Gamma(y+\alpha)\left(\frac{\beta}{n\beta+1}\right)^{y+\alpha}}\\
\sim gamma\left(y+\alpha, \frac{\beta}{n\beta+1}\right)\]</span></p>
</div>
<div id="the-em-expectation-maximization-algorithm" class="section level3">
<h3>The EM (Expectation-Maximization) Algorithm</h3>
<p>In general, we can move in either direction, from the complete-data problem to the <strong>incomplete-data</strong> problem or the reverse. If <span class="math inline">\(\mathbf Y = (Y_1, \cdots , Y_n)\)</span> are the incomplete data, and <span class="math inline">\(\mathbf X = (X_1, \cdots, X_m)\)</span> are the augmented data, making <span class="math inline">\((\mathbf Y, \mathbf X)\)</span> the <strong>complete data</strong>,
the densities <span class="math inline">\(g(\cdot\lvert \theta)\)</span> of <span class="math inline">\(\mathbf Y\)</span> and <span class="math inline">\(f(\cdot\lvert\theta)\)</span> of <span class="math inline">\((\mathbf Y, \mathbf X)\)</span> have the relationship <span class="math display">\[g(\mathbf y\lvert\theta) = \int f(\mathbf y, \mathbf x\lvert\theta) d\mathbf x\]</span>
with sums replacing integrals in the discrete case.<br />
If we turn these into likelihoods, <span class="math display">\[L(\theta\lvert\mathbf y) = g(\mathbf y\lvert\theta)\]</span> is the incomplete-data likelihood and <span class="math display">\[L(\theta\lvert\mathbf y, \mathbf x)=f(\mathbf y,\mathbf x\lvert\theta)\]</span> is the complete-data likelihood.</p>
</div>
</div>
<div id="methods-of-evaluating-estimators" class="section level2">
<h2>Methods of Evaluating Estimators</h2>
<div id="mean-squared-error" class="section level3">
<h3>Mean Squared Error</h3>
<p>The <strong>mean squared error</strong> (MSE) of an estimator <span class="math inline">\(W\)</span>
of a parameter <span class="math inline">\(\theta\)</span> is defined by <span class="math inline">\(\mathbb E_{\theta}[(W - \theta)^2]\)</span>.</p>
<p><strong>Bias-Variance Decomposition</strong>
<span class="math display">\[\mathbb E_{\theta}[(W - \theta)^2] = \mathbb V_{\theta} [W] + (\mathbb E_{\theta} [W] - \theta)^2 = \mathbb V_{\theta} [W] + (\text{Bias}_{\theta} [W])^2\]</span></p>
<p><strong>Bias</strong> The <em>bias</em> of a point estimator <span class="math inline">\(W\)</span> of a parameter <span class="math inline">\(\theta\)</span> is
given by <span class="math display">\[\text{Bias}_{\theta} [W] = \mathbb E_{\theta} [W] - \theta.\]</span> An estimator whose bias is <span class="math inline">\(0\)</span> is called an <strong>unbiased estimator</strong> and has <span class="math inline">\(\mathbb E_{\theta}[W] = \theta,\quad\forall \theta\)</span>.</p>
<ul>
<li><p>Clearly, if an estimator is unbiased, then its MSE is equal to its
variance.</p></li>
<li><p>The MSE makes sense for location parameters but not so much for
scale parameters, since it is symmetric and scale parameters have a
natural floor at <span class="math inline">\(0\)</span>.</p></li>
<li><p>The MSE may be a function of the thing you’re trying to estimate. So
which estimator you choose as being the ‘best’ may depend on the
range you expect the parameter to lie within.</p></li>
</ul>
<p>The MLE (biased) estimator of the variance of a normal distribution is
<span class="math display">\[\hat \sigma^2 = \frac{1}{N} \sum_{i=1}^{N}(X_i - \bar X)^2,\]</span> where <span class="math inline">\(\bar X\)</span> is the sample mean and <span class="math inline">\(X_i \sim^{iid} \mathcal{N}(\mu,\sigma^2)\)</span>.
Since <span class="math display">\[\frac{1}{\sigma ^2} \sum_{i=1}^{N}(X_i-\bar X)^2 \sim \chi^2_{N},\]</span> we have that
<span class="math display">\[\begin{align}
\mathbb V\big(\frac{N\hat \sigma^2}{\sigma^2}\big) = \mathbb V(\chi^2_{N-1}) = 2(N-1) \\
\implies \mathbb V(\hat \sigma ^2) = \frac{2(N-1) \sigma^4}{N^2}  
\end{align}
\]</span> The variance of this estimator is equal to <span class="math inline">\(\frac{2(N-1) \sigma^4}{N^2}\)</span>. The MSE is <span class="math display">\[\text{MSE}=\mathbb V(\hat \sigma ^2)+\text{Bias} =\frac{2\sigma ^4}{N}+ \mathbb E [\hat \sigma ^2] - \sigma ^2=\mathbb V(\hat \sigma ^2)+\text{Bias}^2 \\=\mathbb V(\hat \sigma ^2)+(\mathbb E(\hat \sigma ^2)-\sigma ^2)^2=\frac{2(N-1) \sigma^4}{N^2}+ \left(\frac{N-1}{N}\sigma ^2 - \sigma ^2\right)^2\\
=\frac{2N-1}{N^2}\sigma ^4.\]</span> where
<span class="math display">\[\begin{align}
\mathbb E(\hat \sigma ^2)&amp;=\mathbb E\left(\frac{1}{N}\sum_{i=1}^{N}(x_i-\bar{x})^2\right)\\
&amp;=\frac{1}{N}\mathbb E\left(\sum_{i=1}^{N}x_i^2-2\sum_{i=1}^{N}x_i\bar{x}+\sum_{i=1}^{N}\bar{x}^2\right)\\
&amp;=\frac{1}{N}\mathbb E\left(\sum_{i=1}^{N}x_i^2-N\bar{x}^2\right)\\
&amp;=\frac{1}{N}\mathbb E\left(\sum_{i=1}^{N}x_i^2\right)-\mathbb E\left(\bar{x}^2\right)\\
&amp;=\mathbb E\left(x^2\right)-\mathbb E\left(\bar{x}^2\right)\\
&amp;=(\sigma_x^2+\mu^2)-(\sigma_{\bar x}^2+\mu^2)\\
&amp;=\sigma_x^2-\sigma_{\bar x}^2\\
&amp;=\sigma_x^2-\mathbb V(\bar x)\\
&amp;=\sigma_x^2-\mathbb V\left(\frac{1}{N}\sum_{i=1}^{N}x_i\right)\\
&amp;=\sigma_x^2-\frac{1}{N^2}\mathbb V\left(\sum_{i=1}^{N}x_i\right)\\
&amp;=\sigma_x^2-\frac{1}{N^2}\sum_{i=1}^{N}\mathbb V\left(x\right)\\
&amp;=\sigma_x^2-\frac{1}{N}\mathbb V\left(x\right)\\
&amp;=\sigma_x^2-\frac{1}{N}\sigma_x^2\\
&amp;=\frac{N-1}{N}\sigma_x^2\\
\end{align}\]</span></p>
<p>A unbiased estimator of the variance of a normal distribution is
<span class="math display">\[S^2 = \frac{1}{N-1} \sum_{i=1}^{N}(X_i - \bar X)^2,\]</span> since <span class="math inline">\(\mathbb E S^2=\sigma^2\)</span>.
And since <span class="math display">\[\frac{(N-1) S^2}{\sigma^2} \sim \chi_{n-1}^2,\]</span>
<span class="math display">\[\begin{align}
\mathbb V\big(\frac{(N-1)S^2}{\sigma^2}\big) = \mathbb V(\chi^2_{N-1}) = 2(N-1) \\
\implies \mathbb V(S^2) = \frac{2(N-1) \sigma^4}{(N-1)^2}=\frac{2\sigma^4}{N-1}  
\end{align}
\]</span> The variance of this estimator is equal to <span class="math inline">\(\frac{2\sigma^4}{N-1}\)</span>, which is also the MSE.</p>
<p>The variance (or MSE) in the unbiased case <span class="math inline">\(\frac{2\sigma^4}{N-1}\)</span> is greater than the variance in the biased case <span class="math inline">\(\frac{2(N-1) \sigma^4}{N^2}\)</span>. The MSE of the unbiased estimator <span class="math inline">\(\frac{2\sigma^4}{N-1}\)</span> is larger than the MSE of the biased estimator <span class="math inline">\(\frac{2N-1}{N^2}\sigma ^4\)</span>.</p>
<p><span class="math display">\[
\begin{array}{llll}
\text{Estimator} &amp; \text{MLE} &amp; \text{best-unbiased} &amp; \text{Bayes}           \\
\hline
\text{Normal}(\mu) &amp; \bar X        &amp; \bar X  &amp; \frac{\tau^2}{\tau^2+\sigma^2}\bar X+\frac{\sigma^2}{\tau^2+\sigma^2}\mu       \\
\text{Normal}(\sigma^2) &amp; \frac{1}{n}\sum(X_i-\bar X)=\frac{n-1}{n}S^2           &amp; S^2=\frac{1}{n-1}\sum(X_i-\bar X) &amp; \frac{\sigma^2\tau^2}{\sigma^2+\tau^2}       \\
\text{Bernoulli}(p)          &amp; \bar X             &amp; &amp;         \\
\text{Bernoulli}(p(1-p))          &amp;              &amp;  &amp;         \\
\text{Binomial}(np)          &amp; \frac{\sum_{i=1}^n X_i}{n}            &amp; &amp; \frac{(\sum_{i=1}^n X_i) +\alpha}{\alpha+\beta+n}         \\
\text{Binomial}(np(1-p))          &amp;             &amp; &amp;         \\
\end{array}
\]</span></p>
</div>
<div id="best-unbiased-estimators" class="section level3">
<h3>Best Unbiased Estimators</h3>
<p>[Best Unbiased Estimator] An estimator <span class="math inline">\(W^*\)</span> is <em>best unbiased
estimator</em> of of <span class="math inline">\(\tau(\theta)\)</span> if it satisfies
<span class="math inline">\(\mathbb E[W^*] = \tau(\theta) \,\, \forall \theta\)</span>, and for any other
estimator with <span class="math inline">\(\mathbb E[W] = \tau(\theta) \,\, \forall \theta\)</span> we have
<span class="math inline">\(\mathbb V[W*] \leq \mathbb V[W] \,\, \forall \theta\)</span>. We also call <span class="math inline">\(W^*\)</span> the
<strong>uniform minimum variance unbiased estimator</strong> (UMVUE) of <span class="math inline">\(\tau(\theta)\)</span>.</p>
<p>Suppose that we are trying to estimate <span class="math inline">\(\theta\)</span> and consider the class
of estimators <span class="math display">\[\mathcal{C}_\tau = \{W: \mathbb E[W] = \tau(\theta)\}.\]</span> All
estimators in this class have the same bias, so we can compare their
MSEs by comparing their variances alone. (So the best estimator in this
class is just the minimum variance one.) This means that the
considerations of this chapter can be applied to classes like
<span class="math inline">\(\mathcal{C}_\tau\)</span>, even if <span class="math inline">\(\tau(\theta) \neq \theta\)</span>.</p>
<p>The best unbiased estimator, if it exists, could be hard to find. The
following lower bound at least gives a stopping criterion to our search.</p>
<p><strong>Cramér-Rao Inequality</strong> Let <span class="math inline">\(X_1, \dots, X_n\)</span> be a sample with pdf
<span class="math inline">\(f(\mathbf x\vert{}\theta)\)</span> and let <span class="math inline">\(W(\mathbf X) = W(X_1, \dots, X_n)\)</span> be
any estimator with finite variance satisfying
<span class="math display">\[\frac{d}{d \theta} \mathbb E_{\theta}[W(\mathbf X)] = \int_{\mathcal X{}} \frac{\partial}{\partial \theta} \left(W(\mathbf X) f(\mathbf x\vert{}\theta) \right)d\mathbf x.\]</span> and <span class="math display">\[\mathbb V_{\theta}[W(\mathbf X)]&lt;\infty\]</span>
Then
<span class="math display">\[\mathbb V_{\theta}[W(\mathbf X)] \geq \frac{\left(\frac{d}{d \theta} \mathbb E_{\theta}[W(\mathbf X)] \right)^2}{\mathbb E_{\theta}\left[ \left(\frac{\partial}{\partial \theta} \log f(\mathbf X\vert{} \theta) \right)^2 \right]}.\]</span></p>
<p>The proof of this considers the correlation between the gradient of the log likelihood and the statistic. Note that the sample in the above theorem is not necessarily iid.</p>
<p><strong>proof</strong></p>
<p><strong>Covariance inequality</strong> For any two random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, <span class="math display">\[(\text{Cov}[X, Y])^2 \leq \sigma_X^2 \sigma_Y^2=\mathbb V(X)\mathbb V(Y)\]</span> and <span class="math display">\[\mathbb V(X)\ge\frac{(\text{Cov}[X, Y])^2}{\mathbb V(Y)}\]</span> The cleverness in this theorem follows from choosing <span class="math inline">\(X\)</span> to be the estimator <span class="math inline">\(W(\mathbf X)\)</span> and <span class="math inline">\(Y\)</span> to be the quantity <span class="math inline">\(\frac{\partial}{\partial\theta} \log f(\mathbf X|\theta)\)</span> and applying the Cauchy-Schwarz IneqUality.
First note that <span class="math display">\[\begin{align}
\frac{d}{d\theta}\mathbb E_{\theta}W(\mathbf X)&amp;=\int_{\mathcal X}W(\mathbf X)\left[\frac{\partial}{\partial\theta}f(\mathbf X|\theta)\right]dx\\
&amp;=\int_{\mathcal X}W(\mathbf X)\frac{\frac{\partial}{\partial\theta}f(\mathbf X|\theta)}{f(\mathbf X|\theta)}f(\mathbf X|\theta)dx\\
&amp;=\mathbb E_{\theta}\left[W(\mathbf X)\frac{\frac{\partial}{\partial\theta}f(\mathbf X|\theta)}{f(\mathbf X|\theta)}\right]\\
&amp;=\mathbb E_{\theta}\left[W(\mathbf X)\frac{\partial}{\partial\theta}\log f(\mathbf X|\theta)\right]\\
\end{align}\]</span> which suggests a covariance between <span class="math inline">\(W(\mathbf X)\)</span> and <span class="math inline">\(\frac{\partial}{\partial\theta}\log f(\mathbf X|\theta)\)</span>. For it to be a covariance, we need to subtract the product of the expected values, so we calculate <span class="math inline">\(\mathbb E_{\theta}\left[\frac{\partial}{\partial\theta}\log f(\mathbf X|\theta)\right]\)</span>. If <span class="math inline">\(W(\mathbf X) = 1\)</span>, we have <span class="math display">\[\frac{d}{d\theta}\mathbb E_{\theta}W(\mathbf X)=\frac{d}{d\theta}\mathbb E_{\theta}[1]=0=\mathbb E_{\theta}\left[\frac{\partial}{\partial\theta}\log f(\mathbf X|\theta)\right]\]</span></p>
<p>Therefore <span class="math display">\[\begin{align}
\text{Cox}_{\theta}\left(W(\mathbf X), \frac{\partial}{\partial\theta}\log f(\mathbf X|\theta)\right)&amp;=\mathbb E_{\theta}\left[W(\mathbf X)\frac{\partial}{\partial\theta}\log f(\mathbf X|\theta)\right]+\mathbb E_{\theta}W(\mathbf X)\mathbb E_{\theta}\left[\frac{\partial}{\partial\theta}\log f(\mathbf X|\theta)\right]\\
&amp;=\mathbb E_{\theta}\left[W(\mathbf X)\frac{\partial}{\partial\theta}\log f(\mathbf X|\theta)\right]\\
&amp;=\frac{d}{d\theta}\mathbb E_{\theta}W(\mathbf X)
\end{align}\]</span> Also since <span class="math display">\[\mathbb E_{\theta}\left[\frac{\partial}{\partial\theta}\log f(\mathbf X|\theta)\right]=0\]</span> we have <span class="math display">\[\mathbb V_{\theta}\left(\frac{\partial}{\partial\theta}\log f(\mathbf X|\theta)\right)=\mathbb E_{\theta}\left(\left(\frac{\partial}{\partial\theta}\log f(\mathbf X|\theta)\right)^2\right)\]</span> Using the Cauchy-Schwarz Inequality we obtain <span class="math display">\[\mathbb V_{\theta}(W(\mathbf X))\ge \frac{\left(\frac{d}{d\theta}\mathbb E_{\theta}W(\mathbf X)\right)^2}{\mathbb E_{\theta}\left(\left(\frac{\partial}{\partial\theta}\log f(\mathbf X|\theta)\right)^2\right)}=\frac{\left(\frac{d}{d\theta}\mathbb E_{\theta}W(\mathbf X)\right)^2}{\text{(information number)}}\]</span></p>
<p><strong>Cramer-Rao Inequality, iid case</strong> If the assumptions of last Theorem are satisfied and, additionally, if <span class="math inline">\(X_1,\cdots ,X_n\)</span> are iid with pdf <span class="math inline">\(f(x|\theta)\)</span>, then
<span class="math display">\[\begin{align}
\mathbb E_{\theta}\left[ \left(\frac{\partial}{\partial \theta} \log f(\mathbf X \vert{} \theta) \right)^2 \right]&amp;=\mathbb E_{\theta}\left[ \left(\frac{\partial}{\partial \theta} \log \prod_{i=1}^{n}f(X_i \vert{} \theta) \right)^2 \right]
\end{align}\]</span></p>
<p><strong>proof</strong>
Since <span class="math inline">\(X_1, \dots, X_n\)</span> are independent,
<span class="math display">\[\begin{align}
\mathbb E_{\theta}\left[ \left(\frac{\partial}{\partial \theta} \log f(\mathbf X \vert{} \theta) \right)^2 \right]&amp;=\mathbb E_{\theta}\left[ \left(\frac{\partial}{\partial \theta} \log \prod_{i=1}^{n}f(X_i \vert{} \theta) \right)^2 \right]\\
&amp;=\mathbb E_{\theta}\left[ \left(\sum_{i=1}^{n}\frac{\partial}{\partial \theta} \log f(X_i \vert{} \theta) \right)^2 \right]\\
&amp;=\sum_{i=1}^{n}\mathbb E_{\theta}\left[ \left(\frac{\partial}{\partial \theta} \log f(X_i \vert{} \theta) \right)^2 \right]\\
&amp;+\sum_{i\ne j}^{}\mathbb E_{\theta}\left[ \left(\frac{\partial}{\partial \theta} \log f(X_i \vert{} \theta) \right) \left(\frac{\partial}{\partial \theta} \log f(X_j \vert{} \theta) \right)\right]\\
\end{align}\]</span> since <span class="math inline">\(X_1, \dots, X_n\)</span> are independent, for <span class="math inline">\(i\ne j\)</span> we have <span class="math display">\[\mathbb E_{\theta}\left[ \left(\frac{\partial}{\partial \theta} \log f(X_i \vert{} \theta) \right) \left(\frac{\partial}{\partial \theta} \log f(X_j \vert{} \theta) \right)\right]
=\mathbb E_{\theta}\left(\frac{\partial}{\partial \theta} \log f(X_i \vert{} \theta) \right) \mathbb E_{\theta}\left(\frac{\partial}{\partial \theta} \log f(X_j \vert{} \theta) \right)=0\]</span> So <span class="math display">\[\begin{align}
\mathbb E_{\theta}\left[ \left(\frac{\partial}{\partial \theta} \log f(\mathbf X \vert{} \theta) \right)^2 \right]=\sum_{i=1}^{n}\mathbb E_{\theta}\left[ \left(\frac{\partial}{\partial \theta} \log f(X_i \vert{} \theta) \right)^2 \right]=n\mathbb E_{\theta}\left[ \left(\frac{\partial}{\partial \theta} \log f(X \vert{} \theta) \right)^2 \right]\\
\end{align}\]</span></p>
</div>
<div id="fisher-information" class="section level3">
<h3>Fisher Information</h3>
<p><strong>Fisher Information</strong> The quantity
<span class="math display">\[\mathbb E_{\theta}\left[ \left(\frac{\partial}{\partial \theta} \log f(\mathbf X \vert{} \theta) \right)^2 \right]\]</span> is called the <strong>Fisher Information</strong> or <strong>Information number</strong> of the sample <span class="math inline">\(\mathbf X\)</span>. This terminology reflects the fact that the information number gives a bound on the variance of the best unbiased estimator of <span class="math inline">\(\theta\)</span>. As the
information number gets bigger and we have more information about <span class="math inline">\(\theta\)</span>, we have a smaller bound on the variance of the best unbiased estimator.</p>
<p>If <span class="math inline">\(f(x \vert{} \theta)\)</span> satisfies
<span class="math display">\[\frac{d}{d \theta} \mathbb E_{\theta}\left[ \frac{\partial}{\partial \theta} \log f(X \vert{} \theta) \right] = \int \frac{\partial}{\partial \theta} \left[ \left( \frac{\partial}{\partial \theta} \log f(x \vert{} \theta) \right) f(x \vert{} \theta) \right] dx\]</span>
(as is true for an exponential family), the partial derivative with respect to <span class="math inline">\(\theta\)</span> of the natural logarithm of the likelihood function is called the <strong>score</strong>. Under certain regularity conditions, if <span class="math inline">\(\theta\)</span> is the true parameter (i.e. <span class="math inline">\(X\)</span> is actually distributed as <span class="math inline">\(f(X; \theta)\)</span>), it can be shown that the expected value (the first moment) of the score, evaluated at the true parameter value <span class="math inline">\(\theta\)</span> , is <span class="math inline">\(0\)</span></p>
<p><span class="math display">\[\begin{align}
\mathbb E_{\theta}\left[\frac{\partial}{\partial \theta} \log f(X \vert{} \theta) \right]&amp;=\int_{\mathbb R}^{}\left[\frac{\partial}{\partial \theta} \log f(X \vert{} \theta) \right]f(X \vert{} \theta) dx\\
&amp;=\int_{\mathbb R}^{}\frac{\frac{\partial}{\partial \theta} f(X \vert{} \theta)}{f(X \vert{} \theta)} f(X \vert{} \theta) dx\\
&amp;=\frac{\partial}{\partial \theta}\int_{\mathbb R}^{} f(X \vert{} \theta) dx\\
&amp;=\frac{\partial}{\partial \theta}\cdot 1\\
&amp;=0\\
\end{align}\]</span> The variance of the score is defined to be the <strong>Fisher information</strong>:
<span class="math display">\[\mathcal I_{\theta}=\mathbb E_{\theta}\left[ \left(\frac{\partial}{\partial \theta} \log f(X \vert{} \theta) \right)^2 \right]=\int_{\mathbb R}\left(\frac{\partial}{\partial \theta} \log f(X \vert{} \theta) \right)^2f(X \vert{} \theta)dx\]</span>
Since
<span class="math display">\[\begin{align}
\frac{\partial^2}{\partial \theta^2} \log f(X \vert{} \theta)&amp;=\frac{\partial}{\partial \theta}\frac{\frac{\partial}{\partial \theta} f(X \vert{} \theta)}{f(X \vert{} \theta)}\\
&amp;=\frac{\left[\frac{\partial^2}{\partial \theta^2} f(X \vert{} \theta)\right]f(X \vert{} \theta)-\left[\frac{\partial}{\partial \theta} f(X \vert{} \theta)\right]^2}{(f(X \vert{} \theta))^2}\\
&amp;=\frac{\frac{\partial^2}{\partial \theta^2} f(X \vert{} \theta)}{f(X \vert{} \theta)}-\frac{\left[\frac{\partial}{\partial \theta} f(X \vert{} \theta)\right]^2}{(f(X \vert{} \theta))^2}\\
&amp;=\frac{\frac{\partial^2}{\partial \theta^2} f(X \vert{} \theta)}{f(X \vert{} \theta)}-\left(\frac{\partial}{\partial \theta} \log f(X \vert{} \theta)\right)^2\\
\end{align}\]</span> and <span class="math display">\[\mathbb E_{\theta}\left[\frac{\frac{\partial^2}{\partial \theta^2} f(X \vert{} \theta)}{f(X \vert{} \theta)}\right]=\frac{\partial^2}{\partial \theta^2}\int_{\mathbb R}f(X \vert{} \theta)dx=0\]</span>
then the Fisher information can
be written
<span class="math display">\[\mathbb E_{\theta}\left[ \left(\frac{\partial}{\partial \theta} \log f(X \vert{} \theta) \right)^2 \right] =\mathbb E_{\theta}\left[ \left(\frac{\frac{\partial}{\partial \theta} f(X \vert{} \theta)}{f(X \vert{} \theta)}\right)^2 \right]= - \mathbb E_{\theta}\left[ \frac{\partial^2}{\partial \theta^2} \log f(X \vert{} \theta) \right].\]</span></p>
<p>Even if the Cramér-Rao bound is applicable, it may not be sharp – there
may not be an estimator that attains this bound.</p>
<ul>
<li>For Bernoulli RVs, the Fisher information is
<span class="math display">\[\begin{align}
I_X(p)&amp;=E_p\left[\left(\frac{d}{dp}\log\left(p^X(1-p)^{1-X}\right)\right)^2\right]\\
&amp;=E_p\left[\left(\frac{d}{dp}\left[X\log p+(1-X)\log (1-p)\right]\right)^2\right]\\
&amp;=E_p\left[\left(\frac Xp-\frac{1-X}{1-p}\right)^2\right]\\
&amp;=E_p\left[\left(\frac Xp\right)^2-2\left(\frac Xp\frac{1-X}{1-p}\right)+\left(\frac{1-X}{1-p}\right)^2\right]\\
&amp;=E_p\left(\frac{X^2}{p^2}\right)-2E_p\left(\frac{X(1-X)}{p(1-p)}\right)+E_p\left(\frac{(1-X)^2}{(1-p)^2}\right)\\
&amp;=\frac{1}{p}-0+\frac{1}{(1-p)}\\
&amp;=\frac{1}{p(1-p)}\\
\end{align}\]</span></li>
</ul>
<p>Since
<span class="math display">\[E_p(X^2)=0^2\cdot p_X(0)+1^2\cdot p_X(1)=0^2(1-p)+1^2p=p,\]</span>
<span class="math display">\[E_p((1-X)^2)=1-p,\]</span>
- For Exponential RVs <span class="math inline">\(X \sim \exp(\beta)\)</span>, If the density is <span class="math display">\[f(X|\beta) = \frac1\beta e^{-\frac1\beta X}\]</span>. The Fisher information is
<span class="math display">\[\begin{align*}
\mathcal I_{\beta} &amp; = n\mathbb E_{\beta}\left( \left(\frac{\partial \log f(X| \beta)}{\partial \beta}\right)^2\right) \\
&amp; = n\int_0^\infty \left(\frac{\partial \log f(X|\beta)}{\partial \beta}\right)^2 \, f(X|\beta) \, dx \\
&amp;=n\int_0^\infty \left(\frac{\partial}{\partial \beta}\left(-\frac{1}{\beta}x+\log\frac{1}{\beta}\right)\right)^2 \, \frac{1}{\beta}e^{-\frac{1}{\beta}x} \, dx \\
&amp; = n\int_0^\infty \left(\frac{x}{\beta^2} -\frac{1}{\beta}\right)^2 \, \frac{1}{\beta}e^{-\frac{1}{\beta}x} \, dx \\
&amp; = n\int_0^\infty \left(\frac{x^2}{\beta^4} -2\frac{x}{\beta^3}+\frac{1}{\beta^2}\right) \, \frac{1}{\beta}e^{-\frac{1}{\beta}x} \, dx \\
&amp;=n\left(\frac{2}{\beta^2}-\frac{2}{\beta^2}+\frac{1}{\beta^2}\right)\\
&amp;=\frac{n}{\beta^2}\\
\end{align*}\]</span></p>
<ul>
<li>For Exponential RVs <span class="math inline">\(X \sim \exp(\lambda)\)</span>, If the density is <span class="math display">\[f(X|\lambda) = \lambda e^{-\lambda X}\]</span>. The Fisher information is
<span class="math display">\[\begin{align*}
\mathcal I_{\lambda} &amp; = n\mathbb E_{\lambda}\left( \left(\frac{\partial \log f(X| \lambda)}{\partial \lambda}\right)^2\right) \\
&amp; = n\int_0^\infty \left(\frac{\partial \log f(X|\lambda)}{\partial \lambda}\right)^2 \, f(X|\lambda) \, dx \\
&amp;=n\int_0^\infty \left(\frac{\partial}{\partial \lambda}\left(-\lambda x+\log\lambda\right)\right)^2 \, \lambda e^{-\lambda X}\, dx \\
&amp; = n\int_0^\infty \left(-x +\frac{1}{\lambda}\right)^2 \, \lambda e^{-\lambda X} \, dx \\
&amp; = n\int_0^\infty \left(x^2 -2\frac{x}{\lambda}+\frac{1}{\lambda^2}\right) \, \lambda e^{-\lambda X} \, dx \\
&amp;=n\left(\frac{2}{\lambda^2}-\frac{2}{\lambda^2}+\frac{1}{\lambda^2}\right)\\
&amp;=\frac{n}{\lambda^2}\\
\end{align*}\]</span></li>
</ul>
<p><span class="math display">\[
\begin{array}{llll}
 &amp; \text{Fisher Information} &amp; \text{Cramér-Rao lower bound}           \\
\hline
\text{Exponential}(\beta) &amp; \frac{n}{\beta^2}        &amp;    \frac{\beta^2}{n}   \\
\text{Bernoulli}(p)          &amp; \frac{1}{p(1-p)}             &amp;   p(1-p)       \\
\text{Normal}(\sigma^2)          &amp; \frac{n}{2\sigma^4}             &amp;   \frac{2\sigma^4}{n}       \\
\end{array}
\]</span></p>
<p>The Matrix form</p>
<p>When there are <span class="math inline">\(N\)</span> parameters, so that <span class="math inline">\(\theta\)</span> is an <span class="math inline">\(N \times 1\)</span> vector <span class="math display">\[\displaystyle\boldsymbol\theta =\begin{bmatrix}
\theta _{1}&amp;\theta _{2}&amp;\dots &amp;\theta _{N}
\end{bmatrix}^{T}\]</span> then the Fisher information takes the form of an <span class="math inline">\(N \times N\)</span> matrix. This matrix is called the <strong>Fisher information matrix</strong> (FIM) and has typical element <span class="math display">\[[\mathcal I(\theta)]_{i,j}=\mathbb E_{\theta}\left[\left(\frac{\partial}{\partial\theta_i}\log f(X|\theta)\right)\left(\frac{\partial}{\partial\theta_j}\log f(X|\theta)\right)\right]\]</span> The FIM is a <span class="math inline">\(N \times N\)</span> positive semidefinite matrix. If it is positive definite, then it defines a Riemannian metric on the <span class="math inline">\(N\)</span>-dimensional parameter space.</p>
<p>Under certain regularity conditions, the Fisher information matrix may also be written as <span class="math display">\[[\mathcal I(\theta)]_{i,j}=-\mathbb E_{\theta}\left[\frac{\partial}{\partial\theta_i}\frac{\partial}{\partial\theta_j}\log f(X|\theta)\right]\]</span></p>
<p>[Attainment] Let <span class="math inline">\(X_1, \dots, X_n\)</span> be iid <span class="math inline">\(X \sim f(x \vert{} \theta)\)</span>,
where <span class="math inline">\(f(x \vert{} \theta)\)</span> satisfies the conditions of the Cramér-Rao
Theorem. Let
<span class="math inline">\(L(\theta \vert{} \mathbf x) = \prod_{i=1}^n f(x_i \vert{} \theta)\)</span> denote
the likelihood function. If <span class="math inline">\(W(\mathbf X)=W(X_1,\cdots,X_n)\)</span> is any unbiased estimator of <span class="math inline">\(\tau(\theta)\)</span> then <span class="math inline">\(W(\mathbf X)\)</span> attains the Cramér-Rao lower bound if and only if exist <span class="math inline">\(a(\theta)\)</span> such that
<span class="math display">\[a(\theta) [W(\mathbf x) - \tau(\theta)] = \frac{\partial}{\partial \theta} \log L(\theta \vert{} \mathbf x).\]</span></p>
<p><strong>Proof:</strong> Recalling that <span class="math inline">\(\mathbb E_{\theta}W=\tau(\theta)\)</span>, <span class="math inline">\(\mathbb E_{\theta}\left(\frac{\partial}{\partial\theta}\log\prod_{i=1}^{n}f(X_i|\theta)\right)=0\)</span>, the Cramer-Rao Inequality, can be written as <span class="math display">\[\begin{align}
\left[\text{Cov}_{\theta}\left(W(\mathbf X),\frac{\partial}{\partial\theta}\log\prod_{i=1}^{n}f(X_i|\theta)\right)\right]^2&amp;=\left[\mathbb E_{\theta}\bigg(W(\mathbf X)-\tau(\theta)\bigg)\left(\frac{\partial}{\partial\theta}\log\prod_{i=1}^{n}f(X_i|\theta)-0\right)\right]^2\\
&amp;=\left[\mathbb E_{\theta}\left(W(\mathbf X)\frac{\partial}{\partial\theta}\log\prod_{i=1}^{n}f(X_i|\theta)\right)\right]^2\\
&amp;\le\mathbb V_{\theta}\left[W(\mathbf X)\right]\mathbb V_{\theta}\left[\frac{\partial}{\partial\theta}\log\prod_{i=1}^{n}f(X_i|\theta)\right]\end{align}\]</span> We can have equality if and only if <span class="math display">\[W(\mathbf X)-\tau(\theta)\]</span> is proportional to <span class="math inline">\(\frac{\partial}{\partial\theta}\log\prod_{i=1}^{n}f(X_i|\theta)\)</span>.</p>
</div>
<div id="sufficiency-and-unbiasedness" class="section level3">
<h3>Sufficiency and Unbiasedness</h3>
<p><strong>Rao-Blackwell</strong> Let <span class="math inline">\(W\)</span> be any unbiased estimator of <span class="math inline">\(\tau(\theta)\)</span> and
let <span class="math inline">\(T\)</span> be a sufficient statistic for <span class="math inline">\(\theta\)</span>. Define
<span class="math inline">\(\phi(T) = \mathbb E[W \vert{} T]\)</span>. Then <span class="math inline">\(\mathbb E_{\theta}[\phi(T)] = \tau(\theta)\)</span> and <span class="math inline">\(\mathbb V_{\theta}[\phi(T)] \leq \mathbb V_{\theta}[W] \,\, \forall \theta\)</span>. That is, <span class="math inline">\(\phi(T)\)</span> is a <em>uniformly better unbiased estimator</em> of <span class="math inline">\(\tau(\theta)\)</span>.</p>
<p><strong>Proof:</strong> Since <span class="math display">\[\mathbb E X=\mathbb E[\mathbb E(X|Y)]\]</span>
<span class="math display">\[\mathbb V X=\mathbb V[\mathbb E(X|Y)]+\mathbb E[\mathbb V (X|Y)]\]</span> then we have <span class="math display">\[\tau(\theta)=\mathbb E_{\theta}W=\mathbb E_{\theta}[\mathbb E(W|T)]=\mathbb E_{\theta}\phi(T)\]</span> so <span class="math inline">\(\phi(T)\)</span> is unbiased for <span class="math inline">\(\tau(\theta)\)</span>. Also
<span class="math display">\[\begin{align}
\mathbb V_{\theta}W&amp;=\mathbb V_{\theta}[\mathbb E(W|T)]+\mathbb E_{\theta}[\mathbb V (W|T)]\\
&amp;=\mathbb V_{\theta}[\phi(T)]+\mathbb E_{\theta}[\mathbb V (W|T)]\\
&amp;\ge\mathbb V_{\theta}[\phi(T)]
\end{align}\]</span> Hence <span class="math inline">\(\phi(T)\)</span> is uniformly better than <span class="math inline">\(W\)</span>, and it only remains to show that <span class="math inline">\(\phi(T)\)</span> is indeed an estimator. That is, we must show that <span class="math inline">\(\phi(T) = \mathbb E[W \vert{} T]\)</span> is a function of only the sample and, in particular, is independent of <span class="math inline">\(\theta\)</span>. But it follows from the definition of sufficiency, and the fact that <span class="math inline">\(W\)</span> is a function only of the sample, that the distribution of <span class="math inline">\(W|T\)</span> is independent of <span class="math inline">\(\theta\)</span>. Hence <span class="math inline">\(\phi(T)\)</span> is a uniformly better unbiased estimator of <span class="math inline">\(\tau(\theta)\)</span>.</p>
<ul>
<li><p>Conditioning any unbiased estimator on a sufficient statistic
will result in a uniform improvement, so we need consider only
statistics that are functions of a sufficient statistic when looking for best unbiased estimators.</p></li>
<li><p>The proof doesn’t require that the statistic we condition on is
sufficient, but if it isn’t then the resulting quantity will
probably depend on the parameter we are trying to estimate and not be
an estimator..</p></li>
</ul>
</div>
<div id="uniqueness-of-best-unbiased-estimator" class="section level3">
<h3>Uniqueness of best unbiased estimator</h3>
<p>If <span class="math inline">\(W\)</span> is a best unbiased estimator of <span class="math inline">\(\tau(\theta)\)</span> then <span class="math inline">\(W\)</span> is
unique.</p>
<p><strong>Proof:</strong> Suppose <span class="math inline">\(W&#39;\)</span> is another best unbiased estimator, and consider the estimator <span class="math inline">\(W^*=\frac{1}{2}(W + W&#39;)\)</span>. Note that <span class="math inline">\(\mathbb E_{\theta}W^* = \tau(\theta)\)</span> and<br />
<span class="math display">\[\begin{align}
\mathbb V_{\theta}W^*&amp;=\mathbb V_{\theta}\left(\frac{1}{2}W+\frac{1}{2}W&#39;\right)\\
&amp;=\frac{1}{4}\mathbb V_{\theta}W+\frac{1}{4}\mathbb V_{\theta}W&#39;+\frac{1}{2}\text{Cov}_{\theta}(W,W&#39;)\\
&amp;\le\frac{1}{4}\mathbb V_{\theta}W+\frac{1}{4}\mathbb V_{\theta}W&#39;+\frac{1}{2}\left[\mathbb V_{\theta}(W)\mathbb V_{\theta}(W&#39;)\right]^{1/2}\\
&amp;=\mathbb V_{\theta}W \quad\quad(\mathbb V_{\theta}W=\mathbb V_{\theta}W&#39;)\\
\end{align}\]</span> But if the above inequality is strict, then the best unbiasedness of <span class="math inline">\(W\)</span> is contradicted, so we must have equality for all <span class="math inline">\(\theta\)</span>. Since the inequality is an application of Cauchy-Schwarz, we can have equality only if <span class="math inline">\(W&#39; = a(\theta) W + b(\theta)\)</span>. Now using properties of covariance, we have <span class="math display">\[\begin{align}
\text{Cov}_{\theta}(W + W&#39;)&amp;=\text{Cov}_{\theta}(W + a(\theta) W + b(\theta))\\
&amp;=\text{Cov}_{\theta}(W + a(\theta) W)\\
&amp;=a(\theta)\mathbb V_{\theta}(W)\\
\end{align}\]</span> But <span class="math display">\[\text{Cov}_{\theta}(W + W&#39;)=\mathbb V_{\theta}(W)\]</span> since we had equality in last Cauchy-Schwarz, hence <span class="math inline">\(a(\theta)=1\)</span> and since <span class="math inline">\(\mathbb E_{\theta}W^* = \tau(\theta)\)</span>, we must have <span class="math inline">\(b(\theta)=0\)</span> and <span class="math inline">\(W=W&#39;\)</span>.</p>
<p>The following theorem is mostly useful to show that a given estimator
<em>isn’t</em> best unbiased.</p>
<p>If <span class="math inline">\(\mathbb E_{\theta}[W] = \tau(\theta)\)</span>, <span class="math inline">\(W\)</span> is the best unbiased estimator of
<span class="math inline">\(\tau(\theta)\)</span> if and only if <span class="math inline">\(W\)</span> is uncorrelated with all unbiased estimators of <span class="math inline">\(0\)</span>.</p>
<p><strong>Proof:</strong> If <span class="math inline">\(W\)</span> satisfies <span class="math inline">\(\mathbb E_{\theta}[W] = \tau(\theta)\)</span>, and we have another estimator, <span class="math inline">\(U\)</span>, that satisfies <span class="math inline">\(\mathbb E_{\theta}U=0\)</span> for all <span class="math inline">\(\theta\)</span>, that is, <span class="math inline">\(U\)</span> is an unbiased estimator of <span class="math inline">\(0\)</span>. The estimator <span class="math display">\[\phi_a=W+aU,\]</span> where <span class="math inline">\(a\)</span> is a constant, satisfies <span class="math inline">\(\mathbb E_{\theta}\phi_a = \tau(\theta)\)</span> and hence is also an unbiased estimator of <span class="math inline">\(\tau(\theta)\)</span>. The variance of <span class="math inline">\(\phi_a\)</span> is
<span class="math display">\[\mathbb V_{\theta}\phi_a=\mathbb V_{\theta}(W+aU)\\
=\mathbb V_{\theta}W+a^2\mathbb V_{\theta}U+2a\text{Cov}_{\theta}(W,U)\]</span> Now, if for some <span class="math inline">\(\theta=\theta_0,\text{Cov}_{\theta_0}(W,U)&lt;0\)</span>, then we can male <span class="math inline">\(2a\text{Cov}_{\theta_0}(W,U)+a^2\mathbb V_{\theta}U&lt;0\)</span> by choosing <span class="math inline">\(a\in(0, -2\text{Cov}_{\theta_0}(W,U)/\mathbb V_{\theta_0}U)\)</span>. Hence, <span class="math inline">\(\phi_a\)</span> will be better than <span class="math inline">\(W\)</span> at <span class="math inline">\(\theta=\theta_0\)</span> and <span class="math inline">\(W\)</span> can’t be best unbiased. A similar argument will show that if <span class="math inline">\(\text{Cov}_{\theta_0}(W,U)&gt;0\)</span> for any <span class="math inline">\(\theta_0\)</span>, <span class="math inline">\(W\)</span> also cannot be best unbiased.</p>
<p>If <span class="math inline">\(W\)</span> is best unbiased, the above argument shows that <span class="math inline">\(W\)</span> must satisfy
<span class="math inline">\(\text{Cov}_{\theta}(W,U)=0\)</span> for all <span class="math inline">\(\theta\)</span>, for any <span class="math inline">\(U\)</span> satisfying <span class="math inline">\(\mathbb E_{\theta}U=0\)</span>. Hence the necessity is established.</p>
<p>Suppose now that we have an unbiased estimator <span class="math inline">\(W\)</span> that is uncorrelated with all unbiased estimators of <span class="math inline">\(0\)</span>. Let <span class="math inline">\(W&#39;\)</span> be any other estimator satisfying <span class="math inline">\(\mathbb E_{\theta}W&#39;=\mathbb E_{\theta}W=\tau(\theta)\)</span> we will show that <span class="math inline">\(W\)</span> is better than <span class="math inline">\(W&#39;\)</span>. Write <span class="math display">\[W&#39;=W+(W&#39;-W),\]</span> and calculate <span class="math display">\[\begin{align}
\mathbb V_{\theta}W&#39;&amp;=\mathbb V_{\theta}W+\mathbb V_{\theta}(W&#39;-W)+2\text{Cov}_{\theta}(W,W&#39;-W)\\
&amp;=\mathbb V_{\theta}W+\mathbb V_{\theta}(W&#39;-W)
\end{align}\]</span> where the last equality is true because <span class="math inline">\(W&#39;-W\)</span> is an unbiased estimator of <span class="math inline">\(0\)</span> and is uncorrelated with <span class="math inline">\(W\)</span> by assumption. Since <span class="math inline">\(\mathbb V_{\theta}(W&#39;-W)\ge 0\)</span>, then <span class="math inline">\(\mathbb V_{\theta}W&#39;\ge \mathbb V_{\theta}W\)</span>. Since <span class="math inline">\(W&#39;\)</span> is arbitrary, it follows that <span class="math inline">\(W\)</span> is the best unbiased estimator of <span class="math inline">\(\tau(\theta)\)</span>.</p>
<p>The idea comes from considering <span class="math inline">\(\phi_a = W + aU\)</span> where <span class="math inline">\(\mathbb E[U] = 0\)</span>,
then considering the variance.</p>
<p><strong>Unbiased estimator of <span class="math inline">\(0\)</span></strong> Note that an unbiased estimator of <span class="math inline">\(0\)</span> is simply
<strong>random noise</strong> (there is no information in an estimator of <span class="math inline">\(0\)</span>). If an estimator can be improved by adding noise, then it is probably defective.</p>
<p>We are now in a position such that, if we can characterise all of the
unbiased estimators of <span class="math inline">\(0\)</span> then we can check if a given estimator is best
unbiased. In general this is not easy and requires conditions on the
distribution. However, if a distribution is complete then it admits no
unbiased estimators of <span class="math inline">\(0\)</span> other than <span class="math inline">\(0\)</span> itself (by definition), so we will
be done.</p>
<p>Note that due to the Rao-Blackwell theorem, only the distribution of the
sufficient statistic needs to be complete (not the underlying population
distribution).</p>
</div>
<div id="completeness-and-best-unbiasedness" class="section level3">
<h3>completeness and best unbiasedness</h3>
<p>Let <span class="math inline">\(T\)</span> be a complete sufficient statistic for a parameter <span class="math inline">\(\theta\)</span> and
let <span class="math inline">\(\phi(T)\)</span> be any estimator based only on <span class="math inline">\(T\)</span>. Then <span class="math inline">\(\phi(T)\)</span> is the
unique best unbiased estimator of its expected value.</p>
<p><strong>Lehmann-Scheffé</strong> Unbiased estimators based on complete sufficient
statistics are unique.</p>
</div>
<div id="loss-function-optimality" class="section level3">
<h3>Loss Function Optimality</h3>
</div>
</div>
</div>
<div id="hypothesis-testing" class="section level1">
<h1>8. Hypothesis Testing</h1>
<div id="introduction" class="section level2">
<h2>Introduction</h2>
<ul>
<li>Test statistic: <span class="math inline">\(W(X_1,..., X_n) = W(\mathbf X)\)</span></li>
<li><span class="math inline">\(H_0: \theta \in \Theta_0\)</span></li>
<li><span class="math inline">\(H_1: \theta \in \Theta^c_0\)</span></li>
</ul>
</div>
<div id="methods-of-finding-tests" class="section level2">
<h2>Methods of Finding Tests</h2>
<div id="likelihood-ratio-tests" class="section level3">
<h3>Likelihood Ratio Tests</h3>
<p><strong>Likelihood ratio test (LRT)</strong> statistic for testing <span class="math inline">\(H_0: \theta \in \Theta_0\)</span> versus <span class="math inline">\(H_1: \theta \in \Theta^c_0\)</span> is:</p>
<p><span class="math display">\[\lambda(\mathbf x) = \frac{\underset{\Theta_0}{\sup}L(\theta|\mathbf x)}{\underset{\Theta}{\sup}L(\theta|\mathbf x)}\]</span> A likelihood ratio test (LRT) is any test that has a rejection region of the form
<span class="math inline">\(\{\mathbf x : \lambda(\mathbf x)\le c\}\)</span> , where <span class="math inline">\(c\)</span> is any number satisfying <span class="math inline">\(0\le c \le 1\)</span>.
<span class="math display">\[L(\theta|x_1,..., x_n) = f(\mathbf x|\theta) = \prod^n_{i=1}f(x_i|\theta)\]</span>
the numerator is the maximum probability of the observed sample, the maximum being computed over parameters in the null hypothesis. The denominator is maximum probability of observed sample over all possible parameters. Small likelihood implies rejecting <span class="math inline">\(H_0\)</span>.</p>
<p>Suppose <span class="math inline">\(\hat\theta\)</span> an MLE of <span class="math inline">\(\theta\)</span>, exists; <span class="math inline">\(\hat\theta\)</span> is obtained by doing an unrestricted maximization of <span class="math inline">\(L(\theta|\mathbf x)\)</span>.
We can also consider the MLE of <span class="math inline">\(\theta\)</span>, call it <span class="math inline">\(\hat\theta_0\)</span>, obtained by doing a restricted maximization, assuming <span class="math inline">\(\Theta_0\)</span> is the parameter space. That is, <span class="math inline">\(\hat\theta_0=\hat\theta_0(\mathbf x)\)</span> is the value of <span class="math inline">\(\theta\in\Theta_0\)</span> that maximizes <span class="math inline">\(L(\theta|\mathbf x)\)</span>. Then, the LRT statistic is
<span class="math display">\[\lambda(\mathbf x)=\frac{L(\hat\theta_0|\mathbf x)}{L(\hat\theta|\mathbf x)}\]</span></p>
</div>
<div id="likelihood-ratio-tests-using-the-sufficiency-statistic" class="section level3">
<h3>Likelihood Ratio Tests using the sufficiency statistic</h3>
<p>If <span class="math inline">\(T(\mathbf X)\)</span> is a sufficient statistic for <span class="math inline">\(\theta\)</span> and <span class="math inline">\(\lambda^*(t)\)</span> and <span class="math inline">\(\lambda(\mathbf x)\)</span> are the LRT statistics based on <span class="math inline">\(T\)</span> and <span class="math inline">\(\mathbf X\)</span>, respectively, then <span class="math inline">\(\lambda^*(T(\mathbf x))=\lambda(\mathbf x)\)</span> for every <span class="math inline">\(\mathbf x\)</span> in the sample space.</p>
<p><strong>Proof:</strong>
From the <a href="#theorem1">Factorization Theorem</a>, the pdf or pmf of <span class="math inline">\(\mathbf X\)</span> can
be written as <span class="math inline">\(f(\mathbf x|\theta) = g(T(\mathbf x)|\theta)h(\mathbf x)\)</span>, where <span class="math inline">\(g(t|\theta)\)</span> is the pdf or pmf of <span class="math inline">\(T\)</span> and <span class="math inline">\(h(\mathbf x)\)</span>
does not depend on <span class="math inline">\(\theta\)</span>. Thus
<span class="math display">\[\begin{align}
\lambda(\mathbf x) &amp;= \frac{\underset{\Theta_0}{\sup}L(\theta|\mathbf x)}{\underset{\Theta}{\sup}L(\theta|\mathbf x)}\\
&amp;=\frac{\underset{\Theta_0}{\sup}f(\mathbf x|\theta)}{\underset{\Theta}{\sup}f(\mathbf x|\theta)}\\
&amp;=\frac{\underset{\Theta_0}{\sup}g(T(\mathbf x)|\theta)h(\mathbf x)}{\underset{\Theta}{\sup}g(T(\mathbf x)|\theta)h(\mathbf x)}\\
&amp;=\frac{\underset{\Theta_0}{\sup}g(T(\mathbf x)|\theta)}{\underset{\Theta}{\sup}g(T(\mathbf x)|\theta)}\\
&amp;= \frac{\underset{\Theta_0}{\sup}L^*(\theta|T(\mathbf x))}{\underset{\Theta}{\sup}L^*(\theta|T(\mathbf x))}\\
&amp;=\lambda^*(T(\mathbf x))\\
\end{align}\]</span></p>
</div>
<div id="bayesian-tests" class="section level3">
<h3>Bayesian tests</h3>
<p>Normal Bayesian test: accept <span class="math inline">\(H_0\)</span> if and only if
<span class="math display">\[\frac{1}{2} \leq P(\theta \in \Theta_0|\mathbf X) = P(\theta \leq \theta_0|\mathbf X)\]</span></p>
</div>
<div id="union-intersection-and-intersection-union-tests" class="section level3">
<h3>Union-intersection and intersection-union tests</h3>
<p>Union-intersection test:
<span class="math display">\[H_0: \theta \in \underset{\gamma \in \Gamma}\bigcap \Theta_{\gamma}\]</span>
intersection-union test:
<span class="math display">\[H_0: \theta \in \underset{\gamma \in \Gamma}\bigcup \Theta_{\gamma}\]</span></p>
</div>
</div>
<div id="methods-of-evaluating-tests" class="section level2">
<h2>Methods of Evaluating Tests</h2>
<div id="error-probabilities-and-the-power-function" class="section level3">
<h3>Error Probabilities and the Power Function</h3>
<ul>
<li>Type I Error (FP): If <span class="math inline">\(\theta \in \Theta_0\)</span> but the hypothesis test incorrectly decides to reject <span class="math inline">\(H_0\)</span>,</li>
<li>Type II Error (FN): If <span class="math inline">\(\theta \in \Theta^c_0\)</span> but the test decides to accept <span class="math inline">\(H_0\)</span>,</li>
</ul>
<p><strong>Power function</strong>: The power function of a hypothesis test with rejection region <span class="math inline">\(R\)</span> is the function of <span class="math inline">\(\theta\)</span> defined by <span class="math display">\[\beta(\theta) = P_{\theta}(\mathbf X \in R)\]</span>
Ideally;
- <span class="math inline">\(\beta(\theta) = 0\)</span> for all <span class="math inline">\(\theta \in \Theta_0\)</span>
- <span class="math inline">\(\beta(\theta) = 1\)</span> for all <span class="math inline">\(\theta \in \Theta^c_0\)</span></p>
<p>Let <span class="math inline">\(X_1,\cdots,X_n\)</span> be a random sample from a <span class="math inline">\(n(\theta, \sigma^2)\)</span> population, <span class="math inline">\(\sigma^2\)</span> known. An LRT of <span class="math inline">\(H_0 : \theta\le\theta_0\)</span> versus <span class="math inline">\(H_1 : \theta &gt; \theta_0\)</span> is a test that rejects <span class="math inline">\(H_0\)</span> if
<span class="math display">\[\frac{\bar{X} - \theta_0}{\sigma/ \sqrt{n}} &gt; c\]</span> The constant <span class="math inline">\(c\)</span> can be any positive number. The power function of this test is:
<span class="math display">\[\begin{align}
\beta(\theta) &amp;= P_{\theta}\left(\frac{\bar{X} - \theta_0}{\sigma/ \sqrt{n}} &gt; c\right)\\
&amp;= P_{\theta}\left(\frac{\bar{X} - \theta}{\sigma/ \sqrt{n}} &gt; c+\frac{\theta_0 - \theta}{\sigma/ \sqrt{n}}\right)\\
&amp;=P\left(Z &gt; c + \frac{\theta_0 - \theta}{\sigma/ \sqrt{n}}\right)
\end{align}\]</span> where <span class="math inline">\(Z\)</span> is a standard normal random variable, since <span class="math display">\[\frac{\bar{X} - \theta}{\sigma/ \sqrt{n}}\sim n(0,1)\]</span> As <span class="math inline">\(\theta\)</span>
increases from <span class="math inline">\(-\infty\to \infty\)</span>, it is easy to see that this normal probability increases from <span class="math inline">\(0\)</span> to <span class="math inline">\(1\)</span>. Therefore, it follows that <span class="math inline">\(\beta(\theta)\)</span> is an increasing function of <span class="math inline">\(\theta\)</span>, with
<span class="math display">\[\underset{\theta\to-\infty}{\lim}\beta(\theta)=0, \;\;\underset{\theta\to+\infty}{\lim}\beta(\theta)=1,\]</span> and <span class="math display">\[\beta(\theta_0)=\alpha\quad\text{if}\quad P(Z&gt;c)=\alpha\]</span> <span class="math inline">\(\alpha\)</span> is the Type I Error probability.</p>
<ul>
<li><p>For <span class="math inline">\(0\le\alpha\le 1\)</span>, a test with power function <span class="math inline">\((\beta(\theta)\)</span> is a <strong>size <span class="math inline">\(\alpha\)</span> test</strong> if <span class="math inline">\(\sup_{\theta\in\Theta_0}\beta(\theta) = \alpha\)</span>.</p></li>
<li><p>For <span class="math inline">\(0\le\alpha\le 1\)</span>, a test with power function <span class="math inline">\((\beta(\theta)\)</span> is a <strong>level <span class="math inline">\(\alpha\)</span> test</strong> if <span class="math inline">\(\sup_{\theta\in\Theta_0}\beta(\theta) \le \alpha\)</span>.</p></li>
<li><p><strong>Size of LRT</strong> a size <span class="math inline">\(\alpha\)</span> LRT is constructed by choosing
<span class="math inline">\(c\)</span> such that <span class="math inline">\(\sup_{\theta\in\Theta_0}P_{\theta}(\lambda(\mathbf X)\le c) = \alpha\)</span>. How that <span class="math inline">\(c\)</span> is determined depends on the
particular problem. For example, <span class="math inline">\(\Theta_0\)</span> consists of the single point <span class="math inline">\(\theta=\theta_0\)</span> and <span class="math inline">\(\sqrt{n}(\bar{X} - \theta_0) \sim n(0,1)\)</span> if <span class="math inline">\(\theta=\theta_0\)</span>. So the test
<span class="math display">\[\text{reject }H_0\text{ if }|\bar{X}-\theta_0|\ge z_{\alpha/2}/\sqrt{n}\]</span> where <span class="math inline">\(z_{\alpha/2}\)</span> satisfies <span class="math inline">\(P(Z\ge z_{\alpha/2})=\alpha/2\)</span> with <span class="math inline">\(Z\sim n(0,1)\)</span> is a size <span class="math inline">\(\alpha\)</span> LRT.</p></li>
<li><p><strong>Size of union-intersection test</strong> Finding constants <span class="math inline">\(t_L\)</span> and <span class="math inline">\(t_U\)</span> such that <span class="math display">\[\underset{\theta\in\Theta_0}{\sup}P_{\theta}\left(\frac{\bar{X}-\mu_0}{\sqrt{S^2/n}}\ge t_L\quad\text{ or }\quad\frac{\bar{X}-\mu_0}{\sqrt{S^2/n}}\le t_U\right)=\alpha\]</span> But for any <span class="math inline">\((\mu,\sigma^2)=\theta\in\Theta_0,\;\mu=\mu_0\)</span> and thus <span class="math display">\[\frac{\bar{X}-\mu_0}{\sqrt{S^2/n}}=\frac{\bar{X}-\mu}{\sqrt{S^2/n}}\]</span> has a Student’s <span class="math inline">\(t\)</span> distribution with <span class="math inline">\(n-1\)</span> degree of freedom. So any choice of <span class="math inline">\(t_U=t_{n-1,1-\alpha_1}\)</span> and <span class="math inline">\(t_L=t_{n-1,\alpha_2}\)</span> with <span class="math inline">\(\alpha_1+\alpha_2=\alpha\)</span> will yield a test with Type I Error probability of exactly <span class="math inline">\(\alpha\)</span> for all <span class="math inline">\(\theta\in\Theta_0\)</span>.</p></li>
<li><p><strong>Unbiased power function</strong> A test with power function <span class="math inline">\(\beta(\theta)\)</span> is unbiased if <span class="math inline">\(\beta(\theta&#39;)\ge\beta(\theta&#39;&#39;)\)</span> for every <span class="math inline">\(\theta&#39;\in\Theta_0^c\)</span> and <span class="math inline">\(\theta&#39;&#39;\in\Theta_0\)</span></p></li>
</ul>
</div>
<div id="most-powerful-tests-ump" class="section level3">
<h3>Most Powerful Tests (UMP)</h3>
<p>Let <span class="math inline">\(\mathcal C\)</span> be a class of tests for testing <span class="math inline">\(H_0:\theta\in\Theta_0\)</span> versus <span class="math inline">\(H_1:\theta\in\Theta_0^c\)</span>. A test in class <span class="math inline">\(\mathcal C\)</span>, with power function <span class="math inline">\(\beta(\theta)\)</span>, is a <strong>uniformly most powerful (UMP) class <span class="math inline">\(\mathcal C\)</span> test</strong> if <span class="math inline">\(\beta(\theta)\ge \beta&#39;(\theta)\)</span> for every <span class="math inline">\(\theta\in\Theta_0^c\)</span> and every <span class="math inline">\(\beta&#39;(\theta)\)</span> that is a power function of a test in class <span class="math inline">\(\mathcal C\)</span>.</p>
<div id="neyman-pearson-lemma" class="section level4">
<h4>Neyman-Pearson lemma</h4>
<p>Consider testing <span class="math inline">\(H_0:\theta=\theta_0\)</span> versus <span class="math inline">\(H_1:\theta=\theta_1\)</span>, where the pdf or pmf corresponding to <span class="math inline">\(\theta_i\)</span> is <span class="math inline">\(f(\mathbf x|\theta_i), i = 0, 1\)</span>, using a test with rejection region <span class="math inline">\(R\)</span> that satisfies <span class="math display">\[\mathbf x\in R\quad\text{if}\quad f(\mathbf x|\theta_1)&gt;kf(\mathbf x|\theta_0)\\
\text{and}\\
\mathbf x\in R^c\quad\text{if}\quad f(\mathbf x|\theta_1)&lt;kf(\mathbf x|\theta_0)\tag1\]</span> for some <span class="math inline">\(K\ge 0\)</span> and <span class="math display">\[\alpha=P_{\theta_0}(\mathbf X\in R)\tag2\]</span> Then</p>
<ul>
<li><ol style="list-style-type: lower-alpha">
<li><em>Sufficiency</em>, Any test that satisfies the conditions above is a UMP level <span class="math inline">\(\alpha\)</span> test.</li>
</ol></li>
<li><ol start="2" style="list-style-type: lower-alpha">
<li><em>Necessity</em>, If there exists a test satisfying the conditions above with <span class="math inline">\(k &gt; 0\)</span>, then every UMP level <span class="math inline">\(\alpha\)</span> test is a size <span class="math inline">\(\alpha\)</span> test (satisfies (2)) and every UMP level <span class="math inline">\(\alpha\)</span> test satisfies (1) except perhaps on a set <span class="math inline">\(A\)</span> satisfying <span class="math display">\[P_{\theta_0}(\mathbf X\in A)=P_{\theta_1}(\mathbf X\in A)=0\]</span></li>
</ol></li>
</ul>
<p><strong>Proof:</strong> Note first that any test satisfying (2) is a size <span class="math inline">\(\alpha\)</span> and, hence, a level <span class="math inline">\(\alpha\)</span> test because
<span class="math display">\[\sup_{\theta\in\Theta_0}P_{\theta}(\mathbf X\in R)=P_{\theta_0}(\mathbf X\in R)=\alpha\]</span> since <span class="math inline">\(\Theta_0\)</span> has only one point.</p>
<p>To ease notation, we define a <em>test junction</em>, a function on the sample space that is <span class="math inline">\(1\)</span> if <span class="math inline">\(\mathbf x \in R\)</span> and <span class="math inline">\(0\)</span> if <span class="math inline">\(\mathbf x \in R^c\)</span>. That is, it is the indicator function of the rejection
region. Let <span class="math inline">\(\phi(\mathbf x)\)</span> be the test function of a test satisfying (1) and (2). Let <span class="math inline">\(\phi&#39;(\mathbf x)\)</span> be the test function of any other level <span class="math inline">\(\alpha\)</span> test, and let <span class="math inline">\(\beta(\theta)\)</span> and <span class="math inline">\(\beta&#39;(\theta)\)</span> be the power functions corresponding to the tests <span class="math inline">\(\phi\)</span> and <span class="math inline">\(\phi&#39;\)</span>, respectively. Because <span class="math inline">\(0\le\phi&#39;(\mathbf x)\le 1\)</span>, (1) implies that <span class="math display">\[(\phi(\mathbf x)-\phi&#39;(\mathbf x))(f(\mathbf x|\theta_1)-kf(\mathbf x|\theta_0))\ge 0\]</span> for every <span class="math inline">\(\mathbf x\)</span> (since <span class="math inline">\(\phi=1\)</span> if <span class="math inline">\(f(\mathbf x|\theta_1)&gt;kf(\mathbf x|\theta_0)\)</span> and <span class="math inline">\(\phi = 0\)</span> if <span class="math inline">\(f(\mathbf x|\theta_1) &lt; kf(\mathbf x|\theta_0)\)</span>. Thus <span class="math display">\[0\le\int\left[\phi(\mathbf x)-\phi&#39;(\mathbf x)\right]\left[f(\mathbf x|\theta_1)-kf(\mathbf x|\theta_0)\right]d\mathbf x\\
=\beta(\theta_1)-\beta&#39;(\theta_1)-k(\beta(\theta_0)-\beta&#39;(\theta_0))\tag3\]</span> Statement (a) is proved by noting that, since <span class="math inline">\(\phi&#39;\)</span> is a level <span class="math inline">\(\alpha\)</span> test and <span class="math inline">\(\phi\)</span> is a size <span class="math inline">\(\alpha\)</span> test, <span class="math inline">\(\beta(\theta_0)-\beta&#39;(\theta_0)=\alpha-\beta&#39;(\theta_0)\ge 0\)</span> Thus (3) and <span class="math inline">\(k\ge 0\)</span> imply that <span class="math display">\[0\le\beta(\theta_1)-\beta&#39;(\theta_1)-k(\beta(\theta_0)-\beta&#39;(\theta_0))\le\beta(\theta_1)-\beta&#39;(\theta_1)\]</span> showing that <span class="math inline">\(\beta(\theta_1)\ge \beta&#39;(\theta_1)\)</span> and hence <span class="math inline">\(\phi\)</span> has greater power than <span class="math inline">\(\phi&#39;\)</span>. Since <span class="math inline">\(\phi&#39;\)</span> was an arbitrary level <span class="math inline">\(\alpha\)</span> test and <span class="math inline">\(\theta_1\)</span> is the only point in <span class="math inline">\(\Theta_0^c\)</span>, <span class="math inline">\(\phi\)</span> is a UMP level <span class="math inline">\(\alpha\)</span> test.</p>
<p>To prove statement (b) , let <span class="math inline">\(\phi&#39;\)</span> now be the test function for any UMP level <span class="math inline">\(\alpha\)</span> test. By part (a), <span class="math inline">\(\phi\)</span>, the test satisfying (1) and (2), is also a UMP level <span class="math inline">\(\alpha\)</span> test, thus
<span class="math inline">\(\beta(\theta_1)=\beta&#39;(\theta_1)\)</span>. This fact, (3), and <span class="math inline">\(k &gt; 0\)</span> imply <span class="math display">\[\alpha-\beta&#39;(\theta_0)=\beta(\theta_0)-\beta&#39;(\theta_0)\leq0\]</span></p>
<p>Now, since <span class="math inline">\(\phi&#39;\)</span> is a level <span class="math inline">\(\alpha\)</span> test, <span class="math inline">\(\beta&#39;(\theta_0)\le\alpha\)</span>. Thus <span class="math inline">\(\beta&#39;(\theta_0)=\alpha\)</span>, that is, <span class="math inline">\(\phi&#39;\)</span> is a size <span class="math inline">\(\alpha\)</span> test, and this also implies that (3) is an equality in this case. But the nonnegative integrand <span class="math inline">\((\phi(\mathbf x)-\phi&#39;(\mathbf x))(f(\mathbf x|\theta_1)-kf(\mathbf x|\theta_0))\)</span> will have a zero integral only if <span class="math inline">\(\phi&#39;\)</span> satisfies
(1), except perhaps on a set <span class="math inline">\(A\)</span> with <span class="math inline">\(\int_Af(\mathbf x|\theta_i)d\mathbf x=0\)</span>. This implies that the last
assertion in statement (b) is true.</p>
</div>
<div id="corollary-neyman-pearson-lemma-and-sufficiency" class="section level4">
<h4>Corollary: Neyman-Pearson lemma and sufficiency</h4>
<p>Consider the hypothesis problem posed in Theorem <a href="#neyman-pearson-lemma">Neyman-Pearson lemma</a>. Suppose <span class="math inline">\(T(\mathbf X)\)</span> is a sufficient statistic for <span class="math inline">\(\theta\)</span> and <span class="math inline">\(g(t|\theta_i)\)</span> is the pdf or pmf of <span class="math inline">\(T\)</span> corresponding to <span class="math inline">\(\theta_i,\;i=0,1\)</span>. Then any test based on <span class="math inline">\(T\)</span> with rejection region <span class="math inline">\(S\)</span> (a subset of the sample space of <span class="math inline">\(T\)</span>) is a UMP level <span class="math inline">\(\alpha\)</span> test if it satisfies <span class="math display">\[t\in S\quad\text{if}\quad g(t|\theta_1)&gt;kg(t|\theta_0)\\
\text{and}\\
t\in S^c\quad\text{if}\quad g(t|\theta_1)&lt;kg(t|\theta_0),\tag4\]</span> for some <span class="math inline">\(k\ge0\)</span> where <span class="math display">\[\alpha=P_{\theta_0}(T\in S)\tag5\]</span></p>
<p><strong>Proof:</strong> In terms of the original sample <span class="math inline">\(\mathbf X\)</span>, the test based on <span class="math inline">\(T\)</span> has the rejection region <span class="math inline">\(R = \{\mathbf x: T(\mathbf x) \in S\}\)</span>. By the <a href="#theorem1">Factorization Theorem</a>, the pdf or pmf of <span class="math inline">\(\mathbf X\)</span> can be written as <span class="math inline">\(f(\mathbf x|\theta_i)=g(T(\mathbf x)|\theta_i)h(\mathbf x),\quad i=0,1,\)</span> for some nonnegative function <span class="math inline">\(h(\mathbf x)\)</span>.
Multiplying the inequalities in (4) by this non-negative function, we see that <span class="math inline">\(R\)</span> satisfies <span class="math display">\[\mathbf x\in R\quad\text{if}\quad f(\mathbf x|\theta_1)=g(T(\mathbf x)|\theta_1)h(\mathbf x)&gt;kg(T(\mathbf x)|\theta_0)h(\mathbf x)=kf(\mathbf x|\theta_0)\]</span>
and <span class="math display">\[\mathbf x\in R^c\quad\text{if}\quad f(\mathbf x|\theta_1)=g(T(\mathbf x)|\theta_1)h(\mathbf x)&lt;kg(T(\mathbf x)|\theta_0)h(\mathbf x)=kf(\mathbf x|\theta_0)\]</span>
Also by (5), <span class="math display">\[P_{\theta_0}(\mathbf X\in R)=P_{\theta_0}(T(\mathbf X)\in S)=\alpha.\]</span> So, by the sufficiency part of the <a href="#neyman-pearson-lemma">Neyma-Pearson Lemma</a>, the test based on <span class="math inline">\(T\)</span> is a
UMP level <span class="math inline">\(\alpha\)</span> test.</p>
</div>
<div id="monotone-likelihood-ratio-mlr" class="section level4">
<h4>Monotone Likelihood Ratio (MLR)</h4>
<p>A family of pdfs or pmfs <span class="math inline">\(\{g(t|\theta):\theta\in\Theta\}\)</span> for a univariate random variable <span class="math inline">\(T\)</span> with real-valued parameter <span class="math inline">\(\theta\)</span> has a <strong>monotone likelihood ratio (MLR)</strong> if, for every <span class="math inline">\(\theta_2 &gt; \theta_1\)</span>, <span class="math inline">\(g(t|\theta_2)/g(t|\theta_1)\)</span> is a monotone (nonincreasing or nondecreasing) function of <span class="math inline">\(t\)</span> on
<span class="math inline">\(\{t:g(t|\theta_1)&gt;0\text{ or }g(t|\theta_2)&gt;0\}\)</span>. Note that <span class="math inline">\(c/0\)</span> is defined as <span class="math inline">\(\infty\)</span> if <span class="math inline">\(0 &lt; c\)</span>.</p>
<blockquote>
<p>Many common families of distributions have an MLR. For example, the normal (known variance, unknown mean), Poisson, and binomial all have an MLR. Indeed, any regular exponential family with <span class="math inline">\(g(t|\theta)=h(t)c(\theta)e^{w(\theta)t}\)</span> has an MLR if <span class="math inline">\(w(\theta)\)</span> is a nondecreasing function.</p>
</blockquote>
</div>
<div id="karlin-rubin-theorem" class="section level4">
<h4>Karlin-Rubin Theorem</h4>
<p>Consider testing <span class="math inline">\(H_0:\theta\le\theta_0\)</span> versus <span class="math inline">\(H_1:\theta&gt;\theta_0\)</span>. Suppose that <span class="math inline">\(T\)</span> is a sufficient statistic for <span class="math inline">\(\theta\)</span> and the family of pdfs or pmfs
<span class="math inline">\(\{g(t|\theta):\theta\in\Theta\}\)</span> of <span class="math inline">\(T\)</span> has an MLR. Then for any <span class="math inline">\(t_0\)</span>, the test that rejects <span class="math inline">\(H_0\)</span> if and only if <span class="math inline">\(T &gt; t_0\)</span> is a UMP level <span class="math inline">\(\alpha\)</span> test, where <span class="math inline">\(\alpha = P_{\theta_0}(T &gt; t_0)\)</span>.</p>
<p><strong>Proof:</strong> Let <span class="math inline">\(\beta(\theta)=P_{\theta}(T&gt;t_0)\)</span> be the power function of the test. Fix <span class="math inline">\(\theta&#39;&gt;\theta_0\)</span> and consider testing <span class="math inline">\(H_0&#39;:\theta=\theta_0\)</span> versus <span class="math inline">\(H_1&#39;:\theta=\theta&#39;\)</span>. Since the family of pdfs or pmfs of <span class="math inline">\(T\)</span> has an MLR, <span class="math inline">\(\beta(\theta)\)</span> is nondecreasing, so</p>
<ul>
<li><ol style="list-style-type: lower-roman">
<li><span class="math inline">\(\sup_{\theta&lt;\theta_0}\beta(\theta)=\beta(\theta_0)=\alpha\)</span>, and this is a level <span class="math inline">\(\alpha\)</span> test.<br />
</li>
</ol></li>
<li><ol start="2" style="list-style-type: lower-roman">
<li>If we define <span class="math display">\[k&#39;=\inf_{t\in\mathcal T}\frac{g(t|\theta&#39;)}{g(t|\theta_0)}\]</span> where <span class="math inline">\(\mathcal T=\{t:t&gt;t_0\text{  and either  } g(t|\theta&#39;)&gt;0\text{  or  }g(t|\theta_0)&gt;0\}\)</span>, it follows that <span class="math display">\[T&gt;t_0\iff\frac{g(t|\theta&#39;)}{g(t|\theta_0)}&gt;k&#39;.\]</span></li>
</ol></li>
</ul>
<p>Together with <a href="#corollary-neyman-pearson-lemma-and-sufficiency">Corollary: Neyman-Pearson lemma and sufficiency</a>, (i) and (ii) imply that <span class="math inline">\(\beta(\theta&#39;)\ge\beta^*(\theta&#39;)\)</span>, where <span class="math inline">\(\beta^*(\theta)\)</span> is the power function for any other level <span class="math inline">\(\alpha\)</span> test of <span class="math inline">\(H_0&#39;\)</span>, that is, any test satisfying <span class="math inline">\(\beta(\theta_0)\le\alpha\)</span>. However, any level <span class="math inline">\(\alpha\)</span> test of <span class="math inline">\(H_0\)</span> satisfies <span class="math inline">\(\beta^*(\theta_0)\le\sup_{\theta\in\Theta_0}\beta^*(\theta)\le\alpha.\)</span> Thus, <span class="math inline">\(\beta(\theta&#39;)\ge\beta^*(\theta&#39;)\)</span> for any level <span class="math inline">\(\alpha\)</span> test of <span class="math inline">\(H_0\)</span>. Since <span class="math inline">\(\theta&#39;\)</span> was arbitrary, the test is a UMP level <span class="math inline">\(\alpha\)</span> test.</p>
</div>
</div>
<div id="sizes-of-union-intersection-and-intersection-union-tests" class="section level3">
<h3>Sizes of Union-Intersection and Intersection-Union Tests</h3>
<p><strong>Union-Intersection tests ( UIT)</strong> and <strong>Intersection-Union tests (IUT)</strong> can often be bounded above by the sizes of some other tests. First consider UITs. In this situation, we are testing a null hypothesis of the form <span class="math inline">\(H_0:\theta\in\Theta_0\)</span>, where <span class="math inline">\(\Theta_0=\underset{\gamma\in\Gamma}{\bigcap}\Theta_{\gamma}\)</span>. To be specific, let <span class="math inline">\(\lambda_{\gamma}(\mathbf x)\)</span> be the LRT statistic for testing <span class="math display">\[H_{0\gamma}:\theta\in\Theta_{\gamma}\]</span> versus <span class="math display">\[H_{1\gamma}:\theta\in\Theta_{\gamma}^c\]</span> and let <span class="math inline">\(\lambda(\mathbf x)\)</span> be the LRT statistic for testing <span class="math inline">\(H_{0}:\theta\in\Theta_{0}\)</span> versus <span class="math inline">\(H_{1}:\theta\in\Theta_{0}^c\)</span> Then we have the following
relationships between the overall LRT and the UIT based on <span class="math inline">\(\lambda_{\gamma}(\mathbf x)\)</span>.</p>
<div id="theorem-lrt-is-uniformly-more-powerful-than-the-union-intersection-tests-uits" class="section level4">
<h4>Theorem: LRT is uniformly more powerful than the Union-Intersection Tests (UITs)</h4>
<p>Consider testing <span class="math inline">\(H_{0}:\theta\in\Theta_{0}\)</span> versus <span class="math inline">\(H_{1}:\theta\in\Theta_{0}^c\)</span>, where <span class="math inline">\(\Theta_0=\underset{\gamma\in\Gamma}{\bigcap}\Theta_{\gamma}\)</span> and <span class="math inline">\(\lambda_{\gamma}(\mathbf x)\)</span> is defined in the previous paragraph. Define <span class="math inline">\(T(\mathbf x)=\inf_{\gamma\in\Gamma}\lambda_{\gamma}(\mathbf x)\)</span>, and form the UIT with rejection region
<span class="math display">\[\{\mathbf x:\lambda_{\gamma}(\mathbf x)&lt;c\quad\text{for some}\quad\gamma\in\Gamma\}=\{\mathbf x:T(\mathbf x)&lt;c\}.\]</span> Also consider the usual LRT with rejection region <span class="math display">\[\{\mathbf x:\lambda(\mathbf x)&lt;c\}.\]</span> Then</p>
<ul>
<li><ol style="list-style-type: lower-alpha">
<li><span class="math inline">\(T(\mathbf x)\ge\lambda(\mathbf x)\quad\text{for every}\quad \mathbf x\)</span>;</li>
</ol></li>
<li><ol start="2" style="list-style-type: lower-alpha">
<li>If <span class="math inline">\(\beta_T(\theta)\)</span> and <span class="math inline">\(\beta_{\lambda}(\theta)\)</span> are the power functions for the tests based on <span class="math inline">\(T\)</span> and <span class="math inline">\(\lambda\)</span>, respectively, then <span class="math inline">\(\beta_T(\theta)\le\beta_{\lambda}(\theta)\quad\text{for every}\quad\theta\in\Theta\)</span>;</li>
</ol></li>
<li><ol start="3" style="list-style-type: lower-alpha">
<li>If the LRT is a level <span class="math inline">\(\alpha\)</span> test, then the UIT is a level <span class="math inline">\(\alpha\)</span> test.</li>
</ol></li>
</ul>
<p><strong>Proof:</strong> Since <span class="math inline">\(\Theta_0=\underset{\gamma\in\Gamma}{\bigcap}\Theta_{\gamma}\subset\Theta_{\gamma}\)</span> for any <span class="math inline">\(\gamma\)</span>, from Definition of <a href="#likelihood-ratio-tests">Likelihood Ratio Tests</a>, we see that for any <span class="math inline">\(\mathbf x\)</span>, <span class="math display">\[\lambda_{\gamma}(\mathbf x)\ge\lambda(\mathbf x)\quad \text{for each }\quad \gamma\in\Gamma\]</span> because the region of maximization is bigger for the individual <span class="math inline">\(\lambda_{\gamma}\)</span>, thus <span class="math inline">\(T(\mathbf x)=\inf_{\gamma\in\Gamma}\lambda_{\gamma}(\mathbf x)\ge\lambda(\mathbf x)\)</span>, proving (a).</p>
<p>By (a), <span class="math inline">\(\{\mathbf x:T(\mathbf x)&lt;c\}\subset\{\mathbf x:\lambda(\mathbf x)&lt;c\}\)</span>, so <span class="math display">\[\beta_T(\theta)=P_{\theta}(T(\mathbf X)&lt;c)\le P_{\theta}(\lambda(\mathbf X)&lt;c)=\beta_{\lambda}(\theta),\]</span> proving (b). Since (b) holds for every <span class="math inline">\(\theta, \;\;\sup_{\theta\in\Theta_0}\beta_T(\theta)\le\sup_{\theta\in\Theta_0}\beta_{\lambda}(\theta)\le\alpha\)</span>, proving (c).</p>
</div>
<div id="theorem-level-alpha-test-iut" class="section level4">
<h4>Theorem: Level <span class="math inline">\(\alpha\)</span> test IUT</h4>
<p>A simple bound for the size of an IUT is related to the sizes of the individual tests that are used to define the IUT. Recall that
in this situation the null hypothesis is expressible as a <em>union</em>; that is, we are testing <span class="math display">\[H_0:\theta\in\Theta_0\quad\text{versus}\quad H_1:\theta\in\Theta_0^c,\quad\text{where}\quad\Theta_0=\bigcup_{\gamma\in\Gamma}\Theta_{\gamma}.\]</span> An IUT has a rejection region of the form <span class="math inline">\(R=\bigcap_{\gamma\in\Gamma}R_{\gamma},\)</span> where <span class="math inline">\(R_{\gamma}\)</span> is the rejection region for a test of <span class="math inline">\(H_{0\gamma}:\theta\in\Theta_{\gamma}\)</span>H</p>
<p>Let <span class="math inline">\(\alpha_{\gamma}\)</span> be the size of the test of <span class="math inline">\(H_{0\gamma}\)</span> with rejection region <span class="math inline">\(R_{\gamma}\)</span>. Then the IUT with rejection region
<span class="math inline">\(R=\bigcap_{\gamma\in\Gamma}R_{\gamma}\)</span> is a level <span class="math inline">\(\alpha=\sup_{\gamma\in\Gamma}\alpha_{\gamma}\)</span> test.</p>
<p><strong>Proof:</strong> Let <span class="math inline">\(\theta\in\Theta_0\)</span>. Then <span class="math inline">\(\theta\in\Theta_{\gamma}\)</span> for some <span class="math inline">\(\gamma\)</span> and <span class="math display">\[P_{\theta}(\mathbf X\in R)\le P_{\theta}(\mathbf X\in R_{\gamma})\le\alpha_{\gamma}\le\alpha.\]</span> Since <span class="math inline">\(\theta\in\Theta_0\)</span> was arbitrary, the IUT is a level <span class="math inline">\(\alpha\)</span> test.</p>
<p>Consider testing <span class="math inline">\(H_0:\theta\in\bigcup_{j=1}^{k}\Theta_j\)</span>, where <span class="math inline">\(k\)</span> is a finite positive integer. For each <span class="math inline">\(j = 1 ,\cdots, k\)</span>, let <span class="math inline">\(R_j\)</span> be the rejection region of a level <span class="math inline">\(\alpha\)</span> test of <span class="math inline">\(H_{0j}\)</span>. Suppose that for some <span class="math inline">\(i= 1,\cdots, k\)</span>, there exists a sequence of parameter points, <span class="math inline">\(\theta_l\in\Theta_i,\quad l=1,2,\cdots\)</span> such that</p>
<ul>
<li><ol style="list-style-type: lower-roman">
<li><span class="math inline">\(\lim_{l\to\infty}P_{\theta_l}(\mathbf X\in R_i)=\alpha\)</span>,</li>
</ol></li>
<li><ol start="2" style="list-style-type: lower-roman">
<li>for each <span class="math inline">\(j=1,\cdots,k,j\ne i,\lim_{l\to\infty}P_{\theta_l}(\mathbf X\in R_j)=1.\)</span></li>
</ol></li>
</ul>
<p>Then, the IUT with rejection region <span class="math inline">\(R=\bigcap_{j=1}^{k}R_j\)</span> is a size <span class="math inline">\(\alpha\)</span> test.</p>
<p><strong>Proof:</strong> By Theorem <a href="#theorem-level-alpha-test-iut">Level <span class="math inline">\(\alpha\)</span> test IUT</a> , <span class="math inline">\(R\)</span> is a level <span class="math inline">\(\alpha\)</span> test, that is,
<span class="math display">\[\sup_{\theta\in\Theta_0}P_{\theta}(\mathbf X\in R)\le\alpha.\]</span> But, because all the parameter points <span class="math inline">\(\theta_l\)</span> satisfy <span class="math inline">\(\theta_l\in\Theta_i\subset\Theta_0,\)</span> <span class="math display">\[\begin{align}\sup_{\theta\in\Theta_0}P_{\theta}(\mathbf X\in R)&amp;\ge\lim_{l\to\infty}P_{\theta_l}(\mathbf X\in R)\\
&amp;=\lim_{l\to\infty}P_{\theta_l}\left(\mathbf X\in \bigcap_{j=1}^{k}R_j\right)\\
&amp;\ge\lim_{l\to\infty}\sum_{j=1}^{k}P_{\theta_l}(\mathbf X\in R_j)-(k-1)\quad\quad\quad(\text{Bonferroni&#39;s Inequality})\\
&amp;=(k-1)+\alpha-(k-1)\\
&amp;=\alpha.
\end{align}\]</span> <a href="#theorem-1.2.9-and-bonferroni-s-inequality">Bonferroni ’s Inequality</a> which and <span class="math inline">\(\sup_{\theta\in\Theta_0}P_{\theta}(\mathbf X\in R)\le\alpha\)</span> imply the test has size exactly equal to <span class="math inline">\(\alpha\)</span>.</p>
</div>
</div>
<div id="p-values" class="section level3">
<h3><span class="math inline">\(p\)</span>-Values</h3>
<p><strong>Definition</strong> A <span class="math inline">\(p\)</span>-value <span class="math inline">\(p(\mathbf X)\)</span> is a test statistic satisfying <span class="math inline">\(0\le p(\mathbf x)\le 1\)</span> for every sample point <span class="math inline">\(\mathbf x\)</span>. Small values of <span class="math inline">\(p(\mathbf X)\)</span> give evidence that <span class="math inline">\(H_1\)</span> is true. A <span class="math inline">\(p\)</span>-value is <em>valid</em> if, for every <span class="math inline">\(\theta\in\Theta_0\)</span> and every <span class="math inline">\(0\le\alpha\le1\)</span>, <span class="math display">\[P_{\theta}(p(\mathbf X)\le\alpha)\le\alpha\]</span></p>
<div id="theorem-valid-p-value" class="section level4">
<h4>Theorem: valid <span class="math inline">\(p\)</span>-value</h4>
<p>Let <span class="math inline">\(W(\mathbf X)\)</span> be a test statistic such that large values of <span class="math inline">\(W\)</span> give evidence that <span class="math inline">\(H_1\)</span> is true. For each sample point <span class="math inline">\(\mathbf x\)</span>, define <span class="math display">\[p(\mathbf x)=\sup_{\theta\in\Theta_0}P_{\theta}(W(\mathbf X)\ge W(\mathbf x)).\]</span> Then <span class="math inline">\(p(\mathbf X)\)</span> is a valid <span class="math inline">\(p\)</span>-value.</p>
<p><strong>Proof:</strong> Fix <span class="math inline">\(\theta\in\Theta_0\)</span>. Let <span class="math inline">\(F_{\theta}(w)\)</span> denote the cdf of <span class="math inline">\(-W(\mathbf X)\)</span>. Define <span class="math display">\[p_{\theta}(\mathbf x)=p_{\theta}(W(\mathbf X)\ge W(\mathbf x))=P_{\theta}(-W(\mathbf X)\le -W(\mathbf x))=F_{\theta}(-W(\mathbf x))\]</span> Then the random variable <span class="math inline">\(p_{\theta}(\mathbf X)\)</span> is equal to <span class="math inline">\(F_{\theta}(-W(\mathbf X))\)</span>. Hence, by the <a href="#probability-integral-transformation">Probability integral transformation</a>, the distribution of <span class="math inline">\(p_{\theta}(\mathbf X)\)</span> is stochastically greater than or equal to a <span class="math inline">\(uniform(0,1)\)</span> distribution. That is, for every <span class="math inline">\(0\le\alpha\le1\)</span>, <span class="math inline">\(P_{\theta}(p_{\theta}(\mathbf X)\le\alpha)\le\alpha\)</span>. Because <span class="math inline">\(p(\mathbf x)=\sup_{\theta&#39;\in\Theta_0}p_{\theta&#39;}(\mathbf x)\ge p_{\theta}(\mathbf x)\)</span> for every <span class="math inline">\(\mathbf x\)</span>, <span class="math display">\[P_{\theta}(p(\mathbf X)\le\alpha)\le P_{\theta}(p_{\theta}(\mathbf X)\le\alpha)\le\alpha\]</span> This is true for every <span class="math inline">\(\theta\in\Theta_0\)</span> and for every <span class="math inline">\(0\le\alpha\le1\)</span>, <span class="math inline">\(p(\mathbf X)\)</span> is a valid p-value.</p>
</div>
</div>
<div id="loss-function-optimality-1" class="section level3">
<h3>Loss Function Optimality</h3>
</div>
</div>
</div>
<div id="interval-estimation" class="section level1">
<h1>9. Interval Estimation</h1>
<div id="introduction-1" class="section level2">
<h2>Introduction</h2>
<p><strong>Definition: coverage probability</strong> For an interval estimator <span class="math inline">\([L(\mathbf X), U(\mathbf X)]\)</span> of a parameter <span class="math inline">\(\theta\)</span>, the <em>coverage probability</em> of <span class="math inline">\([L(\mathbf X), U(\mathbf X)]\)</span> is the probability that the random interval
<span class="math inline">\([L(\mathbf X), U(\mathbf X)]\)</span> covers the true parameter, <span class="math inline">\(\theta\)</span>. In symbols, it is denoted by either <span class="math inline">\(P_{\theta}(\theta\in[L(\mathbf X), U(\mathbf X)])\)</span> or <span class="math inline">\(P(\theta\in[L(\mathbf X), U(\mathbf X)]|\theta)\)</span>.</p>
<p><strong>Definition: confidence coefficient</strong><br />
(Confidence coefficient is a measure of confidence.)
For an interval estimator <span class="math inline">\([L(\mathbf X), U(\mathbf X)]\)</span> of a parameter <span class="math inline">\(\theta\)</span>, the
confidence coefficient of <span class="math inline">\([L(\mathbf X), U(\mathbf X)]\)</span> is the infimum of the coverage probabilities,
<span class="math inline">\(\inf_{\theta}P_{\theta}(\theta\in[L(\mathbf X), U(\mathbf X)])\)</span>.</p>
<p>It is important to keep in mind that tbe <strong>interval</strong> is the random quantity, not the parameter. Therefore, when we write probability statements such as <span class="math inline">\(P_{\theta}(\theta\in[L(\mathbf X), U(\mathbf X)])\)</span>, these probability statements refer to <span class="math inline">\(\mathbf X\)</span>, not <span class="math inline">\(\theta\)</span>.</p>
<p><strong>Interval estimators</strong>, together with a measure of confidence (usually a confidence coefficient) , are sometimes known as <strong>confidence internals</strong>.</p>
</div>
<div id="methods-of-finding-interval-estimators" class="section level2">
<h2>Methods of Finding Interval Estimators</h2>
<div id="inverting-a-test-statistic" class="section level3">
<h3>Inverting a Test Statistic</h3>
<div id="inverting-a-normal-test" class="section level4">
<h4>Inverting a normal test</h4>
<p>Let <span class="math inline">\(X_1,\cdots, X_n\)</span> be iid <span class="math inline">\(n(\mu, \sigma^2)\)</span>. The <em>acceptance region</em> of the hypothesis test, the test in the sample space for which <span class="math inline">\(H_0:\mu=\mu_0\)</span> is accepted, is given by <span class="math display">\[A(\mu_0)=\bigg\{(x_1,\cdots,x_n):\mu_0-z_{\alpha/2}\frac{\sigma}{\sqrt{n}}\le\bar x\le \mu_0+z_{\alpha/2}\frac{\sigma}{\sqrt{n}}\bigg\}\]</span> and the <em>confidence interval</em>, the set in the parameter sapce with plausible values of <span class="math inline">\(\mu\)</span>, is given by <span class="math display">\[C(x_1,\cdots,x_n)=\bigg\{\mu:\bar x-z_{\alpha/2}\frac{\sigma}{\sqrt{n}}\le\mu\le \bar x+z_{\alpha/2}\frac{\sigma}{\sqrt{n}}\bigg\}\]</span> These sets are connected to each other by the tautology: <span class="math display">\[(x_1,\cdots,x_n)\in A(\mu_0)\iff\mu_0\in C(x_1,\cdots,x_n).\]</span></p>
</div>
<div id="theorem-acceptance-regions-of-tests-and-confidence-sets" class="section level4">
<h4>Theorem: acceptance regions of tests and confidence sets</h4>
<p>For each <span class="math inline">\(\theta_0\in\Theta\)</span>, let <span class="math inline">\(A(\theta_0)\)</span> be the acceptance region of a level <span class="math inline">\(\alpha\)</span> test of <span class="math inline">\(H_0:\theta=\theta_0\)</span>. For each <span class="math inline">\(\mathbf x \in\mathcal X\)</span>, define a set <span class="math inline">\(C(\mathbf x)\)</span> in the parameter space by <span class="math display">\[C(\mathbf x)=\{\theta_0:\mathbf x\in A(\theta_0)\}\]</span> Then the random set <span class="math inline">\(C(\mathbf X)\)</span> is a <span class="math inline">\(1 - \alpha\)</span> confidence set. Conversely, let <span class="math inline">\(C(\mathbf X)\)</span> be a <span class="math inline">\(1 - \alpha\)</span>
confidence set. For any <span class="math inline">\(\theta_0\in\Theta\)</span>, define <span class="math display">\[A(\theta_0)=\{\mathbf x:\theta_0\in C(\mathbf x)\}\]</span> Then <span class="math inline">\(A(\theta_0)\)</span> is the acceptance region of a level <span class="math inline">\(\alpha\)</span> test of <span class="math inline">\(H_0:\theta=\theta_0\)</span>.</p>
<p><strong>Proof:</strong> For the first part, since <span class="math inline">\(A(\theta_0)\)</span> is the acceptance region of a level <span class="math inline">\(\alpha\)</span> test, <span class="math display">\[P_{\theta_0}(\mathbf X\notin A(\theta_0))\le\alpha\quad\text{and hence}\quad P_{\theta_0}(\mathbf X\in A(\theta_0))\ge 1-\alpha\]</span> Since <span class="math inline">\(\theta_0\)</span> is arbitrary, write <span class="math inline">\(\theta\)</span> instead of <span class="math inline">\(\theta_0\)</span>. The above inequality, together with <span class="math display">\[C(\mathbf x)=\{\theta_0:\mathbf x\in A(\theta_0)\}\]</span>
shows that the coverage probability of the set <span class="math inline">\(C(\mathbf X)\)</span> is given by <span class="math display">\[P_{\theta}(\theta\in C(\mathbf X))=P_{\theta}(\mathbf X\in A(\theta_0))\ge 1-\alpha,\]</span> showing that <span class="math inline">\(C(\mathbf X)\)</span> is a <span class="math inline">\(1-\alpha\)</span> confidence set.</p>
<p>For the second part, the Type I Error probability for the test of <span class="math inline">\(H_0:\theta=\theta_0\)</span> with acceptance region <span class="math inline">\(A(\theta_0)\)</span> is <span class="math display">\[P_{\theta_0}(\mathbf X\notin A(\theta_0))=P_{\theta_0}(\theta_0\notin C(\mathbf X))\le\alpha.\]</span> So this is a level <span class="math inline">\(\alpha\)</span> test.</p>
</div>
</div>
<div id="pivotal-quantities" class="section level3">
<h3>Pivotal Quantities</h3>
<p><strong>Definition: Pivotal Quantities</strong> A random variable
<span class="math inline">\(Q(\mathbf X, \theta) = Q(X_1, \cdots, X_n, \theta)\)</span> is a pivotal quantity (or pivot) if the distribution of <span class="math inline">\(Q(\mathbf X, \theta)\)</span> is independent of all parameters. That is, if <span class="math inline">\(\mathbf X \sim F(\mathbf x|\theta)\)</span>, then <span class="math inline">\(Q(\mathbf X, \theta)\)</span> has the same distribution for all values of <span class="math inline">\(\theta\)</span>.</p>
</div>
<div id="pivoting-the-cdf" class="section level3">
<h3>Pivoting the CDF</h3>
<div id="pivoting-a-continuous-cdf" class="section level4">
<h4>Pivoting a continuous cdf</h4>
<p>Let <span class="math inline">\(T\)</span> be a statistic with continuous cdf <span class="math inline">\(F_{T}(t|\theta)\)</span>. Let <span class="math inline">\(\alpha_1 + \alpha_2 = \alpha\)</span> with <span class="math inline">\(0 &lt; \alpha &lt; 1\)</span> be fixed values. Suppose that for each <span class="math inline">\(t \in \mathcal T\)</span>, the functions <span class="math inline">\(\theta_L(t)\)</span> and <span class="math inline">\(\theta_U(t)\)</span> can be defined as follows.</p>
<ul>
<li><p><strong>i.</strong> If <span class="math inline">\(F_{T}(t|\theta)\)</span> is a decreasing function of <span class="math inline">\(\theta\)</span> for each <span class="math inline">\(t\)</span>, define <span class="math inline">\(\theta_L(t)\)</span> and <span class="math inline">\(\theta_U(t)\)</span> by <span class="math display">\[F_{T}(t|\theta_U(t))=\alpha_1;\quad F_{T}(t|\theta_L(t))=1-\alpha_2\]</span></p></li>
<li><p><strong>ii.</strong> If <span class="math inline">\(F_{T}(t|\theta)\)</span> is an increasing function of <span class="math inline">\(\theta\)</span> for each <span class="math inline">\(t\)</span>, define <span class="math inline">\(\theta_L(t)\)</span> and <span class="math inline">\(\theta_U(t)\)</span> by <span class="math display">\[F_{T}(t|\theta_L(t))=\alpha_1;\quad F_{T}(t|\theta_U(t))=1-\alpha_2\]</span> Then the random interval <span class="math inline">\([\theta_L(t), \theta_U(t)]\)</span> is a <span class="math inline">\(1-\alpha\)</span> confidence interval for <span class="math inline">\(\theta\)</span>.</p></li>
</ul>
<p><strong>Proof:</strong> Assume that we have constructed the <span class="math inline">\(1-\alpha\)</span> acceptance region <span class="math display">\[\big\{t:\alpha_1\le F_{T}(t|\theta_0)\le1-\alpha_2\big\}\]</span> Since <span class="math inline">\(F_{T}(t|\theta)\)</span> is a decreasing function of <span class="math inline">\(\theta\)</span> for each <span class="math inline">\(t\)</span> and <span class="math inline">\(1-\alpha_2&gt;\alpha_1,\quad \theta_L(t) &lt; \theta_U(t)\)</span>, and the values <span class="math inline">\(\theta_L(t)\)</span> and <span class="math inline">\(\theta_U(t)\)</span> are unique. Also,
<span class="math display">\[F_{T}(t|\theta)&lt;\alpha_1\iff \theta&gt;\theta_U(t),\]</span>
<span class="math display">\[F_{T}(t|\theta)&gt;1-\alpha_2\iff \theta&lt;\theta_L(t),\]</span> and hence <span class="math display">\[\big\{\theta:\alpha_1\le F_{T}(t|\theta)\le1-\alpha_2\big\}=\big\{\theta:\theta_L(T) \le\theta\le \theta_U(T)\big\}.\]</span></p>
</div>
<div id="pivoting-a-discrete-cdf" class="section level4">
<h4>Pivoting a discrete cdf</h4>
<p>Let <span class="math inline">\(T\)</span> be a discrete statistic with cdf
<span class="math inline">\(F_T(t|\theta)=P(T\le t|\theta)\)</span>. <span class="math inline">\(\alpha_1 + \alpha_2 = \alpha\)</span> with <span class="math inline">\(0 &lt; \alpha &lt; 1\)</span> be fixed values. Suppose that for each <span class="math inline">\(t \in \mathcal T\)</span>, the functions <span class="math inline">\(\theta_L(t)\)</span> and <span class="math inline">\(\theta_U(t)\)</span> can be defined as follows.</p>
<ul>
<li><p><strong>i.</strong> If <span class="math inline">\(F_{T}(t|\theta)\)</span> is a decreasing function of <span class="math inline">\(\theta\)</span> for each <span class="math inline">\(t\)</span>, define <span class="math inline">\(\theta_L(t)\)</span> and <span class="math inline">\(\theta_U(t)\)</span> by <span class="math display">\[P(T\le t|\theta_U(t))=\alpha_1;\quad P(T\ge t|\theta_L(t))=\alpha_2\]</span></p></li>
<li><p><strong>ii.</strong> If <span class="math inline">\(F_{T}(t|\theta)\)</span> is an increasing function of <span class="math inline">\(\theta\)</span> for each <span class="math inline">\(t\)</span>, define <span class="math inline">\(\theta_L(t)\)</span> and <span class="math inline">\(\theta_U(t)\)</span> by <span class="math display">\[P(T\ge t|\theta_U(t))=\alpha_1;\quad P(T\le t|\theta_L(t))=\alpha_2\]</span> Then the random interval <span class="math inline">\([\theta_L(t), \theta_U(t)]\)</span> is a <span class="math inline">\(1-\alpha\)</span> confidence interval for <span class="math inline">\(\theta\)</span>.</p></li>
</ul>
<p><strong>Proof:</strong> Since <span class="math inline">\(F_T(T|\theta)\)</span> is stochastically greater
than a uniform random variable, that is, <span class="math inline">\(P_{\theta}(F_T(T|\theta)\le x)\le x\)</span>. Furthermore, this
property is shared by <span class="math inline">\(\bar F_T(T|\theta)=P(T\ge t|\theta)\)</span>, and this implies that the set
<span class="math display">\[\big\{\theta: F_{T}(T|\theta)\le\alpha_1\quad\text{and}\quad\bar F_T(T|\theta)\le\alpha_2\big\}.\]</span> is a <span class="math inline">\(1-\alpha\)</span> confidence set. The fact that <span class="math inline">\(F_T(t|\theta)\)</span> is a decreasing function of <span class="math inline">\(\theta\)</span> for each <span class="math inline">\(t\)</span> implies that <span class="math inline">\(\bar F_T(t|\theta)\)</span> is a nondecreasing function of <span class="math inline">\(\theta\)</span> for each <span class="math inline">\(t\)</span>. It therefore follows that
<span class="math display">\[\theta&gt;\theta_U(t)\Rightarrow F_{T}(t|\theta)&lt;\frac{\alpha}{2},\]</span>
<span class="math display">\[\theta&lt;\theta_L(t)\Rightarrow \bar F_{T}(t|\theta)&lt;\frac{\alpha}{2},\]</span> and hence
<span class="math display">\[\big\{\theta: F_{T}(T|\theta)\le\alpha_1\quad\text{and}\quad\bar F_T(T|\theta)\le\alpha_2\big\}=\big\{\theta:\theta_L(T)\le\theta\le\theta_U(T)\big\}.\]</span></p>
</div>
</div>
</div>
<div id="methods-of-evaluating-interval-estimators" class="section level2">
<h2>Methods of Evaluating Interval Estimators</h2>
<div id="size-and-coverage-probability" class="section level3">
<h3>Size and Coverage Probability</h3>
<div id="unimodal" class="section level4">
<h4>unimodal:</h4>
<p>A pdf <span class="math inline">\(f (x)\)</span> is <strong>unimodal</strong> if there exists <span class="math inline">\(x^*\)</span> such that <span class="math inline">\(f(x)\)</span> is nondecreasing for <span class="math inline">\(x \le x^*\)</span> and <span class="math inline">\(f (x)\)</span> is nonincreasing for
<span class="math inline">\(x \ge x^*\)</span>.</p>
</div>
<div id="shortest-interval-theorem" class="section level4">
<h4>shortest interval Theorem</h4>
<p>Let <span class="math inline">\(f(x)\)</span> be a unimodal pdf. If the interval <span class="math inline">\([a, b]\)</span> satisfies</p>
<ul>
<li><strong>i.</strong> <span class="math inline">\(\int_{a}^{b} f (x) dx 1-\alpha,\)</span><br />
</li>
<li><strong>ii.</strong> <span class="math inline">\(f(a) = f(b) &gt; 0,\)</span><br />
</li>
<li><strong>iii.</strong> <span class="math inline">\(a \le x^* \le b, \text{ where } x^* \text{ is a mode of } f(x),\)</span> then <span class="math inline">\([a, b]\)</span> is the shortest among all intervals that satisfy (<strong>i</strong>).</li>
</ul>
<p><strong>Proof:</strong> Let <span class="math inline">\([a&#39;, b&#39;]\)</span> be any interval with <span class="math inline">\(b&#39; - a&#39; &lt; b - a\)</span>. We will show that this implies <span class="math inline">\(\int_{a&#39;}^{b&#39;} f(x) dx &lt; 1 - \alpha\)</span>. The result will be proved only for <span class="math inline">\(a&#39; \le a\)</span>, the proof being similar if <span class="math inline">\(a &lt; a&#39;\)</span>.
If <span class="math inline">\(b&#39; \le a\)</span>, then <span class="math inline">\(a&#39; \le b&#39; \le a \le x^*\le b\)</span> and <span class="math display">\[\begin{align}
\int_{a&#39;}^{b&#39;} f(x) dx &amp;\le f(b&#39;)(b&#39;-a&#39;)\\
&amp;\le f(a)(b&#39;-a&#39;)\\
&amp;&lt;f(a)(b-a)\\
&amp;\le\int_{a}^{b}f(x)dx\\
&amp;=1-\alpha
\end{align}\]</span></p>
<p>If <span class="math inline">\(b&#39;&gt;a\)</span>, then <span class="math inline">\(a&#39;\le a&lt;b&#39;&lt;b\)</span>. In this case, we can write
<span class="math display">\[\begin{align}
\int_{a&#39;}^{b&#39;} f(x) dx &amp;= \int_{a}^{b} f(x) dx+\int_{a&#39;}^{a} f(x)dx-\int_{b&#39;}^{b} f(x) dx\\
&amp;= 1-\alpha+\int_{a&#39;}^{a} f(x)dx-\int_{b&#39;}^{b} f(x) dx\\
&amp;&lt;1-\alpha
\end{align}\]</span>
Since
<span class="math display">\[\begin{align}
\int_{a&#39;}^{a} f(x) dx &amp;\le f(a)(a-a&#39;)\\
\end{align}\]</span> and
<span class="math display">\[\begin{align}
\int_{b&#39;}^{b} f(x) dx &amp;\ge f(b)(b-b&#39;)\\
\end{align}\]</span> Thus,</p>
<p><span class="math display">\[\begin{align}
\int_{a&#39;}^{a} f(x) dx -\int_{b&#39;}^{b} f(x) dx &amp;\le f(a)(a-a&#39;)-f(b)(b-b&#39;)\\
&amp;=f(a)[(a-a&#39;)-(b-b&#39;)]\quad (f(a)=f(b))\\
&amp;=f(a)[(b&#39;-a&#39;)-(b-a)]
\end{align}\]</span> which is negative if <span class="math inline">\((b&#39;-a&#39;)&lt;(b-a)\)</span> and <span class="math inline">\(f(a)&gt;0\)</span>.</p>
</div>
</div>
<div id="test-related-optimality" class="section level3">
<h3>Test-Related Optimality</h3>
<p><span class="math inline">\(\mathbf X\sim f(\mathbf x|\theta)\)</span>, and we construct a <span class="math inline">\(1 - \alpha\)</span>
confidence set for <span class="math inline">\(\theta\)</span>, <span class="math inline">\(C(\mathbf x)\)</span>, by inverting an acceptance region, <span class="math inline">\(A(\theta)\)</span>. The probability of coverage of <span class="math inline">\(C(\mathbf x)\)</span>, that is, the <strong>probability of true coverage</strong>, is the function of <span class="math inline">\(\theta\)</span> given by <span class="math inline">\(P_{\theta}(\theta \in C(\mathbf X))\)</span>. The <strong>Probability of false coverage</strong> is the function of <span class="math inline">\(\theta\)</span> and <span class="math inline">\(\theta&#39;\)</span> defined by
<span class="math display">\[P_{\theta}(\theta&#39; \in C(\mathbf X)),\theta\ne \theta&#39;, \text{ if }C(\mathbf X))=[L(\mathbf X), U(\mathbf X)],\]</span>
<span class="math display">\[P_{\theta}(\theta&#39; \in C(\mathbf X)),\theta&#39;&lt; \theta, \text{ if }C(\mathbf X))=[L(\mathbf X), \infty),\]</span>
<span class="math display">\[P_{\theta}(\theta&#39; \in C(\mathbf X)),\theta&#39;&gt; \theta, \text{ if }C(\mathbf X))=(-\infty, U(\mathbf X)],\]</span> the probability of covering <span class="math inline">\(\theta&#39;\)</span> when <span class="math inline">\(\theta\)</span> is the true parameter.</p>
<p>A <span class="math inline">\(1-\alpha\)</span> confidence set that minimizes the probability of false coverage over a class of <span class="math inline">\(1-\alpha\)</span> confidence sets is called a <strong>uniformly most accurate (UMA)</strong> confidence set.</p>
<div id="ump-yields-uma-theorem" class="section level4">
<h4>UMP yields UMA Theorem</h4>
<p>Let <span class="math inline">\(\mathbf X\sim f(\mathbf x|\theta)\)</span>, where <span class="math inline">\(\theta\)</span> is a real-valued parameter. For each <span class="math inline">\(\theta_0\in\Theta\)</span>, let <span class="math inline">\(A^*(\theta_0)\)</span> be the UMP level <span class="math inline">\(\alpha\)</span> acceptance region of a test of <span class="math inline">\(H_0 : \theta = \theta_0\)</span> versus <span class="math inline">\(H_1: \theta &gt; \theta_0\)</span>. Let <span class="math inline">\(C^*(\mathbf x)\)</span> be the <span class="math inline">\(1 - \alpha\)</span> confidence set formed by inverting the UMP acceptance regions. Then for any other <span class="math inline">\(1 - \alpha\)</span> confidence set <span class="math inline">\(C\)</span>, <span class="math display">\[P_{\theta}(\theta&#39; \in C^*(\mathbf X))\le P_{\theta}(\theta&#39; \in C(\mathbf X))\;\;\forall \theta&#39;&lt;\theta\]</span></p>
<p><strong>Proof:</strong> Let <span class="math inline">\(\theta&#39;\)</span> be any value less than <span class="math inline">\(\theta\)</span>. Let <span class="math inline">\(A(\theta&#39;)\)</span> be the acceptance region of the level <span class="math inline">\(\alpha\)</span> test of <span class="math inline">\(H_0 : \theta = \theta&#39;\)</span> obtained by inverting <span class="math inline">\(C\)</span>. Since <span class="math inline">\(A^*(\theta&#39;)\)</span> is the UMP acceptance region for testing <span class="math inline">\(H_0 : \theta = \theta&#39;\)</span> versus <span class="math inline">\(H_1 : \theta &gt; \theta&#39;\)</span>, and since <span class="math inline">\(\theta&#39;&lt;\theta\)</span>, we have
<span class="math display">\[\begin{align}
P_{\theta}(\theta&#39; \in C^*(\mathbf X))&amp;=P_{\theta}(\mathbf X \in A^*(\theta&#39;))\\
&amp;\le P_{\theta}(\mathbf X \in A(\theta&#39;))\quad (\text{Since }A^*\text{ is UMP})\\
&amp;=P_{\theta}(\theta&#39; \in C(\mathbf X)).
\end{align}\]</span><br />
Therefore, we have established that for <span class="math inline">\(\theta&#39;&lt;\theta\)</span>, the probability of false coverage is minimized by the interval obtained from inverting the UMP test.</p>
</div>
<div id="unbiased-test" class="section level4">
<h4>unbiased test</h4>
<p>A <span class="math inline">\(1-\alpha\)</span> confidence set <span class="math inline">\(C(\mathbf x)\)</span> is <strong>unbiased</strong> if
<span class="math inline">\(P_{\theta}(\theta&#39; \in C(\mathbf X))\le1-\alpha\quad\forall\theta\ne\theta&#39;\)</span>. Unbiased test is one in which the power in the alternative is always greater than the power in the null.</p>
<p>Thus, for an unbiased confidence set, the probability of false coverage is never more than the minimum probability of true coverage. Unbiased confidence sets can be obtained by inverting unbiased tests. That is, if <span class="math inline">\(A(\theta_0)\)</span> is an unbiased level <span class="math inline">\(\alpha\)</span> acceptance region of a test of <span class="math inline">\(H_0:\theta=\theta_0\)</span> versus <span class="math inline">\(H_1:\theta\ne\theta_0\)</span> and <span class="math inline">\(C(\mathbf x)\)</span> is the <span class="math inline">\(1-\alpha\)</span> confidence set formed by inverting the acceptance regions, then <span class="math inline">\(C(\mathbf x)\)</span> is an unbiased <span class="math inline">\(1-\alpha\)</span> confidence set.</p>
</div>
<div id="pratt-theorem" class="section level4">
<h4>Pratt Theorem</h4>
<p>Let <span class="math inline">\(X\)</span> be a real-valued random variable with <span class="math inline">\(X\sim f(x|\theta)\)</span>,
where <span class="math inline">\(\theta\)</span> is a real-valued parameter. Let
<span class="math inline">\(C(x) = [L(x) , U(x)]\)</span> be a confidence interval for <span class="math inline">\(\theta\)</span>. If <span class="math inline">\(L(x)\)</span> and <span class="math inline">\(U(x)\)</span> are both increasing junctions of <span class="math inline">\(x\)</span>, then for any value <span class="math inline">\(\theta^*\)</span>, <span class="math display">\[\mathbb E_{\theta^*}(\text{Length}[C(\mathbf X)])=\int_{\theta\ne\theta^*}P_{\theta^*}(\theta\in C(\mathbf X))d\theta.\]</span></p>
<p><strong>Proof:</strong> From the definition of expected value we can write <span class="math display">\[\begin{align}
\mathbb E_{\theta^*}(\text{Length}[C(X)])&amp;=\int_{\mathcal X}\text{Length}[C(x)]f(x|\theta^*)dx\\
&amp;=\int_{\mathcal X}[U(x)-L(x)]f(x|\theta^*)dx\\
&amp;=\int_{\mathcal X}\left[\int_{L(x)}^{U(x)}d\theta\right]f(x|\theta^*)dx\\
&amp;=\int_{\Theta}\left[\int_{U^{-1}(\theta)}^{L^{-1}(\theta)}f(x|\theta^*)dx\right]d\theta\\
&amp;=\int_{\Theta}\left[P_{\theta^*}(U^{-1}(\theta)\le X\le L^{-1}(\theta))\right]d\theta\\
&amp;=\int_{\Theta}\left[P_{\theta^*}(\theta\in C(X))\right]d\theta\\
&amp;=\int_{\theta\ne\theta^*}P_{\theta^*}(\theta\in C(\mathbf X))d\theta\\
\end{align}\]</span> The inversion of the confidence interval is standard, where we use the relationship
<span class="math display">\[\theta\in\{\theta:L(x)\le\theta\le U(x)\}\iff x\in\{x:U^{-1}(\theta)\le x\le L^{-1}(\theta)\},\]</span> which is valid because of the assumption that <span class="math inline">\(L\)</span> and <span class="math inline">\(U\)</span> are increasing.</p>
</div>
</div>
</div>
</div>
<div id="asymptotic-evaluations" class="section level1">
<h1>10. Asymptotic Evaluations</h1>
<div id="point-estimation-1" class="section level2">
<h2>Point Estimation</h2>
<div id="consistency" class="section level3">
<h3>Consistency</h3>
<div id="definition-consistent-sequence-of-estimators" class="section level4">
<h4>Definition: consistent sequence of estimators</h4>
<p>A sequence of estimators <span class="math inline">\(W_n = W_n(X_1, \cdots, X_n)\)</span> is a <strong>consistent sequence of estimators</strong> of the parameter <span class="math inline">\(\theta\)</span> if, for every <span class="math inline">\(\epsilon &gt; 0\)</span> and every <span class="math inline">\(\theta\in\Theta\)</span>, <span class="math display">\[\lim_{n\to\infty}P_{\theta}(|W_n-\theta|&lt;\epsilon)=1.\]</span> An equivalent statement is this: For every <span class="math inline">\(\epsilon &gt; 0\)</span> and every <span class="math inline">\(\theta\in\Theta\)</span>, a consistent sequence <span class="math inline">\(W_n\)</span> will satisfy <span class="math display">\[\lim_{n\to\infty}P_{\theta}(|W_n-\theta|\ge\epsilon)=0.\]</span> Definition says that a consistent sequence of estimators converges
in probability to the parameter <span class="math inline">\(\theta\)</span> it is estimating.</p>
<p>This definition should be compared with <a href="#convergence-in-probability">Convergence in Probability</a>. Definition of Convergence in Probability dealt with one sequence of random variables with one probability structure, Definition of consistent sequence of estimators deals with an entire family o f probability structures, indexed by <span class="math inline">\(\theta\)</span>. For each different value of <span class="math inline">\(\theta\)</span>, the probability structure associated with the sequence <span class="math inline">\(W_n\)</span> is different. And the definition says that for each value of <span class="math inline">\(\theta\)</span>, the probability structure is such that the sequence converges in probability to the true <span class="math inline">\(\theta\)</span>. This is the usual difference between a probability definition and a statistics definition. The probability definition deals with one probability structure, but the statistics definition deals with an entire family.</p>
<p>Recall that, for an estimator <span class="math inline">\(W_n\)</span> <a href="#chebyshevs-inequality">Chebychev’s Inequality</a> states
<span class="math display">\[P_{\theta}(|W_{\theta}-\theta|\ge \epsilon\sigma)\le \frac{\mathbb E_{\theta}[(W_n-\theta)^2]}{\sigma^2\epsilon^2}\]</span> so if for every <span class="math inline">\(\theta\in\Theta\)</span>, <span class="math display">\[\lim_{n\to\infty}\mathbb E_{\theta}[(W_n-\theta)^2]=0\]</span> then the sequence of estimators is consistent.</p>
<p>Since <span class="math inline">\(\mathbb E_{\theta}[(W_n-\theta)^2]=\mathbb V_{\theta}W_n+[\text{Bias}_{\theta}W_n]^2\)</span>, Then the following theorem:</p>
<p>If <span class="math inline">\(W_n\)</span> is a sequence of estimators of a parameter <span class="math inline">\(\theta\)</span> satisfying</p>
<ul>
<li><span class="math inline">\(\lim_{n\to\infty}\mathbb V_{\theta}W_n=0\)</span>,</li>
<li><span class="math inline">\(\lim_{n\to\infty}\text{Bias}_{\theta}W_n=0\)</span>,</li>
</ul>
<p>for every <span class="math inline">\(\theta\in\Theta\)</span>, then <span class="math inline">\(W_n\)</span> is a consistent sequence of estimators of <span class="math inline">\(\theta\)</span>.</p>
</div>
</div>
</div>
</div>
<div id="analysis-of-variance-and-regression" class="section level1">
<h1>11. Analysis of Variance and Regression</h1>
<ul>
<li><p>One way ANOVA equation: <span class="math display">\[Y_{ij} = \theta_{i} + \epsilon_{ij}, \text{ with: } i = 1,..., k \text{ and } j = 1,..., n_{i}\]</span></p>
<ul>
<li><span class="math inline">\(i\)</span> = treatment</li>
<li><span class="math inline">\(j\)</span> = observation</li>
</ul></li>
<li><p>Assumptions:</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(E[\epsilon_{ij}] = 0 , \text{ Var}\epsilon_{ij} = \sigma^2_{i} &lt; \infty\)</span></p>
<p><span class="math inline">\(\text{Cov}(\epsilon_{ij}\epsilon_{i&#39;j&#39;}) = 0, \text{ unless } i=i&#39; \text{ and } j=j&#39;\)</span></p></li>
<li><p><span class="math inline">\(\epsilon_{ij}\)</span> are independently and normally distributed</p></li>
<li><p>Homoscedasicity</p></li>
</ol></li>
<li><p>If data doesn’t meet the assumptions can transform or rely on CLT</p></li>
<li><p>Requires identifiability, each <span class="math inline">\(\theta\)</span> has a unique distribution</p>
<ul>
<li>Otherwise it’s impossible to tell label future observations<br />
</li>
</ul></li>
<li><p>Hypothesises:</p>
<ul>
<li><span class="math inline">\(H_0: \theta_1 = \theta_2 = ... = \theta_k\)</span></li>
<li><span class="math inline">\(H_1: \theta_i \neq \theta_j\)</span> for some <span class="math inline">\(i, j\)</span></li>
</ul></li>
<li><p>Pooled variance: <span class="math display">\[S^{2}_{p} = \frac{1}{N-k}\sum^{k}_{i=1}(n_{1}-1)S^{2}_{1} = \frac{1}{N-k}\sum^{k}_{i=1}\sum^{n_{i}}_{j=1}(Y_{ij}-\bar{Y_{i}})^{2}\]</span></p></li>
<li><p>t statistic: <span class="math display">\[\abs{\frac{\bar{Y_{1.}}\bar{Y_{2.}}}{\sqrt{S^{2}_{p}(\frac{1}{n_1} + \frac{1}{n_2}})}} &gt; t_{N-k, \alpha/2}\]</span></p></li>
<li><p>ANOVA F test: <span class="math display">\[\text{reject } H_0 \text{ if } F = \frac{\sum^{k}_{i=1}n_{i}((\bar{Y_{i.}} - \bar{\bar{Y}}))^{2}/(k-1)}{S^{2}_{p}} &gt; F_{k-1, N-K, \alpha}\]</span></p></li>
<li><p>Sum of squares = (SS within) + (SS between)</p>
<ul>
<li>This is easy to look at than some complicated equation</li>
<li>Are chi-square random variables</li>
<li>If <span class="math inline">\(H_0\)</span> is true:
<ul>
<li>(SS between) $ ^2_{k-1}$</li>
<li>(SS total) $ ^2_{N-1}$</li>
</ul></li>
</ul></li>
</ul>
<div id="simple-linear-regression" class="section level3">
<h3>Simple Linear Regression</h3>
<ul>
<li>Population regression function: <span class="math inline">\(E[Y_i] = \beta_0 + \beta x_i\)</span>
<ul>
<li>Assumes: <span class="math inline">\(E[\epsilon] = 0\)</span></li>
</ul></li>
<li>Another form: <span class="math inline">\(E[Y_i|x_i] \approx \beta_0 + \beta x_i\)</span></li>
</ul>
<div id="algebraic-solution" class="section level4">
<h4>Algebraic solution</h4>
<ul>
<li>Estimating perameters:
<ul>
<li><span class="math inline">\(\beta_0 = \bar{y} - \beta\bar{x}\)</span></li>
<li><span class="math inline">\(\beta = \frac{S_{xy}}{S_{xx}}\)</span></li>
</ul></li>
</ul>
</div>
<div id="statistical-solution" class="section level4">
<h4>Statistical solution</h4>
<ul>
<li>Assume the form: <span class="math inline">\(Y_i = \beta_0 + \beta x_i + \epsilon_i \text{ for } i = 1,..., n\)</span></li>
<li><span class="math inline">\(\epsilon_i,..., \epsilon_n\)</span> are uncorrelated random variables with: <span class="math display">\[E[\epsilon_i] = 0 \text{ and } \text{Var}\epsilon_i = \sigma^2\]</span>
<ul>
<li><span class="math inline">\(\epsilon_i\)</span> are random errors, therefore <span class="math inline">\(Y_i\)</span> are uncorrelated</li>
</ul></li>
<li>Linear estimator: <span class="math display">\[\sum^{n}_{i=1}d_iY_i\]</span>
<ul>
<li>Must satisfy <span class="math inline">\(E[\sum^{n}_{i=1}d_iY_i] = \beta\)</span>
<ul>
<li>Holds true for <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(beta\)</span> if and only if: <span class="math inline">\(\sum^n_{i=1}d_i = 0\)</span> and <span class="math inline">\(\sum^n_{i=1}d_{i}x_{i} = 1\)</span></li>
</ul></li>
<li><span class="math inline">\(d_i\)</span> is BLUE if it minimizes the variance</li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="regression-models" class="section level1">
<h1>12. Regression Models</h1>
</div>
<div id="references" class="section level1 unnumbered">
<h1>References</h1>
<div id="refs" class="references csl-bib-body">
<div id="ref-casella2021statistical" class="csl-entry">
1. Casella G, Berger RL. Statistical inference. Cengage Learning; 2021.
</div>
</div>
</div>

    </div>
  </article>

  


</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
          </li>
          <li>
          </li>
        </ul>
      </footer>

    </div>
    

    
<script src="../../../../js/math-code.js"></script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
  </body>
</html>

