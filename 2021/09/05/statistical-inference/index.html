<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.80.0" />


<title>Statistical Inference - A Hugo website</title>
<meta property="og:title" content="Statistical Inference - A Hugo website">


  <link href='../../../../favicon.ico' rel='icon' type='image/x-icon'/>



  







<link rel="stylesheet" href="../../../../css/fonts.css" media="all">
<link rel="stylesheet" href="../../../../css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="../../../../" class="nav-logo">
    <img src="../../../../images/logo.png"
         width="50"
         height="50"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="../../../../about/">about</a></li>
    
    <li><a href="../../../../vitae/">CV</a></li>
    
    <li><a href="https://github.com/danli349">GitHub</a></li>
    
    <li><a href="https://scholar.google.com/citations?user=mNjLK8EAAAAJ&amp;hl=en">Googlr Scholar</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">28 min read</span>
    

    <h1 class="article-title">Statistical Inference</h1>

    
    <span class="article-date">2021-09-05</span>
    

    <div class="article-content">
      
<script src="../../../../2021/09/05/statistical-inference/index_files/header-attrs/header-attrs.js"></script>

<div id="TOC">
<ul>
<li><a href="#common-families-of-distributions">3. Common Families of Distributions</a>
<ul>
<li><a href="#some-distributions">Some Distributions</a>
<ul>
<li><a href="#chi-squared-distribution">Chi-Squared Distribution</a></li>
<li><a href="#students-t-distribution">Student’s <span class="math inline">\(t\)</span>-Distribution</a></li>
<li><a href="#snedcors-f-distribution">Snedcor’s <span class="math inline">\(F\)</span>-Distribution</a></li>
<li><a href="#multinomial-distribution">Multinomial Distribution</a></li>
</ul></li>
<li><a href="#exponential-families">Exponential Families</a></li>
<li><a href="#location-and-scale-families">Location and Scale Families</a></li>
<li><a href="#inequalities-and-identities">Inequalities and Identities</a></li>
</ul></li>
<li><a href="#multiple-random-variables">4. Multiple Random Variables</a>
<ul>
<li><a href="#facts">Facts</a></li>
<li><a href="#bivariate-relations">Bivariate Relations</a></li>
<li><a href="#inequalities">Inequalities</a>
<ul>
<li><a href="#numerical-inequalities">Numerical Inequalities</a></li>
<li><a href="#functional-inequalities">Functional Inequalities</a></li>
</ul></li>
</ul></li>
<li><a href="#properties-of-a-random-sample">5. Properties of a Random Sample</a>
<ul>
<li><a href="#sampling-from-the-normal-distribution">Sampling from the Normal Distribution</a></li>
<li><a href="#convergence-concepts">Convergence Concepts</a>
<ul>
<li><a href="#convergence-in-probability">Convergence in Probability</a></li>
<li><a href="#almost-sure-convergence">Almost Sure Convergence</a></li>
<li><a href="#convergence-in-distribution">Convergence in Distribution</a></li>
<li><a href="#the-delta-method">The Delta Method</a></li>
<li><a href="#the-second-order-delta-method">The second-order Delta Method</a></li>
<li><a href="#multivariate-delta-method">Multivariate Delta Method</a></li>
</ul></li>
<li><a href="#generating-a-random-sample">Generating A Random Sample</a>
<ul>
<li><a href="#direct-method">Direct Method</a></li>
<li><a href="#indirect-methods">Indirect Methods</a></li>
<li><a href="#the-acceptreject-algorithm">The Accept/Reject Algorithm</a></li>
</ul></li>
</ul></li>
<li><a href="#principles-of-data-reduction">6. Principles of Data Reduction</a>
<ul>
<li><a href="#the-sufficiency-principle">The Sufficiency Principle</a></li>
<li><a href="#ancillary-statistic">Ancillary Statistic</a></li>
<li><a href="#complete-statistic">Complete Statistic</a></li>
<li><a href="#the-likelihood-principle">The Likelihood Principle</a></li>
<li><a href="#a-slightly-more-formal-construction">A Slightly More Formal Construction</a></li>
<li><a href="#the-equivariance-principle">The Equivariance Principle</a></li>
</ul></li>
<li><a href="#references">References</a></li>
</ul>
</div>

<div id="common-families-of-distributions" class="section level1">
<h1>3. Common Families of Distributions</h1>
<div id="some-distributions" class="section level2">
<h2>Some Distributions</h2>
<p>[Normal Distribution] If <span class="math inline">\(\mathbf{X} \sim \text{n}(\mathbf{\mu}, \boldsymbol{\Sigma})\)</span> then
<span class="math inline">\(\mathbf{X}\)</span> has pdf
<span class="math display">\[f_{\mathbf{X}}(x_1, \dots, x_k \vert{} \mathbf{\mu}, \boldsymbol{\Sigma})  = \frac{1}{\sqrt{(2\pi)^k \det\boldsymbol{\Sigma}}} \exp\left(-\frac12 (\mathbf{ x}-\mathbf{\mu})^\top \boldsymbol{\Sigma}^{-1}(\mathbf{x}-\mathbf{\mu})\right)\]</span>.</p>
<div id="chi-squared-distribution" class="section level3">
<h3>Chi-Squared Distribution</h3>
<p>The <em>chi-squared distribution with <span class="math inline">\(p\)</span> degrees of freedom</em> has pdf
<span class="math display">\[\chi_p^2 \sim \frac{1}{\Gamma(p/2)2^{p/2}} x^{(p/2)-1} e^{-x/2}, \quad 0&lt; x&lt; \infty.\]</span></p>
<p>[Some facts]</p>
<ol style="list-style-type: lower-alpha">
<li><p>If <span class="math inline">\(Z \sim \text{n}(0, 1)\)</span> then <span class="math inline">\(Z^2 \sim \chi_1^2\)</span></p></li>
<li><p>If <span class="math inline">\(X_1, \dots, X_n\)</span> are independent <span class="math inline">\(X_i \sim \chi_{p_i}^2\)</span> then
<span class="math inline">\(X_1 + \cdots + X_n \sim \chi_{p_1 + \cdots + p_n}^2\)</span>.</p></li>
</ol>
</div>
<div id="students-t-distribution" class="section level3">
<h3>Student’s <span class="math inline">\(t\)</span>-Distribution</h3>
<p><a href="#students-t-distribution">Student’s <span class="math inline">\(t\)</span>-distribution</a> <span class="math inline">\(T \sim t_p\)</span>, a <em><span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(p\)</span>
degrees of freedom</em> if it has pdf
<span class="math display">\[f_T(t) = \frac{\Gamma\left(\frac{p+1}{2}\right)}{\Gamma\left(\frac{p}{2}\right)} \frac{1}{\sqrt{p\pi}} \frac{1}{(1 + t^2/p)^{(p+1)/2}}, \quad t \in \mathbb R{}\]</span></p>
<p>If <span class="math inline">\(p = 1\)</span> then this is the Cauchy distribution.</p>
<p>If <span class="math inline">\(X_1, \dots, X_n\)</span> are a random sample from <span class="math inline">\(\text{n}(\mu, \sigma^2)\)</span> then
<span class="math display">\[\frac{\bar{X} - \mu}{S/\sqrt{n}} \sim t_{n-1}.\]</span> This is often taken
as the definition. Note that the denominator is independent of the
numerator.</p>
<p>[Moments and mgf of <span class="math inline">\(t\)</span>-distribution] Student’s <span class="math inline">\(t\)</span> has no mgf because
it does not have moments of all orders: <span class="math inline">\(t_p\)</span> has only <span class="math inline">\(p-1\)</span> moments. If
<span class="math inline">\(T_p \sim t_p\)</span> then</p>
<p><span class="math display">\[\begin{aligned}
        \mathbb E{}[T_p] &amp;= 0 \quad p &gt; 1 \\
        \mathbb V{}[T_p] &amp;= \frac{p}{p-2} \quad p &gt; 2
    \end{aligned}\]</span></p>
</div>
<div id="snedcors-f-distribution" class="section level3">
<h3>Snedcor’s <span class="math inline">\(F\)</span>-Distribution</h3>
<p><a href="#snedcors-f-distribution">Snedcor’s <span class="math inline">\(F\)</span>-distribution</a> A random variable <span class="math inline">\(X \sim F_{p,q}\)</span> has
<em><span class="math inline">\(F\)</span>-distribution with p and q degrees of freedom</em> if its pdf is
<span class="math display">\[f_X(x) = \frac{\Gamma\left(\frac{p+q}{2}\right)}{\Gamma\left(\frac p2 \right)\Gamma\left(\frac q2 \right)} (p/q)^{p/2} \frac{x^{(p/2) - 1}}{(1 + px/q)^{(p+q)/2}}, \quad 0 &lt; x &lt; \infty.\]</span></p>
<p>If <span class="math inline">\(X_1, \dots, X_n\)</span> is a random sample from <span class="math inline">\(\text{n}(\mu_X, \sigma_X^2)\)</span> and
<span class="math inline">\(Y_1, \dots, Y_m\)</span> is an independent random sample from
<span class="math inline">\(\text{n}(\mu_Y, \sigma_Y^2)\)</span>, then
<span class="math display">\[\frac{S^2_X/\sigma^2_X}{S_Y^2/\sigma_Y^2} \sim F_{n-1, m-1}.\]</span> This is
often taken as the definition.</p>
<p>[Some facts]</p>
<ol style="list-style-type: lower-alpha">
<li><p><span class="math inline">\(X \sim F_{p,q} \,\, \implies \,\, 1/X \sim F_{q,p}\)</span></p></li>
<li><p><span class="math inline">\(X \sim t_q \,\, \implies \,\, X^2 \sim F_{1,q}\)</span></p></li>
<li><p><span class="math inline">\(X \sim F_{p,q} \,\, \implies \,\, \frac{(p/q)X}{1 + (p/q)X} \sim \text{beta}(p/2, q/2)\)</span></p></li>
</ol>
</div>
<div id="multinomial-distribution" class="section level3">
<h3>Multinomial Distribution</h3>
<p><a href="#multinomial-distribution">Multinomial Distribution</a> Let <span class="math inline">\(m\)</span> and <span class="math inline">\(n\)</span> be positive integers and let
<span class="math inline">\(p_1, \dots, p_n \in [0, 1]\)</span> satisfy <span class="math inline">\(\sum_{i=1}^n p_i = 1\)</span>. Then the
random vector <span class="math inline">\((X_1, \dots, X_n)\)</span> has <em>multinomial distribution with m
trials and cell probabilities <span class="math inline">\(p_1, \dots, p_n\)</span></em> if the joint pmf of
<span class="math inline">\((X_1, \dots, X_n)\)</span> is
<span class="math display">\[f(x_1, \dots , x_n) = \frac{m!}{x_1! \cdots x_n!} p_1^{x_1} \cdots
p_n^{x_n} = m! \prod_{i=1}^n \frac{p_i^{x_i}}{x_i!}\]</span> on the set of
<span class="math inline">\((x_1, \dots, x_n)\)</span> such that each <span class="math inline">\(x_i\)</span> is a nonnegative integer and
<span class="math inline">\(\sum_{i=1}^n x_i = m\)</span>.</p>
<p>The marginal distributions have <span class="math inline">\(X_i \sim \text{binomial}(m, p_i)\)</span>.</p>
<p>[Multinomial Theorem] Let <span class="math inline">\(m\)</span> and <span class="math inline">\(n\)</span> be positive integers and let
<span class="math inline">\(\mathcal{A}\)</span> be the set of vectors <span class="math inline">\(\mathbf{x} = (x_1, \dots, x_n)\)</span> such
that each <span class="math inline">\(x_i\)</span> is a nonnegative integer and <span class="math inline">\(\sum_{i=1}^n x_i = m\)</span>.
Then for any real numbers <span class="math inline">\(p_1, \dots, p_n\)</span>,
<span class="math display">\[(p_1 + \cdots + p_n)^m = \sum_{\mathbf{x} \in \mathcal{A}} \frac{m!}{x_1! \cdots x_n!} p_1^{x_1}\cdots p_n^{x_n}.\]</span></p>
</div>
</div>
<div id="exponential-families" class="section level2">
<h2>Exponential Families</h2>
<p>[Exponential family 1] A family of pmfs/pdfs is called an <em>exponential
family</em> if it can be expressed
<span class="math display">\[f(x|\mathbf{\theta}) = h(x)c(\mathbf{\theta})\exp\left( \sum_{i=1}^k w_i(\mathbf{\theta}) t_i(x) \right)\]</span>
where <span class="math inline">\(h(x) \geq 0\)</span>, the <span class="math inline">\(t_i\)</span> are real valued functions of the
observation <span class="math inline">\(x\)</span> that do not depend on <span class="math inline">\(\mathbf{\theta}\)</span> and
<span class="math inline">\(c(\theta) \geq 0\)</span> and the <span class="math inline">\(w_i(\mathbf{\theta})\)</span> are real valued functions
of <span class="math inline">\(\mathbf{\theta}\)</span> that do not depend on <span class="math inline">\(x\)</span>.</p>
<p>If <span class="math inline">\(X\)</span> is a random variable from an exponential family distribution then
<span class="math display">\[\mathbb E{}\left[ \sum_{i=1}^k \frac{\partial w_i(\mathbf{\theta})}{\partial \theta_j} t_i(X) \right] = - \frac{\partial}{\partial \theta_j} \log c(\mathbf{\theta})\]</span>
and
<span class="math display">\[\mathbb V\left[ \frac{\partial w_i(\mathbf{\theta})}{\partial \theta_j} t_i(X) \right] = - \frac{\partial^2}{\partial \theta_j^2} \log c(\mathbf{\theta}) - \mathbb E{}\left[ \sum_{i=1}^{k} \frac{\partial^2 w_i(\mathbf{\theta})}{\partial \theta_j^2} t_i(X) \right]\]</span></p>
<p>[Exponential family 2] We can write another parameterisation of the
exponential family
<span class="math display">\[f(x | \mathbf{\eta}) = h(x) c^{*}(\mathbf{\eta}) \exp(\mathbf{\eta} \cdot \mathbf{t}(x))\]</span>
where <span class="math inline">\(\mathbf{\eta}\)</span> is called the <em>natural parameter</em> and the set
<span class="math inline">\(\mathcal{H} = \{\mathbf{\eta} : \int_\mathbb R{} f(x|\eta) \text{d}x &lt; \infty \}\)</span> is called
the <em>natural parameter space</em> and is convex.</p>
<p><span class="math inline">\(\{\mathbf{\eta}: \mathbf{\eta} = \mathbf{w}(\mathbf{\theta}), \,\, \mathbf{\theta} \in \Theta\} \subseteq \mathcal{H}\)</span>.
So there may be more parameterisations here than previously.<br />
</p>
<p>The natural parameter provides a convenient mathematical formulation,
but sometimes lacks simple interpretation.</p>
<p>[Curved exponential family] A <em>curved exponential family</em> distribution
is one for which the dimension of <span class="math inline">\(\mathbf{\theta}\)</span> is <span class="math inline">\(d &lt; k\)</span>. If <span class="math inline">\(d = k\)</span>
then we have a <em>full exponential family</em>.</p>
</div>
<div id="location-and-scale-families" class="section level2">
<h2>Location and Scale Families</h2>
<p>[Location family] Let <span class="math inline">\(f(x)\)</span> be any pdf. The family of pdfs <span class="math inline">\(f(x - \mu)\)</span>
for <span class="math inline">\(\mu \in \mathbb R{}\)</span> is called the <em>location family with standard pdf
<span class="math inline">\(f(x)\)</span></em> and <span class="math inline">\(\mu\)</span> is the <em>location parameter</em> of the family.</p>
<p>[Scale family] Let <span class="math inline">\(f(x)\)</span> be any pdf. For any <span class="math inline">\(\sigma &gt; 0\)</span> the family of
pdfs <span class="math inline">\(\frac{1}{\sigma} f(x/\sigma)\)</span> is called the <em>scale family with
standard pdf <span class="math inline">\(f(x)\)</span></em> and <span class="math inline">\(\sigma\)</span> is the <em>scale parameter</em> of the
family.</p>
<p>[Location-Scale family] Let <span class="math inline">\(f(x)\)</span> be any pdf. For <span class="math inline">\(\mu \in \mathbb R{}\)</span> and
<span class="math inline">\(\sigma &gt; 0\)</span> the family of pdfs
<span class="math inline">\(\frac{1}{\sigma} f(\frac{x - \mu}{\sigma})\)</span> is called the
<em>location-scale family with standard pdf <span class="math inline">\(f(x)\)</span></em>; <span class="math inline">\(\mu\)</span> is the <em>location
parameter</em> and <span class="math inline">\(\sigma\)</span> is the <em>scale parameter</em>.</p>
<p>[Standardisation] Let <span class="math inline">\(f\)</span> be any pdf, <span class="math inline">\(\mu \in \mathbb R{}\)</span> and
<span class="math inline">\(\sigma \in \mathbb R{}_{&gt;0}\)</span>. Then <span class="math inline">\(X\)</span> is a random variable with pdf
<span class="math inline">\(\frac{1}{\sigma}f(\frac{x - \mu}{\sigma})\)</span> if and only if there exists
a random variable <span class="math inline">\(Z\)</span> with pdf <span class="math inline">\(f(z)\)</span> and <span class="math inline">\(X = \sigma Z + \mu\)</span>.</p>
<p>Probabilities of location-scale families can be computed in terms of
their standard variables <span class="math inline">\(Z\)</span>
<span class="math display">\[\text{P}(X \leq x) = \text{P}\left(Z \leq \frac{x - \mu}{\sigma} \right)\]</span></p>
</div>
<div id="inequalities-and-identities" class="section level2">
<h2>Inequalities and Identities</h2>
<p>[Chebychev’s inequality] Let <span class="math inline">\(X\)</span> be a random variable and let <span class="math inline">\(g(x)\)</span> be
a nonnegative function. Then, for any <span class="math inline">\(r &gt; 0\)</span>,
<span class="math display">\[\text{P}(g(X) \geq r) \leq \frac{\mathbb E{}[g(X)]}{r}.\]</span></p>
<p>This bound is conservative and almost never attained.</p>
<p>[Markov inequality] The Markov inequality is the special case with
<span class="math inline">\(g = \mathbb{I}\)</span>.</p>
<p>Let <span class="math inline">\(X_{\alpha, \beta}\)</span> denote a gamma<span class="math inline">\((\alpha, \beta)\)</span> random variable
with pdf <span class="math inline">\(f(x \vert{} \alpha, \beta)\)</span>, where <span class="math inline">\(\alpha &gt; 1\)</span>. Then for any
constants <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>:
<span class="math display">\[\text{P}(a &lt; X_{\alpha, \beta} &lt; b) = \beta (f(a \vert{} \alpha, \beta) - f(b \vert{} \alpha, \beta)) + \text{P}(a &lt; X_{\alpha - 1, \beta} &lt; b)\]</span></p>
<p>[Stein’s Lemma] Let <span class="math inline">\(X \sim \text{n}(\theta, \sigma^2)\)</span> and let <span class="math inline">\(g\)</span> be a
differentiable function with <span class="math inline">\(\mathbb E{}[g&#39;(x)] &lt; \infty\)</span>. Then
<span class="math display">\[\mathbb E{}[g(X)(X - \theta)] = \sigma^2 \mathbb E{}[g&#39;(X)]\]</span></p>
<p>The proof is just integration by parts.</p>
<p>Stein’s lemma is useful for moment calculations</p>
<p>Let <span class="math inline">\(\chi^2_p\)</span> denote a chi squared distribution with <span class="math inline">\(p\)</span> degrees of
freedom. For any function <span class="math inline">\(h(x)\)</span>,
<span class="math display">\[\mathbb E{}[h(\chi^2_p)] = p \mathbb E{}\left[\frac{h(\chi_{p+2}^2)}{\chi_{p+2}^2}\right]\]</span>
provided the expressions exist.</p>
<p>Let <span class="math inline">\(g(x)\)</span> be a function that is bounded at <span class="math inline">\(-1\)</span> and has finite
expectation, then</p>
<ol style="list-style-type: lower-alpha">
<li><p>If <span class="math inline">\(X \sim \text{Poisson}(\lambda)\)</span>,
<span class="math display">\[\mathbb E{}[\lambda g(X)] = \mathbb E{}[Xg(X-1)].\]</span></p></li>
<li><p>If <span class="math inline">\(X\sim \text{negative-binomial}(r, p)\)</span>,
<span class="math display">\[\mathbb E{}[(1-p)g(X)] = \mathbb E{}\left[ \frac{X}{r + X - 1}g(X) \right].\]</span></p></li>
</ol>
</div>
</div>
<div id="multiple-random-variables" class="section level1">
<h1>4. Multiple Random Variables</h1>
<div id="facts" class="section level2">
<h2>Facts</h2>
<ul>
<li><p>RVs are independent if and only if their pdfs factorise</p></li>
<li><p>Functions of independent RVs are independent</p></li>
<li><p>Expectations of products (and hence mgfs, etc. of sums) of
independent RVs factor</p></li>
<li><p>Variance of sum of independent RVs is sum of variances.</p></li>
<li><p>Independent RVs have vanishing covariance/correlation, but the
converse is not true in general.</p></li>
</ul>
</div>
<div id="bivariate-relations" class="section level2">
<h2>Bivariate Relations</h2>
<p>[Conditional Expectation] <span class="math display">\[\mathbb E{}[\mathbb E{}[X|Y]] = \mathbb E{}[X]\]</span> provided the
expectations exist.</p>
<p>[Conditional variance]
<span class="math display">\[\mathbb V[X] = \mathbb E{}[\mathbb V[X \vert{} Y]] + \mathbb V[\mathbb E{}[X \vert{} Y]]\]</span>
provided the expectations exist.</p>
<p>[Covariance] <span class="math display">\[\text{ Cov }[X, Y] = \mathbb E{}[(X - \mu_X)(Y - \mu_Y)]\]</span></p>
<p><span class="math display">\[\text{ Cov }[X, Y] = \mathbb E{}[XY] - \mu_X\mu_Y\]</span></p>
<p><span class="math display">\[\mathbb V[aX + bY] = a^2\mathbb V[X] + b^2\mathbb V[Y] + 2ab\text{Cov }[X, Y]\]</span></p>
<p>[Correlation] <span class="math display">\[\rho_XY = \frac{\text{Cov }[X, Y]}{\sigma_X \sigma_Y}\]</span></p>
<p>The correlation measures the strength of <em>linear</em> relation between two
RVs. It is possible to have strong non-linear relationships but with
<span class="math inline">\(\rho = 0\)</span>.</p>
<p>We can use an argument similar to the standard proof of Cauchy-Schwarz
to show the following</p>
<p>Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be any RVs, then</p>
<ol style="list-style-type: lower-alpha">
<li><p><span class="math inline">\(-1 \leq \rho_{XY} \leq 1\)</span>,</p></li>
<li><p><span class="math inline">\(\lvert{\rho_{XY}}\lvert = 1\)</span> if and only if there are constants
<span class="math inline">\(a \text{n}eq 0, b\)</span> such that <span class="math inline">\(\text{P}{}(Y = aX + b) = 1\)</span>. If
<span class="math inline">\(\lvert{\rho_{XY}}\lvert = 1\)</span> then <span class="math inline">\(\text{sign}(\rho) = \text{sign}(a)\)</span>.</p></li>
</ol>
<p>Let <span class="math inline">\(X_1, \cdots , X_n\)</span> be independent random vectors. Let <span class="math inline">\(g_i(x_i)\)</span> be a function only of <span class="math inline">\(x_i, i = 1 , \cdots, n\)</span>. Then the random variables <span class="math inline">\(U_i = g_i(X_i), i = 1,\cdots, n\)</span>, are mutually independent.</p>
</div>
<div id="inequalities" class="section level2">
<h2>Inequalities</h2>
<div id="numerical-inequalities" class="section level3">
<h3>Numerical Inequalities</h3>
<p>Let <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> be any positive numbers and let <span class="math inline">\(p, q &gt; 1\)</span> satisfy
<span class="math inline">\(1/p + 1/q = 1\)</span>, then <span class="math display">\[p+q=pq\]</span>
<span class="math display">\[q(p-1)=p\]</span>
<span class="math display">\[1-1/q=1/p\]</span>
<span class="math display">\[\frac1p a^p + \frac1q b^q \geq ab\]</span> with
equality if and only if <span class="math inline">\(a^p = b^q\)</span>.</p>
<p><strong>H<span class="math inline">\(\ddot{o}\)</span>lder’s Inequality</strong> Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be any random
variables and let <span class="math inline">\(p, q &gt; 1\)</span> satisfy <span class="math inline">\(1/p + 1/q = 1\)</span>, then
<span class="math display">\[\lvert{\mathbb E{}[XY]}\lvert \leq \mathbb E{}[\lvert{XY}\lvert] \leq \mathbb E{}[\lvert{X}\lvert^p]^{1/p} \mathbb E{}[\lvert{Y}\lvert^q]^{1/q}\]</span></p>
<ul>
<li><p><strong>Cauchy-Schwarz Inequality</strong> is the special case <span class="math inline">\(p = q = 2\)</span></p></li>
<li><p><span class="math inline">\(\text{Cov }[X, Y]^2 \leq \sigma_X^2 \sigma_Y^2\)</span></p></li>
<li><p><span class="math inline">\(\mathbb E{}[\lvert{X}\lvert] \leq \mathbb E{}[\lvert{X}\lvert^p]^{1/p}\)</span></p></li>
<li><p><strong>Liapounov’s Inequality</strong>
<span class="math inline">\(\mathbb E{}[\lvert{X}\lvert^r]^{1/r} \leq \mathbb E{}[\lvert{X}\lvert^s]^{1/s}\)</span> where
<span class="math inline">\(1 &lt; r &lt; s &lt; \infty\)</span>.</p></li>
<li><p>For numbers <span class="math inline">\(a_i,b_i,i=1,\cdots,n\)</span>, <span class="math display">\[\sum_{i=1}^{n}|a_ib_i|\leq\left(\sum_{i=1}^{n}a_i^p\right)^{1/p}\left(\sum_{i=1}^{n}b_i^q\right)^{1/q}\]</span> where <span class="math inline">\(\frac{1}{p}+\frac{1}{q}=1\)</span></p></li>
<li><p><span class="math display">\[\frac{1}{n}\left(\sum_{i=1}^{n}|a_i|\right)^2\leq\sum_{i=1}^{n}a_i^2\]</span></p></li>
</ul>
<p><strong>Minkowski’s Inequality</strong><br />
Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be any two random variables. Then for <span class="math inline">\(1\le p&lt;\infty\)</span>
<span class="math display">\[(\mathbb E|X+Y|^p)^{1/p}\le (\mathbb E|X|^p)^{1/p}+(\mathbb E|Y|^p)^{1/p}\]</span></p>
</div>
<div id="functional-inequalities" class="section level3">
<h3>Functional Inequalities</h3>
<p><strong>Convex Function</strong> A function <span class="math inline">\(g(x)\)</span> is <em>convex</em> on a set <span class="math inline">\(S\)</span> if for all
<span class="math inline">\(x, y \in S\)</span> and <span class="math inline">\(0&lt; \lambda &lt; 1\)</span>
<span class="math display">\[g(\lambda x + (1 - \lambda)y) \leq \lambda g(x) + (1 - \lambda)g(y).\]</span>
<strong>Strictly convex</strong> is when the inequality is strict. <span class="math inline">\(g\)</span> is <strong>concave</strong> if
<span class="math inline">\(-g\)</span> is convex.</p>
<p><span class="math inline">\(g(x)\)</span> is convex on <span class="math inline">\(S\)</span> if <span class="math inline">\(g&#39;&#39;(x) \geq 0\)</span> <span class="math inline">\(\forall x \in S\)</span>.</p>
<p><strong>Jensen’s Inequality</strong> If <span class="math inline">\(g(x)\)</span> is convex, then for any random variable
<span class="math inline">\(X\)</span> <span class="math display">\[\mathbb E{}[g(X)] \leq g(\mathbb E{}[X]).\]</span> Equality holds if and only if, for
every line <span class="math inline">\(a + bx\)</span> that is tangent to <span class="math inline">\(g(x)\)</span> at <span class="math inline">\(x = \mathbb E{}[X]\)</span>,
<span class="math inline">\(\text{P}{}\{g(X) = a + bX\} = 1\)</span>. (So if and only if <span class="math inline">\(g\)</span> is affine with
probability 1.)</p>
<ul>
<li><p><span class="math inline">\(x^2\)</span> is convex, <span class="math inline">\(\mathbb E[X^2] \geq \mathbb E{}[X]^2\)</span></p></li>
<li><p><span class="math inline">\(1/x\)</span> is convex, <span class="math inline">\(\mathbb E[1/X] \geq 1/ \mathbb E{}[X]\)</span></p></li>
<li><p><span class="math inline">\(\log x\)</span> is concave, <span class="math inline">\(\mathbb E(\log X) \leq \log (\mathbb E X)\)</span></p></li>
<li><p>Arithmetic mean: <span class="math display">\[a_A=\frac{1}{n}(a_1+a_2+\cdots+a_n)\]</span>
Geometric mean: <span class="math display">\[a_G=(a_1a_2\cdots a_n)^{1/n}\]</span>
Harmonic mean: <span class="math display">\[a_H=\frac{1}{\frac{1}{n}\left(\frac{1}{a_1}+\frac{1}{a_2}+\cdots+\frac{1}{a_n}\right)}\]</span>
<span class="math display">\[a_H\leq a_G\leq a_A\]</span></p></li>
</ul>
<p><strong>Covariance Inequality</strong> Let <span class="math inline">\(X\)</span> be any random variable and <span class="math inline">\(g(x)\)</span> and <span class="math inline">\(h(x)\)</span> any functions such that <span class="math inline">\(\mathbb E g(X), \mathbb E h(X)\)</span> and <span class="math inline">\(\mathbb E (g(X)h(X))\)</span> exist.<br />
- If <span class="math inline">\(g(x)\)</span> is a nondecreasing function and <span class="math inline">\(h(x)\)</span> is a nonincreasing function, then
<span class="math display">\[\mathbb E \bigg(g(X)h(X)\bigg)\leq\bigg(\mathbb E g(X)\bigg)\bigg(\mathbb E h(X)\bigg)\]</span></p>
<ul>
<li>If <span class="math inline">\(g(x)\)</span> and <span class="math inline">\(h(x)\)</span> are both nondecreasing or nonincreasing functions, then
<span class="math display">\[\mathbb E \bigg(g(X)h(X)\bigg)\geq\bigg(\mathbb E g(X)\bigg)\bigg(\mathbb E h(X)\bigg)\]</span></li>
</ul>
</div>
</div>
</div>
<div id="properties-of-a-random-sample" class="section level1">
<h1>5. Properties of a Random Sample</h1>
<div class="definition">
<p><span id="def:unlabeled-div-1" class="definition"><strong>Definition 1  </strong></span>A collection of random variables <span class="math inline">\(X_1, \dots, X_n\)</span> is a <em>random sample
of size n from population <span class="math inline">\(f(x)\)</span></em> if they are iid with pdf/pmf f(x).</p>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-2" class="definition"><strong>Definition 2  </strong></span><span class="math inline">\(Y = T(X_1, \dots, X_n)\)</span> is a <em>statistic</em> if the domain of <span class="math inline">\(T\)</span> contains
the sample space of <span class="math inline">\((X_1, \dots, X_n)\)</span>. The distribution of <span class="math inline">\(Y\)</span> is the
<em>sampling distribution of <span class="math inline">\(Y\)</span></em>.</p>
</div>
<div class="remark">
<p><span id="unlabeled-div-3" class="remark"><em>Remark</em>. </span>A statistic is any function of the data. The only restriction is that
the statistic is not also a function of some other parameters.</p>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-4" class="definition"><strong>Definition 3  </strong></span>The <em>sample mean</em> <span class="math inline">\(\bar{X}\)</span> and <em>sample variance</em> <span class="math inline">\(S^2\)</span> of a random
sample <span class="math inline">\(X_1, \dots, X_n\)</span> are, respectively,</p>
<ul>
<li><p><span class="math inline">\(\bar{X} = \frac1n \sum_{i=1}^n X_i\)</span></p></li>
<li><p><span class="math inline">\(S^2 = \frac{1}{n-1}\sum_{i=1}^n (X_i - \bar{X})^2\)</span>.</p></li>
</ul>
<p>The <em>sample standard deviation</em> is <span class="math inline">\(S = \sqrt{S^2}\)</span>.</p>
</div>
<div class="theorem">
<p><span id="thm:unlabeled-div-5" class="theorem"><strong>Theorem 1  </strong></span>Let <span class="math inline">\(X_1, \dots, X_n\)</span> be a random sample from a population with mean
<span class="math inline">\(\mu\)</span> and finite variance <span class="math inline">\(\sigma^2\)</span>, then</p>
<ol style="list-style-type: lower-alpha">
<li><p><span class="math inline">\(\mathbb E[\bar{X}] = \mu\)</span></p></li>
<li><p><span class="math inline">\(\mathbb V[\bar{X}] = \frac{\sigma^2}{n}\)</span></p></li>
<li><p><span class="math inline">\(\mathbb E[S^2] = \sigma ^2\)</span>.</p></li>
</ol>
</div>
<div class="theorem">
<p><span id="thm:unlabeled-div-6" class="theorem"><strong>Theorem 2  </strong></span>Take <span class="math inline">\(x_1, \dots, x_n \in \mathbb R\)</span> and let <span class="math inline">\(\bar{x}\)</span> be their mean. Then,</p>
<ol style="list-style-type: lower-alpha">
<li><p><span class="math inline">\(\min_a \sum_{i=1}^n (x_i - a)^2 = \sum_{i=1}^n (x_i - \bar{x})^2\)</span></p></li>
<li><p><span class="math inline">\((n-1)s^2 = \sum_{i=1}^n(x_i - \bar{x})^2 = \sum_{i=1}^n x_i^2 - n\bar{x}^2\)</span>.</p></li>
<li><p>The mgf <span class="math inline">\(M_{\bar{X}}(t)=\mathbb E e^{t\bar{X}}=\mathbb E e^{t(X_1+\cdots+X_n)/n}=\mathbb E e^{(t/n)(X_1+\cdots+X_n)}=M_{(X_1+\cdots+X_n)}(t/n)=[M_{X}(t/n)]^n\)</span>. The last equal sign is caused by <span class="math inline">\(X_1,\cdots,X_n\)</span> are identically distributed, <span class="math inline">\(M_{X_i}(t)\)</span> is the same function for each <span class="math inline">\(i\)</span>.</p></li>
<li><p>Let <span class="math inline">\(X_1, \dots, X_n\)</span> be a random sample from a <span class="math inline">\(\text{n}(\mu, \sigma^2)\)</span>
distribution. Then the mgf of the sample mean is <span class="math display">\[\begin{align}
M_{\bar{X}}(t)&amp;=\left[\exp\left(\mu\frac{t}{n}+\frac{\sigma^2(t/n)^2}{2}\right)\right]^n\\
&amp;=\exp\left(n\left(\mu\frac{t}{n}+\frac{\sigma^2(t/n)^2}{2}\right)\right)\\
&amp;=\exp\left(\mu t+\frac{(\sigma^2/n)t^2}{2}\right)
\end{align}\]</span> Thus <span class="math inline">\(\bar{X}\)</span> has a <span class="math inline">\(\text{n}(\mu, \sigma^2/n)\)</span> distribution.</p></li>
</ol>
</div>
<div class="theorem">
<p><span id="thm:unlabeled-div-7" class="theorem"><strong>Theorem 3  </strong></span>Let <span class="math inline">\(X_1, \dots, X_n\)</span> be a random sample from population
<span class="math inline">\(f(x\vert{}\mathbf{\theta})\)</span> belonging to an exponential family
<span class="math display">\[f(x \vert{} \mathbf{\theta}) = h(x)c(\mathbf{\theta}) \exp\left(\sum_{i=1}^k w_i(\mathbf{\theta})t_i(x)\right).\]</span>
Define the statistics
<span class="math display">\[T_i(X_1, \dots, X_n) = \sum_{j=1}^n t_i(X_j), \quad i=1, \dots, k.\]</span>
Then if the set
<span class="math inline">\(\{(w_1(\mathbf{\theta}), \dots, w_n(\mathbf{\theta}), \, \theta \in \Theta\}\)</span>
contains and open subset of <span class="math inline">\(\mathbb R^k\)</span> then the distribution of
<span class="math inline">\((X_1, \dots, X_n)\)</span> is an exponential family of the form
<span class="math display">\[f(u_1, \dots, u_n \vert{} \mathbf{\theta}) = H(u_1, \dots, u_n) c(\mathbf{\theta})^n \exp\left(\sum_{i=1}^kw_i(\mathbf{\theta})u_i\right).\]</span></p>
</div>
<div class="remark">
<p><span id="unlabeled-div-8" class="remark"><em>Remark</em>. </span>The open condition eliminates curved exponential families from this
result.</p>
</div>
<div id="sampling-from-the-normal-distribution" class="section level2">
<h2>Sampling from the Normal Distribution</h2>
<div class="theorem">
<p><span id="thm:unlabeled-div-9" class="theorem"><strong>Theorem 4  </strong></span>Let <span class="math inline">\(X_1, \dots, X_n\)</span> be a random sample from a <span class="math inline">\(\text{n}(\mu, \sigma^2)\)</span>
distribution. Then,</p>
<ul>
<li><p><span class="math inline">\(\bar{X}\)</span> and <span class="math inline">\(S^2\)</span> are independent</p></li>
<li><p><span class="math inline">\(\bar{X} \sim \text{n}(\mu, \sigma^2/n)\)</span></p></li>
<li><p><span class="math inline">\((n-1)S^2/\sigma^2 \sim \chi^2_{n-1}\)</span>.</p></li>
</ul>
</div>
<div class="lemma">
<p><span id="lem:unlabeled-div-10" class="lemma"><strong>Lemma 1  </strong></span>In the case of samples from a multivariate normal</p>
<ul>
<li><p>Independence <span class="math inline">\(\iff\)</span> vanishing covariance</p></li>
<li><p>Pairwise independence <span class="math inline">\(\iff\)</span> independence</p></li>
</ul>
</div>
</div>
<div id="convergence-concepts" class="section level2">
<h2>Convergence Concepts</h2>
<div id="convergence-in-probability" class="section level3">
<h3>Convergence in Probability</h3>
<div class="definition">
<p><span id="def:unlabeled-div-11" class="definition"><strong>Definition 4  </strong></span>A sequence of random variables <span class="math inline">\(\{X_i: i \in \mathbb N \}\)</span> <em>converges in
probability</em> to a random variable <span class="math inline">\(X\)</span> if <span class="math inline">\(\forall \epsilon &gt; 0\)</span>
<span class="math display">\[\lim_{n\to\infty}\text{P }(|{X_n - X}| \geq \epsilon) = 0\]</span> (or
equivalently if <span class="math inline">\(\lim_{n\to\infty}\text{P }(|{X_n - X}| &lt; \epsilon) = 1\)</span>).</p>
</div>
<div class="theorem">
<p><span id="thm:unlabeled-div-12" class="theorem"><strong>Theorem 5  </strong></span><strong>Weak Law of Large Numbers, WLLN</strong>
Let <span class="math inline">\(X_1, X_2, \dots\)</span> be iid RVs with mean <span class="math inline">\(\mu\)</span> and
<span class="math inline">\(\mathbb V X_i = \sigma^2 &lt; \infty\)</span>. Define <span class="math display">\[\bar{X}_n = \frac1n \sum_{i=1}^n X_i\]</span>
Then the sequence <span class="math inline">\(\bar{X}_n\)</span> converges in probability to <span class="math inline">\(\mu\)</span>. That
is, <span class="math inline">\(\forall \epsilon &gt; 0\)</span>
<span class="math display">\[\lim_{n\to\infty} \text{P }(|{\bar X_n - \mu}| &lt; \epsilon) = 1\]</span></p>
</div>
<p>Proof is using Chebychev’s inequality.</p>
<p>We have for every <span class="math inline">\(\epsilon&gt;0\)</span>, <span class="math display">\[P(|\bar X_n - \mu|\ge\epsilon)=P((\bar X_n - \mu)^2\ge\epsilon^2)\leq\frac{\mathbb E(\bar X_n - \mu)^2}{\epsilon^2}=\frac{\mathbb V\bar X_n}{\epsilon^2}=\frac{\sigma^2}{n\epsilon^2}\]</span>
Hence, <span class="math display">\[P(|\bar X_n - \mu|&lt;\epsilon)=1-P(|\bar X_n - \mu|\ge\epsilon)\ge 1-\frac{\sigma^2}{n\epsilon^2}\to 1 \text{ as } n\to \infty\]</span></p>
<p>The property summarized by the WLLN, that a sequence of the “same” sample quantity approaches a constant as <span class="math inline">\(n \to \infty\)</span>, is known as <strong>consistency</strong>.</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-13" class="theorem"><strong>Theorem 6  </strong></span>If <span class="math inline">\(X_1, X_2, \dots\)</span> converges in probability to <span class="math inline">\(X\)</span> and <span class="math inline">\(h\)</span> is a
continuous function, then <span class="math inline">\(h(X_1), h(X_2), \dots\)</span> converges in
probability to <span class="math inline">\(h(X)\)</span>.</p>
</div>
</div>
<div id="almost-sure-convergence" class="section level3">
<h3>Almost Sure Convergence</h3>
<div class="definition">
<p><span id="def:unlabeled-div-14" class="definition"><strong>Definition 5  </strong></span>A sequence of random variables <span class="math inline">\(X_1, X_2, \dots\)</span> <em>converges almost surely</em> to a random variable <span class="math inline">\(X\)</span> if <span class="math inline">\(\forall \epsilon &gt; 0\)</span>
<span class="math display">\[\text{P }\left(\lim_{n\to\infty} |{\bar{X}_n - X}| &lt; \epsilon\right) = 1.\]</span></p>
</div>
<div class="comments">
<ul>
<li><p>Almost sure convergence is much stronger than convergence in probability. Convergence in probability states that the sequence of measures of the sets on which the sequence has finite difference from its the limit converges to <span class="math inline">\(0\)</span>. Almost sure convergence states that any place where the sequence has finite difference from its limit must have measure <span class="math inline">\(0\)</span>. It’s like a sequence of integrals converging vs. whether the integrands converge.</p></li>
<li><p>Almost sure convergence implies convergence in probability but not the other way around.</p></li>
</ul>
</div>
<div class="theorem">
<p><span id="thm:unlabeled-div-15" class="theorem"><strong>Theorem 7  </strong></span>If a sequence converges in probability then it is possible to find a subsequence that converges almost surely.</p>
</div>
<div class="theorem">
<p><span id="thm:unlabeled-div-16" class="theorem"><strong>Theorem 8  </strong></span><strong>Strong Law of Large Numbers (SLLN)</strong>
Let <span class="math inline">\(X_1, X_2, \dots\)</span> be iid RVs with mean <span class="math inline">\(\mathbb E {X_i} = \mu\)</span> and
<span class="math inline">\(\mathbb V{X_i} = \sigma^2 &lt; \infty\)</span>. Define <span class="math display">\[\bar{X}_n = \frac{1}{n} \sum_{i=1}^n X_i\]</span>.
Then the sequence <span class="math inline">\(\bar{X}_n\)</span> converges almost surely to <span class="math inline">\(\mu\)</span>. That is,
<span class="math inline">\(\forall \epsilon &gt; 0\)</span>
<span class="math display">\[\text{P }\left(\lim_{n\to\infty}|{\bar{X}_n - \mu}| &lt; \epsilon\right) = 1.\]</span></p>
</div>
</div>
<div id="convergence-in-distribution" class="section level3">
<h3>Convergence in Distribution</h3>
<div class="definition">
<p><span id="def:unlabeled-div-17" class="definition"><strong>Definition 6  </strong></span>A sequence of random variables <span class="math inline">\(X_1, X_2, \dots\)</span> <em>converges in
distribution</em> to a random variable <span class="math inline">\(X\)</span> if
<span class="math display">\[\lim_{n \to \infty} F_{X_n}(x) = F_X(x)\]</span> at all points where <span class="math inline">\(F_X\)</span> is
continuous.</p>
</div>
<div class="remark">
<p><span id="unlabeled-div-18" class="remark"><em>Remark</em>. </span>Here it is really the cdfs that converge, rather than the random
variables. In this way convergence in distribution differs from the
previous two concepts.</p>
</div>
<div class="theorem">
<p><span id="thm:unlabeled-div-19" class="theorem"><strong>Theorem 9  </strong></span>Convergence in probability implies convergence in distribution</p>
</div>
<div class="theorem">
<p><span id="thm:unlabeled-div-20" class="theorem"><strong>Theorem 10  </strong></span><strong>Central Limit Theorem</strong>
Let <span class="math inline">\(X_1, X_2, \dots\)</span> be a sequence of iid random variables with
<span class="math inline">\(\mathbb E[X_i] = \mu\)</span> and finite variance <span class="math inline">\(\mathbb V[X_i] = \sigma^2 &lt; \infty\)</span>.
Define <span class="math inline">\(\bar{X}_n = \frac1n \sum_{i=1}^n X_i\)</span>. Let <span class="math inline">\(G_n(x)\)</span> denote the
cdf of <span class="math inline">\(\sqrt{n}(\bar{X}_n - \mu)/\sigma\)</span>. Then <span class="math inline">\(\forall x \in \mathbb R\)</span>,
<span class="math display">\[\lim_{n\to\infty} G_n(x) = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^x e^{-y^2/2}dy.\]</span>
That is, <span class="math inline">\(\sqrt{n}(\bar{X}_n - \mu)/\sigma\)</span> converges in distribution to
the standard normal.</p>
</div>
<div class="theorem">
<p><span id="thm:unlabeled-div-21" class="theorem"><strong>Theorem 11  </strong></span><strong>Slutsky’s Theorem</strong>
If <span class="math inline">\(X_n \to X\)</span> in distribution and <span class="math inline">\(Y_n \to a\)</span> in probability with <span class="math inline">\(a\)</span>
constant, then</p>
<ol style="list-style-type: lower-alpha">
<li><p><span class="math inline">\(Y_nX_n \to aX\)</span> in distribution</p></li>
<li><p><span class="math inline">\(X_n + Y_n \to X + a\)</span> in distribution</p></li>
</ol>
</div>
<div class="remark">
<p><span id="unlabeled-div-22" class="remark"><em>Remark</em>. </span>This tells us, for instance, that
<span class="math display">\[\frac{\sqrt{n}(\bar{X}_n - \mu)}{S_n} \to \text{n}(0, 1)\]</span> in distribution,
since we know that <span class="math inline">\(S_n \to \sigma\)</span> in probability.</p>
</div>
</div>
<div id="the-delta-method" class="section level3">
<h3>The Delta Method</h3>
<p>If we are interested in the convergence of some function of a sequence
of RVs, rather than the RVs themselves, then we can use the Delta Method
(follows from an application of Taylor’s theorem and Slutsky’s theorem).</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-23" class="theorem"><strong>Theorem 12  </strong></span>Let <span class="math inline">\(Y_n\)</span> be a sequence of random variables that satisfies
<span class="math inline">\(\sqrt{n}(Y_n - \theta) \to \text{n}(0, \sigma^2)\)</span> in distribution. For a
given function <span class="math inline">\(g\)</span> and a specific value of <span class="math inline">\(\theta\)</span>, suppose that
<span class="math inline">\(g&#39;(\theta)\)</span> exists and is non-zero. Then
<span class="math display">\[\sqrt{n}[g(Y_n) - g(\theta)] \to \text{n}(0, \sigma^2g&#39;(\theta)^2)\]</span> in
distribution.</p>
</div>
<div class="remark">
<p><span id="unlabeled-div-24" class="remark"><em>Remark</em>. </span>There exists a corresponding multivariate result.</p>
</div>
<p>If <span class="math inline">\(g&#39;(\theta) = 0\)</span> then we take the next term in the Taylor series.</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-25" class="theorem"><strong>Theorem 13  </strong></span>Let <span class="math inline">\(Y_n\)</span> be a sequence of random variables that satisfies
<span class="math inline">\(\sqrt{n}(Y_n - \theta) \to \text{n}(0, \sigma^2)\)</span> in distribution. For a
given function <span class="math inline">\(g\)</span> and a specific value of <span class="math inline">\(\theta\)</span>, suppose that
<span class="math inline">\(g&#39;(\theta) = 0\)</span> and <span class="math inline">\(g&#39;&#39;(\theta)\)</span> exists and is non-zero. Then
<span class="math display">\[\sqrt{n}[g(Y_n) - g(\theta)] \to \text{n}(0, \sigma^2[g&#39;(\theta)]^2)\]</span> in
distribution.</p>
<p>Furthermore, as both <span class="math inline">\(\bar Y\)</span> and <span class="math inline">\(\sigma^2\)</span> are consistent estimators, we can again apply Slutsky’s Theorem to conclude that
<span class="math display">\[\sqrt{n}[g(Y_n) - g(\bar Y)] \to \text{n}(0, S^2[g&#39;(\bar Y)]^2)\]</span></p>
</div>
<p><strong>Proof</strong> The Taylor expansion of <span class="math inline">\(g(Y_n)\)</span> around <span class="math inline">\(Y_n = 0\)</span> is
<span class="math display">\[g(Y_n) = g(\theta) + g&#39;(\theta) (Y_n - \theta) + \text{Remainder},\]</span>
where the remainder <span class="math inline">\(\to 0\)</span> as <span class="math inline">\(Y_n \to \theta\)</span>. Since <span class="math inline">\(Y_n \to \theta\)</span> in probability it follows that the remainder <span class="math inline">\(\to 0\)</span> in probability. By applying <strong>Slutsky’s Theorem</strong> to
<span class="math display">\[\sqrt{n}[g(Y_n) - g(\theta)] = g&#39;(\theta)\sqrt{n}(Y_n-\theta),\]</span>
the result now follows.</p>
</div>
<div id="the-second-order-delta-method" class="section level3">
<h3>The second-order Delta Method</h3>
<p>If <span class="math inline">\(g&#39;(\theta) = 0\)</span>, we take one more term in the Taylor expansion to get
<span class="math display">\[g(Y_n) = g(\theta) + g&#39;(\theta)(Y_n -\theta) +\frac{g&#39;&#39;(\theta)}{2} (Y_n -\theta)^2 + \text{Remainder}.\]</span>
If we do some rearranging (setting <span class="math inline">\(g&#39; = 0\)</span>), we have
<span class="math display">\[ g(Y_n) -g(\theta) = \frac{g&#39;&#39;(\theta)}{2} (Y_n -\theta)^2 + \text{Remainder}.\]</span>
Now recall that the square of a <span class="math inline">\(n(0, 1)\)</span> is a <span class="math inline">\(\chi_1^2\)</span> , which implies that
<span class="math display">\[\frac{n(Y_n-\theta)^2}{\sigma^2}\to\chi_1^2\]</span>
in distribution. Therefore,</p>
<p>(<strong>Second-order Delta Method</strong>) Let <span class="math inline">\(Y_n\)</span> be a sequence of random variables that satisfies <span class="math inline">\(\sqrt{n}(Y_n -\theta) \to n(0, \sigma^2)\)</span> in distribution. For a given function <span class="math inline">\(g\)</span>
and a specific value of <span class="math inline">\(\theta\)</span>, suppose that <span class="math inline">\(g&#39;(\theta)=0\)</span> and <span class="math inline">\(g&#39;&#39;(\theta)\)</span> exists and is not <span class="math inline">\(0\)</span>. Then
<span class="math display">\[n[g(Y_n) -g(\theta)]\approx n\frac{g&#39;&#39;(\theta)}{2} (Y_n -\theta)^2\to \sigma^2\frac{g&#39;&#39;(\theta)}{2}\chi_1^2\]</span> in distribution.</p>
</div>
<div id="multivariate-delta-method" class="section level3">
<h3>Multivariate Delta Method</h3>
<p>Note that we must deal with multiple random variables although the ultimate CLT is a univariate one. Suppose the vector-valued random variable <span class="math inline">\(\mathbf X=(X_1,\cdots, X_p)\)</span> has mean <span class="math inline">\(\boldsymbol\mu = (\mu_1, \cdots , \mu_p)\)</span> and covariances <span class="math inline">\(Cov(X_i, X_j) = \sigma_{ij}\)</span>, and we observe an independent
random sample <span class="math inline">\(\mathbf X_1, \cdots, \mathbf X_n\)</span> and calculate the means <span class="math display">\[\bar X_i = \frac{1}{n}\sum_{k=1}^{n}X_{ik},\quad i=1,\cdots,p\]</span>
For a function <span class="math inline">\(g(\mathbf x) = g(x_1, \cdots, x_p)\)</span> we can write
<span class="math display">\[g(\bar x_1, \cdots, \bar x_p) = g(\mu_1, \cdots, \mu_p) + \sum_{k=1}^{p}g&#39;_k(\mathbf x)(\bar x_k - \mu_k),\]</span>
and we then have the following theorem:</p>
<p><strong>Multivariate Delta Method</strong> Let <span class="math inline">\(\mathbf X_1, \cdots, \mathbf X_n\)</span> be a random sample with <span class="math inline">\(E(X_{ij})=\mu_i\)</span> and <span class="math inline">\(Cov(X_{ik}, X_{jk}) = \sigma_{ij}\)</span> . For a given function <span class="math inline">\(g\)</span> with continuous first partial derivatives and a specific value of <span class="math inline">\(\boldsymbol\mu = (\mu_1, \cdots , \mu_p)\)</span> for which
<span class="math display">\[\tau^2=\sum\sum\sigma_{ij}\frac{\partial g(\boldsymbol\mu)}{\partial{\mu_i}}\frac{\partial g(\boldsymbol\mu)}{\partial{\mu_j}}&gt;0,\]</span>
<span class="math display">\[\sqrt{n}\big[g(\bar x_1,\cdots,\bar x_p)-g(\mu_1,\cdots,\mu_p)\big]\to n(0,\tau^2)\;\;\text {in distribution}\]</span></p>
</div>
</div>
<div id="generating-a-random-sample" class="section level2">
<h2>Generating A Random Sample</h2>
<div id="direct-method" class="section level3">
<h3>Direct Method</h3>
<p><strong>Probability integral transformation</strong> Let <span class="math inline">\(X\)</span> have continuous
cdf <span class="math inline">\(F_X(x)\)</span> and define the random variable <span class="math inline">\(Y\)</span> as <span class="math inline">\(Y = F_X (X)\)</span>. Then <span class="math inline">\(Y\)</span> is uniformly distributed on <span class="math inline">\((0, 1)\)</span>, that is, <span class="math inline">\(P(Y \leq y) = y, 0 &lt; y &lt; 1\)</span>.</p>
<p><strong>Proof</strong> For <span class="math inline">\(Y = F_X (X)\)</span> we have, for <span class="math inline">\(0 &lt; y &lt; 1\)</span>,
<span class="math display">\[\begin{align} P(Y\leq y)&amp;=P(F_X (X)\leq y)\\
&amp;=P(F_X^{-1} [F_X (X)]\leq F_X^{-1}(y))\quad (F_X^{-1} \text{ is increasing})\\
&amp;=P(X\leq F_X^{-1}(y))\\
&amp;=F_X(F_X^{-1}(y))\\
&amp;=y
\end{align}\]</span>
At the endpoints we have <span class="math inline">\(P (Y \leq y) = 1\)</span> for <span class="math inline">\(y \ge 1\)</span> and <span class="math inline">\(P (Y \leq y)=0\)</span> for <span class="math inline">\(y \leq 0\)</span>, showing that <span class="math inline">\(Y\)</span> has a uniform distribution.</p>
<p>Let <span class="math inline">\(X\)</span> have cdf <span class="math inline">\(F_X(x)\)</span>, let <span class="math inline">\(Y=g(X)\)</span>, and let <span class="math inline">\(\mathcal X\)</span> and <span class="math inline">\(\mathcal Y\)</span> be defined as <span class="math inline">\(\mathcal X \{x: f_X(x) &gt; 0\}\)</span> and <span class="math inline">\(\mathcal Y = \{y: y = g(x)\},\text{ for some }x \in \mathcal X\}\)</span>.</p>
<ul>
<li><ol style="list-style-type: lower-alpha">
<li>If <span class="math inline">\(g\)</span> is an increasing function on <span class="math inline">\(\mathcal X\)</span>, <span class="math display">\[F_Y (y) =\int_{x\in \mathcal X:x\leq g^{-1}(y)}f_X(x)dx=\int_{-\infty}^{g^{-1}(y)}f_X(x)dx= F_X (g^{-1}(y)) \text{ for }y \in \mathcal Y\]</span></li>
</ol></li>
<li><ol start="2" style="list-style-type: lower-alpha">
<li>If <span class="math inline">\(g\)</span> is a decreasing function on <span class="math inline">\(\mathcal X\)</span> and <span class="math inline">\(X\)</span> is a continuous random variable, <span class="math display">\[F_Y (y)=\int_{g^{-1}(y)}^{\infty}f_X(x)dx=1 - F_X(g^{-1} (y)) \text{ for } y \in \mathcal Y\]</span></li>
</ol></li>
</ul>
<div class="definition">
<p><span id="def:unlabeled-div-26" class="definition"><strong>Definition 7  </strong></span>A <strong>Direct Method</strong> of generating a random sample uses the probability
integral transform to map draws from a <span class="math inline">\(\text{uniform}(0, 1)\)</span> random
variable to draws from the distribution of interest.<br />
The Probability Integral Transform states that if <span class="math inline">\(X\)</span> has continuous cdf
<span class="math inline">\(F_X(x)\)</span> then <span class="math display">\[F_X(X) \sim \text{uniform}(0, 1).\]</span></p>
</div>
</div>
<div id="indirect-methods" class="section level3">
<h3>Indirect Methods</h3>
</div>
<div id="the-acceptreject-algorithm" class="section level3">
<h3>The Accept/Reject Algorithm</h3>
<div class="definition">
<p><span id="def:unlabeled-div-27" class="definition"><strong>Definition 8  </strong></span>Let <span class="math inline">\(Y \sim f_Y(y)\)</span> and <span class="math inline">\(V \sim f_V(v)\)</span> where <span class="math inline">\(f_Y\)</span> and <span class="math inline">\(f_V\)</span> have
common support with <span class="math display">\[M = \sup_y f_Y(y) / f_V(y) &lt; \infty.\]</span> To generate
a random variable <span class="math inline">\(Y \sim f_Y\)</span>:</p>
<ol style="list-style-type: lower-alpha">
<li><p>Generate <span class="math inline">\(U \sim \text{uniform}(0, 1)\)</span>, <span class="math inline">\(V \sim f_V\)</span> independent.</p></li>
<li><p>If <span class="math inline">\(U &lt; \frac 1M f_Y(V)/f_V(V)\)</span>, return <span class="math inline">\(V\)</span> as a sample of <span class="math inline">\(Y\)</span>;
otherwise go back to (a.).</p></li>
</ol>
</div>
<ul>
<li><p>It is typical to call <span class="math inline">\(V\)</span> the <em>candidate density</em> and <span class="math inline">\(Y\)</span> the
<em>target density</em>.</p></li>
<li><p>One would normally try to choose a candidate density with heavier
tails than the target density (e.g. Cauchy and normal) to ensure
that the tails of the target are well represented. If the target has
heavy tails, however, it can be hard to find a candidate that
results in finite <span class="math inline">\(M\)</span>. In this case people turn to MCMC methods.</p></li>
<li><p>Note that <span class="math inline">\(\text{P }(\texttt{terminate}) = 1/M\)</span>. The number of trials to
generate one sample of <span class="math inline">\(Y\)</span> is therefore <span class="math inline">\(\text{geometric}(1/M)\)</span>,
with <span class="math inline">\(M\)</span> the expected number of trials.</p></li>
<li><p>The intuition behind this algorithm is that if we consider placing
the density of a random variable <span class="math inline">\(Y\)</span> in a box (2d for simplicity)
with coordinates <span class="math inline">\((v,u)\)</span>, we express the cdf of <span class="math inline">\(Y\)</span> using
<span class="math inline">\(V, U \sim \text{uniform}(0, 1)\)</span>
<span class="math display">\[\text{P }(Y \leq y) = \text{P }(V \leq y \vert{} U \leq \frac1c f_Y(V))\]</span>
where <span class="math inline">\(c = \sup_y f_Y(y)\)</span>. In the actual algorithm we take
<span class="math inline">\(U \sim \text{uniform}(0,1)\)</span> and <span class="math inline">\(V\)</span> to be an RV that has common
support with <span class="math inline">\(Y\)</span>.</p></li>
</ul>
</div>
</div>
</div>
<div id="principles-of-data-reduction" class="section level1">
<h1>6. Principles of Data Reduction</h1>
<p>In this chapter, we explore how we can use functions of a sample
<span class="math inline">\(\mathbf{X}\)</span> to make inferences about an unknown parameter (of the
distribution of the sample) <span class="math inline">\(\theta\)</span>.</p>
<p>[Statistic] A statistic is any function of the data.</p>
<p>A statistic <span class="math inline">\(T(X)\)</span> forms a partition of the sample space <span class="math inline">\(\mathcal X\)</span> according to
its image,
<span class="math inline">\(\mathcal{T} = \{t: \exists \mathbf{x} \in \mathcal X{} \text{  so that  } t = T(\mathbf{x})\}\)</span>.
In this way a statistic provides a method of data reduction. An
experimenter who observes only <span class="math inline">\(T\)</span> will treat as equal two samples
<span class="math inline">\(\mathbf{x}, \mathbf{y}\)</span> for which <span class="math inline">\(T(\mathbf{x}) = T(\mathbf{y})\)</span>.</p>
<div id="the-sufficiency-principle" class="section level3">
<h3>The Sufficiency Principle</h3>
<p><strong>Sufficient Statistic</strong> A statistic <span class="math inline">\(T(\mathbf{X})\)</span> is a <strong>sufficient
statistic</strong> for <span class="math inline">\(\theta\)</span> if the conditional distribution of the sample
<span class="math inline">\(\mathbf{X}\)</span> given <span class="math inline">\(T(\mathbf{X})\)</span> does not depend on <span class="math inline">\(\theta\)</span>.</p>
<p>We ignore the fact that all points have <span class="math inline">\(0\)</span> probability for continuous
distributions.</p>
<p><strong>The Sufficiency Principle</strong> If <span class="math inline">\(T(\mathbf{X})\)</span> is a sufficient statistic
for <span class="math inline">\(\theta\)</span>, then any inference about <span class="math inline">\(\theta\)</span> should depend on the
sample <span class="math inline">\(\mathbf{X}\)</span> only through <span class="math inline">\(T(\mathbf{X})\)</span>.</p>
<p>If <span class="math inline">\(p(\mathbf{x}\vert{}\theta)\)</span> is the pmf/pdf of the sample <span class="math inline">\(\mathbf{X}\)</span> and
<span class="math inline">\(q(t\vert{} \theta)\)</span> is the pmf/pdf of <span class="math inline">\(T(\mathbf{X})\)</span>, then <span class="math inline">\(T(\mathbf{X})\)</span>
is a sufficient statistic for <span class="math inline">\(\theta\)</span> if <span class="math inline">\(\forall \mathbf{x} \in \mathcal X{}\)</span>,
<span class="math display">\[f(\mathbf{x}\vert{}\theta) / q(T(\mathbf{X})\lvert\theta)\]</span> is constant as a function
of <span class="math inline">\(\theta\)</span>.</p>
<p><strong>Niceness of the exponential family</strong> It turns out that outside of the
exponential family, it is rare to have a sufficient statistic that is of
smaller dimension than the size of the sample.</p>
<p><strong>Factorization Theorem</strong> Let <span class="math inline">\(f(\mathbf{x} \vert{} \theta)\)</span> denote the
pmf/pdf of a sample <span class="math inline">\(\mathbf{X}\)</span>. A statistic <span class="math inline">\(T(\mathbf{X})\)</span> is a sufficient
statistic for <span class="math inline">\(\theta\)</span> if and only if there exists functions
<span class="math inline">\(g(T(\mathbf{X})\vert{} \theta)\)</span> and <span class="math inline">\(h(\mathbf{x})\)</span> such that
<span class="math inline">\(\forall \mathbf{x} \in \mathcal X{}\)</span>, <span class="math inline">\(\forall \theta \in \Theta\)</span>
<span class="math display">\[f(\mathbf{x} \vert{} \theta) = g(T(\mathbf{x}) \vert{} \theta)h(\mathbf{x}).\]</span></p>
<p>This theorem shows that the identity is a sufficient statistic. It is
straightforward to show from this that any bijection of a sufficient
statistic is a sufficient statistic.</p>
<p><strong>Proof</strong> Suppose <span class="math inline">\(T(\mathbf X)\)</span> is a sufficient statistic. Choose <span class="math inline">\(g(t\lvert \theta) = P_{\theta}(T(\mathbf X) = t)\)</span> and <span class="math inline">\(h(\mathbf x) =P\big(\mathbf X = \mathbf x\lvert T(\mathbf X) = T(\mathbf x)\big)\)</span>. Because <span class="math inline">\(T(\mathbf X)\)</span> is sufficient, the conditional probability
defining <span class="math inline">\(h(\mathbf x)\)</span> does not depend on <span class="math inline">\(\theta\)</span>. Thus this choice of <span class="math inline">\(h(\mathbf x)\)</span> and <span class="math inline">\(g(t\lvert \theta)\)</span> is legitimate,
and for this choice we have
<span class="math display">\[\begin{align}
f(\mathbf x\lvert \theta) &amp;= P_{\theta}(\mathbf X = \mathbf x)\\
&amp;= P_{\theta}\bigg(\mathbf X = \mathbf x \text{ and } T(\mathbf X) = T(\mathbf x)\bigg)\\
&amp;= P_{\theta}(T(\mathbf X) = T(\mathbf x)) P(\mathbf X=\mathbf x\lvert T(\mathbf X) = T(\mathbf x))\quad \text{(sufficiency)}\\
&amp;= g(T(\mathbf x)\lvert \theta)h(\mathbf x).
\end{align}\]</span></p>
<p>So factorization has been exhibited. We also see from the last two lines above that
<span class="math display">\[P_{\theta}\big(T(\mathbf X) = T(\mathbf x)\big) =g(T(\mathbf x)\lvert \theta),\]</span> so <span class="math inline">\(g(T(\mathbf x)\lvert \theta)\)</span> is the pmf of <span class="math inline">\(T(\mathbf X).\)</span></p>
<p>Now assume the factorization exists. Let <span class="math inline">\(q(t\lvert \theta)\)</span> be the pmf of <span class="math inline">\(T(\mathbf X)\)</span>. To show that <span class="math inline">\(T(\mathbf X)\)</span> is sufficient we examine the ratio <span class="math inline">\(f(\mathbf x\lvert \theta)/q(T(\mathbf x)\lvert \theta)\)</span>. Define <span class="math inline">\(A_{T(\mathbf x)} =\{\mathbf y: T(\mathbf y) = T(\mathbf x)\}\)</span>. Then
<span class="math display">\[\begin{align}
\frac{f(\mathbf x\lvert \theta)}{q(T(\mathbf x)\lvert \theta)}&amp;=\frac{g(T(\mathbf x)\lvert \theta)h(\mathbf x)}{q(T(\mathbf x)\lvert \theta)}\\
&amp;=\frac{g(T(\mathbf x)\lvert \theta)h(\mathbf x)}{\sum_{A_{T(\mathbf x)}}g(T(\mathbf y)\lvert \theta)h(\mathbf y)}\quad \text{(definition of the pmf of T)}\\
&amp;=\frac{g(T(\mathbf x)\lvert \theta)h(\mathbf x)}{g(T(\mathbf y)\lvert \theta)\sum_{A_{T(\mathbf x)}}h(\mathbf y)}\quad \text{(since T is constant on} \;A_{T(\mathbf x)})\\
&amp;=\frac{g(T(\mathbf x)\lvert \theta)h(\mathbf x)}{g(T(\mathbf x)\lvert \theta)\sum_{A_{T(\mathbf x)}}h(\mathbf y)}\\
&amp;=\frac{h(\mathbf x)}{\sum_{A_{T(\mathbf x)}}h(\mathbf y)}\\
\end{align}\]</span></p>
<p>Since the ratio does not depend on <span class="math inline">\(\theta\)</span>, <span class="math inline">\(T(\mathbf X)\)</span> is a sufficient statistic for <span class="math inline">\(\theta\)</span>.</p>
<p>Let <span class="math inline">\(X_1, \dots, X_n\)</span> be iid observations from a pmf/pdf
<span class="math inline">\(f(x \vert{} \boldsymbol\theta)\)</span> from an exponential family
<span class="math display">\[f(x|\boldsymbol{\theta}) = h(x)c(\boldsymbol{\theta})\exp\left( \sum_{i=1}^k w_i(\boldsymbol{\theta}) t_i(x) \right),\]</span>
where <span class="math inline">\(\boldsymbol{\theta} = (\theta_1, \dots, \theta_d)\)</span>, <span class="math inline">\(d \leq k\)</span>. Then
<span class="math inline">\(T(\mathbf{X})\)</span> defined by
<span class="math display">\[T(\mathbf{X}) = \left(\sum_{j=1}^n t_1(\mathbf{X}_j),\cdots,\sum_{j=1}^n t_k(\mathbf{X}_j)\right)\]</span> is a sufficient statistic for <span class="math inline">\(\boldsymbol{\theta}\)</span>.</p>
<p><strong>Minimal sufficient statistic</strong> A sufficient statistic <span class="math inline">\(T(\mathbf{X})\)</span> is called a <strong>minimal sufficient statistic</strong> if, for any other sufficient statistic <span class="math inline">\(T&#39;(\mathbf{X})\)</span>, <span class="math inline">\(T(\mathbf{x})\)</span> is a function of <span class="math inline">\(T&#39;(\mathbf{x})\)</span>.</p>
<p>By ‘function of’ we mean that if <span class="math inline">\(T&#39;(\mathbf{x}) = T&#39;(\mathbf{y})\)</span> then
<span class="math inline">\(T(\mathbf{x}) = T(\mathbf{y})\)</span>, <span class="math inline">\(T\)</span> varies with respect to <span class="math inline">\(X\)</span> only as it varies with <span class="math inline">\(T&#39;\)</span>. This means that each tile of the partition <span class="math inline">\(\{B_{t&#39;}:t&#39;\in \mathcal T&#39;\}\)</span> of the sample space according to the image of <span class="math inline">\(T&#39;\)</span> is a subset of some tile in the partition <span class="math inline">\(\{A_{t}:t\in \mathcal T\}\)</span> according to <span class="math inline">\(T\)</span>. This means that minimal sufficient statistics provide the <em>coarsest</em> possible tiling of the sample space and thus are the sufficient statistics that provide the greatest data reduction.</p>
<p><strong>Theorem</strong> Let <span class="math inline">\(f(\mathbf{x} \vert{} \theta)\)</span> be the pmf/pdf of a sample <span class="math inline">\(\mathbf{X}\)</span>.
Suppose there exists a function <span class="math inline">\(T(\mathbf{X})\)</span> such that
<span class="math inline">\(\forall \mathbf{x}, \mathbf{y} \in \mathcal X{}\)</span> the ratio
<span class="math inline">\(f(\mathbf{x}\vert{}\theta)/f(\mathbf{y}\vert{}\theta)\)</span> is constant as a function of <span class="math inline">\(\theta\)</span> if an only if <span class="math inline">\(T(\mathbf{x}) = T(\mathbf{y})\)</span>. Then <span class="math inline">\(T(\mathbf{X})\)</span> is a minimal sufficient statistic for <span class="math inline">\(\theta\)</span>.</p>
<p><strong>Proof</strong> To simplify the proof, we assume <span class="math inline">\(f(\mathbf x\lvert \theta) &gt; 0\)</span> for all <span class="math inline">\(\mathbf x \in \mathcal X\)</span> and <span class="math inline">\(\theta\)</span>.</p>
<p>First we show that <span class="math inline">\(T(\mathbf X)\)</span> is a sufficient statistic. Let <span class="math inline">\(\mathcal T = \{t : t=T(\mathbf x)\; \text{for some }\; \mathbf x \in \mathcal X\}\)</span> be the image of <span class="math inline">\(\mathcal X\)</span> under <span class="math inline">\(T(\mathbf x)\)</span>. Define the partition sets induced by <span class="math inline">\(T(\mathbf x)\)</span> as <span class="math inline">\(A_t = \{\mathbf x: T(\mathbf x) = t\}\)</span>. For each <span class="math inline">\(A_t\)</span>, choose and fix one element <span class="math inline">\(\mathbf x_t \in A_t\)</span>. For any <span class="math inline">\(\mathbf x \in \mathcal X\)</span>,
<span class="math inline">\(\mathbf x_{T(\mathbf x)}\)</span> is the fixed element that is in the same set, <span class="math inline">\(A_t\)</span> , as <span class="math inline">\(\mathbf x\)</span>. Since <span class="math inline">\(\mathbf x\)</span> and <span class="math inline">\(\mathbf x_{T(\mathbf x)}\)</span> are in the same set <span class="math inline">\(A_t\)</span> , <span class="math inline">\(T(\mathbf x) = T(\mathbf x_{T(\mathbf x)})\)</span> and, hence, <span class="math inline">\(f(\mathbf x\lvert\theta)/ f(\mathbf x_{T(\mathbf x)}\lvert\theta)\)</span> is constant as a
function of <span class="math inline">\(\theta\)</span>. Thus, we can define a function on <span class="math inline">\(\mathcal X\)</span> by <span class="math inline">\(h(\mathbf x) = f(\mathbf x\lvert\theta)/ f(\mathbf x_{T(\mathbf x)}\lvert\theta)\)</span> and <span class="math inline">\(h\)</span> does not depend on <span class="math inline">\(\theta\)</span>. Define a function on <span class="math inline">\(\mathcal T\)</span> by <span class="math inline">\(g(t\lvert\theta) = f(\mathbf x_t\lvert\theta)\)</span>. Then it can be seen that
<span class="math display">\[f(\mathbf x_t\lvert\theta)=\frac{f(\mathbf x_{T(\mathbf x)}\lvert\theta)f(\mathbf x\lvert\theta)}{f(\mathbf x_{T(\mathbf x)}\lvert\theta)} = g(T(\mathbf x)\lvert\theta)h(\mathbf x)\]</span><br />
and, by the Factorization Theorem, <span class="math inline">\(T(\mathbf X)\)</span> is a sufficient statistic for <span class="math inline">\(\theta\)</span>.</p>
<p>Now to show that <span class="math inline">\(T(\mathbf X)\)</span> is minimal, let <span class="math inline">\(T&#39;(\mathbf X)\)</span> be any other sufficient statistic. By the Factorization Theorem, there exist functions <span class="math inline">\(g&#39;\)</span> and <span class="math inline">\(h&#39;\)</span> such that <span class="math inline">\(f(\mathbf x\lvert\theta) =g&#39;(T&#39;(\mathbf x)\lvert\theta)h&#39;(\mathbf x)\)</span>. Let <span class="math inline">\(\mathbf x\)</span> and <span class="math inline">\(\mathbf y\)</span> be any two sample points with <span class="math inline">\(T&#39;(\mathbf x) = T&#39;(\mathbf y)\)</span>. Then <span class="math display">\[\frac{f(\mathbf x\lvert\theta)}{f(\mathbf y\lvert\theta)}=\frac{g&#39;(T&#39;(\mathbf x)\lvert\theta)h&#39;(\mathbf x)}{g&#39;(T&#39;(\mathbf y)\lvert\theta)h&#39;(\mathbf y)}=\frac{h&#39;(\mathbf x)}{h&#39;(\mathbf y)}\]</span> Since this ratio does not depend on <span class="math inline">\(\theta\)</span>, the assumptions of the theorem imply that <span class="math inline">\(T(\mathbf x) = T(\mathbf y)\)</span>. Thus, <span class="math inline">\(T(\mathbf x)\)</span> is a function of <span class="math inline">\(T&#39;(\mathbf x)\)</span> and <span class="math inline">\(T(\mathbf x)\)</span> is minimal.</p>
<p><strong>Necessary Statistic</strong> A statistic is <em>necessary</em> if it can be written as
a function of every sufficient statistic.</p>
<p>A statistic is minimal sufficient if and only if it is a necessary and
sufficient statistic.</p>
</div>
<div id="ancillary-statistic" class="section level3">
<h3>Ancillary Statistic</h3>
<p><strong>Ancillary Statistic</strong> A statistic <span class="math inline">\(S(\mathbf{X})\)</span> whose distribution does not depend on the parameter <span class="math inline">\(\theta\)</span> is called an <strong>ancillary statistic</strong>.</p>
<p><strong>First Order Ancillary</strong> A statistic <span class="math inline">\(V(\mathbf{X})\)</span> is <strong>first order ancillary</strong> if <span class="math inline">\(\mathbb E[V(\mathbf{X})]\)</span> is independent of <span class="math inline">\(\theta\)</span>.</p>
</div>
<div id="complete-statistic" class="section level3">
<h3>Complete Statistic</h3>
<p><strong>Complete Statistic</strong> Let <span class="math inline">\(f(t \vert{} \theta)\)</span> be a family of pdfs or
pmfs for a statistic <span class="math inline">\(T(\mathbf{X})\)</span>. The family of probability
distributions is called <strong>complete</strong> if for every (measurable) function
<span class="math inline">\(g\)</span>
<span class="math display">\[\mathbb E[g(T)] = 0  \,\, \forall \theta \implies P(g(T) = 0) = 1 \,\, \forall \theta.\]</span>
Equivalently, <span class="math inline">\(T(\mathbf{X})\)</span> is called a <strong>complete statistic</strong>.</p>
<p>(It is left unsaid that the function <span class="math inline">\(g\)</span> must be independent of
<span class="math inline">\(\theta\)</span>.)</p>
<p><strong>Basu’s Theorem</strong> If <span class="math inline">\(T(\mathbf{X})\)</span> is a complete and minimal sufficient
statistic, then <span class="math inline">\(T(\mathbf{X})\)</span> is independent of every ancillary
statistic.</p>
<p><strong>Complete statistics in the exponential family</strong> Let <span class="math inline">\(X_1, \dots, X_n\)</span> be
iid observations from a pmf/pdf <span class="math inline">\(f(x \vert{} \theta)\)</span> from an
exponential family
<span class="math display">\[f(x|\mathbf{\theta}) = h(x)c(\mathbf{\theta})\exp\left( \sum_{i=1}^k w_i(\mathbf{\theta}) t_i(x) \right),\]</span>
where <span class="math inline">\(\mathbf{\theta} = (\theta_1, \dots, \theta_d)\)</span>, <span class="math inline">\(d \leq k\)</span>. Then
<span class="math inline">\(\mathbf{T}(\mathbf{X})\)</span> defined by
<span class="math display">\[T_i(\mathbf{X}) = \sum_{j=1}^k t_i(\mathbf{X}_j)\]</span> is complete if
<span class="math inline">\(\{(w_1(\mathbf{\theta}), \dots, w_n(\mathbf{\theta}))\}\)</span> contains an open set
in <span class="math inline">\(\mathbb R^k\)</span>.</p>
<p>The open set criteria excludes curved exponential families.</p>
<p>If a minimal sufficient statistic exists, then every complete statistic
is minimal sufficient.</p>
</div>
<div id="the-likelihood-principle" class="section level3">
<h3>The Likelihood Principle</h3>
<p><strong>Likelihood</strong> Let <span class="math inline">\(f(\mathbf{x}\vert{} \theta)\)</span> denote the pmf/pdf of the
sample <span class="math inline">\(\mathbf{X}\)</span>. Then the <em>likelihood function</em>, given an observation
<span class="math inline">\(\mathbf{X} = \mathbf{x}\)</span>, is
<span class="math display">\[L(\theta \vert{} \mathbf{x}) = f(\mathbf{x} \vert{} \theta)\]</span> as a function
of <span class="math inline">\(\theta\)</span>.</p>
<p><strong>Likelihood Principle</strong> If <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(\mathbf{y}\)</span> are two sample points
such that <span class="math inline">\(L(\theta \vert{} \mathbf{x})\)</span> is proportional to
<span class="math inline">\(L(\theta \vert{} \mathbf{y})\)</span>, that is, there exists a constant
<span class="math inline">\(C(\mathbf{x}, \mathbf{y})\)</span> such that
<span class="math display">\[L(\theta \vert{} \mathbf{x}) = C(\mathbf{x}, \mathbf{y})L(\theta \vert{} \mathbf{y}) \quad \forall \theta,\]</span>
then the conclusions drawn from <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(\mathbf{y}\)</span> should be
identical.</p>
</div>
<div id="a-slightly-more-formal-construction" class="section level3">
<h3>A Slightly More Formal Construction</h3>
<p><strong>Experiment</strong> We define an <strong>experiment</strong> <span class="math inline">\(E\)</span> to be a triple
<span class="math inline">\((\mathbf{X}, \theta, f(\mathbf{x}\vert{}\theta))\)</span>, where <span class="math inline">\(\mathbf{X}\)</span> is a
random vector with pmf/pdf <span class="math inline">\(f\)</span>.<br />
</p>
<p>Knowing what experiment <span class="math inline">\(E\)</span> was performed, an experimenter will observer
<span class="math inline">\(\mathbf{X} = \mathbf{x}\)</span>. The conclusions they draw about <span class="math inline">\(\theta\)</span> will be
denoted <span class="math inline">\(\text{Ev}(E, \mathbf{x})\)</span>, which stands for <em>the evidence about
<span class="math inline">\(\theta\)</span> arising from <span class="math inline">\(E\)</span> and <span class="math inline">\(\mathbf{x}\)</span></em>.</p>
<p><strong>Formal Sufficiency Principle</strong> Consider an experiment
<span class="math inline">\(E = (\mathbf{X}, \theta, f(\mathbf{x}\vert{}\theta))\)</span> and suppose that
<span class="math inline">\(T(\mathbf{X})\)</span> is a sufficient statistic for <span class="math inline">\(\theta\)</span>. If <span class="math inline">\(\mathbf{x}\)</span> and
<span class="math inline">\(\mathbf{y}\)</span> are sample points satisfying <span class="math inline">\(T(\mathbf{x}) = T(\mathbf{y})\)</span>, then
<span class="math inline">\(\text{Ev}(E, \mathbf{x}) = \text{Ev}(E, \mathbf{y})\)</span>.</p>
<p><strong>Conditionality Principle</strong> Suppose that
<span class="math inline">\(E_1 = \{X_1, \theta, f_1(x_1\vert{} \theta)\}\)</span> and
<span class="math inline">\(E_2 = \{X_2, \theta, f_2(x_2\vert{} \theta)\}\)</span> are two experiments,
where only the unknown parameter <span class="math inline">\(\theta\)</span> need be common between the two
experiments. Consider the mixed experiment in which the random variable
<span class="math inline">\(J\)</span> is observed, where <span class="math inline">\(P(J = 1) = P(J = 2) = \frac12\)</span>
(independent of <span class="math inline">\(\mathbf{X}_1, \mathbf{X}_2, \theta\)</span>), and then the experiment
<span class="math inline">\(E_J\)</span> is performed. Formally, the experiment performed is
<span class="math inline">\(E^* = (\mathbf{X}^*, \theta, f(\mathbf{x}^* \vert{} \theta))\)</span>, where
<span class="math inline">\(\mathbf{X}^* = (j, \mathbf{X})j\)</span> and
<span class="math inline">\(f^*(\mathbf{x}^* \vert{} \theta) = f^*((j, \mathbf{x}_j)\vert{} \theta) = \frac12 f_j(\mathbf{x}_j \vert{}\theta)\)</span>.
Then <span class="math display">\[\text{Ev}(E^*, (j, \mathbf{x}_j)) = \text{Ev}(E_j, \mathbf{x_j}).\]</span>
That is, information about <span class="math inline">\(\theta\)</span> depends only on the experiment run
(not on the fact that the particular experiment was chosen).</p>
<p><strong>Formal Likelihood Principle</strong> Suppose that we have two
experiments, <span class="math inline">\(E_1 = (\mathbf{X}_1, \theta, f_1(\mathbf{x}_1 \vert{} \theta))\)</span>
and <span class="math inline">\(E_2 = (\mathbf{X}_2, \theta, f_2(\mathbf{x}_2 \vert{} \theta))\)</span> where the
unknown parameter <span class="math inline">\(\theta\)</span> is the same in both experiments. Suppose that
<span class="math inline">\(\mathbf{x}_1^*\)</span> and <span class="math inline">\(\mathbf{x}_2^*\)</span> are sample points from <span class="math inline">\(E_1\)</span> and <span class="math inline">\(E_2\)</span>
respectively, such that
<span class="math display">\[L(\theta \vert{} \mathbf{x}_2^*) = CL(\theta \vert{} \mathbf{x}_1^*)\]</span> for
all <span class="math inline">\(\theta\)</span> and for some constant <span class="math inline">\(C(\mathbf{x}_1^*, \mathbf{x}_2^*)\)</span> that is
independent of <span class="math inline">\(\theta\)</span>. Then
<span class="math display">\[\text{Ev}(E_1, \mathbf{x}_1^*) = \text{Ev}(E_2, \mathbf{x}_2^*).\]</span></p>
<p>Note that this is more general from the other likelihood principle since
it concerns two experiments (that we can of course set to be equal).</p>
<p><strong>Likelihood Principle Corollary</strong> If
<span class="math inline">\(E = (\mathbf{X}, \theta, f(\mathbf{x} \vert{} \theta))\)</span> is an experiment then
<span class="math inline">\(\text{Ev}(E, \mathbf{x})\)</span> should depend on <span class="math inline">\(E\)</span> and <span class="math inline">\(\mathbf{x}\)</span> only through
<span class="math inline">\(L(\theta \vert{} \mathbf{x})\)</span>.</p>
<p><strong>Birnbaum’s Theorem</strong> The Formal Likelihood Principle follows from the
Formal Sufficiency Principle and the Conditionality Principle. The
converse is also true.</p>
<p>Many common statistical procedures violate the Formal Likelihood
Principle – the topic of the applicability of these principles is not
settled. For instance, checking the residuals of a model (to grade the
model) violates the Sufficiency Principle. These notions are model
dependent, so may not be applicable until <em>after</em> we have decided on a
model.</p>
</div>
<div id="the-equivariance-principle" class="section level3">
<h3>The Equivariance Principle</h3>
<p><strong>Measurement Equivariance</strong> Inferences should not depend on the
measurement scale used.</p>
<p><strong>Formal Invariance</strong> If two inference problems have the same formal
structure, in terms of the mathematical model used, then the same
inference procedure should be used, regardless of the physical
realisation.</p>
<p><strong>Equivariance Principle</strong> If <span class="math inline">\(\mathbf{Y} = g(\mathbf{X})\)</span> is a change of
measurement scale such that the model <span class="math inline">\(\mathbf{Y}\)</span> has the same formal
structure as the model <span class="math inline">\(\mathbf{X}\)</span>, then an inference procedure should be
both measurement equivariant and formally invariant.</p>
<p>Let <span class="math inline">\(\mathcal{F} = \{f(\mathbf{x} \vert{} \theta): \theta \in \Theta\}\)</span> be
a set of pdfs or pmfs for <span class="math inline">\(\mathbf{X}\)</span>, and let <span class="math inline">\(\mathcal{G}\)</span> be group of
transformations on the sample space <span class="math inline">\(\mathcal X{}\)</span>. Then <span class="math inline">\(\mathcal{F}\)</span> is
<em>invariant under the group</em> <span class="math inline">\(\mathcal{G}\)</span> if for every
<span class="math inline">\(\theta \in \Theta\)</span> and <span class="math inline">\(g \in \mathcal{G}\)</span> there exists a unique
<span class="math inline">\(\theta&#39; \in \Theta\)</span> such that <span class="math inline">\(\mathbf{Y} = g(\mathbf{X})\)</span> has the
distribution <span class="math inline">\(f(\mathbf{y} \vert{} \theta&#39;)\)</span> if <span class="math inline">\(\mathbf{X}\)</span> has the
distribution <span class="math inline">\(f(\mathbf{x} \vert{} \theta&#39;)\)</span>.</p>
</div>
</div>
<div id="references" class="section level1 unnumbered">
<h1>References</h1>
<div id="refs" class="references csl-bib-body">
<div id="ref-casella2021statistical" class="csl-entry">
1. Casella G, Berger RL. Statistical inference. Cengage Learning; 2021.
</div>
</div>
</div>

    </div>
  </article>

  


</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
          </li>
          <li>
          </li>
        </ul>
      </footer>

    </div>
    

    
<script src="../../../../js/math-code.js"></script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
  </body>
</html>

