<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.80.0" />


<title>AOS chapter12 Bayesian Inference - A Hugo website</title>
<meta property="og:title" content="AOS chapter12 Bayesian Inference - A Hugo website">


  <link href='../../../../favicon.ico' rel='icon' type='image/x-icon'/>



  








<link href='//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css' rel='stylesheet' type='text/css' />



<link rel="stylesheet" href="../../../../css/fonts.css" media="all">
<link rel="stylesheet" href="../../../../css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="../../../../" class="nav-logo">
    <img src="../../../../images/logo.png"
         width="50"
         height="50"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="../../../../about/">about</a></li>
    
    <li><a href="../../../../vitae/">CV</a></li>
    
    <li><a href="https://github.com/danli349">GitHub</a></li>
    
    <li><a href="https://scholar.google.com/citations?user=mNjLK8EAAAAJ&amp;hl=en">Googlr Scholar</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">23 min read</span>
    

    <h1 class="article-title">AOS chapter12 Bayesian Inference</h1>

    
    <span class="article-date">2021-04-21</span>
    

    <div class="article-content">
      
<script src="../../../../2021/04/21/aos-chapter12-bayesian-inference/index_files/header-attrs/header-attrs.js"></script>

<div id="TOC">
<ul>
<li><a href="#bayesian-inference">12. Bayesian Inference</a>
<ul>
<li><a href="#bayesian-philosophy">12.1 Bayesian Philosophy</a></li>
<li><a href="#the-bayesian-method">12.2 The Bayesian Method</a></li>
<li><a href="#functions-of-parameters">12.3 Functions of Parameters</a></li>
<li><a href="#simulation">12.4 Simulation</a></li>
<li><a href="#large-sample-properties-for-bayes-procedures">12.5 Large Sample Properties for Bayes’ Procedures</a></li>
<li><a href="#flat-priors-improper-priors-and-noninformative-priors">12.6 Flat Priors, Improper Priors and “Noninformative” Priors</a></li>
<li><a href="#multiparameter-problems">12.7 Multiparameter Problems</a></li>
<li><a href="#strenghts-and-weaknesses-of-bayesian-inference">12.8 Strenghts and Weaknesses of Bayesian Inference</a></li>
<li><a href="#appendix">12.9 Appendix</a></li>
<li><a href="#exercises">12.11 Exercises</a></li>
</ul></li>
<li><a href="#references">References</a></li>
</ul>
</div>

<div id="bayesian-inference" class="section level2">
<h2>12. Bayesian Inference</h2>
<div id="bayesian-philosophy" class="section level3">
<h3>12.1 Bayesian Philosophy</h3>
<p>Postulates of <strong>frequentist (or classical)</strong> inference:</p>
<ul>
<li>Probabilty refers to limiting relative frequencies. Probabilities are objective properties of the real world.</li>
<li>Parameters are fixed, usually unknown constants. Because they are not fluctuating, no probability statements can be made about parameters.</li>
<li>Statistical procedures should be designed to have well defined long run frequency properties. For example, a 95% confidence interval should trap the true value of the parameter with limiting frequency at least 95%.</li>
</ul>
<p>Postulates of <strong>Bayesian</strong> inference:</p>
<ul>
<li>Probability indicates degrees of belief, not limiting frequency. As such, we can make probability statements about lots of things, not just data which are subject to random variation. For example, I might say that ‘the probability that Albert Einstein drank a cup of tea on August 1 1948 is 35%.’ This does not refer to limiting frequency. It reflects my strength of belief that the proposition is true.</li>
<li>We can make probability statements about parameters, even though they are fixed constants.</li>
<li>We can make inferences about a parameter <span class="math inline">\(\theta\)</span>, by producing a probability distribution for <span class="math inline">\(\theta\)</span>. Inferences, such as point estimates and interval estimates, may then be extracted from this distribution.</li>
</ul>
</div>
<div id="the-bayesian-method" class="section level3">
<h3>12.2 The Bayesian Method</h3>
<p>Bayesian inference is usually carried out in the following way:</p>
<ol style="list-style-type: decimal">
<li><p>Choose a probability density <span class="math inline">\(f(\theta)\)</span> – called the <strong>prior distribution</strong> – that expresses our degrees of belief about a parameter <span class="math inline">\(\theta\)</span> before we see any data.</p></li>
<li><p>We choose a statistical model <span class="math inline">\(f(x | \theta)\)</span> that reflects our beliefs about <span class="math inline">\(x\)</span> given <span class="math inline">\(\theta\)</span>.</p></li>
<li><p>After observing data <span class="math inline">\(X_1, \dots, X_n\)</span>, we update our beliefs and form the <strong>posterior distribution</strong> <span class="math inline">\(f(\theta | X_1, \dots, X_n)\)</span>.</p></li>
</ol>
<p>Bayesian update:</p>
<p><span class="math display">\[f(\theta | x^n) = \frac{f(x^n | \theta) f(\theta)}{\int f(x^n | \theta) f(\theta) d\theta} 
= \frac{\mathcal{L}_n(\theta) f(\theta)}{\int \mathcal{L}_n f(\theta) d\theta}
\propto \mathcal{L}_n(\theta) f(\theta)\]</span></p>
<p>We throw away the denominator, which is a constant that does not depend on <span class="math inline">\(\theta\)</span>, called a <strong>normalizing constant</strong>. We can summarize this by writing:</p>
<p><span class="math display">\[ \text{&quot;posterior is proportional to likelihood times prior&quot;} \]</span></p>
<p>We can obtain a <strong>point estimate</strong> using the posterior mean:</p>
<p><span class="math display">\[ \overline{\theta} = \int \theta f(\theta | x^n) d\theta = \frac{\int \theta \mathcal{L}_n(\theta) f(\theta)}{\int \mathcal{L}_n(\theta) f(\theta) d\theta}\]</span></p>
<p>We can also obtain a Bayesian interval estimate. Define <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> by $ _{-}^a f(| x^n) d= _b^f(| x^n) d= / 2$. Let <span class="math inline">\(C = (a, b)\)</span>. Then</p>
<p><span class="math display">\[\mathbb{P}(\theta \in C | x^n) = \int_a^b f(\theta | x^n) d\theta = 1 - \alpha\]</span></p>
<p>so <strong><span class="math inline">\(C\)</span> is a <span class="math inline">\(1 - \alpha\)</span> posterior interval</strong>.</p>
<p>When the prior and the posterior are in the same family, we say the prior is <strong>conjugate</strong>.</p>
</div>
<div id="functions-of-parameters" class="section level3">
<h3>12.3 Functions of Parameters</h3>
<p>The posterior CDF for <span class="math inline">\(\tau = g(\theta)\)</span> is</p>
<p><span class="math display">\[ H(\tau | x^n) = \mathbb{P}(g(\theta) \leq \tau) = \int_A f(\theta | x^n) d\theta\]</span></p>
<p>where <span class="math inline">\(A = \{ \theta : g(\theta) \leq \tau \}\)</span>. The posterior density is <span class="math inline">\(h(\tau | x^n) = H&#39;(\tau | x^n)\)</span>.</p>
</div>
<div id="simulation" class="section level3">
<h3>12.4 Simulation</h3>
<p>The posterior can often be approximated by simulation. Suppose we draw <span class="math inline">\(\theta_1, \dots, \theta_B \sim p(\theta | x^n)\)</span>. Then a histogram of <span class="math inline">\(\theta_1, \dots, \theta_n\)</span> approximates the posterior density <span class="math inline">\(p(\theta | x^n)\)</span>. An approximation to the posterior mean <span class="math inline">\(\overline{\theta}_n = \mathbb{E}(\theta | x^n)\)</span> is <span class="math inline">\(B^{-1} \sum_{j=1}^B \theta_j\)</span>. The posterior <span class="math inline">\(1 - \alpha\)</span> interval can be approximated by <span class="math inline">\((\theta_{\alpha/2}, \theta_{1 - \alpha/2})\)</span> where <span class="math inline">\(\theta_{\alpha/2}\)</span> is the <span class="math inline">\(\alpha/2\)</span> sample quantile of <span class="math inline">\(\theta_1, \dots, \theta_B\)</span>.</p>
<p>Once we have a sample <span class="math inline">\(\theta_1, \dots, \theta_B\)</span> from <span class="math inline">\(f(\theta | x^n)\)</span>, let <span class="math inline">\(\tau_i = g(\theta_i)\)</span>. Then <span class="math inline">\(\tau_1, \dots, \tau_B\)</span> is a sample from <span class="math inline">\(f(\tau | x^n)\)</span>. This avoids the need to do analytical calculations. Simulation is discussed in more detail later in the book.</p>
</div>
<div id="large-sample-properties-for-bayes-procedures" class="section level3">
<h3>12.5 Large Sample Properties for Bayes’ Procedures</h3>
<p><strong>Theorem 12.5</strong>. Under appropriate regularity conditions, we have that the posterior is approximately <span class="math inline">\(N(\hat{\theta}, \hat{\text{se}}^2)\)</span> where <span class="math inline">\(\hat{\theta}_n\)</span> is the MLE and <span class="math inline">\(\hat{\text{se}} = 1 / \sqrt{nI(\hat{\theta}_n)}\)</span>. Hence, <span class="math inline">\(\overline{\theta}_n \approx \hat{\theta}_n\)</span>. Also, if <span class="math inline">\(C_n = (\hat{\theta}_n - z_{\alpha/2} \hat{\text{se}}, \hat{\theta}_n + z_{\alpha/2} \hat{\text{se}})\)</span> is the asymptotic frequentist <span class="math inline">\(1 - \alpha\)</span> confidence interval, then <span class="math inline">\(C_n\)</span> is also an approximate <span class="math inline">\(1 - \alpha\)</span> Bayesian posterior interval:</p>
<p><span class="math display">\[\mathbb{P}(\theta \in C_n | X^n) \rightarrow 1 - \alpha\]</span></p>
<p>There is also a <strong>Bayesian delta method</strong>. Let <span class="math inline">\(\tau = g(\theta)\)</span>. Then</p>
<p><span class="math display">\[ \tau | X^n \approx N(\hat{\tau}, \tilde{\text{se}}^2)\]</span></p>
<p>where <span class="math inline">\(\hat{\tau} = g(\hat{\theta})\)</span> and <span class="math inline">\(\tilde{\text{se}} = \text{se} | g&#39;(\hat{\theta}) |\)</span>.</p>
</div>
<div id="flat-priors-improper-priors-and-noninformative-priors" class="section level3">
<h3>12.6 Flat Priors, Improper Priors and “Noninformative” Priors</h3>
<p><strong>Improper priors</strong>. We can adopt a flat prior <span class="math inline">\(f(\theta) \propto c\)</span> where <span class="math inline">\(c &gt; 0\)</span> is a constant. Note that <span class="math inline">\(\int f(\theta) d\theta = \infty\)</span> so this is not a real probability density function in a proper sense. We call such prior an improper prior. In general, improper priors are not a problem as long as the resulting posterior is a well defined probability distribution.</p>
<p><strong>Flat priors are not invariant</strong>. The notion of a flat prior is not well-defined because a flat prior on a parameter does not imply a flat prior on a transformed version of a parameter.</p>
<p><strong>Jeffreys’ prior</strong>. Take <span class="math inline">\(f(\theta) \propto I(\theta)^{1/2}\)</span> where <span class="math inline">\(I(\theta)\)</span> is the Fisher information function. This rule turns out to be transformation invariant.</p>
<p>In a multiparameter problem, the Jeffreys’ prior is defined to be <span class="math inline">\(f(\theta) \propto \sqrt{\text{det} I(\theta)}\)</span>, where <span class="math inline">\(\text{det}(A)\)</span> denotes the determinant of a matrix <span class="math inline">\(A\)</span>.</p>
</div>
<div id="multiparameter-problems" class="section level3">
<h3>12.7 Multiparameter Problems</h3>
<p>Suppose that <span class="math inline">\(\theta = (\theta_1, \dots, \theta_p)\)</span>. The posterior density is still given by</p>
<p><span class="math display">\[p(\theta | x^n) \propto \mathcal{L}(\theta) f(\theta)\]</span></p>
<p>Suppose we want to make inferences for <span class="math inline">\(\theta_1\)</span>. The marginal posterior for <span class="math inline">\(\theta_1\)</span> is</p>
<p><span class="math display">\[ f(\theta_1 | x^n) = \int \cdots \int f(\theta_1, \dots, \theta_p) d\theta_2 \dots d\theta_p\]</span></p>
<p>In practice, it might not be feasible to do this integral. Simulation can help. Draw randomly from the posterior:</p>
<p><span class="math display">\[\theta^1, \dots, \theta^B \sim f(\theta | x^n)\]</span></p>
<p>where the superscripts index the different draws; each <span class="math inline">\(\theta^j\)</span> is a vector <span class="math inline">\(\theta^j = (\theta^j_1, \dots, \theta^j_p)\)</span>. Now collect together the first component of each draw:</p>
<p><span class="math display">\[\theta^1_1, \dots, \theta^B_1\]</span></p>
<p>These form a sample from <span class="math inline">\(f(\theta_1 | x^n)\)</span> and we have avoided doing any integrals.</p>
</div>
<div id="strenghts-and-weaknesses-of-bayesian-inference" class="section level3">
<h3>12.8 Strenghts and Weaknesses of Bayesian Inference</h3>
<p>Frequentist and Bayesian methods are answering different questions:</p>
<ul>
<li>To combine prior beliefs with data in a principled way, use bayesian inference.</li>
<li>To construct procedures with guaranteed long run performance, such as confidence intervals, use frequentist methods.</li>
</ul>
</div>
<div id="appendix" class="section level3">
<h3>12.9 Appendix</h3>
<p><strong>Proof of Theorem 12.5</strong>.</p>
<p>It can be shown that the effect of the prior diminishes as <span class="math inline">\(n\)</span> increases so that <span class="math inline">\(f(\theta | X^n) \propto \mathcal{L}_n(\theta)f(\theta) \approx \mathcal{L}_n(\theta)\)</span>. Hence, <span class="math inline">\(\log f(\theta | X^n) \approx \ell(\theta)\)</span>. Now, <span class="math inline">\(\ell(\theta) \approx \ell(\hat{\theta}) + (\theta - \hat{\theta})\ell&#39;(\hat{\theta}) + [(\theta - \hat{\theta})^2/2]\ell&#39;&#39;(\hat{\theta}) = \ell(\hat{\theta}) + [(\theta - \hat{\theta})^2/2] \ell&#39;&#39;(\hat{\theta})\)</span> since <span class="math inline">\(\ell&#39;(\hat{\theta}) = 0\)</span>. Exponentiating, we get approximately that</p>
<p><span class="math display">\[ f(\theta | X^n) \propto \exp \left\{ - \frac{1}{2} \frac{(\theta - \hat{\theta})^2}{\sigma_n^2} \right\} \]</span></p>
<p>where <span class="math inline">\(\sigma_n^2 = -1 / \ell&#39;&#39;(\hat{\theta}_n)\)</span>. So the posterior of <span class="math inline">\(\theta\)</span> is approximately Normal with mean <span class="math inline">\(\hat{\theta}\)</span> and variance <span class="math inline">\(\sigma^2_n\)</span>. Let <span class="math inline">\(\ell_i = \log f(X_i | \theta)\)</span>, then</p>
<p><span class="math display">\[
\begin{align}
\sigma_n^{-2} &amp;= -\ell&#39;&#39;(\hat{\theta}_n) = \sum_i -\ell&#39;&#39;_i(\hat{\theta}_n) \\
&amp;= n \left( \frac{1}{n} \right) \sum_i -\ell&#39;&#39;_i(\hat{\theta}_n) \approx n \mathbb{E}_{\theta}\left[-\ell&#39;&#39;_i(\hat{\theta}_n)\right] \\
&amp;= n I(\hat{\theta}_n)
\end{align}
\]</span></p>
<p>and hence <span class="math inline">\(\sigma_n \approx \text{se}(\hat{\theta})\)</span>.</p>
</div>
<div id="exercises" class="section level3">
<h3>12.11 Exercises</h3>
<p><strong>Exercise 12.11.1</strong>. Verify (12.5).</p>
<p><em>Let <span class="math inline">\(X_1, \dots, X_n \sim N(\theta, \sigma^2)\)</span>. For simplicity, let us assume that <span class="math inline">\(\sigma\)</span> is known. Suppose that we take as a prior <span class="math inline">\(\theta \sim N(a, b^2)\)</span>. In problem 1 of the homework, it is shown that the posterior for <span class="math inline">\(\theta\)</span> is</em></p>
<p><span class="math display">\[\theta | X^n \sim N(\overline{\theta}, \tau^2)\]</span></p>
<p><em>where</em></p>
<p><span class="math display">\[\overline{\theta} = w \overline{X} + (1 - w) a\]</span></p>
<p><em>where</em></p>
<p><span class="math display">\[w = \frac{\frac{1}{\text{se}^2}}{\frac{1}{\text{se}^2} + \frac{1}{\text{b}^2}} \quad \text{and} \quad \frac{1}{\tau^2} = \frac{1}{\text{se}^2} + \frac{1}{b^2}\]</span></p>
<p><em>and <span class="math inline">\(\text{se} = \sigma / \sqrt{n}\)</span> is the standard error of the MLE <span class="math inline">\(\overline{X}\)</span>.</em></p>
<p><strong>Solution</strong>.</p>
<p>The posterior is proportional to the likelihood times the prior:</p>
<p><span class="math display">\[ p(\theta | x^n) \propto \mathcal{L}_n(\theta) f(\theta)\]</span></p>
<p>The likelihood is the product of <span class="math inline">\(n\)</span> Normal PDFs evaluated at different data points:</p>
<p><span class="math display">\[ 
\mathcal{L}_n(\theta) = \prod_{i=1}^n f_X(X_i, \theta)
\]</span></p>
<p>Looking at the case <span class="math inline">\(n = 2\)</span>:</p>
<p><span class="math display">\[
\begin{align} 
f(X_1, \theta) f(X_2, \theta) &amp;= 
\frac{1}{\sigma \sqrt{2 \pi}} \exp \left\{-\frac{1}{2} \left(\frac{X_1 - \mu}{\sigma} \right)^2 \right\}
\frac{1}{\sigma \sqrt{2 \pi}} \exp \left\{-\frac{1}{2} \left(\frac{X_2 - \mu}{\sigma} \right)^2 \right\} \\
&amp;= \left( \frac{1}{\sigma \sqrt{2 \pi}} \right)^2 \exp \left\{-\frac{1}{2\sigma^2} \left((X_1 - \mu)^2 + (X_2 - \mu)^2 \right) \right\} \\
&amp;= \left( \frac{1}{\sigma \sqrt{2 \pi}} \right)^2 \exp \left\{-\frac{1}{2\sigma^2} \left(
X_1^2 + 2 X_1 X_2 + X_2^2 - 2 (X_1 + X_2)\mu + 2\mu^2 - 2 X_1 X_2
\right) \right\} \\
&amp;= \left( \frac{1}{\sigma \sqrt{2 \pi}} \right)^2 \exp \left\{-\frac{1}{\sigma^2}\left(
\left(\mu - \frac{X_1 + X_2}{2}\right)^2 + \left(\frac{X_1 + X_2}{2}\right)^2 - X_1 X_2
\right) \right\} \\
&amp;= \left( \frac{1}{\sigma \sqrt{2 \pi}} \right)^2 \exp \left\{-\frac{1}{2} \left(
\frac{\left(\mu - \frac{X_1 + X_2}{2}\right)^2 + \left(\frac{X_1 + X_2}{2}\right)^2 - X_1 X_2}{\sigma^2 / 2}
\right) \right\} \\
&amp;= \left( \frac{1}{\sigma \sqrt{2 \pi}} \right)^2 \exp \left\{-\frac{1}{2} \left(
\frac{\left(\mu - \frac{X_1 + X_2}{2}\right)^2}{\sigma^2 / 2} + C
\right) \right\} \\
&amp;\propto \exp \left\{-\frac{1}{2}
\left(\frac{\mu - \frac{X_1 + X_2}{2}}{\sigma / \sqrt{2}}\right)^2
\right\}
\end{align}
\]</span></p>
<p>The exponent is a quadratic form in <span class="math inline">\(\mu\)</span>, and that makes this expression is proportional to the PDF of a Normal with mean <span class="math inline">\(\mu\)</span> and standard error <span class="math inline">\(\sigma / \sqrt{2}\)</span>, observed at the mean, <span class="math inline">\(X = (X_1 + X_2)/2\)</span>.</p>
<p>This strongly suggests that, for the general case, the product is proportional to the PDF of a Normal with mean <span class="math inline">\(\mu\)</span> and standard error <span class="math inline">\(\sigma / \sqrt{n}\)</span>, observed at the mean, <span class="math inline">\(X = n^{-1} \sum_{i=1}^n X_i\)</span>. We can prove it by induction, going through similar steps:</p>
<p><span class="math display">\[
\begin{align} 
f((X_1, \dots, X_n), \theta_n) f(X_{n+1}, \theta) &amp;= 
\frac{1}{\sigma_n \sqrt{2 \pi}} \exp \left\{-\frac{1}{2} \left(\frac{\overline{X}_n - \mu}{\sigma_n} \right)^2 \right\}
\frac{1}{\sigma \sqrt{2 \pi}} \exp \left\{-\frac{1}{2} \left(\frac{X_{n+1} - \mu}{\sigma} \right)^2 \right\} \\
&amp;\propto \exp \left\{-\frac{1}{2} \left( \left(\frac{\overline{X}_n - \mu}{\sigma_n} \right)^2 + \left(\frac{X_{n+1} - \mu}{\sigma} \right)^2 \right) \right\} \\
&amp;= \exp \left\{-\frac{1}{2} \left( n \left(\frac{\overline{X}_n - \mu}{\sigma} \right)^2 + \left(\frac{X_{n+1} - \mu}{\sigma} \right)^2 \right) \right\} \\
&amp;= \exp \left\{-\frac{1}{2 \sigma^2} \left( n \left(\overline{X}_n - \mu\right)^2 + \left(X_{n+1} - \mu \right)^2 \right) \right\} \\
&amp;= \exp \left\{-\frac{1}{2 \sigma^2} \left( n \left(\overline{X}_n^2 - 2\overline{X}_n\mu+\mu^2\right) + \left(X_{n+1}^2 - 2X_{n+1}\mu +\mu^2\right) \right) \right\} \\
&amp;\propto \exp \left\{-\frac{1}{2} \left( \frac{\mu - \frac{n\overline{X}_n + X_{n+1}}{{n+1}}}{\sigma / \sqrt{n+1}} \right)^2 \right\}\\
&amp;= \exp \left\{-\frac{1}{2}
\left(\frac{\mu - \overline{X}_{n+1}}{\sigma / \sqrt{n+1}}\right)^2
\right\}
\end{align}
\]</span></p>
<p>Finally, we just need to prove one final result – that the product of the likelihood and the prior, both of which are PDFs of normals, is proportional to yet another normal, with the parameters given. The steps are again similar – except now we are completing the squares in <span class="math inline">\(\theta\)</span>, rather than <span class="math inline">\(\mu\)</span>, as each PDF has distinct mean (<span class="math inline">\(\overline{X}\)</span> and <span class="math inline">\(a\)</span>) and standard error (<span class="math inline">\(\text{se}\)</span> and <span class="math inline">\(b\)</span>).</p>
<p><span class="math display">\[
\begin{align}
\mathcal{L}_n(\theta) f(\theta) 
&amp;= \frac{1}{\text{se} \sqrt{2 \pi}} \exp \left\{-\frac{1}{2} \left(\frac{\overline{X} - \theta}{\text{se}}\right)^2 \right\} \frac{1}{b \sqrt{2 \pi}} \exp \left\{-\frac{1}{2} \left( \frac{a - \theta}{b} \right)^2 \right\} \\
&amp; \propto \exp \left\{-\frac{1}{2} \left( \left(\frac{\overline{X} - \theta}{\text{se}}\right)^2 + \left( \frac{a - \theta}{b} \right)^2 \right) \right\} \\
&amp; \propto \exp \left\{-\frac{1}{2} \left( \frac{\overline{\theta} - \theta}{\tau} \right)^2 \right\}
\end{align}
\]</span></p>
<p><em>where</em></p>
<p><span class="math display">\[\overline{\theta} = w \overline{X} + (1 - w) a\]</span></p>
<p><em>where</em></p>
<p><span class="math display">\[w = \frac{\frac{1}{\text{se}^2}}{\frac{1}{\text{se}^2} + \frac{1}{\text{b}^2}}= \frac{1}{1 + \frac{\text{se}^2}{\text{b}^2}}=\frac{\text{b}^2}{\text{b}^2 + \text{se}^2}\]</span> and
<span class="math display">\[\frac{1}{\tau^2} = \frac{1}{\text{se}^2} + \frac{1}{\text{b}^2}\]</span>
<span class="math display">\[\tau^2 = \frac{\text{se}^2\text{b}^2}{\text{se}^2+\text{b}^2}\]</span></p>
<p><em>and <span class="math inline">\(\text{se} = \sigma / \sqrt{n}\)</span> is the standard error of the MLE <span class="math inline">\(\overline{X}\)</span>.</em>
which is the desired result.</p>
<p>An <strong>excruciatingly complete</strong> derivation of the results used here – products of Normal PDFs are proportional to Normal PDFs – and more general versions of it can be seen in Bromiley, Paul. “Products and convolutions of Gaussian probability density functions.” Tina-Vision Memo 3.4 (2003): 1, which is available online.</p>
<p><strong>Exercise 12.11.2</strong>. Let <span class="math inline">\(X_1, \dots, X_n \sim N(\mu, 1)\)</span>.</p>
<p><strong>(a)</strong> Simulate a dataset (using <span class="math inline">\(\mu = 5\)</span>) consisting of <span class="math inline">\(n = 100\)</span> observations.</p>
<p><strong>(b)</strong> Take <span class="math inline">\(f(\mu) = 1\)</span> and find the posterior density. Plot the density.</p>
<p><strong>(c)</strong> Simulate 1000 draws from the posterior. Plot a histogram of the simulated values and compare the histogram to the answer in (b).</p>
<p><strong>(d)</strong> Let <span class="math inline">\(\theta = e^\mu\)</span>. Find the posterior density for <span class="math inline">\(\theta\)</span> analytically and by simulation.</p>
<p><strong>(e)</strong> Find a 95% posterior interval for <span class="math inline">\(\theta\)</span>.</p>
<p><strong>(f)</strong> Find a 95% confidence interval for <span class="math inline">\(\theta\)</span>.</p>
<p><strong>Solution</strong>.</p>
<p><strong>(a)</strong></p>
<pre class="python"><code>import numpy as np
from scipy.stats import norm

import matplotlib.pyplot as plt

n = 100
mu = 5
sigma = 1
X = norm.rvs(loc=mu, scale=sigma, size=n)
plt.scatter(np.arange(1,101,1),X)
plt.plot(np.arange(1,101,1), np.zeros_like(np.arange(1,101,1))+X.mean(),color=&#39;r&#39;)</code></pre>
<pre><code>[&lt;matplotlib.lines.Line2D at 0x7fd9a5f08a90&gt;]</code></pre>
<div class="figure">
<img src="Chapter%2012%20-%20Bayesian%20Inference_files/Chapter%2012%20-%20Bayesian%20Inference_34_1.png" alt="" />
<p class="caption">png</p>
</div>
<p><strong>(b)</strong> The posterior is proportional to likelihood times the prior:</p>
<p><span class="math display">\[f(\mu | X^n) \propto \mathcal{L}_n(\mu) f(\mu) = \mathcal{L}_n(\mu)\]</span></p>
<p>But the likelihood is the product of the PDFs for each data point:</p>
<p><span class="math display">\[\mathcal{L}_n(\mu) = \prod_{i=1}^n f(X_i; \mu)\]</span></p>
<p>We can plot this with “brute force” – calculating this product for each sample point – or use the analytic result that the likelihood is proportional to a random variable following a Normal distribution in <span class="math inline">\(\mu\)</span> with mean <span class="math inline">\(\hat{\mu} = \overline{X}\)</span> and standard error <span class="math inline">\(\sigma / \sqrt{n}\)</span>:</p>
<p><span class="math display">\[\mathcal{L}_n(\mu) \propto N(\overline{X}, \sigma^2 / n)\]</span></p>
<pre class="python"><code># posterior is proportional to likelihood times f(\mu)
# likelihood is \prod_i f(X_i; \mu)
# pdf is f(x; (\mu, \sigma^2)), the density of the normal function

mu_hat = X.mean()
mu_values = np.linspace(4, 6, 100)</code></pre>
<pre class="python"><code># Brute-force solution: explicitly compute the PDF for each sample, and multiply everything 
# (or take logs, add, and exponentiate):
likelihood = np.vectorize(lambda mu_hat: np.exp(np.log(norm.pdf(X, loc=mu_hat, scale=sigma)).sum()))
L_i = likelihood(mu_values)

plt.plot(mu_values, L_i / L_i.sum());</code></pre>
<div class="figure">
<img src="Chapter%2012%20-%20Bayesian%20Inference_files/Chapter%2012%20-%20Bayesian%20Inference_37_0.png" alt="" />
<p class="caption">png</p>
</div>
<pre class="python"><code># Alternatively, use the analytic solution, \mathcal{L}_n(\mu) \sim N(\mu_hat, \sigma^2/n)

L_i2 = norm.pdf(mu_values, loc=mu_hat, scale=sigma/np.sqrt(n))

plt.plot(mu_values, L_i2 / L_i2.sum());</code></pre>
<div class="figure">
<img src="Chapter%2012%20-%20Bayesian%20Inference_files/Chapter%2012%20-%20Bayesian%20Inference_38_0.png" alt="" />
<p class="caption">png</p>
</div>
<p><strong>(c)</strong></p>
<pre class="python"><code>fig, (ax1,ax2) = plt.subplots(2, 1, sharex=&#39;col&#39;)
ax1.plot(mu_values, L_i2 / L_i2.sum())

posterior_samples = norm.rvs(loc=mu_hat, scale=sigma/np.sqrt(n), size=10000)
ax2.hist(posterior_samples, density=True, bins=mu_values);</code></pre>
<div class="figure">
<img src="Chapter%2012%20-%20Bayesian%20Inference_files/Chapter%2012%20-%20Bayesian%20Inference_40_0.png" alt="" />
<p class="caption">png</p>
</div>
<p><strong>(d)</strong></p>
<p>Let <span class="math inline">\(Y = e^X\)</span>. Analytically:</p>
<ul>
<li>The CDF is <span class="math inline">\(F_Y(z) = \mathbb{P}_\theta(e^X \leq z) = \mathbb{P}_\mu(X \leq \log z) = \mathbb{P}_\mu \left( \frac{X - \mu}{\sigma} \leq \frac{\log z - \mu}{\sigma} \right) = \mathbb{P}(Z \leq \log z - \mu) = \Phi(\log z - \mu)\)</span>, where <span class="math inline">\(\Phi\)</span> is the CDF of a standard normal distribution.</li>
<li>The PDF is <span class="math inline">\(f_Y(z) = F&#39;_Y(z) = \partial \Phi(\log z - \mu) / \partial z = \phi(\log z - \mu) / z\)</span>, where <span class="math inline">\(\phi = \Phi&#39;\)</span> is the PDF of a standard normal function.</li>
</ul>
<pre class="python"><code>def posterior_density(z):
    # Suppress warnings about log(z) when z &lt; 0 and division by zero 
    # np.where will filter out invalid values
    with np.errstate(divide=&#39;ignore&#39;, invalid=&#39;ignore&#39;):
        return np.where(z &gt; 0, norm.pdf(np.log(z) - mu_hat) / z, 0)
    
z_values = np.linspace(0, 500, 100)
f_values = posterior_density(z_values)

plt.plot(z_values, f_values);</code></pre>
<div class="figure">
<img src="Chapter%2012%20-%20Bayesian%20Inference_files/Chapter%2012%20-%20Bayesian%20Inference_42_0.png" alt="" />
<p class="caption">png</p>
</div>
<pre class="python"><code># By simulation:

# Resample from the estimated parametric distribution for X, and calculate Y = exp(X)
Y = np.exp(norm.rvs(loc=mu_hat, scale=sigma, size=10000))

# Recompute ranges for plot based on observed Y values
z_values = np.linspace(0, max(Y), 100)
f_values = posterior_density(z_values)

fig, (ax1,ax2) = plt.subplots(2, 1, sharex=&#39;col&#39;)
ax1.plot(z_values, f_values)
ax2.hist(Y, density=True, bins=z_values);</code></pre>
<div class="figure">
<img src="Chapter%2012%20-%20Bayesian%20Inference_files/Chapter%2012%20-%20Bayesian%20Inference_43_0.png" alt="" />
<p class="caption">png</p>
</div>
<p><strong>(e)</strong></p>
<p>Analytic solution: <span class="math inline">\(Y = g(X) = e^X\)</span> and <span class="math inline">\(X \sim N(\mu, 1)\)</span>. Since <span class="math inline">\(g\)</span> is a monotonically increasing function, the quantiles of <span class="math inline">\(Y\)</span> are the exponentials of the quantiles of <span class="math inline">\(X\)</span>; that is,</p>
<p><span class="math display">\[F_Y^{-1}(q) = g(F_X^{-1}(q)) = e^{F_X^{-1}(q)}\]</span></p>
<p>But <span class="math inline">\(X\)</span> follows a Normal distribution – we can plug in the MLE for <span class="math inline">\(\mu\)</span>, <span class="math inline">\(\hat{\mu} = \overline{X}\)</span>, obtain the quantiles for <span class="math inline">\(X\)</span>, and then obtain the quantiles for <span class="math inline">\(Y\)</span>.</p>
<p><span class="math display">\[
\begin{align}
F_X^{-1}(q) &amp;= \Phi^{-1}(q) / \sigma + \mu \\
F_Y^{-1}(q) &amp;= e^{\Phi^{-1}(q) / \sigma + \mu}
\end{align}
\]</span></p>
<pre class="python"><code>from scipy.stats import norm

z_025 = norm.ppf(0.025)
z_975 = norm.ppf(0.975)

posterior_interval_analytic = (np.exp(z_025 + mu_hat), np.exp(z_975 + mu_hat))
print(&quot;95%% posterior interval (analytic):  %.3f, %.3f&quot; % posterior_interval_analytic)</code></pre>
<pre><code>95% posterior interval (analytic):  21.183, 1067.580</code></pre>
<p>Alternatively, we can just sample from our simulation draws to get an approximation:</p>
<pre class="python"><code># Find percentile from simulated draws

posterior_interval_simulation = (
    np.quantile(Y, 0.025),
    np.quantile(Y, 0.975)
)

print(&quot;95%% posterior interval (simulation):  %.3f, %.3f&quot; % posterior_interval_simulation)</code></pre>
<pre><code>95% posterior interval (simulation):  21.432, 1049.800</code></pre>
<p><strong>(f)</strong></p>
<p>For the Bayesian interval estimate, we need to find <span class="math inline">\(C = (a, b)\)</span> such that</p>
<p><span class="math display">\[ \int_{-\infty}^a f_Y(\theta | X^n) d\theta = \int_b^\infty f_Y(\theta | X^n) d\theta = \frac{\alpha}{2}\]</span></p>
<p>or, using the cumulative density functions,</p>
<p><span class="math display">\[ F_Y(a) = 1 - F_Y(b) = \frac{\alpha}{2}\]</span></p>
<p><span class="math display">\[\Phi(\log a - \mu) = 1 - \Phi(\log b - \mu) = \frac{\alpha}{2}\]</span></p>
<p>Solving for <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>,</p>
<p><span class="math display">\[
a = e^{\mu + \Phi^{-1}(\alpha / 2)}
\quad \text{and} \quad
b = e^{\mu + \Phi^{-1}(1 - \alpha / 2)}
\]</span></p>
<p>This is exactly the same calculation as the analytic solution for the posterior interval.</p>
<pre class="python"><code>from scipy.stats import norm

z_025 = norm.ppf(0.025)
z_975 = norm.ppf(0.975)

confidence_interval_analytic = (np.exp(z_025 + mu_hat), np.exp(z_975 + mu_hat))
print(&quot;95%% confidence interval (analytic):  %.3f, %.3f&quot; % confidence_interval_analytic)</code></pre>
<pre><code>95% confidence interval (analytic):  21.183, 1067.580</code></pre>
<p><strong>Exercise 12.11.3</strong>. Let <span class="math inline">\(X_1, \dots, X_n \sim \text{Uniform}(0, \theta)\)</span>. Let <span class="math inline">\(f(\theta) \propto 1/\theta\)</span>. Find the posterior density.</p>
<p><strong>Solution</strong>. The posterior density is proportional to the likelihood times the prior:</p>
<p><span class="math display">\[
\begin{align}
f(\theta | X^n) &amp;\propto \mathcal{L}_n(\theta) f(\theta) \\
&amp;= \frac{1}{\theta} \prod_{i=1}^n f(X_i; \theta)  \\
&amp;= \frac{1}{\theta} \prod_{i=1}^n \frac{I(X_i \leq \theta)}{\theta} \\
&amp;= \theta^{-(n+1)} I( \max \{ X_1, \dots, X_n \} \leq \theta \} ) \\
&amp;= \begin{cases}
\theta^{-(n+1)} &amp; \text{if } \theta \geq \max \{ X_1, \dots, X_n \} \\
0 &amp; \text{otherwise}
\end{cases}
\end{align}
\]</span></p>
<p>Now we just need to normalize the posterior density so it integrates to 1. Let <span class="math inline">\(m = \max \{ X_1, \dots, X_n \}\)</span>. Then:</p>
<p><span class="math display">\[ 1 = \int_{-\infty}^\infty f(\theta) d\theta  = \int_m^\infty c \theta^{-(n+1)} d\theta = c m^{-n} / n\]</span></p>
<p>Solving this we get <span class="math inline">\(c = n m^n\)</span>, so the posterior density is:</p>
<p><span class="math display">\[
f(\theta) = \begin{cases}
\frac{n}{\theta} \left(\frac{m}{\theta}\right)^n &amp; \text{if } \theta \geq m \\
0 &amp; \text{otherwise}
\end{cases}
\]</span></p>
<p>where <span class="math inline">\(m = \max \{ X_1, \dots, X_n \}\)</span>.</p>
<p><strong>Exercise 12.11.4</strong>. Suppose that 50 people are given a placebo and 50 are given a new treatment. 30 placebo patients show improvement while 40 treated patients show improvement. Let <span class="math inline">\(\tau = p_2 - p_1\)</span> where <span class="math inline">\(p_2\)</span> is the probability of improving under treatment and <span class="math inline">\(p_1\)</span> is the probability of improving under placebo.</p>
<p><strong>(a)</strong> Find the MLE of <span class="math inline">\(\tau\)</span>. Find the standard error and 90% confidence interval using the delta method.</p>
<p><strong>(b)</strong> Find the standard error and 90% confidence interval using the parametric bootstrap.</p>
<p><strong>(c)</strong> Use the prior <span class="math inline">\(f(p_1, p_2) = 1\)</span>. Use simulation to find the posterior mean and posterior 90% interval for <span class="math inline">\(\tau\)</span>.</p>
<p><strong>(d)</strong> Let</p>
<p><span class="math display">\[ \psi = \log \left( \left( \frac{p_1}{1 - p_1} \div \frac{p2}{1 - p2} \right) \right) \]</span></p>
<p>be the log-odds ratio. Note that <span class="math inline">\(\psi = 0\)</span> if <span class="math inline">\(p_1 = p_2\)</span>. Find the MLE of <span class="math inline">\(\psi\)</span>. Use the delta method to find a 90% confidence interval for <span class="math inline">\(\psi\)</span>.</p>
<p><strong>(e)</strong> Use simulation to find the posterior mean and posterior 90% interval for <span class="math inline">\(\psi\)</span>.</p>
<p><strong>Solution</strong>.</p>
<p><strong>(a)</strong></p>
<p>We have two random variables, <span class="math inline">\(X_1 \sim \text{Binomial}(n_1, p_1)\)</span> and <span class="math inline">\(X_2 \sim \text{Binomial}(n_2, p_2)\)</span>. We are given <span class="math inline">\(n_1 = n_2 = 50\)</span>, and the measurements <span class="math inline">\(X_1 = 30\)</span> and <span class="math inline">\(X_2 = 40\)</span>.</p>
<p>We wish to estimate <span class="math inline">\(\tau = g(p_1, p_2) = p_2 - p_1\)</span>. The MLE is simply <span class="math inline">\(\hat{\tau} = g(\hat{p}_1, \hat{p}_2) = \hat{p}_2 - \hat{p}_1 = X_2 / n_2 - X_1 / n_1\)</span>.</p>
<p>Using the multiparameter delta method, the gradient of <span class="math inline">\(g\)</span> is</p>
<p><span class="math display">\[ \nabla g = 
\begin{pmatrix} \partial g / \partial p_1 \\ \partial g / \partial p_2 \end{pmatrix} 
= \begin{pmatrix} -1 \\ 1 \end{pmatrix} 
\]</span></p>
<p>The log likelihood is:</p>
<p><span class="math display">\[ 
\begin{align}
\ell(p_1, p_2) &amp;= \log f_{X_1}(X_1; p_1) + \log f_{X_2}(X_2; p_2) \\
&amp; = \log \binom{n_1}{X_1} p_1^{X_1} (1 - p_1)^{n_1 - X_1}
+ \log \binom{n_2}{X_2} p_2^{X_2} (1 - p_2)^{n_2 - X_2} \\
&amp;= \log \binom{n_1}{X_1} + X_1 \log p_1 + (n_1 - X_1) \log (1 - p_1)
+ \log \binom{n_2}{X_2} + X_2 \log p_2 + (n_2 - X_2) \log (1 - p_2) \\
&amp;= X_1 \log p_1 + (n_1 - X_1) \log (1 - p_1) + X_2 \log p_2 + (n_2 - X_2) \log (1 - p_2) + C
\end{align}
\]</span></p>
<p>where <span class="math inline">\(C\)</span> does not depend on <span class="math inline">\(p_1\)</span> or <span class="math inline">\(p_2\)</span>.</p>
<p>The partial derivatives of the log likelihood are:</p>
<p><span class="math display">\[
\begin{align}
\frac{\partial^2 \ell}{\partial p_1^2} &amp; = - \frac{X_1}{p_1^2} - \frac{n_1 - X_1}{(1 - p_1)^2} \\
\frac{\partial^2 \ell}{\partial p_2^2} &amp; = - \frac{X_2}{p_2^2} - \frac{n_2 - X_2}{(1 - p_2)^2} \\
\frac{\partial^2 \ell}{\partial p_1 \partial p_2} &amp; = 0
\end{align}
\]</span></p>
<p>The Fisher Information Matrix is</p>
<p><span class="math display">\[ I_n(p_1, p_2) = -\begin{pmatrix}
\mathbb{E}\left[ \frac{\partial^2 \ell}{\partial p_1^2} \right]
&amp; \mathbb{E}\left[ \frac{\partial^2 \ell}{\partial p_1 \partial p_2} \right]  \\
\mathbb{E}\left[ \frac{\partial^2 \ell}{\partial p_1 \partial p_2} \right]
&amp; \mathbb{E}\left[ \frac{\partial^2 \ell}{\partial p_2^2} \right]
\end{pmatrix} = \begin{pmatrix}
\frac{n_1}{p_1(1 - p_1)} &amp; 0 \\
0 &amp; \frac{n_2}{p_2(1 - p_2)}
\end{pmatrix}\]</span></p>
<p>and its inverse is</p>
<p><span class="math display">\[J_n(p_1, p_2) = I_n^{-1}(p_1, p_2) = \begin{pmatrix} 
\frac{p_1(1 - p_1)}{n_1} &amp; 0 \\
0 &amp; \frac{p_2(1 - p_2)}{n_2}
\end{pmatrix}\]</span></p>
<p>Then,</p>
<p><span class="math display">\[\hat{\text{se}}(\hat{\tau}) = \sqrt{(\hat{\nabla}g)^T \hat{J}_n (\hat{\nabla}g)}
= \sqrt{\frac{\hat{p}_1(1 - \hat{p}_1)}{n_1} + \frac{\hat{p}_2(1 - \hat{p}_2)}{n_2}}
\]</span></p>
<p>And we get the confidence interval:</p>
<p><span class="math display">\[ \hat{\tau} \pm z_{1 - \alpha/2}\hat{\text{se}}(\hat{\tau})\]</span></p>
<pre class="python"><code>import numpy as np
from scipy.stats import norm

n1 = 50
n2 = 50

X1 = 30
X2 = 40

p1_hat = X1 / n1
p2_hat = X2 / n2

tau_hat = p2_hat - p1_hat
se_hat = np.sqrt((p1_hat * (1 - p1_hat) / n1) + (p2_hat * (1 - p2_hat) / n2))

z_90 = norm.ppf(0.95)

confidence_interval = (tau_hat - z_90 * se_hat, tau_hat + z_90 * se_hat)

print(&#39;Estimated difference of means: \t\t %.3f&#39; % tau_hat)
print(&#39;Estimated 90%% confidence interval:\t %.3f, %.3f&#39; % confidence_interval)</code></pre>
<pre><code>Estimated difference of means:       0.200
Estimated 90% confidence interval:   0.053, 0.347</code></pre>
<p><strong>(b)</strong></p>
<pre class="python"><code>import numpy as np
from scipy.stats import binom

B = 100000

n1 = 50
n2 = 50

X1 = 30
X2 = 40

p1_hat = X1 / n1
p2_hat = X2 / n2

XX1 = binom.rvs(n1, p1_hat, size=B)
XX2 = binom.rvs(n2, p2_hat, size=B)
tau_boot = XX2 / n2 - XX1 / n1

tau_boot_hat = tau_boot.mean()
q_05 = np.quantile(tau_boot, 0.05)
q_95 = np.quantile(tau_boot, 0.95)

boot_confidence_interval = (q_05, q_95)

print(&#39;Estimated difference of means: \t\t %.3f&#39; % tau_boot_hat)
print(&#39;Estimated 90%% confidence interval:\t %.3f, %.3f&#39; % boot_confidence_interval)</code></pre>
<pre><code>Estimated difference of means:       0.200
Estimated 90% confidence interval:   0.060, 0.340</code></pre>
<p><strong>(c)</strong> The posterior density is proportional to the likelihood times the prior:</p>
<p><span class="math display">\[
\begin{align}
f((p_1, p_2) | X^2) &amp;\propto \mathcal{L}(p_1, p_2 | X^2) f(p_1, p_2) \\
&amp;= f_{X_1}(X_1 | p_1) f_{X_2}(X_2 | p_2) \cdot 1 \\
&amp;= \binom{n_1}{X_1} p_1^{X_1} (1 - p_1)^{n_1 - X_1} \binom{n_2}{X_2} p_2^{X_2} (1 - p_2)^{n_2 - X_2} \\
&amp;\propto p_1^{X_1} (1 - p_1)^{n_1 - X_1} p_2^{X_2} (1 - p_2)^{n_2 - X_2} \\
&amp;\propto f(p_1 | X_1) f(p_2 | X_2)
\end{align}
\]</span></p>
<p>where</p>
<p><span class="math display">\[p_1 | X_1 \sim \text{Beta}(X_1 + 1, n_1 - X_1 + 1) 
\quad \text{and} \quad
p_2 | X_2 \sim \text{Beta}(X_2 + 1, n_2 - X_2 + 1)\]</span></p>
<p>We can now sample from the Beta distributions to sample <span class="math inline">\(p_1\)</span>, <span class="math inline">\(p_2\)</span>, and for each pair of samples, compute the sample <span class="math inline">\(\tau = p_2 - p_1\)</span>.</p>
<pre class="python"><code>from scipy.stats import beta

B = 100000

n1 = 50
n2 = 50

X1 = 30
X2 = 40

p1_boot = beta.rvs(X1 + 1, n1 - X1 + 1, size=B)
p2_boot = beta.rvs(X2 + 1, n2 - X2 + 1, size=B)
tau_boot = p2_boot - p1_boot

tau_boot_hat = tau_boot.mean()
q_05 = np.quantile(tau_boot, 0.05)
q_95 = np.quantile(tau_boot, 0.95)

boot_confidence_interval = (q_05, q_95)

print(&#39;Estimated posterior mean: \t\t %.3f&#39; % tau_boot_hat)
print(&#39;Estimated 90%% confidence interval:\t %.3f, %.3f&#39; % boot_confidence_interval)</code></pre>
<pre><code>Estimated posterior mean:        0.192
Estimated 90% confidence interval:   0.047, 0.335</code></pre>
<p><strong>(d)</strong></p>
<p>Let <span class="math inline">\(\psi = h(p_1, p_2) = \log p_1 - \log (1 - p_1) - \log p_2 + \log (1 - p_2)\)</span>. The MLE is just <span class="math inline">\(\hat{\psi} = h(\hat{p_1}, \hat{p_2})\)</span>.</p>
<p>Using the multiparameter delta method, the gradient of <span class="math inline">\(h\)</span> is:</p>
<p><span class="math display">\[\nabla h = 
\begin{pmatrix}
\frac{\partial h}{\partial p_1} \\ 
\frac{\partial h}{\partial p_2} \end{pmatrix}
= \begin{pmatrix}
\frac{1}{p_1(1 - p_1)} \\
-\frac{1}{p_2(1 - p_2)}
\end{pmatrix}
\]</span></p>
<p>The inverse Fisher Information Matrix is still:</p>
<p><span class="math display">\[J_n(p_1, p_2) = \begin{pmatrix}
\frac{p_1(1 - p_1)}{n_1} &amp; 0 \\
0 &amp; \frac{p_2(1 - p_2)}{n_2}
\end{pmatrix}\]</span></p>
<p>and so the estimated standard error is:</p>
<p><span class="math display">\[
\begin{align}
\hat{\text{se}}(\hat{\psi}) &amp;= \sqrt{(\hat{\nabla}h)^T \hat{J}_n (\hat{\nabla}h)} \\
&amp;= 
\sqrt{\begin{pmatrix}
\frac{1}{\hat{p}_1(1 - \hat{p}_1)} &amp;
-\frac{1}{\hat{p}_2(1 - \hat{p}_2)}
\end{pmatrix}
\begin{pmatrix}
\frac{\hat{p}_1(1 - \hat{p}_1)}{n_1} &amp; 0 \\
0 &amp; \frac{\hat{p}_2(1 - \hat{p}_2)}{n_2}
\end{pmatrix}
\begin{pmatrix}
\frac{1}{\hat{p}_1(1 - \hat{p}_1)} \\
-\frac{1}{\hat{p}_2(1 - \hat{p}_2)}
\end{pmatrix}} \\
&amp;= \sqrt{\frac{1}{n_1 \hat{p}_1(1 - \hat{p}_1)} + \frac{1}{n_2 \hat{p}_2(1 - \hat{p}_2)}}
\end{align}
\]</span></p>
<p>and we get the confidence interval:</p>
<p><span class="math display">\[\hat{\psi} \pm z_{1 - \alpha/2} \hat{\text{se}}(\hat{\psi})\]</span></p>
<pre class="python"><code>import numpy as np
from scipy.stats import norm

n1 = 50
n2 = 50

X1 = 30
X2 = 40

p1_hat = X1 / n1
p2_hat = X2 / n2

psi_hat = np.log((p1_hat / (1 - p1_hat)) / (p2_hat / (1 - p2_hat)))
se_hat = np.sqrt(1/(n1 * p1_hat * (1 - p1_hat)) + 1/(n2 * p2_hat * (1 - p2_hat)))

z_90 = norm.ppf(0.95)

confidence_interval = (psi_hat - z_90 * se_hat, psi_hat + z_90 * se_hat)

print(&#39;Estimated log-odds ratio: \t\t %.3f&#39; % psi_hat)
print(&#39;Estimated 90%% confidence interval:\t %.3f, %.3f&#39; % confidence_interval)</code></pre>
<pre><code>Estimated log-odds ratio:        -0.981
Estimated 90% confidence interval:   -1.732, -0.230</code></pre>
<p><strong>(e)</strong> The probability distributions for <span class="math inline">\(p_1 | X_1\)</span> and <span class="math inline">\(p_2 | X_2\)</span> are still the same – we are just computing <span class="math inline">\(\psi = h(p_1, p_2)\)</span> for each simulation sample now, rather than <span class="math inline">\(\tau = g(p_1, p_2)\)</span>.</p>
<pre class="python"><code>from scipy.stats import beta

B = 100000

n1 = 50
n2 = 50

X1 = 30
X2 = 40

p1_boot = beta.rvs(X1 + 1, n1 - X1 + 1, size=B)
p2_boot = beta.rvs(X2 + 1, n2 - X2 + 1, size=B)
psi_boot = np.log((p1_boot / (1 - p1_boot)) / (p2_boot / (1 - p2_boot)))

psi_boot_hat = psi_boot.mean()
q_05 = np.quantile(psi_boot, 0.05)
q_95 = np.quantile(psi_boot, 0.95)

boot_confidence_interval = (q_05, q_95)

print(&#39;Estimated posterior mean: \t\t %.3f&#39; % psi_boot_hat)
print(&#39;Estimated 90%% confidence interval:\t %.3f, %.3f&#39; % boot_confidence_interval)</code></pre>
<pre><code>Estimated posterior mean:        -0.954
Estimated 90% confidence interval:   -1.703, -0.228</code></pre>
<p><strong>Exercise 12.11.5</strong>. Consider the <span class="math inline">\(\text{Bernoulli}(p)\)</span> observations</p>
<p><span class="math display">\[ 0\; 1\; 0\; 1\; 0\; 0\; 0\; 0\; 0\; 0 \]</span></p>
<p>Plot the posterior for <span class="math inline">\(p\)</span> using these priors: <span class="math inline">\(\text{Beta}(1/2, 1/2)\)</span>, <span class="math inline">\(\text{Beta}(1, 1)\)</span>, <span class="math inline">\(\text{Beta}(10, 10)\)</span>, <span class="math inline">\(\text{Beta}(100, 100)\)</span>.</p>
<p><strong>Solution</strong>.</p>
<p>The observations include <span class="math inline">\(n = 10\)</span> samples, of which <span class="math inline">\(k = 2\)</span> samples with observed value 1. Assume a prior of the form <span class="math inline">\(\text{Beta}(\alpha, \beta)\)</span>.</p>
<p>The posterior is proportional to the likelihood times the prior.</p>
<p><span class="math display">\[
\begin{align}
f(p | X^n) &amp; \propto \mathcal{L}(p | x^n) f(p) \\
&amp;= \binom{n}{k} p^k (1 - p)^{n-k} \frac{p^{\alpha - 1}(1-p)^{\beta - 1}}{B(\alpha, \beta)}\\
&amp;\propto p^{k + \alpha - 1} (1 - p)^{n - k + \beta - 1}
\end{align}
\]</span></p>
<p>This density is proportional to a Beta density function in p, so</p>
<p><span class="math display">\[p | X^n \sim \text{Beta}(k + \alpha - 1, n - k + \beta - 1)\]</span></p>
<p>Now, plotting the posterior becomes just a matter of plotting these beta distributions.</p>
<p>When <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> are integers, we can also reinterpret this posterior as equivalent to starting with a flat prior and observing extra <span class="math inline">\(\alpha + \beta\)</span> events, with <span class="math inline">\(\alpha\)</span> extra events producing outcome 1 and <span class="math inline">\(\beta\)</span> extra events producing outcome 0. This is shown on the plots below – the larger the number of “extra observations,” the less impact the actual observations make on shifting the belief about the true parameter value.</p>
<pre class="python"><code>import numpy as np
from scipy.stats import beta
import matplotlib.pyplot as plt

n = 10
k = 2

x = np.linspace(0, 1, 200)

plt.figure(figsize=(12, 8))
for a, b in [(1/2, 1/2), (1, 1), (10, 10), (100, 100)]:
    plt.plot(x, beta.pdf(x, k + a - 1, n - k + b - 1), label=&#39;prior = Beta(&#39; + str(a) + &#39;, &#39; + str(b) +&#39;)&#39;)
    plt.legend()

plt.show();</code></pre>
<div class="figure">
<img src="Chapter%2012%20-%20Bayesian%20Inference_files/Chapter%2012%20-%20Bayesian%20Inference_66_0.png" alt="" />
<p class="caption">png</p>
</div>
<p><strong>Exercise 12.11.6</strong>. Let <span class="math inline">\(X_1, \dots, X_n \sim \text{Poisson}(\lambda)\)</span>.</p>
<p><strong>(a)</strong> Let <span class="math inline">\(\lambda \sim \text{Gamma}(\alpha, \beta)\)</span> be the prior. Show that the posterior is also a Gamma. Find the posterior mean.</p>
<p><strong>(b)</strong> Find the Jeffreys’ prior. Find the posterior.</p>
<p><strong>Solution</strong>.</p>
<p><strong>(a)</strong> The posterior is proportional to the likelihood times the prior:</p>
<p><span class="math display">\[
\begin{align}
f(\lambda | X^n) &amp;\propto \mathcal{L}(\lambda | X^n) f(\lambda) \\
&amp;= \left(\prod_{i=1}^n \frac{\lambda^{X_i}e^{-\lambda}}{X_i!} \right) \frac{\beta^{\alpha}}{\Gamma(\alpha)}\lambda^{\alpha - 1}e^{-\beta \lambda} \\
&amp; \propto \lambda^{\sum_i X_i} e^{-n\lambda} \lambda^{\alpha - 1}e^{-\beta \lambda} \\
&amp;= \lambda^{\left(\alpha + \sum_i X_i \right) - 1} e^{-(\beta + n) \lambda}
\end{align}
\]</span></p>
<p>Therefore, the posterior is proportional to a Gamma distribution with parameters <span class="math inline">\(\left(\alpha + \sum_{i=1}^n X_i, \beta + n\right)\)</span> – and thus it must be drawn from that distribution.</p>
<p><span class="math display">\[\lambda | X^n \sim \text{Gamma}\left(\alpha + \sum_{i=1}^n X_i, \beta + n \right)\]</span></p>
<p>The posterior mean is the mean of Gamma distribution it is drawn from,</p>
<p><span class="math display">\[ \overline{\lambda} | X^n = \frac{\alpha + \sum_{i=1}^n X_i}{\beta + n} \]</span></p>
<p><strong>(b)</strong> For a Poisson distribution, the log likelihood is</p>
<p><span class="math display">\[
\begin{align}
\ell_n(\lambda) &amp;= \sum_{i=1}^n \log \left( \frac{\lambda^{X_i} e^{-\lambda}}{X_i!}\right)  \\
&amp;= \sum_{i=1}^n \left( X_i \log \lambda - \lambda - \log (X_i!)\right) \\
&amp;= \log \lambda \left( \sum_{i=1}^n X_i \right) - n\lambda - \sum_{i=1}^n \log (X_i!)
\end{align}
\]</span></p>
<p>Its second derivative is <span class="math inline">\(\ell_n&#39;&#39;(\lambda) = -n \overline{X} / \lambda^2\)</span>, so Fisher’s information is <span class="math inline">\(I(\lambda) = -\mathbb{E}[\ell&#39;&#39;(\lambda)] = 1 / \lambda\)</span>.</p>
<p>Jeffreys’ prior is:</p>
<p><span class="math display">\[f(\lambda) \propto I(\lambda)^{1/2} = \lambda^{-1/2}\]</span></p>
<p>The posterior is proportional to the likelihood times the prior:</p>
<p><span class="math display">\[
\begin{align}
f(\lambda | X^n) &amp;\propto \mathcal{L}(\lambda | X^n) f(\lambda) \\
&amp;\propto \left(\prod_{i=1}^n \frac{\lambda^{X_i}e^{-\lambda}}{X_i!} \right) \lambda^{-1/2} \\
&amp;\propto \lambda^{\left(1/2 + \sum_i X_i \right) - 1} e^{-n\lambda}
\end{align}
\]</span></p>
<p>Therefore, the posterior is proportional to a Gamma distribution with parameters <span class="math inline">\(\left(\left(1/2 + \sum_{i=1}^n X_i \right), n\right)\)</span> – and thus it must be drawn from that distribution.</p>
<p><span class="math display">\[\lambda | X^n \sim \text{Gamma}\left(\frac{1}{2} + \sum_{i=1}^n X_i, n \right)\]</span></p>
</div>
</div>
<div id="references" class="section level2 unnumbered">
<h2>References</h2>
<div id="refs" class="references csl-bib-body">
<div id="ref-wasserman2013all" class="csl-entry">
1. Wasserman L. All of statistics: A concise course in statistical inference. Springer Science &amp; Business Media; 2013.
</div>
<div id="ref-telmo-correa/all-of-statistics" class="csl-entry">
2. Https://github.com/telmo-correa/all-of-statistics.
</div>
</div>
</div>

    </div>
  </article>

  


</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
          </li>
          <li>
          </li>
        </ul>
      </footer>

    </div>
    



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/r.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/yaml.min.js"></script>
<script>hljs.configure({languages: []}); hljs.initHighlightingOnLoad();</script>



    
<script src="../../../../js/math-code.js"></script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
  </body>
</html>

