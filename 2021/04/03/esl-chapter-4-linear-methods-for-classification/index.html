<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.80.0" />


<title>ESL chapter 4 Linear Methods for Classification - A Hugo website</title>
<meta property="og:title" content="ESL chapter 4 Linear Methods for Classification - A Hugo website">


  <link href='../../../../favicon.ico' rel='icon' type='image/x-icon'/>



  








<link href='//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css' rel='stylesheet' type='text/css' />



<link rel="stylesheet" href="../../../../css/fonts.css" media="all">
<link rel="stylesheet" href="../../../../css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="../../../../" class="nav-logo">
    <img src="../../../../images/logo.png"
         width="50"
         height="50"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="../../../../about/">about</a></li>
    
    <li><a href="../../../../vitae/">CV</a></li>
    
    <li><a href="https://github.com/danli349">GitHub</a></li>
    
    <li><a href="https://scholar.google.com/citations?user=mNjLK8EAAAAJ&amp;hl=en">Googlr Scholar</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">102 min read</span>
    

    <h1 class="article-title">ESL chapter 4 Linear Methods for Classification</h1>

    
    <span class="article-date">2021-04-03</span>
    

    <div class="article-content">
      
<script src="../../../../2021/04/03/esl-chapter-4-linear-methods-for-classification/index_files/header-attrs/header-attrs.js"></script>

<div id="TOC">
<ul>
<li><a href="#chapter-4.-linear-methods-for-classification">Chapter 4. Linear Methods for Classification</a></li>
<li><a href="#s-4.1.-introduction"><span class="math inline">\(\S\)</span> 4.1. Introduction</a>
<ul>
<li><a href="#linear-regression">Linear regression</a></li>
<li><a href="#discriminant-function">Discriminant function</a></li>
<li><a href="#logit-transformation">Logit transformation</a></li>
<li><a href="#separating-hyperplanes">Separating hyperplanes</a></li>
<li><a href="#scope-for-generalization">Scope for generalization</a></li>
</ul></li>
<li><a href="#s-4.2.-linear-regression-of-an-indicator-matrix"><span class="math inline">\(\S\)</span> 4.2. Linear Regression of an Indicator Matrix</a>
<ul>
<li><a href="#rationale">Rationale</a></li>
<li><a href="#a-more-simplistic-viewpoint">A more simplistic viewpoint</a></li>
<li><a href="#masked-class-with-the-regression-approach">Masked class with the regression approach</a></li>
</ul></li>
<li><a href="#s-4.3.-linear-discriminant-analysis"><span class="math inline">\(\S\)</span> 4.3. Linear Discriminant Analysis</a>
<ul>
<li><a href="#lda-from-multivariate-gaussian">LDA from multivariate Gaussian</a></li>
<li><a href="#estimating-parameters">Estimating parameters</a></li>
<li><a href="#simple-correspondence-between-lda-and-linear-regression-with-two-classes">Simple correspondence between LDA and linear regression with two classes</a></li>
<li><a href="#practice-beyond-the-gaussian-assumption">Practice beyond the Gaussian assumption</a></li>
<li><a href="#quadratic-discriminant-analysis">Quadratic Discriminant Analysis</a></li>
<li><a href="#why-lda-and-qda-have-such-a-good-track-record">Why LDA and QDA have such a good track record?</a></li>
<li><a href="#s-4.3.1.-regularized-discriminant-analysis"><span class="math inline">\(\S\)</span> 4.3.1. Regularized Discriminant Analysis</a>
<ul>
<li><a href="#sigma_k-leftrightarrow-sigma"><span class="math inline">\(\Sigma_k \leftrightarrow \Sigma\)</span></a></li>
<li><a href="#sigma-leftrightarrow-sigma"><span class="math inline">\(\Sigma \leftrightarrow \sigma\)</span></a></li>
<li><a href="#to-be-continued">To be continued</a></li>
</ul></li>
<li><a href="#s-4.3.2.-computations-for-lda"><span class="math inline">\(\S\)</span> 4.3.2. Computations for LDA</a></li>
<li><a href="#s-4.3.3.-reduced-rank-linear-discriminant-analysis"><span class="math inline">\(\S\)</span> 4.3.3. Reduced-Rank Linear Discriminant Analysis</a>
<ul>
<li><a href="#what-if-k3-principal-components-subspace">What if <span class="math inline">\(K&gt;3\)</span>? Principal components subspace</a></li>
<li><a href="#maximize-between-class-variance-relative-to-within-class">Maximize between-class variance relative to within-class</a></li>
<li><a href="#algorithm-for-the-generalized-eigenvalue-problem">Algorithm for the generalized eigenvalue problem</a></li>
<li><a href="#summary">Summary</a></li>
<li><a href="#dimension-reduction-for-classification">Dimension reduction for classification</a></li>
<li><a href="#impact-of-prior-information-pi_k">Impact of prior information <span class="math inline">\(\pi_k\)</span></a></li>
<li><a href="#connection-between-fishers-reduced-rank-discriminant-analysis-and-regression-of-an-indicator-response-matrix">Connection between Fisher’s reduced-rank discriminant analysis and regression of an indicator response matrix</a></li>
</ul></li>
</ul></li>
<li><a href="#s-4.4.-logistic-regression"><span class="math inline">\(\S\)</span> 4.4. Logistic Regression</a>
<ul>
<li><a href="#sum-to-one">Sum to one</a></li>
</ul></li>
<li><a href="#s-4.4.1.-fitting-logistic-regression-models"><span class="math inline">\(\S\)</span> 4.4.1. Fitting Logistic Regression Models</a>
<ul>
<li><a href="#maximum-likelihood">Maximum likelihood</a></li>
<li><a href="#maximum-likelihood-for-k2-case">Maximum likelihood for <span class="math inline">\(K=2\)</span> case</a></li>
<li><a href="#newton-raphson-algorithm">Newton-Raphson algorithm</a></li>
<li><a href="#the-same-thing-in-matrix-notation">The same thing in matrix notation</a></li>
<li><a href="#iteratively-reweighted-least-squares">Iteratively reweighted least squares</a></li>
<li><a href="#multiclass-case-with-kge-3">Multiclass case with <span class="math inline">\(K\ge 3\)</span></a></li>
<li><a href="#goal-of-logistic-regression">Goal of logistic regression</a></li>
<li><a href="#s-4.4.2.-example-south-african-heart-disease"><span class="math inline">\(\S\)</span> 4.4.2. Example: South African Heart Disease</a>
<ul>
<li><a href="#correlations-between-predictors">Correlations between predictors</a></li>
<li><a href="#model-selection">Model selection</a></li>
<li><a href="#interpretation-of-a-coefficient">Interpretation of a coefficient</a></li>
</ul></li>
<li><a href="#s-4.4.3.-quadratic-approximations-and-inference"><span class="math inline">\(\S\)</span> 4.4.3. Quadratic Approximations and Inference</a></li>
<li><a href="#s-4.4.4-l_1-regularized-logistic-regression"><span class="math inline">\(\S\)</span> 4.4.4 <span class="math inline">\(L_1\)</span> Regularized Logistic Regression</a></li>
<li><a href="#s-4.4.5-logistic-regression-or-lda"><span class="math inline">\(\S\)</span> 4.4.5 Logistic Regression or LDA?</a>
<ul>
<li><a href="#common-linearity">Common linearity</a></li>
<li><a href="#different-assumptions">Different assumptions</a></li>
<li><a href="#role-of-the-marginal-density-textprx-in-lda">Role of the marginal density <span class="math inline">\(\text{Pr}(X)\)</span> in LDA</a></li>
<li><a href="#marginal-likelihood-as-a-regularizer">Marginal likelihood as a regularizer</a></li>
<li><a href="#in-real-world">In real world</a></li>
</ul></li>
</ul></li>
<li><a href="#s-4.5.-separating-hyperplanes"><span class="math inline">\(\S\)</span> 4.5. Separating Hyperplanes</a>
<ul>
<li><a href="#perceptrons">Perceptrons</a></li>
<li><a href="#review-on-vector-algebra">Review on vector algebra</a></li>
<li><a href="#s-4.5.1.-rosenblatts-perceptron-learning-algorithm"><span class="math inline">\(\S\)</span> 4.5.1. Rosenblatt’s Perceptron Learning Algorithm</a>
<ul>
<li><a href="#stochastic-gradient-descent">Stochastic gradient descent</a></li>
<li><a href="#issues">Issues</a></li>
</ul></li>
<li><a href="#s-4.5.2.-optimal-separating-hyperplanes"><span class="math inline">\(\S\)</span> 4.5.2. Optimal Separating Hyperplanes</a>
<ul>
<li><a href="#formulation">Formulation</a></li>
<li><a href="#convex-optimization">Convex optimization</a></li>
<li><a href="#implications-of-the-algorithm">Implications of the algorithm</a></li>
<li><a href="#classification">Classification</a></li>
<li><a href="#dependency-on-model-assumption">Dependency on model assumption</a></li>
<li><a href="#when-the-data-are-not-separable">When the data are not separable</a></li>
</ul></li>
</ul></li>
<li><a href="#exercises">Exercises</a>
<ul>
<li><a href="#ex.-4.1">Ex. 4.1</a></li>
<li><a href="#ex.-4.2">Ex. 4.2</a></li>
<li><a href="#ex.-4.3">Ex. 4.3</a></li>
<li><a href="#ex.-4.4-multidimensional-logistic-regression">Ex. 4.4 (multidimensional logistic regression)</a></li>
<li><a href="#ex.-4.5">Ex. 4.5</a></li>
<li><a href="#ex.-4.6">Ex. 4.6</a></li>
<li><a href="#ex.-4.7">Ex. 4.7</a></li>
<li><a href="#ex.-4.8">Ex. 4.8</a></li>
<li><a href="#ex.-4.9">Ex. 4.9</a></li>
</ul></li>
<li><a href="#references">References</a></li>
</ul>
</div>

<div id="chapter-4.-linear-methods-for-classification" class="section level1">
<h1>Chapter 4. Linear Methods for Classification</h1>
</div>
<div id="s-4.1.-introduction" class="section level1">
<h1><span class="math inline">\(\S\)</span> 4.1. Introduction</h1>
<p>Since our predictor <span class="math inline">\(G(x)\)</span> takes values in a discrete set <span class="math inline">\(\mathcal{G}\)</span>, we can always divide the input space into a collection of regions labeled according to the classification. We saw in Chapter 2 that the boundaries of these regions can be rough or smooth, depending on the prediction function. For an important class of procedures, these <em>decision boundaries</em> are linear; this is what we will mean by linear methodds for classification.</p>
<div id="linear-regression" class="section level3">
<h3>Linear regression</h3>
<p>In Chapter 2 we fit linear regression models to the class indicator variable, and classify to the largest fit. Suppose there are <span class="math inline">\(K\)</span> classes labeled <span class="math inline">\(1,\cdots,K\)</span>, and the fitted linear model for the <span class="math inline">\(k\)</span>th indicator response variable is</p>
<p><span class="math display">\[\begin{equation}
\hat{f}_k(x) = \hat\beta_{k0} + \hat\beta_k^Tx.
\end{equation}\]</span></p>
<p>The decision boundary between class <span class="math inline">\(k\)</span> and <span class="math inline">\(l\)</span> is that set of points</p>
<p><span class="math display">\[\begin{equation}
\left\lbrace x: \hat{f}_k(x) = \hat{f}_l(x) \right\rbrace = \left\lbrace x: (\hat\beta_{k0}-\hat\beta_{l0}) + (\hat\beta_k-\hat\beta_l)^Tx = 0 \right\rbrace,
\end{equation}\]</span></p>
<p>which is an affine set or hyperplane. Since the same is true for any pair of classes, the input space is divided into regions of constant classification, with piecewise hyperplanar decision boundaries.</p>
</div>
<div id="discriminant-function" class="section level3">
<h3>Discriminant function</h3>
<p>The regression approach is a member of a class of methods that model <em>discriminant functions</em> <span class="math inline">\(\delta_k(x)\)</span> for each class, and then classify <span class="math inline">\(x\)</span> to the class with the largest value for its discriminant function. Methods that model the posterior probabilities <span class="math inline">\(\text{Pr}(G=k|X=x)\)</span> are also in this class. Clearly, if either the <span class="math inline">\(\delta_k(x)\)</span> or <span class="math inline">\(\text{Pr}(G=k|X=x)\)</span> are linear in <span class="math inline">\(x\)</span>, then the decision boundaries will be linear.</p>
</div>
<div id="logit-transformation" class="section level3">
<h3>Logit transformation</h3>
<p>Actually, all we require is that some monotone transformation of <span class="math inline">\(\delta_k\)</span> or <span class="math inline">\(\text{Pr}(G=k|X=x)\)</span> be linear for the decision boundaries to be linear. For example, if there are two classes, a popular model for the posterior probabilities is</p>
<p><span class="math display">\[\begin{align}
\text{Pr}(G=1|X=x) &amp;= \frac{\exp(\beta_0+\beta^Tx)}{1+\exp(\beta_0+\beta^Tx)},\\
\text{Pr}(G=2|X=x) &amp;= \frac{1}{1+\exp(\beta_0+\beta^Tx)},\\
\end{align}\]</span></p>
<p>where the monotone transformation is the <em>logit</em> transformation</p>
<p><span class="math display">\[\begin{equation}
\log\frac{p}{1-p},
\end{equation}\]</span></p>
<p>and in fact we see that</p>
<p><span class="math display">\[\begin{equation}
\log\frac{\text{Pr}(G=1|X=x)}{\text{Pr}(G=2|X=x)} = \beta_0 + \beta^Tx.
\end{equation}\]</span></p>
<p>The decision boundary is the set of points for which the <em>log-odds</em> are zero, and this is a hyperplane defined by</p>
<p><span class="math display">\[\begin{equation}
\left\lbrace x: \beta_0+\beta^Tx = 0 \right\rbrace.
\end{equation}\]</span></p>
<p>We discuss two very popular but different methods that result in linear log-odds or logits: Linear discriminant analysis and linear logistic regression. Although they differ in their derivation, the essential difference between them is in the way the lineaer function is fit to the training data.</p>
</div>
<div id="separating-hyperplanes" class="section level3">
<h3>Separating hyperplanes</h3>
<p>A more direct approach is to explicitly model the boundaries between the classes as linear. For a two-class problem, this amounts to modeling the decision boundary as a hyperplane; a normal vector and a cut-point.</p>
<p>We will look at two methods that explicitly look for “separating hyperplanes.”</p>
<ol style="list-style-type: decimal">
<li>The well-known <em>perceptron</em> model of Rosenblatt (1958), with an algorithm that finds a separating hyperplane in the training data, if one exists.</li>
<li>Vapnik (1996) finds an <em>optimally separating hyperplane</em> if one exists, else finds a hyperplane that minimizes some measure of overlap in the training data.</li>
</ol>
<p>We treat separable cases here, and defer the nonseparable case to Chapter 12.</p>
</div>
<div id="scope-for-generalization" class="section level3">
<h3>Scope for generalization</h3>
<p>We can expand the input by including their squares <span class="math inline">\(X_1^2,X_2^2,\cdots\)</span>, and cross-products <span class="math inline">\(X_1X_2,\cdots\)</span>, thereby adding <span class="math inline">\(p(p+1)/2\)</span> additional variables. Linear functions in the augmented space map down to quadratic decision boundaires. FIGURE 4.1 illustrates the idea.</p>
<p>This approach can be used with any basis transformation <span class="math inline">\(h(X): \mathbb{R}^p\mapsto\mathbb{R}^q\)</span> with <span class="math inline">\(q &gt; p\)</span>, and will be explored in later chapters.</p>
</div>
</div>
<div id="s-4.2.-linear-regression-of-an-indicator-matrix" class="section level1">
<h1><span class="math inline">\(\S\)</span> 4.2. Linear Regression of an Indicator Matrix</h1>
<p>Here each of the response categories are coded via an indicator variable. Thus if <span class="math inline">\(\mathcal{G}\)</span> has <span class="math inline">\(K\)</span> classes, there will be <span class="math inline">\(K\)</span> such indicators <span class="math inline">\(Y_k\)</span>, <span class="math inline">\(k=1,\cdots,K\)</span>, with</p>
<p><span class="math display">\[\begin{equation}
Y_k = 1 \text{ if } G = k \text{ else } 0.
\end{equation}\]</span></p>
<p>These are collected together in a vector <span class="math inline">\(Y=(Y_1,\cdots,Y_k)\)</span>, and the <span class="math inline">\(N\)</span> training instances of these form an <span class="math inline">\(N\times K\)</span> <em>indicator response matrix</em> <span class="math inline">\(\mathbf{Y}\)</span>, which is a matrix of <span class="math inline">\(0\)</span>’s and <span class="math inline">\(1\)</span>’s, with each row having a single <span class="math inline">\(1\)</span>.</p>
<p>We fit a linear regression model to each of the columns of <span class="math inline">\(\mathbf{Y}\)</span> simultaneously, and the fit is given by</p>
<p><span class="math display">\[\begin{equation}
\underset{N\times K}{\hat{\mathbf{Y}}} = \mathbf{X}\left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T\mathbf{Y} = \underset{N\times (p+1)}{\mathbf{X}}\underset{(p+1)\times K}{\hat{\mathbf{B}}}
\end{equation}\]</span></p>
<p>Note that we have a coefficient vector for each response columns <span class="math inline">\(\mathbf{y}_k\)</span>, and hence a <span class="math inline">\((p+1)\times K\)</span> coefficient matrix <span class="math inline">\(\hat{\mathbf{B}} = \left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T\mathbf{Y}\)</span>. Here <span class="math inline">\(\mathbf{X}\)</span> is the model matrix with <span class="math inline">\(p+1\)</span> columns with a leading columns of <span class="math inline">\(1\)</span>’s for the intercept.</p>
<p>A new observation with input <span class="math inline">\(x\)</span> is classified as follows:</p>
<ul>
<li>Compute the fitted output <span class="math inline">\(\hat{f}(x)^T = (1, x^T)\hat{\mathbf{B}}\)</span>, a <span class="math inline">\(K\)</span> vector.</li>
<li>Identify the largest component and classify accordingly:</li>
</ul>
<p><span class="math display">\[\begin{equation}
\hat{G}(x) = \arg\max_{k\in\mathcal{G}} \hat{f}_k(x).
\end{equation}\]</span></p>
<div id="rationale" class="section level3">
<h3>Rationale</h3>
<p>One rather formal justification is to view the regression as an estimate of conditional expectation. For the random variable <span class="math inline">\(Y_k\)</span>,</p>
<p><span class="math display">\[\begin{equation}
\text{E}(Y_k|X=x) = \text{Pr}(G=k|X=x),
\end{equation}\]</span></p>
<p>so conditional expectation of each <span class="math inline">\(Y_k\)</span> seems a sensible goal.</p>
<p>The real issue is: How good an approximation to conditional expectation is the rather rigid linear regression model? Alternatively, are the <span class="math inline">\(\hat{f}_k(x)\)</span> reasonable estimates of the posterior probabilities <span class="math inline">\(\text{Pr}(G=k|X=x)\)</span>, and more importantly, does this matter?</p>
<p>It is quite straightforward to verify that, as long as the model has an intercept,</p>
<p><span class="math display">\[\begin{equation}
\sum_{k\in\mathcal{G}}\hat{f}_k(x) = 1.
\end{equation}\]</span></p>
<p>However it is possible that <span class="math inline">\(\hat{f}_k(x) &lt; 0\)</span> or <span class="math inline">\(\hat{f}_k(x) &gt; 1\)</span>, and typically some are. This is a consequence of the rigid nature of linear regression, especially if we make predictions outside the hull of the training data. These violations in themselves do not guarantee that this approach will not work, and in fact on many problems it gives similar results to more standard linear methods for classification.</p>
<p>If we allow linear regression onto basis expansions <span class="math inline">\(h(X)\)</span> of the inputs, this approach can lead to consistent estimates of the probabilities. As the size of the training set <span class="math inline">\(N\)</span> grows bigger, we adaptively include more basis elements so that linear regression onto these basis functions approaches conditional expectation. We discuss such approaches in Chapter 5.</p>
</div>
<div id="a-more-simplistic-viewpoint" class="section level3">
<h3>A more simplistic viewpoint</h3>
<p>Denote <span class="math inline">\(t_k\)</span> as the <span class="math inline">\(k\)</span>th column of <span class="math inline">\(\mathbf{I}_K\)</span>, the <span class="math inline">\(K\times K\)</span> identity matrix, then a more simplistic viewpoint is to construct <em>targets</em> <span class="math inline">\(t_k\)</span> for each class. The response vector (<span class="math inline">\(i\)</span>th row of <span class="math inline">\(\mathbf{Y}\)</span>)</p>
<p><span class="math display">\[\begin{equation}
y_i = t_k \text{ if } g_i = k.
\end{equation}\]</span></p>
<p>We might then fit the linear model by least squares: The criterion is a sum-of-squared Euclidean distances of the fitted vectors from their targets.</p>
<p><span class="math display">\[\begin{equation}
\min_{\mathbf{B}} \sum_{i=1}^N \left\| y_i - \left[ (1,x_i^T)\mathbf{B} \right]^T \right\|^2.
\end{equation}\]</span></p>
<p>Then a new observation is classified by computing its fitted vector <span class="math inline">\(\hat{f}(x)\)</span> and classifying to the closest target:</p>
<p><span class="math display">\[\begin{equation}
\hat{G}(x) = \arg\min_k \left\| \hat{f}(x)-t_k \right\|^2.
\end{equation}\]</span></p>
<p>This is exactly the same as the previous linear regression approach. Below are the reasons:</p>
<ol style="list-style-type: decimal">
<li><p>The sum-of-squared-norm criterion is exactly the same with multiple response linear regression, just viewed slightly differently. The component decouple and can be rearranged as a separate linear model for each element because there is nothing in the model that binds the diferent response together.</p></li>
<li><p>The closest target classification rule is exactly the same as the maximum fitted component criterion.</p></li>
</ol>
</div>
<div id="masked-class-with-the-regression-approach" class="section level3">
<h3>Masked class with the regression approach</h3>
<p>There is a serious problem with the regression approach when the number of class <span class="math inline">\(K\ge 3\)</span>, especially prevalent when <span class="math inline">\(K\)</span> is large. Because of the rigid nature of the regression model, classes can be <em>masked</em> by others. FIGURE 4.2 illustrates an extreme situation when <span class="math inline">\(K=3\)</span>. The three classes are perfectly separated by linear decision boundaries, yet linear regression misses the middle class completely.</p>
<pre class="python"><code>import scipy
import scipy.linalg
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import cvxpy as cp
import math
np.set_printoptions(precision=4, suppress=True)

%load_ext rpy2.ipython</code></pre>
<pre class="python"><code>&quot;&quot;&quot;FIGURE 4.2. (Left) The data come from three classes in R^2 and are easily
separated by linear decision boundaries. This plot shows the boundaires
found by linear regression of the indicator response variables. The middle
class is completely masked (never dominates).

Instead of drawing the decision boundary, showing the classified data is
enough to illustrate masking phenomenon.&quot;&quot;&quot;

# Make the simulation data
size_cluster = 300
mat_cov = np.eye(2)
cluster1 = np.random.multivariate_normal([-4, -4], mat_cov, size_cluster)
cluster2 = np.random.multivariate_normal([0, 0], mat_cov, size_cluster)
cluster3 = np.random.multivariate_normal([4, 4], mat_cov, size_cluster)
target1, target2, target3 = np.eye(3)
print(target1, target2, target3, type(target1))
mat_x0 = np.concatenate((cluster1, cluster2, cluster3))
mat_x = np.hstack((np.ones((size_cluster*3, 1)), mat_x0))
mat_y = np.vstack((np.tile(target1, (size_cluster, 1)),
                      np.tile(target2, (size_cluster, 1)),
                      np.tile(target3, (size_cluster, 1))))
print(mat_x.shape)
print(mat_y.shape)

# Multiple linear regression
mat_beta = scipy.linalg.solve(mat_x.T @ mat_x, mat_x.T @ mat_y)
mat_y_hat = mat_x @ mat_beta
#sum(axis=1) sum the row
assert np.allclose(mat_y_hat.sum(axis=1), 1)
print(mat_y_hat)
#argmax(axis=1) Returns the indices of the maximum values along the row.
idx_classified_y = mat_y_hat.argmax(axis=1)
print(idx_classified_y, idx_classified_y.size)

classified_cluster1 = mat_x0[idx_classified_y == 0]
classified_cluster2 = mat_x0[idx_classified_y == 1]
classified_cluster3 = mat_x0[idx_classified_y == 2]</code></pre>
<pre><code>[1. 0. 0.] [0. 1. 0.] [0. 0. 1.] &lt;class &#39;numpy.ndarray&#39;&gt;
(900, 3)
(900, 3)
[[ 0.79118865  0.33248045 -0.1236691 ]
 [ 0.85014483  0.33261569 -0.18276052]
 [ 0.78423187  0.33116915 -0.11540103]
 ...
 [-0.21401706  0.3339582   0.88005886]
 [-0.21901358  0.33362912  0.88538446]
 [-0.06011645  0.33403974  0.72607671]]
[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 2 0 0 2 2 2 2 2 0 2 0 2 0 2 2 0 0 0 0 2 0 2 2 2 2 2 2 0 2 2 2 0
 2 0 0 2 0 2 0 0 0 2 0 0 0 2 2 0 2 2 0 0 2 2 2 0 0 0 0 0 2 2 0 0 0 0 2 0 0
 2 1 0 2 0 0 2 0 0 2 0 2 0 2 2 2 2 0 0 0 0 2 2 0 2 0 0 2 0 0 2 0 2 2 2 0 0
 0 0 2 2 2 2 2 0 2 2 2 0 2 0 0 2 0 2 0 2 2 2 2 0 2 0 0 0 0 2 2 0 2 2 0 0 0
 2 2 2 0 2 0 2 2 0 0 0 2 2 0 0 2 0 2 2 2 2 2 2 0 0 2 0 0 2 0 2 0 0 2 0 0 0
 2 0 0 2 2 2 0 0 0 0 0 2 0 2 0 2 0 2 0 0 2 2 2 2 2 2 0 0 2 2 2 0 0 0 0 0 2
 2 0 2 2 0 0 0 2 2 2 2 2 2 2 2 2 2 0 0 0 0 2 0 0 2 2 2 2 0 0 2 0 0 2 2 2 0
 2 0 0 2 0 2 0 0 2 2 2 0 0 2 0 2 0 2 0 0 0 2 0 2 0 0 0 0 0 0 2 2 2 0 2 0 0
 2 0 0 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
 2 2 2 2 2 2 2 2 2 2 2 2] 900</code></pre>
<pre class="python"><code>fig42 = plt.figure(0, figsize=(10, 5))
ax1 = fig42.add_subplot(1, 2, 1)
ax1.plot(cluster1[:,0], cluster1[:,1], &#39;o&#39;, color=&#39;C1&#39;, markersize=2)
ax1.plot(cluster2[:,0], cluster2[:,1], &#39;o&#39;, color=&#39;C0&#39;, markersize=2)
ax1.plot(cluster3[:,0], cluster3[:,1], &#39;o&#39;, color=&#39;C2&#39;, markersize=2)
ax1.set_xlabel(&#39;X_1&#39;)
ax1.set_ylabel(&#39;X_2&#39;)
ax1.set_title(&#39;Real Data&#39;)

ax2 = fig42.add_subplot(1, 2, 2)
ax2.plot(classified_cluster1[:,0], classified_cluster1[:,1], &#39;o&#39;, color=&#39;C1&#39;, markersize=2)
ax2.plot(classified_cluster2[:,0], classified_cluster2[:,1], &#39;o&#39;, color=&#39;C0&#39;, markersize=2)
ax2.plot(classified_cluster3[:,0], classified_cluster3[:,1], &#39;o&#39;, color=&#39;C2&#39;, markersize=2)
ax2.set_xlabel(&#39;X_1&#39;)
ax2.set_ylabel(&#39;X_2&#39;)
ax2.set_title(&#39;Classification using Linear Regression&#39;)
plt.show()</code></pre>
<div class="figure">
<img src="merged_files/merged_11_0.png" alt="" />
<p class="caption">png</p>
</div>
<p>FIGURE 4.2. The data come from three classes in IR2 and are easily separated by linear decision boundaries. The right plot shows the boundaries found by linear regression of the indicator response variables. The middle class is completely masked (never dominates).</p>
<pre class="r"><code>%%R

## generate data and reproduce figure 4.2
mu = c(0.25, 0.5, 0.75)
sigma = 0.005*matrix(c(1, 0,
                 0, 1), 2, 2)
print(&quot;sigma&quot;)
print(sigma)
library(MASS)
set.seed(1650)
N = 300
X1 = mvrnorm(n = N, c(mu[1], mu[1]), Sigma = sigma)
X2 = mvrnorm(n = N, c(mu[2], mu[2]), Sigma = sigma)
X3 = mvrnorm(n = N, c(mu[3], mu[3]), Sigma = sigma)
X = rbind(X1, X2, X3)


plot(X1[,1],X1[,2],col=&quot;orange&quot;, xlim = c(0,1),ylim = c(0,1), pch=19,
     xlab = expression(X[1]), ylab = expression(X[2]))
points(X2[,1],X2[,2],col=&quot;blue&quot;, pch=19)
points(X3[,1],X3[,2],col=&quot;green&quot;, pch=19)</code></pre>
<pre><code>[1] &quot;sigma&quot;
      [,1]  [,2]
[1,] 0.005 0.000
[2,] 0.000 0.005</code></pre>
<div class="figure">
<img src="merged_files/merged_13_1.png" alt="" />
<p class="caption">png</p>
</div>
<pre class="python"><code>&quot;&quot;&quot;FIGURE 4.3. The effects of masking on linear regression in R for a three
class problem.

The rug plot at the base indicates the positions and class membership of
each observation. The three curves in each panel are the fitted regression
to the three-class indicator variables.

We see on the left that the middle class line is horizontal and its fitted
values are never dominant! Thus, observations from class 2 are classified
either as class 1 or 3.&quot;&quot;&quot;
# Make the simulation data
size_cluster = 300
# np.random.normal() draw random samples from a normal (Gaussian) distribution with mean -4, 0, 4
cluster1 = np.random.normal(-4, size=(size_cluster,1))
cluster2 = np.random.normal(0, size=(size_cluster,1))
cluster3 = np.random.normal(4, size=(size_cluster,1))
target1, target2, target3 = np.eye(3)
print(target1, target2, target3, type(target1))
mat_x0 = np.concatenate((cluster1, cluster2, cluster3))
#mat_x is 900X2 matrix
mat_x = np.hstack((np.ones((size_cluster*3, 1)), mat_x0))
#mat_y is 900X3 matrix
mat_y = np.vstack((np.tile(target1, (size_cluster, 1)),
                      np.tile(target2, (size_cluster, 1)),
                      np.tile(target3, (size_cluster, 1))))

# Multiple linear regression with degree 1, mat_beta is 2X3 matrix
mat_beta = scipy.linalg.solve(mat_x.T @ mat_x, mat_x.T @ mat_y)
# mat_y_hat is 900X3 matrix
mat_y_hat = mat_x @ mat_beta
print(&quot;mat_y_hat&quot;)
print(mat_y_hat)
idx_classified_y = mat_y_hat.argmax(axis=1)</code></pre>
<pre><code>[1. 0. 0.] [0. 1. 0.] [0. 0. 1.] &lt;class &#39;numpy.ndarray&#39;&gt;
mat_y_hat
[[ 0.81458631  0.33179161 -0.14637792]
 [ 0.82090514  0.33177137 -0.15267651]
 [ 0.85306795  0.33166833 -0.18473629]
 ...
 [-0.13503942  0.33483379  0.80020562]
 [-0.08154107  0.33466241  0.74687866]
 [-0.05041249  0.33456268  0.71584981]]</code></pre>
<pre class="python"><code>fig43 = plt.figure(1, figsize=(10, 5))
ax1 = fig43.add_subplot(1, 2, 1)
ax1.plot(mat_x0, mat_y_hat[:, 0], &#39;o&#39;, color=&#39;C1&#39;, markersize=2)
ax1.plot(mat_x0, mat_y_hat[:, 1], &#39;o&#39;, color=&#39;C0&#39;, markersize=2)
ax1.plot(mat_x0, mat_y_hat[:, 2], &#39;o&#39;, color=&#39;C2&#39;, markersize=2)
y_floor, _ = ax1.get_ylim()
ax1.plot(cluster1, [y_floor]*size_cluster, &#39;o&#39;, color=&#39;C1&#39;, markersize=2)
ax1.plot(cluster2, [y_floor]*size_cluster, &#39;o&#39;, color=&#39;C0&#39;, markersize=2)
ax1.plot(cluster3, [y_floor]*size_cluster, &#39;o&#39;, color=&#39;C2&#39;, markersize=2)
ax1.set_title(&#39;Degree = 1, Error = 0.33&#39;)

# Multiple linear regression with degree 2
# mat_x2 is 900X3 matrix
mat_x2 = np.hstack((mat_x, mat_x0*mat_x0))
# mat_beta2 is 3X3 matrix
mat_beta2 = np.linalg.solve(mat_x2.T @ mat_x2, mat_x2.T @ mat_y)
# mat_y2_hat is 900X3 matrix
mat_y2_hat = mat_x2 @ mat_beta2
print(&quot;mat_y2_hat&quot;)
print(mat_y2_hat)

ax2 = fig43.add_subplot(1, 2, 2)
ax2.plot(mat_x0, mat_y2_hat[:, 0], &#39;o&#39;, color=&#39;C1&#39;, markersize=2)
ax2.plot(mat_x0, mat_y2_hat[:, 1], &#39;o&#39;, color=&#39;C0&#39;, markersize=2)
ax2.plot(mat_x0, mat_y2_hat[:, 2], &#39;o&#39;, color=&#39;C2&#39;, markersize=2)
y_floor, _ = ax2.get_ylim()
ax2.plot(cluster1, [y_floor]*size_cluster, &#39;o&#39;, color=&#39;C1&#39;, markersize=2)
ax2.plot(cluster2, [y_floor]*size_cluster, &#39;o&#39;, color=&#39;C0&#39;, markersize=2)
ax2.plot(cluster3, [y_floor]*size_cluster, &#39;o&#39;, color=&#39;C2&#39;, markersize=2)
ax2.set_title(&#39;Degree = 2, Error = 0.04&#39;)
plt.show()</code></pre>
<pre><code>mat_y2_hat
[[ 0.91692888  0.12167828 -0.03860716]
 [ 0.93141869  0.1048827  -0.03630139]
 [ 1.00683423  0.01598011 -0.02281434]
 ...
 [-0.04129243  0.14236755  0.89892488]
 [-0.05170661  0.27341109  0.77829552]
 [-0.05422768  0.34239542  0.71183226]]</code></pre>
<div class="figure">
<img src="merged_files/merged_15_1.png" alt="" />
<p class="caption">png</p>
</div>
<p>FIGURE 4.3. The effects of masking on linear regression in IR for a three-class
problem. The rug plot at the base indicates the positions and class membership of
each observation. The three curves in each panel are the fitted regressions to the
three-class indicator variables; for example, for the blue class, <span class="math inline">\(y_{blue}\)</span> is 1 for the
blue observations, and 0 for the green and orange. The fits are linear and quadratic
polynomials. Above each plot is the training error rate. The Bayes error rate is
0.025 for this problem, as is the LDA error rate.</p>
<pre class="r"><code>%%R
## generate data and reproduce figure 4.3
mu = c(0.25, 0.5, 0.75)
sigma = 0.005*matrix(c(1, 0,
                 0, 1), 2, 2)
library(MASS)
set.seed(1650)
N = 300
X1 = mvrnorm(n = N, c(mu[1], mu[1]), Sigma = sigma)
X2 = mvrnorm(n = N, c(mu[2], mu[2]), Sigma = sigma)
X3 = mvrnorm(n = N, c(mu[3], mu[3]), Sigma = sigma)
# X is 900X2 matrix
X = rbind(X1, X2, X3)</code></pre>
<pre class="r"><code>%%R

# X.proj is projection to [0.5,0.5]
X.proj = rowMeans(X)
## fit as in figure 4.3
## consider orange
Y1 = c(rep(1, N), rep(0, N*2))
## blue
Y2 = c(rep(0, N), rep(1, N), rep(0, N))
## green
Y3 = c(rep(0, N), rep(0, N), rep(1, N))
## regression
m1 = lm(Y1~X.proj)
print(m1)
print(coef(m1))
pred1 = as.numeric(fitted(m1)[order(X.proj)])
m2 = lm(Y2~X.proj)
print(m2)
print(coef(m2))
pred2 = as.numeric(fitted(m2)[order(X.proj)])
m3 = lm(Y3~X.proj)
print(m3)
print(coef(m3))
pred3 = as.numeric(fitted(m3)[order(X.proj)])
c1 = which(pred1 &lt;= pred2)[1] 
c2 = min(which(pred3 &gt; pred2)) 
# class 1: 1 ~ c1
# class 2: c1+1 ~ c2
# class 3: c2+1 ~ end
# actually, c1 = c2
err1 = (abs(c2 - 2*N) + abs(c1 - N))/(3*N)

## reproduce figure 4.3 left
plot(0, 0, type = &quot;n&quot;, 
     xlim = c(0, 1), ylim = c(0,1), xlab = &quot;&quot;, ylab = &quot;&quot;,
     main = paste0(&quot;Degree = 1; Error = &quot;, round(err1, digits = 4)))
abline(coef(m1), col = &quot;orange&quot;)
abline(coef(m2), col = &quot;blue&quot;)
abline(coef(m3), col = &quot;green&quot;)
points(X.proj, fitted(m1), pch=&quot;1&quot;, col=&quot;orange&quot;)
points(X.proj, fitted(m2), pch = &quot;2&quot;, col = &quot;blue&quot;)
points(X.proj, fitted(m3), pch = &quot;3&quot;, col = &quot;green&quot;)
rug(X.proj[1:N], col = &quot;orange&quot;)
rug(X.proj[(N+1):(2*N)], col = &quot;blue&quot;)
rug(X.proj[(2*N+1):(3*N)], col = &quot;green&quot;)
abline(h=c(0.0, 0.5, 1.0), lty=5, lwd = 0.4)
abline(v=c(sort(X.proj)[N], sort(X.proj)[N*2]), lwd = 0.4)
</code></pre>
<pre><code>Call:
lm(formula = Y1 ~ X.proj)

Coefficients:
(Intercept)       X.proj  
      1.283       -1.901  

(Intercept)      X.proj 
   1.282927   -1.901234 

Call:
lm(formula = Y2 ~ X.proj)

Coefficients:
(Intercept)       X.proj  
    0.31258      0.04155  

(Intercept)      X.proj 
 0.31257989  0.04155158 

Call:
lm(formula = Y3 ~ X.proj)

Coefficients:
(Intercept)       X.proj  
    -0.5955       1.8597  

(Intercept)      X.proj 
 -0.5955073   1.8596820 </code></pre>
<div class="figure">
<img src="merged_files/merged_18_1.png" alt="" />
<p class="caption">png</p>
</div>
<pre class="r"><code>%%R
## polynomial regression
pm1 = lm(Y1~X.proj+I(X.proj^2))
pm2 = lm(Y2~X.proj+I(X.proj^2))
pm3 = lm(Y3~X.proj+I(X.proj^2))
## error rate for figure 4.3 right
pred21 = as.numeric(fitted(pm1)[order(X.proj)])
pred22 = as.numeric(fitted(pm2)[order(X.proj)])
pred23 = as.numeric(fitted(pm3)[order(X.proj)])
c1 = which(pred21 &lt;= pred22)[1] - 1
c2 = max(which(pred23 &lt;= pred22)) 
# class 1: 1 ~ c1
# class 2: c1+1 ~ c2
# class 3: c2+1 ~ end
err2 = (abs(c2 - 2*N) + abs(c1 - N))/(3*N)

## reproduce figure 4.3 right
plot(0, 0, type = &quot;n&quot;, 
     xlim = c(0, 1), ylim = c(-1,2), xlab = &quot;&quot;, ylab = &quot;&quot;,
     main = paste0(&quot;Degree = 2; Error = &quot;, round(err2, digits = 4)))
lines(sort(X.proj), fitted(pm1)[order(X.proj)], col=&quot;orange&quot;, type = &quot;o&quot;, pch = &quot;1&quot;)
lines(sort(X.proj), fitted(pm2)[order(X.proj)], col=&quot;blue&quot;, type = &quot;o&quot;, pch = &quot;2&quot;)
lines(sort(X.proj), fitted(pm3)[order(X.proj)], col=&quot;green&quot;, type = &quot;o&quot;, pch = &quot;3&quot;)
abline(h=c(0.0, 0.5, 1.0), lty=5, lwd = 0.4)
## add rug
rug(X.proj[1:N], col = &quot;orange&quot;)
rug(X.proj[(N+1):(2*N)], col = &quot;blue&quot;)
rug(X.proj[(2*N+1):(3*N)], col = &quot;green&quot;)
abline(v=c(sort(X.proj)[N], sort(X.proj)[N*2]), lwd = 0.4)</code></pre>
<div class="figure">
<img src="merged_files/merged_19_0.png" alt="" />
<p class="caption">png</p>
</div>
<p>For this simple example a quadratic rather than linear fit would solve the problem. However, if there were 4 classes, a quadratic would not come down fast enough, and a cubic would be needed as well. A loose but general rule is that if <span class="math inline">\(K\ge 3\)</span> classes are lined up, polynomial terms up to degree <span class="math inline">\(K-1\)</span> might be needed to resolve them.</p>
<p>Note also that these are polynomials along the derived direction passing through the centroids, which can have orbitrary orientation. So in <span class="math inline">\(p\)</span>-dimensional input space, one would need general polynomial terms and cross-products of total degree <span class="math inline">\(K-1\)</span>, <span class="math inline">\(O(p^{K-1})\)</span> terms in all, to resolve such worst-case scenarios.</p>
<p>The example is extreme, but for large <span class="math inline">\(K\)</span> and small <span class="math inline">\(p\)</span> such maskings natrually occur. As a more realistic illustration, FIGURE 4.4 is a projection of the training data for a vowel recognition problem onto an informative two-dimensional subspace.</p>
<pre class="python"><code>&quot;&quot;&quot;FIGURE 4.4. A two-dimensional plot of the vowel training data.

There are K=11 classes in p=10 dimensions, and this is the best view in
terms of a LDA model (Section 4.3.3). The heavy circles are the projected
mean vectors for each class. The class overlap is considerable.

This is a difficult classficiation problem, and the best methods achieve
around 40% errors on the test data. The main point here is summarized in
Table 4.1; masking has hurt in this case. Here simply the first 2 coordinates x.1 and x.2 are used.&quot;&quot;&quot;

vowel_df = pd.read_csv(&#39;../../data/vowel/vowel.train&#39;, index_col=0)
df_y = vowel_df[&#39;y&#39;]
df_x2d = vowel_df[[&#39;x.1&#39;, &#39;x.2&#39;]]
vowel_df</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
y
</th>
<th>
x.1
</th>
<th>
x.2
</th>
<th>
x.3
</th>
<th>
x.4
</th>
<th>
x.5
</th>
<th>
x.6
</th>
<th>
x.7
</th>
<th>
x.8
</th>
<th>
x.9
</th>
<th>
x.10
</th>
</tr>
<tr>
<th>
row.names
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
1
</th>
<td>
1
</td>
<td>
-3.639
</td>
<td>
0.418
</td>
<td>
-0.670
</td>
<td>
1.779
</td>
<td>
-0.168
</td>
<td>
1.627
</td>
<td>
-0.388
</td>
<td>
0.529
</td>
<td>
-0.874
</td>
<td>
-0.814
</td>
</tr>
<tr>
<th>
2
</th>
<td>
2
</td>
<td>
-3.327
</td>
<td>
0.496
</td>
<td>
-0.694
</td>
<td>
1.365
</td>
<td>
-0.265
</td>
<td>
1.933
</td>
<td>
-0.363
</td>
<td>
0.510
</td>
<td>
-0.621
</td>
<td>
-0.488
</td>
</tr>
<tr>
<th>
3
</th>
<td>
3
</td>
<td>
-2.120
</td>
<td>
0.894
</td>
<td>
-1.576
</td>
<td>
0.147
</td>
<td>
-0.707
</td>
<td>
1.559
</td>
<td>
-0.579
</td>
<td>
0.676
</td>
<td>
-0.809
</td>
<td>
-0.049
</td>
</tr>
<tr>
<th>
4
</th>
<td>
4
</td>
<td>
-2.287
</td>
<td>
1.809
</td>
<td>
-1.498
</td>
<td>
1.012
</td>
<td>
-1.053
</td>
<td>
1.060
</td>
<td>
-0.567
</td>
<td>
0.235
</td>
<td>
-0.091
</td>
<td>
-0.795
</td>
</tr>
<tr>
<th>
5
</th>
<td>
5
</td>
<td>
-2.598
</td>
<td>
1.938
</td>
<td>
-0.846
</td>
<td>
1.062
</td>
<td>
-1.633
</td>
<td>
0.764
</td>
<td>
0.394
</td>
<td>
-0.150
</td>
<td>
0.277
</td>
<td>
-0.396
</td>
</tr>
<tr>
<th>
…
</th>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
</tr>
<tr>
<th>
524
</th>
<td>
7
</td>
<td>
-4.065
</td>
<td>
2.876
</td>
<td>
-0.856
</td>
<td>
-0.221
</td>
<td>
-0.533
</td>
<td>
0.232
</td>
<td>
0.855
</td>
<td>
0.633
</td>
<td>
-1.452
</td>
<td>
0.272
</td>
</tr>
<tr>
<th>
525
</th>
<td>
8
</td>
<td>
-4.513
</td>
<td>
4.265
</td>
<td>
-1.477
</td>
<td>
-1.090
</td>
<td>
0.215
</td>
<td>
0.829
</td>
<td>
0.342
</td>
<td>
0.693
</td>
<td>
-0.601
</td>
<td>
-0.056
</td>
</tr>
<tr>
<th>
526
</th>
<td>
9
</td>
<td>
-4.651
</td>
<td>
4.246
</td>
<td>
-0.823
</td>
<td>
-0.831
</td>
<td>
0.666
</td>
<td>
0.546
</td>
<td>
-0.300
</td>
<td>
0.094
</td>
<td>
-1.343
</td>
<td>
0.185
</td>
</tr>
<tr>
<th>
527
</th>
<td>
10
</td>
<td>
-5.034
</td>
<td>
4.993
</td>
<td>
-1.633
</td>
<td>
-0.285
</td>
<td>
0.398
</td>
<td>
0.181
</td>
<td>
-0.211
</td>
<td>
-0.508
</td>
<td>
-0.283
</td>
<td>
0.304
</td>
</tr>
<tr>
<th>
528
</th>
<td>
11
</td>
<td>
-4.261
</td>
<td>
1.827
</td>
<td>
-0.482
</td>
<td>
-0.194
</td>
<td>
0.731
</td>
<td>
0.354
</td>
<td>
-0.478
</td>
<td>
0.050
</td>
<td>
-0.112
</td>
<td>
0.321
</td>
</tr>
</tbody>
</table>
<p>
528 rows × 11 columns
</p>
</div>
<pre class="python"><code>df_x2d.describe()</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
x.1
</th>
<th>
x.2
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
count
</th>
<td>
528.000000
</td>
<td>
528.000000
</td>
</tr>
<tr>
<th>
mean
</th>
<td>
-3.166695
</td>
<td>
1.735343
</td>
</tr>
<tr>
<th>
std
</th>
<td>
0.957965
</td>
<td>
1.160970
</td>
</tr>
<tr>
<th>
min
</th>
<td>
-5.211000
</td>
<td>
-1.274000
</td>
</tr>
<tr>
<th>
25%
</th>
<td>
-3.923000
</td>
<td>
0.916750
</td>
</tr>
<tr>
<th>
50%
</th>
<td>
-3.097000
</td>
<td>
1.733000
</td>
</tr>
<tr>
<th>
75%
</th>
<td>
-2.511750
</td>
<td>
2.403750
</td>
</tr>
<tr>
<th>
max
</th>
<td>
-0.941000
</td>
<td>
5.074000
</td>
</tr>
</tbody>
</table>
</div>
<pre class="python"><code>grouped = df_x2d.groupby(df_y)

fig44 = plt.figure(2, figsize=(10, 10))
ax = fig44.add_subplot(1, 1, 1)

for y, x in grouped:
    x_mean = x.mean()
    print(y)
    print(x_mean)
    color = next(ax._get_lines.prop_cycler)[&#39;color&#39;]
    print(color)
    ax.plot(x[&#39;x.1&#39;], x[&#39;x.2&#39;], &#39;o&#39;, color=color)
    ax.plot(x_mean[0], x_mean[1], &#39;o&#39;, color=color, markersize=10,
            markeredgecolor=&#39;black&#39;, markeredgewidth=3)
ax.set_xlabel(&#39;Coordinate 1 for Training Data&#39;)
ax.set_ylabel(&#39;Coordinate 2 for Training Data&#39;)
ax.set_title(&#39;Linear Discriminant Analysis&#39;)
plt.show()</code></pre>
<pre><code>1
x.1   -3.359563
x.2    0.062938
dtype: float64
#1f77b4
2
x.1   -2.708875
x.2    0.490604
dtype: float64
#ff7f0e
3
x.1   -2.440250
x.2    0.774875
dtype: float64
#2ca02c
4
x.1   -2.226604
x.2    1.525833
dtype: float64
#d62728
5
x.1   -2.756312
x.2    2.275958
dtype: float64
#9467bd
6
x.1   -2.673542
x.2    1.758771
dtype: float64
#8c564b
7
x.1   -3.243729
x.2    2.468354
dtype: float64
#e377c2
8
x.1   -4.051333
x.2    3.233979
dtype: float64
#7f7f7f
9
x.1   -3.876896
x.2    2.345021
dtype: float64
#bcbd22
10
x.1   -4.506146
x.2    2.688562
dtype: float64
#17becf
11
x.1   -2.990396
x.2    1.463875
dtype: float64
#1f77b4</code></pre>
<div class="figure">
<img src="merged_files/merged_23_1.png" alt="" />
<p class="caption">png</p>
</div>
<p>FIGURE 4.4. A two-dimensional plot of the vowel training data. There are
eleven classes with <span class="math inline">\(X \in \mathbb R^{10}\)</span>, and this is the best view in terms of a LDA model
(Section 4.3.3). The heavy circles are the projected mean vectors for each class.
The class overlap is considerable.</p>
<pre class="r"><code>%%R
load_vowel_data &lt;- function(doScaling=FALSE,doRandomization=FALSE){

  # Get the training data:
  #   
  XTrain  = read.csv(&quot;../../data/vowel/vowel.train&quot;, header=TRUE)

  # Delete the column named &quot;row.names&quot;:
  #
  XTrain$row.names = NULL

  # Extract the true classification for each datum
  # 
  labelsTrain = XTrain[,1] 

  # Delete the column of classification labels:
  #
  XTrain$y = NULL 
  
  #
  # We try to scale ALL features so that they have mean zero and a
  # standard deviation of one.
  #
  if( doScaling ){
    XTrain = scale(XTrain, TRUE, TRUE)
    means  = attr(XTrain,&quot;scaled:center&quot;)
    stds   = attr(XTrain,&quot;scaled:scale&quot;)
    XTrain = data.frame(XTrain)
  }

  #
  # Sometime data is processed and stored on disk in a certain order.  When doing cross validation
  # on such data sets we don&#39;t want to bias our results if we grab the first or the last samples.
  # Thus we randomize the order of the rows in the Training data frame to make sure that each
  # cross validation training/testing set is as random as possible.
  # 
  if( doRandomization ){
    nSamples    = dim(XTrain)[1] 
    inds        = sample( 1:nSamples, nSamples )
    XTrain      = XTrain[inds,]
    labelsTrain = labelsTrain[inds]
  }

  # Get the testing data:
  #
  XTest  = read.csv(&quot;../../data/vowel/vowel.test&quot;, header=TRUE)

  # Delete the column named &quot;row.names&quot;:
  #
  XTest$row.names = NULL

  # Extract the true classification for each datum
  # 
  labelsTest = XTest[,1] 

  # Delete the column of classification labels:
  #
  XTest$y = NULL 

  # Scale the testing data using the same transformation as was applied to the training data:
  #  apply &quot;1&quot; for rows and &quot;2&quot; for columns
  if( doScaling ){
    XTest = t( apply( XTest, 1, &#39;-&#39;, means ) ) 
    XTest = t( apply( XTest, 1, &#39;/&#39;, stds ) ) 
  }

  return( list( XTrain, labelsTrain, XTest, labelsTest ) ) 
}</code></pre>
<pre class="r"><code>%%R
linear_regression_indicator_matrix = function(XTrain,yTrain){

  # Inputs:
  #   XTrain = training matrix (without the common column of ones needed to represent the constant offset)
  #   yTrain = training labels matrix of true classifications with indices 1 - K (where K is the number of classes)

  K = max(yTrain) # the number of classes
  N = dim( XTrain )[1] # the number of samples 

  # form the indicator responce matrix Y
  # mat.or.vec(nr, nc) creates an nr by nc zero matrix if nr is greater than 1, and a zero vector of length nr if nc equals 1.
  Y = mat.or.vec( N, K ) 
  for( ii in 1:N ){
    jj       = yTrain[ii] 
    Y[ii,jj] = 1.
  }
  #Y is a 528X11 matrix with 1 in the diagonal and 0 elsewhere
  Y = as.matrix(Y)

  # for the feature vector matrix XTrain ( append a leading column of ones ) and compute Yhat:
  #
  ones = as.matrix( mat.or.vec( N, 1 ) + 1.0 )
  Xm   = as.matrix( cbind( ones, XTrain ) )
  # Bhat is a 11X11 matrix
  Bhat = solve( t(Xm) %*% Xm, t(Xm) %*% Y ) # this is used for predictions on out of sample data  
  Yhat = Xm %*% Bhat # the discriminant predictions on the training data  
  #which.max() Determines the location, i.e., index of the first maximum of a numeric (or logical) vector.
  #apply(&quot;1&quot;) for the row
  # gHat is a 528 vector
  gHat = apply( Yhat, 1, &#39;which.max&#39; ) # classify this data 
  
  return( list(Bhat,Yhat,gHat) )

}</code></pre>
<pre class="r"><code>%%R
library(MASS) # this has functions for lda and qda 

out         = load_vowel_data(doScaling=FALSE, doRandomization=FALSE)
XTrain      = out[[1]]
labelsTrain = out[[2]] 
XTest       = out[[3]]
labelsTest  = out[[4]] 
lda( XTrain, labelsTrain )
predict( lda( XTrain, labelsTrain ), XTrain )$class</code></pre>
<pre><code>  [1] 1  1  3  4  5  5  5  8  9  10 11 1  1  3  4  5  5  5  8  9  10 11 1  1  3 
 [26] 4  5  5  7  8  9  10 11 1  1  3  4  5  5  7  8  9  10 11 1  2  3  4  6  5 
 [51] 7  8  9  10 11 1  2  3  4  11 5  5  8  9  10 11 1  2  3  4  5  6  7  8  8 
 [76] 10 11 1  2  3  4  5  6  7  8  8  10 6  1  2  3  4  5  6  7  8  8  10 6  1 
[101] 2  3  4  5  6  7  8  8  10 11 1  2  3  4  5  6  7  8  8  10 11 1  2  3  4 
[126] 5  6  7  8  10 10 11 3  2  2  6  7  6  7  8  7  10 9  3  3  2  6  7  7  7 
[151] 8  7  10 11 2  3  3  6  7  6  7  8  7  10 11 2  2  3  6  7  6  7  8  9  10
[176] 11 2  2  2  6  7  6  7  8  10 10 11 2  2  2  6  7  6  7  8  9  10 11 2  2 
[201] 3  4  7  6  7  7  9  9  11 2  11 3  4  7  5  7  7  9  9  11 2  2  3  4  5 
[226] 7  11 7  9  9  11 2  2  3  4  5  7  11 7  9  10 11 1  2  3  4  5  11 11 7 
[251] 9  9  11 1  2  3  4  5  11 6  7  9  10 11 1  2  3  4  5  6  7  8  10 10 11
[276] 1  2  3  4  5  6  7  8  10 10 11 1  2  3  4  5  6  7  8  10 10 11 1  2  3 
[301] 4  5  6  7  8  10 10 11 1  2  3  4  5  2  7  8  9  10 11 1  3  3  4  5  2 
[326] 7  8  10 10 3  1  1  3  4  5  6  7  8  10 10 11 1  1  3  4  5  6  5  8  10
[351] 10 11 1  1  3  4  5  6  7  8  9  10 11 1  1  3  4  5  11 7  8  10 10 11 1 
[376] 1  3  4  5  11 5  8  10 10 11 1  1  3  4  5  11 5  8  9  10 11 1  2  3  4 
[401] 5  5  7  8  9  9  3  1  2  3  4  5  5  7  8  9  10 4  1  2  3  4  5  5  7 
[426] 8  9  9  4  1  2  3  4  5  5  7  10 9  9  4  1  2  3  4  5  6  5  10 9  9 
[451] 4  1  2  3  4  5  4  5  10 9  9  4  10 3  3  11 6  6  7  8  9  8  11 10 3 
[476] 3  11 5  6  7  9  9  8  11 10 3  3  11 6  6  9  10 9  8  11 10 3  3  11 6 
[501] 11 9  10 9  8  11 10 3  11 11 6  11 9  10 9  8  11 9  3  11 11 6  11 7  10
[526] 9  8  11
Levels: 1 2 3 4 5 6 7 8 9 10 11</code></pre>
<pre class="r"><code>%%R
library(nnet)
fm = data.frame( cbind(XTrain,labelsTrain) )
#nnet::multinom: Fit Multinomial Log-linear Models
m = multinom( labelsTrain ~ x.1 + x.2 + x.3 + x.4 + x.5 + x.6 + x.7 + x.8 + x.9 + x.10, data=fm )
summary(m)</code></pre>
<pre><code># weights:  132 (110 variable)
initial  value 1266.088704 
iter  10 value 800.991170
iter  20 value 609.647137
iter  30 value 467.787809
iter  40 value 378.168922
iter  50 value 353.235470
iter  60 value 346.430211
iter  70 value 341.952991
iter  80 value 339.249147
iter  90 value 338.593226
iter 100 value 338.518492
final  value 338.518492 
stopped after 100 iterations
Call:
multinom(formula = labelsTrain ~ x.1 + x.2 + x.3 + x.4 + x.5 + 
    x.6 + x.7 + x.8 + x.9 + x.10, data = fm)

Coefficients:
   (Intercept)       x.1      x.2          x.3        x.4       x.5        x.6
2    11.859912  5.015869  9.14065  -0.54668237  -5.842854  5.249095  4.5380736
3    22.991130  8.754650 10.51908  -8.09716943  -6.433698  2.844026 -3.0990773
4    21.211055  9.352021 14.27583 -11.44350228  -9.398762 -5.024023 -9.9120414
5    10.661226  7.994979 18.69133  -3.90686991 -10.227598 -6.879093 -2.3500276
6    17.371541  8.472878 16.80821  -3.55182322  -9.189781 -5.125459 -2.2086056
7    -4.496469  4.108638 19.49488  -2.84114847 -10.997593 -5.176146 -0.2774606
8   -39.710811 -2.641330 21.27174  -3.35135948 -10.611230 -1.537209  5.0932251
9   -25.061995  0.313354 21.23335  -0.04657761  -8.623545  2.898791 10.0112280
10  -56.893368 -6.189874 21.79140   0.53235180  -5.991305 10.081613 14.1554875
11   12.074591  6.162025 15.21914  -2.13476689  -9.304403 -2.280308  1.4693565
           x.7        x.8        x.9      x.10
2   -6.9230019 -1.5923565 -0.1703113  3.852009
3  -11.7597260 -7.9008092 -0.2394799  2.098955
4   -8.8182541 -6.6685842  0.8984416  3.187334
5   -4.1403291 -5.6554346  2.1035335  4.041918
6   -4.4687362 -7.2742827  0.9639560  3.861004
7   -2.3347884  0.3774047  5.5473928  5.496751
8    0.2155082  9.7082600 12.3540217 10.138371
9    3.5998811  6.8699809 11.8096580 10.206290
10   9.1879114  8.2389248 16.1435153 11.816428
11  -6.8252204 -4.9959056 -0.5257904  2.252969

Std. Errors:
   (Intercept)      x.1      x.2      x.3      x.4      x.5      x.6      x.7
2     3.791808 1.580918 2.419225 1.220975 1.500400 1.561514 1.455001 1.748906
3     4.614425 1.841941 2.666256 2.012425 1.600956 2.175967 2.084401 2.360013
4     4.580466 1.799125 2.878256 2.158141 1.790547 2.251240 2.606130 2.143600
5     4.761204 1.813573 2.915254 1.669411 1.780769 2.085541 2.321362 2.083458
6     4.461622 1.772211 2.867150 1.576007 1.713658 2.056081 2.128165 2.016414
7     5.243927 1.843223 2.931897 1.610943 1.726087 2.065063 2.315576 2.087362
8     7.509088 2.155127 3.082591 1.771398 1.816429 2.384532 2.631285 2.315004
9     6.425780 1.956012 3.020526 1.460973 1.728442 2.058597 2.416913 2.190779
10    9.849426 2.554512 3.096157 1.513431 1.819314 2.332869 2.542162 2.536305
11    4.355988 1.753685 2.850742 1.439323 1.710062 1.925247 1.987077 1.984446
        x.8      x.9     x.10
2  1.562736 1.046892 1.512046
3  2.191037 1.466805 1.646052
4  2.371566 1.507766 1.672210
5  2.360183 1.569246 1.705087
6  2.273602 1.392899 1.600117
7  2.520142 1.750712 1.713267
8  2.820331 2.284959 2.050160
9  2.648977 2.217472 1.950013
10 2.969891 2.488993 2.097796
11 2.148410 1.320647 1.552863

Residual Deviance: 677.037 
AIC: 897.037 </code></pre>
<pre class="r"><code>%%R                                                                                                                          
library(MASS) # this has functions for lda and qda 

out         = load_vowel_data(doScaling=FALSE, doRandomization=FALSE)
XTrain      = out[[1]]
labelsTrain = out[[2]] 
XTest       = out[[3]]
labelsTest  = out[[4]] 

# TRAIN A LINEAR REGRESSION BASED MODEL:
#
out      = linear_regression_indicator_matrix(XTrain,labelsTrain)
Bhat     = out[[1]]
Yhat     = out[[2]] 
tpLabels = out[[3]]

numCC      = sum( (tpLabels - labelsTrain) == 0 ) #total correct predictions
numICC     = length(tpLabels)-numCC #total incorrect predictions
eRateTrain = numICC / length(tpLabels) # error rate

# predict on the testing data with this classifier:
#
N         = length(labelsTest)
ones      = as.matrix( mat.or.vec( N, 1 ) + 1.0 )
Xm        = as.matrix( cbind( ones, XTest ) )
tpLabels  = apply( Xm %*% Bhat, 1, &#39;which.max&#39; ) 
numCC     = sum( (tpLabels - labelsTest) == 0 )
numICC    = length(tpLabels)-numCC 
eRateTest = numICC / length(tpLabels) 

print(sprintf(&quot;%40s: %10.6f; %10.6f&quot;,&quot;Linear Regression&quot;,eRateTrain,eRateTest))

# TRAIN A LDA MODEL:
# MASS::lda 
ldam = lda( XTrain, labelsTrain ) 

# get this models predictions on the training data
# 
predTrain = predict( ldam, XTrain ) 
tpLabels  = as.double( predTrain$class ) 

numCC      = sum( (tpLabels - labelsTrain) == 0 )
numICC     = length(tpLabels)-numCC 
eRateTrain = numICC / length(tpLabels) 

# get this models predictions on the testing data
# 
predTest = predict( ldam, XTest ) 
tpLabels = as.double( predTest$class ) 

numCC     = sum( (tpLabels - labelsTest) == 0 )
numICC    = length(tpLabels)-numCC 
eRateTest = numICC / length(tpLabels) 

print(sprintf(&quot;%40s: %10.6f; %10.6f&quot;,&quot;Linear Discriminant Analysis (LDA)&quot;,eRateTrain,eRateTest))

# TRAIN A QDA MODEL:

qdam = qda( XTrain, labelsTrain ) 

# get this models predictions on the training data
# 
predTrain = predict( qdam, XTrain ) 
tpLabels  = as.double( predTrain$class ) 

numCC      = sum( (tpLabels - labelsTrain) == 0 )
numICC     = length(tpLabels)-numCC 
eRateTrain = numICC / length(tpLabels) 

# get this models predictions on the testing data
# 
predTest = predict( qdam, XTest ) 
tpLabels = as.double( predTest$class ) 

numCC     = sum( (tpLabels - labelsTest) == 0 )
numICC    = length(tpLabels)-numCC 
eRateTest = numICC / length(tpLabels) 

print(sprintf(&quot;%40s: %10.6f; %10.6f&quot;,&quot;Quadratic Discriminant Analysis (QDA)&quot;,eRateTrain,eRateTest))

library(nnet)
fm = data.frame( cbind(XTrain,labelsTrain) )
#nnet::multinom: Fit Multinomial Log-linear Models
m = multinom( labelsTrain ~ x.1 + x.2 + x.3 + x.4 + x.5 + x.6 + x.7 + x.8 + x.9 + x.10, data=fm )
yhat_train = predict( m, newdata=XTrain, &quot;class&quot; )
yhat_test = predict( m, newdata=XTest, &quot;class&quot; )

numCC      = sum( (as.integer(yhat_train) - labelsTrain) == 0 )
numICC     = length(labelsTrain)-numCC 
eRateTrain = numICC / length(labelsTrain) 

numCC     = sum( (as.integer(yhat_test) - labelsTest) == 0 )
numICC    = length(labelsTest)-numCC 
eRateTest = numICC / length(labelsTest) 

print(sprintf(&quot;%40s: %10.6f; %10.6f&quot;,&quot;Logistic Regression&quot;,eRateTrain,eRateTest))</code></pre>
<pre><code>[1] &quot;                       Linear Regression:   0.477273;   0.666667&quot;
[1] &quot;      Linear Discriminant Analysis (LDA):   0.316288;   0.556277&quot;
[1] &quot;   Quadratic Discriminant Analysis (QDA):   0.011364;   0.528139&quot;
# weights:  132 (110 variable)
initial  value 1266.088704 
iter  10 value 800.991170
iter  20 value 609.647137
iter  30 value 467.787809
iter  40 value 378.168922
iter  50 value 353.235470
iter  60 value 346.430211
iter  70 value 341.952991
iter  80 value 339.249147
iter  90 value 338.593226
iter 100 value 338.518492
final  value 338.518492 
stopped after 100 iterations
[1] &quot;                     Logistic Regression:   0.221591;   0.512987&quot;</code></pre>
<p>TABLE 4.1. Training and test error rates using a variety of linear techniques
on the vowel data. There are eleven classes in ten dimensions, of which three
account for 90% of the variance (via a principal components analysis). We see
that linear regression is hurt by masking, increasing the test and training error
by over 10%.</p>
</div>
</div>
<div id="s-4.3.-linear-discriminant-analysis" class="section level1">
<h1><span class="math inline">\(\S\)</span> 4.3. Linear Discriminant Analysis</h1>
<p><span class="math inline">\(\S\)</span> 2.4. Decision theory for classification tells us that we need to know the class posteriors <span class="math inline">\(\text{Pr}(G|X)\)</span> for optimal classification. Suppose</p>
<ul>
<li><span class="math inline">\(f_k(x)\)</span> is the class-conditional density of <span class="math inline">\(X\)</span> in class <span class="math inline">\(G=k\)</span>,</li>
<li><span class="math inline">\(\pi_k\)</span> is the prior probability of class <span class="math inline">\(k\)</span>, with <span class="math inline">\(\sum\pi_k=1\)</span>.</li>
</ul>
<p>A simple application of Bayes theorem gives us</p>
<p><span class="math display">\[\begin{equation}
\text{Pr}(G=k|X=x) = \frac{f_k(x)\pi_k}{\sum_{l=1}^K f_l(x)\pi_l}.
\end{equation}\]</span></p>
<p>We see that in terms of ability to classify, it is enough to have the <span class="math inline">\(f_k(x)\)</span>.</p>
<p>Many techniques are based on models for the class densities:</p>
<ul>
<li>linear and quadratic discriminant analysis use Gaussian densities;</li>
<li>more flexible mixtures of Gaussian allow for nonlinear decision boundaires (<span class="math inline">\(\S\)</span> 6.8);</li>
<li>general nonparametric density estimates for each class density allow the most flexibility (<span class="math inline">\(\S\)</span> 6.6.2);</li>
<li><em>Naive Bayes</em> models are a variant of the previous case, and assume that the inputs are conditionally independent in each class; i.e., each of the class densities are products of marginal densities (<span class="math inline">\(\S\)</span> 6.6.3).</li>
</ul>
<div id="lda-from-multivariate-gaussian" class="section level3">
<h3>LDA from multivariate Gaussian</h3>
<p>Suppose that we model each class density as multivariate Gaussian</p>
<p><span class="math display">\[\begin{equation}
f_k(x) = \frac{1}{(2\pi)^{p/2}|\Sigma_k|^{1/2}}\exp\left\lbrace -\frac{1}{2}(x-\mu_k)^T\Sigma_k^{-1}(x-\mu_k) \right\rbrace
\end{equation}\]</span></p>
<p>Linear discriminant analysis (LDA) arises in the special case when we assume that the classes have a common covariance matrix <span class="math inline">\(\Sigma_k=\Sigma,\forall k\)</span>.</p>
<p>In comparing two classes <span class="math inline">\(k\)</span> and <span class="math inline">\(l\)</span>, it is sufficient to look at the log-ratio, and we see that as an equation linear in <span class="math inline">\(x\)</span>,
<span class="math display">\[
\begin{align}
\log\frac{\text{Pr}(G=k|X=x)}{\text{Pr}(G=l|X=x)} &amp;= \log\frac{f_k(x)}{f_l(x)} + \log\frac{\pi_k}{\pi_l} \\
&amp;= \log\frac{\pi_k}{\pi_l} - \frac{1}{2}(\mu_k+\mu_l)^T\Sigma^{-1}(\mu_k-\mu_l)+x^T\Sigma^{-1}(\mu_k-\mu_l) \\
&amp;= \log\frac{\pi_k}{\pi_l} - \frac{1}{2}\mu_k^T\Sigma^{-1}\mu_k + \frac{1}{2}\mu_l^T\Sigma^{-1}\mu_l + x^T\Sigma^{-1}(\mu_k-\mu_l) \\
&amp;= \delta_k(x) - \delta_l(x),
\end{align}
\]</span>
where <span class="math inline">\(\delta_k\)</span> is the <em>linear discriminant function</em></p>
<p><span class="math display">\[\begin{equation}
\delta_k(x) = -\frac{p}{2}\log(2\pi) -\frac{1}{2}\log(|\Sigma|) + x^T\Sigma^{-1}\mu_k - \frac{1}{2}\mu_k^T\Sigma^{-1}\mu_k + \log\pi_k
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
\delta_l(x) = -\frac{p}{2}\log(2\pi) -\frac{1}{2}\log(|\Sigma|) + x^T\Sigma^{-1}\mu_l - \frac{1}{2}\mu_l^T\Sigma^{-1}\mu_l + \log\pi_l
\end{equation}\]</span></p>
<p>This linear log-odds function implies that the decision boundary between classes <span class="math inline">\(k\)</span> and <span class="math inline">\(l\)</span></p>
<p><span class="math display">\[\begin{equation}
\left\lbrace x: \delta_k(x) - \delta_l(x) = 0 \right\rbrace
\end{equation}\]</span></p>
<p>is linear in <span class="math inline">\(x\)</span>; in <span class="math inline">\(p\)</span> dimensions a hyperplane. Also the linear discriminant functions are equivalent description of the decision rule, with</p>
<p><span class="math display">\[\begin{equation}
G(x) = \arg\max_k \delta_k(x).
\end{equation}\]</span></p>
</div>
<div id="estimating-parameters" class="section level3">
<h3>Estimating parameters</h3>
<p>In practice we do not know the parameters of the Gaussian distributions, and will need to estimate them using our training data:</p>
<ul>
<li><span class="math inline">\(\hat\pi_k = N_k/N\)</span>,</li>
<li><span class="math inline">\(\hat\mu_k = \sum_{g_i = k} x_i/N_k\)</span>;</li>
<li><span class="math inline">\(\hat\Sigma = \sum_{k=1}^K \sum_{g_i=k}(x_i-\hat\mu_k)(x_i-\hat\mu_k)^T/(N-K)\)</span>.</li>
</ul>
</div>
<div id="simple-correspondence-between-lda-and-linear-regression-with-two-classes" class="section level3">
<h3>Simple correspondence between LDA and linear regression with two classes</h3>
<p>The LDA rule classifies to class 2 if</p>
<p><span class="math display">\[\delta_2(x)-\delta_1(x)&gt;0\]</span> or</p>
<p><span class="math display">\[
\begin{equation}
x^T\hat\Sigma^{-1}(\hat\mu_2-\hat\mu_1) &gt; \frac{1}{2}(\hat\mu_2+\hat\mu_1)^T\hat\Sigma^{-1}(\hat\mu_2-\hat\mu_1) - \log\frac{N_2}{N_1},
\end{equation}
\]</span>
and class 1 otherwise. If we code the targets in the two classees as <span class="math inline">\(+1\)</span> and <span class="math inline">\(-1\)</span> respectively, then the coefficient vector from least squares is proportional to the LDA direction shown above (Exercise 4.2). However unless <span class="math inline">\(N_1=N_2\)</span> the intercepts are different and hence the resulting decision rules are different.</p>
<p>If <span class="math inline">\(K&gt;2\)</span>, LDA is not the same as linear regression of the class indicator matrix, and it avoids the masking problems (Hastie et al., 1994). A correspondence can be established through the notion of <em>optimal scoring</em>, discussed in <span class="math inline">\(\S\)</span> 12.5.</p>
</div>
<div id="practice-beyond-the-gaussian-assumption" class="section level3">
<h3>Practice beyond the Gaussian assumption</h3>
<p>Since the derivation of the LDA direction via least squares does not use a Gaussian assumption for the features, its applicability extends beyond the realm of Gaussian data. However the derivation of the particular intercept or cut-point given in the above LDA rule <em>does</em> require Gaussian data. Thus it makes sense to instead choose the cut-point that empirically minimizes training error for a given dataset. This something we have found to work well in practive, but have not seen it mentioned in the literature.</p>
<pre class="python"><code>&quot;&quot;&quot;FIGURE 4.5. An idealized example with K=3, p=2, and a common covariance

Here the right panel shows the LDA classification results instead of the
decision boundaries.&quot;&quot;&quot;</code></pre>
<pre class="python"><code>size_cluster = 30
# mat_rand = scipy.rand(2, 2)
# cov = mat_rand.T @ mat_rand / 10
cov = np.array([[1.0876306, 0.2065698],
                   [0.2065698, 0.1157603]])/2

cluster1 = np.random.multivariate_normal([-.5, 0], cov, size_cluster)
cluster2 = np.random.multivariate_normal([.5, 0], cov, size_cluster)
cluster3 = np.random.multivariate_normal([0, .5], cov, size_cluster)

# Estimating parameters axis = 0 is the means of the rows
vec_mean1 = cluster1.mean(axis=0)
print(vec_mean1)
vec_mean2 = cluster2.mean(axis=0)
print(vec_mean2)
vec_mean3 = cluster3.mean(axis=0)
print(vec_mean3)

cluster_centered1 = cluster1 - vec_mean1
cluster_centered2 = cluster2 - vec_mean2
cluster_centered3 = cluster3 - vec_mean3
#mat_cov is 2X2 covariance matrix, which is the \hat\Sigma
mat_cov = (cluster_centered1.T @ cluster_centered1 +
           cluster_centered2.T @ cluster_centered2 +
           cluster_centered3.T @ cluster_centered3)/(3*size_cluster-3)
print(mat_cov)</code></pre>
<pre><code>[-0.50612656 -0.01731414]
[ 0.59327288 -0.01532545]
[0.02818347 0.45984356]
[[0.53340587 0.10735396]
 [0.10735396 0.05431232]]</code></pre>
<pre class="python"><code># Calculate linear discriminant scores
# mat_cov @ sigma_inv_mu123 = np.vstack((vec_mean1, vec_mean2, vec_mean3)).T
# sigma_inv_mu123 is \Sigma^{-1} @ u_{123}^T
sigma_inv_mu123 = scipy.linalg.solve(
    mat_cov,
    np.vstack((vec_mean1, vec_mean2, vec_mean3)).T,
)
print(&quot;sigma_inv_mu123&quot;)
print(sigma_inv_mu123)
print(sigma_inv_mu123.T)
sigma_inv_mu1, sigma_inv_mu2, sigma_inv_mu3 = sigma_inv_mu123.T

# mat_x is 90X2 matrix
# sigma_inv_mu123 is 2X3 matrix
mat_x = np.vstack((cluster1, cluster2, cluster3))
# mat_delta is 90X3 matrix
mat_delta = (mat_x @ sigma_inv_mu123 -
             np.array((vec_mean1 @ sigma_inv_mu1.T,
                          vec_mean2 @ sigma_inv_mu2.T,
                          vec_mean3 @ sigma_inv_mu3.T))/2)
cluster_classified1 = mat_x[mat_delta.argmax(axis=1) == 0]
cluster_classified2 = mat_x[mat_delta.argmax(axis=1) == 1]
cluster_classified3 = mat_x[mat_delta.argmax(axis=1) == 2]</code></pre>
<pre><code>sigma_inv_mu123
[[-1.46914484  1.9413035  -2.74196494]
 [ 2.58512974 -4.1193617  13.88643346]]
[[-1.46914484  2.58512974]
 [ 1.9413035  -4.1193617 ]
 [-2.74196494 13.88643346]]</code></pre>
<pre class="python"><code>fig45 = plt.figure(0, figsize=(10, 5))
ax1 = fig45.add_subplot(1, 2, 1)
ax1.plot(cluster1[:, 0], cluster1[:, 1], &#39;o&#39;, color=&#39;C0&#39;)
ax1.plot(cluster2[:, 0], cluster2[:, 1], &#39;o&#39;, color=&#39;C1&#39;)
ax1.plot(cluster3[:, 0], cluster3[:, 1], &#39;o&#39;, color=&#39;C2&#39;)
ax1.plot(-.5, 0, &#39;o&#39;, color=&#39;C0&#39;, markersize=10, markeredgecolor=&#39;black&#39;,
         markeredgewidth=3)
ax1.plot(.5, 0, &#39;o&#39;, color=&#39;C1&#39;, markersize=10, markeredgecolor=&#39;black&#39;,
         markeredgewidth=3)
ax1.plot(0, .5, &#39;o&#39;, color=&#39;C2&#39;, markersize=10, markeredgecolor=&#39;black&#39;,
         markeredgewidth=3)
ax1.set_title(&#39;Simulation Data&#39;)

ax2 = fig45.add_subplot(1, 2, 2)
ax2.plot(cluster_classified1[:, 0], cluster_classified1[:, 1], &#39;o&#39;, color=&#39;C0&#39;)
ax2.plot(cluster_classified2[:, 0], cluster_classified2[:, 1], &#39;o&#39;, color=&#39;C1&#39;)
ax2.plot(cluster_classified3[:, 0], cluster_classified3[:, 1], &#39;o&#39;, color=&#39;C2&#39;)
ax2.plot(vec_mean1[0], vec_mean1[1], &#39;o&#39;, color=&#39;C0&#39;, markersize=10,
         markeredgecolor=&#39;black&#39;, markeredgewidth=3)
ax2.plot(vec_mean2[0], vec_mean2[1], &#39;o&#39;, color=&#39;C1&#39;, markersize=10,
         markeredgecolor=&#39;black&#39;, markeredgewidth=3)
ax2.plot(vec_mean3[0], vec_mean3[1], &#39;o&#39;, color=&#39;C2&#39;, markersize=10,
         markeredgecolor=&#39;black&#39;, markeredgewidth=3)
ax2.set_title(&#39;LDA Results&#39;)
plt.show()</code></pre>
<div class="figure">
<img src="merged_files/merged_39_0.png" alt="" />
<p class="caption">png</p>
</div>
<pre class="r"><code>%%R
diag(1, 3, 3)</code></pre>
<pre><code>     [,1] [,2] [,3]
[1,]    1    0    0
[2,]    0    1    0
[3,]    0    0    1</code></pre>
<pre class="r"><code>%%R

library(MASS)
library(mvtnorm)
#sigma = diag(1, 2, 2)
sigma = matrix(c(2,1,1,2), nrow = 2)
mu1 = c(-1.5, -0.2)
mu2 = c(1.5, -0.2)
mu3 = c(0, 2)

N = 1000
set.seed(123)
dm1 = mvrnorm(N, mu1, sigma)
dm2 = mvrnorm(N, mu2, sigma)
dm3 = mvrnorm(N, mu3, sigma)
# dmvnorm Calculates the probability density function of the multivariate normal distribution
z1 = dmvnorm(dm1, mu1, sigma)
z2 = dmvnorm(dm2, mu2, sigma)
z3 = dmvnorm(dm3, mu3, sigma)
lev1 = quantile(as.numeric(z1), 0.05)
lev2 = quantile(as.numeric(z2), 0.05)
lev3 = quantile(as.numeric(z3), 0.05)

myContour &lt;- function(mu, sigma, level, col, n=300){
   x.points &lt;- seq(-10,10,length.out=n)
   y.points &lt;- x.points
   z &lt;- matrix(0,nrow=n,ncol=n)
   for (i in 1:n) {
     for (j in 1:n) {
       z[i,j] &lt;- dmvnorm(c(x.points[i],y.points[j]),
                         mean=mu,sigma=sigma)
     }
   }
   contour(x.points,y.points,z, levels = level, col = col, xlim = c(-10, 10), ylim = c(-10, 10), labels = &quot;&quot;)
 }
myContour(mu1, sigma, lev1, &quot;orange&quot;)
par(new=TRUE)
myContour(mu2, sigma, lev2, &quot;blue&quot;)
par(new=TRUE)
myContour(mu3, sigma, lev3, &quot;green&quot;)
points(rbind(mu1, mu2, mu3), pch=&quot;+&quot;, col=&quot;black&quot;)</code></pre>
<div class="figure">
<img src="merged_files/merged_41_0.png" alt="" />
<p class="caption">png</p>
</div>
<pre class="r"><code>%%R

N = 30
sigma = matrix(c(2,1,1,2), nrow = 2)
mu1 = c(-1.5, -0.2)
mu2 = c(1.5, -0.2)
mu3 = c(0, 2)
set.seed(123)
dm1 = mvrnorm(N, mu1, sigma)
dm2 = mvrnorm(N, mu2, sigma)
dm3 = mvrnorm(N, mu3, sigma)

m12 = lda(rbind(dm1, dm2), rep(c(&quot;c1&quot;,&quot;c2&quot;), each=N))
m13 = lda(rbind(dm1, dm3), rep(c(&quot;c1&quot;,&quot;c3&quot;), each=N))
m23 = lda(rbind(dm2, dm3), rep(c(&quot;c2&quot;,&quot;c3&quot;), each=N))

calcY &lt;- function(c, x) { return(-1*c[1]*x/c[2]) }

calcLD &lt;- function(object) {
  mu = object$means
  mu.pool = colSums(mu)/2 ## (mu1+mu2)/2
  scaling = object$scaling
  intercept = sum(scaling * mu.pool)/scaling[2]
  slope = -1* scaling[1]/scaling[2]
  return(c(intercept, slope))
}

#plot
plot(dm1[, 1], dm1[, 2], col = &quot;orange&quot;, pch=&quot;1&quot;, 
     xlim = c(-5, 5), ylim = c(-5, 5), 
     xaxt=&quot;n&quot;, yaxt=&quot;n&quot;, xlab = &quot;&quot;, ylab = &quot;&quot;)
points(dm2[, 1], dm2[, 2], col = &quot;blue&quot;, pch=&quot;2&quot;)
points(dm3[, 1], dm3[, 2], col = &quot;green&quot;, pch=&quot;3&quot;)
points(rbind(mu1, mu2, mu3), pch=&quot;+&quot;, col=&quot;black&quot;)
clip(-5,5,-5,0)
#abline(0, -1*m12$scaling[1]/m12$scaling[2])
abline(calcLD(m12))
clip(-5,0,-5,5)
#abline(0, -1*m13$scaling[1]/m13$scaling[2])
abline(calcLD(m13))
clip(0,5,-5,5)
#abline(0, -1*m23$scaling[1]/m23$scaling[2])
abline(calcLD(m23))
</code></pre>
<div class="figure">
<img src="merged_files/merged_42_0.png" alt="" />
<p class="caption">png</p>
</div>
<p>FIGURE 4.5. The upper panel shows three Gaussian distributions, with the same
covariance and different means. Included are the contours of constant density
enclosing 95% of the probability in each case. The Bayes decision boundaries
between each pair of classes are shown (broken straight lines), and the Bayes
decision boundaries separating all three classes are the thicker solid lines (a subset
of the former). On the lower we see a sample of 30 drawn from each Gaussian
distribution, and the fitted LDA decision boundaries.</p>
<pre class="r"><code>%%R
## #######################################
## directly compute
## ######################################

## sample mean
zmu1 = colMeans(dm1)
zmu2 = colMeans(dm2)
zmu3 = colMeans(dm3)

## sample variance
zs1 = var(dm1)
zs2 = var(dm2)
zs3 = var(dm3)
zs12 = (zs1+zs2)/2 ## ((n1-1)S1+(n2-1)S2)/(n1+n2-2)
zs13 = (zs1+zs3)/2
zs23 = (zs2+zs3)/2

## #############################
## coef:
##   a = S^{-1}(mu1-mu2)
## #############################
za12 = solve(zs12) %*% (zmu1-zmu2)
za12
za13 = solve(zs13) %*% (zmu1-zmu3)
za13
za23 = solve(zs23) %*% (zmu2-zmu3)
za23

## ############################
## constant
##    0.5*a&#39;(mu1+mu2)
## ############################
c12 = sum(za12 * (zmu1+zmu2)/2)
c13 = sum(za13 * (zmu1+zmu3)/2)
c23 = sum(za23 * (zmu2+zmu3)/2)

calcLD2 &lt;- function(za, c) {return(c(c/za[2], -za[1]/za[2]))}
calcLD2(za12, c12)
calcLD2(za13, c13)
calcLD2(za23, c23)
cat(&quot;for class 1 and class 2&quot;,
    &quot;\nuse lda results: &quot;, calcLD(m12), &quot;\ncompute directly: &quot;, calcLD2(za12, c12),
    &quot;\n&quot;,
    &quot;\nfor class 1 and class 3&quot;, 
    &quot;\nuse lda results: &quot;, calcLD(m13), &quot;\ncompute directly: &quot;, calcLD2(za13, c13), 
    &quot;\n&quot;,
    &quot;\nfor class 2 and class 3&quot;,
    &quot;\nuse lda results: &quot;, calcLD(m23), &quot;\ncompute directly: &quot;, calcLD2(za23, c23))</code></pre>
<pre><code>for class 1 and class 2 
use lda results:  -0.1122356 1.641163 
compute directly:  -0.1122356 1.641163 
 
for class 1 and class 3 
use lda results:  0.8667723 -0.009281836 
compute directly:  0.8667723 -0.009281836 
 
for class 2 and class 3 
use lda results:  0.2141177 0.9654518 
compute directly:  0.2141177 0.9654518</code></pre>
</div>
<div id="quadratic-discriminant-analysis" class="section level3">
<h3>Quadratic Discriminant Analysis</h3>
<p>If the <span class="math inline">\(\Sigma_k\)</span> are not assumed to be equal, then the convenient cancellations do not occur. We then get <em>quadratic discriminant functions</em> (QDA),</p>
<p><span class="math display">\[\begin{equation}
\delta_k(x) =\log(p(x|\mathcal G_k))+\log(\pi_k) = -\frac{p}{2}\log(2\pi) -\frac{1}{2}\log|\Sigma_k| -\frac{1}{2}(x-\mu_k)^T\Sigma_k^{-1}(x-\mu_k) + \log\pi_k
\end{equation}\]</span></p>
<p>The decision boundary between each pair of classes <span class="math inline">\(k\)</span> and <span class="math inline">\(l\)</span> is described by a quadratic equation <span class="math inline">\(\left\lbrace x: \delta_k(x) = \delta_l(x) \right\rbrace\)</span>.</p>
<p>This estimates for QDA are similar to those for LDA, except that separate covariance matrices must be estimated for each class. When <span class="math inline">\(p\)</span> is large this can mean a dramatic increase in parameters.</p>
<blockquote>
<p>Since the decision boundaries are functions of the parameters of the densities, counting the number of parameters must be done with care.</p>
</blockquote>
<p>For LDA, it seems there are <span class="math inline">\((K-1)\times(p+1)\)</span> paramters, since we only need the differences <span class="math inline">\(\delta_k(x)-\delta_K(x)\)</span> between the discriminant functions where <span class="math inline">\(K\)</span> is some pre-chosen class (here the last), and each difference requires <span class="math inline">\(p+1\)</span> parameters. Likewise for QDA there will be <span class="math inline">\((K-1)\times\lbrace p(p+3)/2+1 \rbrace\)</span> parameters.</p>
<p>Both LDA and QDA perform well on an amazingly large and diverse set of classification tasks.</p>
</div>
<div id="why-lda-and-qda-have-such-a-good-track-record" class="section level3">
<h3>Why LDA and QDA have such a good track record?</h3>
<p>The data are approximately Gaussian, or for LDA the covariances are approximately equal? Maybe not.</p>
<p>More likely a reason is that the data can only support simple decision boundaries such as linear or quadratic, and the estimates provided via the Guassian models are stable.</p>
<p>This is a bias-variance tradeoff – we can put up with the bias of a linear decision boundary because it can be estimated with much lower variance than more exotic alternatives. This argument is less believable for QDA, since it can have many parameters itself, although perhaps fewer than the non-parametric alternatives.</p>
</div>
<div id="s-4.3.1.-regularized-discriminant-analysis" class="section level2">
<h2><span class="math inline">\(\S\)</span> 4.3.1. Regularized Discriminant Analysis</h2>
<div id="sigma_k-leftrightarrow-sigma" class="section level3">
<h3><span class="math inline">\(\Sigma_k \leftrightarrow \Sigma\)</span></h3>
<p>These methods are very similar in flavor to ridge regression. Friedman (1989) proposed a compromise between LDA and QDA, which allows one to shrink the separate covariances of QDA toward a common covariance <span class="math inline">\(\hat\Sigma\)</span> as in LDA. The regularized covariance matrices have the form</p>
<p><span class="math display">\[\begin{equation}
\hat\Sigma_k(\alpha) = \alpha\hat\Sigma_k + (1-\alpha)\hat\Sigma,
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\hat\Sigma\)</span> is the pooled covariance matrix as used in LDA and <span class="math inline">\(\alpha\in[0,1]\)</span> allows a continuum of models between LDA and QDA, and needs to be specified. In practice <span class="math inline">\(\alpha\)</span> can be chosen based on the performance of the model on validation data, or by cross-validation.</p>
</div>
<div id="sigma-leftrightarrow-sigma" class="section level3">
<h3><span class="math inline">\(\Sigma \leftrightarrow \sigma\)</span></h3>
<p>Similar modifications allow <span class="math inline">\(\hat\Sigma\)</span> itelf to be shrunk toward the scalar covariance,</p>
<p><span class="math display">\[\begin{equation}
\hat\Sigma(\gamma) = \gamma\hat\Sigma + (1-\gamma)\hat\sigma^2\mathbf{I},
\end{equation}\]</span></p>
<p>for <span class="math inline">\(\gamma\in[0,1]\)</span>.</p>
<p>Combining two regularization leads to a more general family of covariances <span class="math display">\[\hat\Sigma(\alpha,\gamma)=\alpha\hat\Sigma_k + (1-\alpha)\left(\gamma\hat\Sigma + (1-\gamma)\hat\sigma^2\mathbf{I}\right)\]</span>.</p>
</div>
<div id="to-be-continued" class="section level3">
<h3>To be continued</h3>
<p>In Chapter 12, we discuss other regularized versions of LDA, which are more suitable when the data arise from digitized analog signals and images. In these situations the features are high-dimensional and correlated, and the LDA coefficients can be regularized to be smooth or sparse in original domain of the signal.</p>
<p>In Chapter 18, we also deal with very high-dimensional problems, where for example, the features are gene-expression measurements in microarray studies.</p>
<pre class="r"><code>%%R
repmat = function(X,m,n){
  ##R equivalent of repmat (matlab)
  mx = dim(X)[1]
  nx = dim(X)[2]
  matrix(t(matrix(X,mx,nx*n)),mx*m,nx*n,byrow=T)
}</code></pre>
<pre class="r"><code>%%R
rda = function( XTrain, yTrain, XTest, yTest, alpha=1.0, gamma=1.0 ){
  #
  # R code to implement classification using Regularized Discriminant Analysis
  # Inputs:
  #   XTrain = training data frame
  #   yTrain = training labels of true classifications with indices 1 - K (where K is the number of classes)
  #   xTest = testing data frame
  #   yTest = testing response
  #
  # Note that
  #   gamma, alpha = (1.0, 1.0) gives quadratic discriminant analysis
  #   gamma, alpha = (1.0, 0.0) gives linear discriminant analysis
  # Check that our class labels are all positive: 
  stopifnot( all( yTrain&gt;0 ) )
  stopifnot( all( yTest&gt;0 ) )
  
  K = length(unique(yTrain)) # the number of classes (expect the classes to be labeled 1, 2, 3, ..., K-1, K 
  N = dim( XTrain )[1] # the number of samples
  p = dim( XTrain )[2] # the number of features 

  # Estimate \hat{sigma}^2 variance of all features:
  #
  XTM = as.matrix( XTrain )
  dim(XTM) = prod(dim(XTM)) # we now have all data in one vector
  sigmaHat2 = var(XTM) #\hat{\sigma}

  # Compute the class independent covariance matrix:
  #
  SigmaHat = cov(XTrain) #\hat{\Sigma} 10X10 matrix

  # Compute the class dependent mean vector and covariance matrices:

  PiK = list()
  MuHatK = list()
  SigmaHatK = list()
  for( ci in 1:K ){
    inds = (yTrain == ci)
    PiK[[ci]] = sum(inds)/N
    MuHatK[[ci]] = as.matrix( colMeans( XTrain[ inds, ] ) ) # 10X1 matrix
    SigmaHatK[[ci]] = cov( XTrain[ inds, ] )
  }

  # Blend the covariances as specified by Regularized Discriminant Analysis:

  RDA_SigmaHatK = list()
  for( ci in 1:K ){
    RDA_SigmaHatK[[ci]] = alpha * SigmaHatK[[ci]] + ( 1 - alpha ) * ( gamma * SigmaHat + ( 1 - gamma ) * sigmaHat2 * diag(p) )
  }

  # Compute some of the things needed for classification via the discriminant functions:
  #
  RDA_SigmaHatK_Det = list()
  RDA_SigmaHatK_Inv = list()
  for( ci in 1:K ){
    RDA_SigmaHatK_Det[[ci]] = det(RDA_SigmaHatK[[ci]])
    RDA_SigmaHatK_Inv[[ci]] = solve(RDA_SigmaHatK[[ci]]) # there are numerically better ways of doing this but ... 
  }

  # Classify Training data:
  #
  XTM = t(as.matrix( XTrain )) # dim= p x N 
  CDTrain = matrix( data=0, nrow=N, ncol=K ) # CDTrain = training class discriminants
  for( ci in 1:K ){
    MU = repmat( MuHatK[[ci]], 1, N ) # dim= p x N
    X_minus_MU = XTM - MU # dim= p x N
    SInv = RDA_SigmaHatK_Inv[[ci]] # dim= p x p
    SX = SInv %*% X_minus_MU # dim= ( p x N ); S^{-1}(X-\mu)
    for( si in 1:N ){
      CDTrain[si,ci] = -0.5 * log(RDA_SigmaHatK_Det[[ci]]) - 0.5 * t(X_minus_MU[,si]) %*% SX[,si] + PiK[[ci]]
    }
  }
  yHatTrain = apply( CDTrain, 1, which.max )
  errRateTrain = sum( yHatTrain != yTrain )/N

  # Classify Testing data:
  #
  N = dim( XTest )[1] 
  XTM = t(as.matrix( XTest )) # dim= p x N 
  CDTest = matrix( data=0, nrow=N, ncol=K ) # CDTest = testing class discriminants
  for( ci in 1:K ){
    MU = repmat( MuHatK[[ci]], 1, N ) # dim= p x N
    X_minus_MU = XTM - MU # dim= p x N
    SInv = RDA_SigmaHatK_Inv[[ci]] # dim= p x p
    SX = SInv %*% X_minus_MU # dim= ( p x N )
    for( si in 1:N ){
      CDTest[si,ci] = -0.5 * log(RDA_SigmaHatK_Det[[ci]]) - 0.5 * t(X_minus_MU[,si]) %*% SX[,si] + log(PiK[[ci]])
    }
  }
  yHatTest = apply( CDTest, 1, which.max )
  errRateTest = sum( yHatTest != yTest )/N
  
  return( list(yHatTrain,errRateTrain, yHatTest,errRateTest) )
}</code></pre>
<pre class="r"><code>%%R                                                                                                                           

out    = load_vowel_data( TRUE, FALSE )
XTrain = out[[1]]
yTrain = out[[2]] 
XTest  = out[[3]]
yTest  = out[[4]] 

alphas = seq(0.0,1.0,length.out=100)
err_rate_train = c()
err_rate_test = c()
for( apha in alphas ){
  out = rda( XTrain, yTrain, XTest, yTest, apha )
  err_rate_train = c(err_rate_train, out[[2]])
  err_rate_test = c(err_rate_test, out[[4]])
}
plot( alphas, err_rate_train, type=&quot;p&quot;, col=&quot;blue&quot;, ylim=range(c(err_rate_train,err_rate_test)), 
     xlab=&quot;alpha&quot;, ylab=&quot;Misclassification Rate&quot;, main=&quot;Regularized Discriminant Analysis on the Vowel Data&quot; )
lines( alphas, err_rate_test, type=&quot;p&quot;, col=&quot;red&quot; )


min_err_rate_spot = which.min( err_rate_test )
print( sprintf( &quot;Min test error rate= %10.6f; alpha= %10.6f&quot;, 
               err_rate_test[min_err_rate_spot], alphas[min_err_rate_spot] ) )


# run model selection with alpha and gamma models to combine to get Sigma_hat:

Nsamples = 100
alphas = seq(0.0,1.0,length.out=Nsamples)
gammas = seq(0.0,1.0,length.out=Nsamples)
err_rate_train = matrix( data=0, nrow=Nsamples, ncol=Nsamples )
err_rate_test = matrix( data=0, nrow=Nsamples, ncol=Nsamples )
for( ii in 1:Nsamples ){
  a = alphas[ii]
  for( jj in 1:Nsamples ){
    g = gammas[jj] 
    out = rda( XTrain, yTrain, XTest, yTest, a, gamma=g )
    err_rate_train[ii,jj] = out[[2]]
    err_rate_test[ii,jj] = out[[4]]
  }
}

inds = which( err_rate_test == min(err_rate_test), arr.ind=TRUE ); ii = inds[1]; jj = inds[2]
print( sprintf( &quot;Min test error rate= %10.6f; alpha= %10.6f; gamma= %10.6f&quot;, 
               err_rate_test[ii,jj], alphas[ii], gammas[jj] ) )</code></pre>
<pre><code>[1] &quot;Min test error rate=   0.478355; alpha=   0.969697&quot;
[1] &quot;Min test error rate=   0.439394; alpha=   0.767677; gamma=   0.050505&quot;</code></pre>
<div class="figure">
<img src="merged_files/merged_53_1.png" alt="" />
<p class="caption">png</p>
</div>
<p>FIGURE 4.7. Test and training errors for the vowel data, using regularized
discriminant analysis with a series of values of <span class="math inline">\(\alpha\in [0, 1]\)</span>. The optimum for the
test data occurs around <span class="math inline">\(\alpha = 0.97\)</span>, close to quadratic discriminant analysis.</p>
</div>
</div>
<div id="s-4.3.2.-computations-for-lda" class="section level2">
<h2><span class="math inline">\(\S\)</span> 4.3.2. Computations for LDA</h2>
<p>Computations for LDA and QDA are simplified by diagonalizing <span class="math inline">\(\hat\Sigma\)</span> or <span class="math inline">\(\hat\Sigma_k\)</span>. For the latter, suppose we compute the eigen-decomposition, for each <span class="math inline">\(k\)</span>,</p>
<p><span class="math display">\[\begin{equation}
\hat\Sigma_k = \mathbf{U}_k\mathbf{D}_k\mathbf{U}_k^T,
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\mathbf{U}_k\)</span> is <span class="math inline">\(p\times p\)</span> orthogonal, and <span class="math inline">\(\mathbf{D}_k\)</span> a diagonal matrix of positive eigenvalues <span class="math inline">\(d_{kl}\)</span>.</p>
<p>Then the ingredients for <span class="math inline">\(\delta_k(x)\)</span> are</p>
<ul>
<li><span class="math inline">\((x-\hat\mu_k)^T\hat\Sigma_k^{-1}(x-\hat\mu_k) = \left[\mathbf{U}_k^T(x-\hat\mu_k)\right]^T\mathbf{D}_k^{-1}\left[\mathbf{U}_k^T(x-\hat\mu_k)\right]\)</span></li>
<li><span class="math inline">\(\log|\hat\Sigma_k| = \sum_l \log d_{kl}\)</span></li>
</ul>
<p>Note that the inversion of diagonal matrices only requires elementwise reciprocals.</p>
<p>The LDA classifier can be implemented by the following pair of steps:</p>
<ul>
<li><em>Sphere</em> the data w.r.t. the common covariance estimate <span class="math inline">\(\hat\Sigma = \mathbf{U}\mathbf{D}\mathbf{U}^T\)</span>:</li>
</ul>
<p><span class="math display">\[\begin{equation}
X^* \leftarrow \mathbf{D}^{-\frac{1}{2}}\mathbf{U}^TX,
\end{equation}\]</span><br />
The common covariance estimate of <span class="math inline">\(X^*\)</span> will now be the identity.
* Classify to the closest class centroid in the transformed space, modulo the effect of the class prior probabilities <span class="math inline">\(\pi_k\)</span>.</p>
</div>
<div id="s-4.3.3.-reduced-rank-linear-discriminant-analysis" class="section level2">
<h2><span class="math inline">\(\S\)</span> 4.3.3. Reduced-Rank Linear Discriminant Analysis</h2>
<p>The <span class="math inline">\(K\)</span> centroids in <span class="math inline">\(p\)</span>-dimensional input space lie in an affine subspace of dimension <span class="math inline">\(\le K-1\)</span>, and if <span class="math inline">\(p \gg K\)</span>, then there will possibly be a considerable drop in dimension. Part of the popularity of LDA is due to such an additional restriction that allows us to view informative low-dimensional projections of the data.</p>
<p>Moreover, in locating the closest centroid, we can ignore distances orthogonal to this subspace, since they will contribute equally to each class. Thus we might just as well project the <span class="math inline">\(X^*\)</span> onto this centroid-spanning subspace <span class="math inline">\(H_{K-1}\)</span>, and make distance comparisons there.</p>
<p>Therefore there is a fundamental dimension reduction in LDA, namely, that we need only consider the data in a subspace of dimension at most <span class="math inline">\(K-1\)</span>. If <span class="math inline">\(K=3\)</span>, e.g., this could allow us to view the data in <span class="math inline">\(\mathbb{R}^2\)</span>, color-coding the classes. In doing so we would not have relinquished any of the information needed for LDA classification.</p>
<div id="what-if-k3-principal-components-subspace" class="section level3">
<h3>What if <span class="math inline">\(K&gt;3\)</span>? Principal components subspace</h3>
<p>We might then ask for a <span class="math inline">\(L&lt;K-1\)</span> dimensional subspace <span class="math inline">\(H_L \subseteq H_{K-1}\)</span> optimal for LDA in some sense. Fisher defined optimal to mean that the projected centroids were spread out as much as possible in terms of variance. This amounts to finding principal component subspaces of the centroids themselves (<span class="math inline">\(\S\)</span> 3.5.1, <span class="math inline">\(\S\)</span> 14.5.1).</p>
<p>In FIGURE 4.4 with the vowel data, there are eleven classes, each a different vowel sound, in a 10D input space. The centroids require the full space in this case, since <span class="math inline">\(K-1=p\)</span>, but we have shown an optimal 2D subspace.</p>
<p>The dimensions are ordered, so we can compute additional dimensions in sequence. FIGURE 4.8 shows four additional pairs of coordinates, a.k.a. <em>canonical</em> or <em>discriminant</em> variables.</p>
<p>In summary then, finding the sequences of optimal subspaces for LDA involves the following steps:</p>
<ul>
<li>Compute the <span class="math inline">\(K\times p\)</span> matrix of class centroids <span class="math inline">\(\mathbf{M}\)</span><br />
the common covariance matrix <span class="math inline">\(\mathbf{W}\)</span> (for <em>within</em>-class covariance).</li>
<li>Compute <span class="math inline">\(\mathbf{M}^* = \mathbf{MW}^{-\frac{1}{2}}\)</span> using the eigen-decomposition of <span class="math inline">\(\mathbf{W}\)</span>.</li>
<li>Compute <span class="math inline">\(\mathbf{B}^*\)</span>, the covariance matrix of <span class="math inline">\(\mathbf{M}^*\)</span> (<span class="math inline">\(\mathbf{B}\)</span> for <em>between</em>-class covariance),<br />
and its eigen-decomposition <span class="math inline">\(\mathbf{B}^* = \mathbf{V}^*\mathbf{D}_B\mathbf{V}^{*T}\)</span>.<br />
The columns <span class="math inline">\(v_l^*\)</span> of <span class="math inline">\(\mathbf{V}^*\)</span> in sequence from first to last define the coordinates of the optimal subspaces.</li>
<li>Then the <span class="math inline">\(l\)</span>th <em>discriminant variable</em> is given by<br />
<span class="math display">\[\begin{equation}
Z_l = v_l^TX \text{ with } v_l = \mathbf{W}^{-\frac{1}{2}}v_l^*.
\end{equation}\]</span></li>
</ul>
<pre class="python"><code>&quot;&quot;&quot;FIGURE 4.8. Four projections onto pairs of canonical variates.

&quot;&quot;&quot;

df_vowel = pd.read_csv(&#39;../../data/vowel/vowel.train&#39;, index_col=0)
print(&#39;A pandas DataFrame of size {} x {} &#39;
      &#39;has been loaded.&#39;.format(*df_vowel.shape))
df_y = df_vowel.pop(&#39;y&#39;)
mat_x = df_vowel.values
df_vowel</code></pre>
<pre><code>A pandas DataFrame of size 528 x 11 has been loaded.</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
x.1
</th>
<th>
x.2
</th>
<th>
x.3
</th>
<th>
x.4
</th>
<th>
x.5
</th>
<th>
x.6
</th>
<th>
x.7
</th>
<th>
x.8
</th>
<th>
x.9
</th>
<th>
x.10
</th>
</tr>
<tr>
<th>
row.names
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
1
</th>
<td>
-3.639
</td>
<td>
0.418
</td>
<td>
-0.670
</td>
<td>
1.779
</td>
<td>
-0.168
</td>
<td>
1.627
</td>
<td>
-0.388
</td>
<td>
0.529
</td>
<td>
-0.874
</td>
<td>
-0.814
</td>
</tr>
<tr>
<th>
2
</th>
<td>
-3.327
</td>
<td>
0.496
</td>
<td>
-0.694
</td>
<td>
1.365
</td>
<td>
-0.265
</td>
<td>
1.933
</td>
<td>
-0.363
</td>
<td>
0.510
</td>
<td>
-0.621
</td>
<td>
-0.488
</td>
</tr>
<tr>
<th>
3
</th>
<td>
-2.120
</td>
<td>
0.894
</td>
<td>
-1.576
</td>
<td>
0.147
</td>
<td>
-0.707
</td>
<td>
1.559
</td>
<td>
-0.579
</td>
<td>
0.676
</td>
<td>
-0.809
</td>
<td>
-0.049
</td>
</tr>
<tr>
<th>
4
</th>
<td>
-2.287
</td>
<td>
1.809
</td>
<td>
-1.498
</td>
<td>
1.012
</td>
<td>
-1.053
</td>
<td>
1.060
</td>
<td>
-0.567
</td>
<td>
0.235
</td>
<td>
-0.091
</td>
<td>
-0.795
</td>
</tr>
<tr>
<th>
5
</th>
<td>
-2.598
</td>
<td>
1.938
</td>
<td>
-0.846
</td>
<td>
1.062
</td>
<td>
-1.633
</td>
<td>
0.764
</td>
<td>
0.394
</td>
<td>
-0.150
</td>
<td>
0.277
</td>
<td>
-0.396
</td>
</tr>
<tr>
<th>
…
</th>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
</tr>
<tr>
<th>
524
</th>
<td>
-4.065
</td>
<td>
2.876
</td>
<td>
-0.856
</td>
<td>
-0.221
</td>
<td>
-0.533
</td>
<td>
0.232
</td>
<td>
0.855
</td>
<td>
0.633
</td>
<td>
-1.452
</td>
<td>
0.272
</td>
</tr>
<tr>
<th>
525
</th>
<td>
-4.513
</td>
<td>
4.265
</td>
<td>
-1.477
</td>
<td>
-1.090
</td>
<td>
0.215
</td>
<td>
0.829
</td>
<td>
0.342
</td>
<td>
0.693
</td>
<td>
-0.601
</td>
<td>
-0.056
</td>
</tr>
<tr>
<th>
526
</th>
<td>
-4.651
</td>
<td>
4.246
</td>
<td>
-0.823
</td>
<td>
-0.831
</td>
<td>
0.666
</td>
<td>
0.546
</td>
<td>
-0.300
</td>
<td>
0.094
</td>
<td>
-1.343
</td>
<td>
0.185
</td>
</tr>
<tr>
<th>
527
</th>
<td>
-5.034
</td>
<td>
4.993
</td>
<td>
-1.633
</td>
<td>
-0.285
</td>
<td>
0.398
</td>
<td>
0.181
</td>
<td>
-0.211
</td>
<td>
-0.508
</td>
<td>
-0.283
</td>
<td>
0.304
</td>
</tr>
<tr>
<th>
528
</th>
<td>
-4.261
</td>
<td>
1.827
</td>
<td>
-0.482
</td>
<td>
-0.194
</td>
<td>
0.731
</td>
<td>
0.354
</td>
<td>
-0.478
</td>
<td>
0.050
</td>
<td>
-0.112
</td>
<td>
0.321
</td>
</tr>
</tbody>
</table>
<p>
528 rows × 10 columns
</p>
</div>
<pre class="python"><code>df_x_grouped = df_vowel.groupby(df_y)
size_class = len(df_x_grouped)
df_mean = df_x_grouped.mean()

print(df_mean)
print(df_vowel[df_y == 1].mean())</code></pre>
<pre><code>         x.1       x.2       x.3       x.4       x.5       x.6       x.7  \
y                                                                          
1  -3.359563  0.062937 -0.294062  1.203333  0.387479  1.221896  0.096375   
2  -2.708875  0.490604 -0.580229  0.813500  0.201938  1.063479 -0.190917   
3  -2.440250  0.774875 -0.798396  0.808667  0.042458  0.569250 -0.280062   
4  -2.226604  1.525833 -0.874437  0.422146 -0.371313  0.248354 -0.018958   
5  -2.756313  2.275958 -0.465729  0.225312 -1.036792  0.389792  0.236417   
6  -2.673542  1.758771 -0.474562  0.350562 -0.665854  0.417000  0.162333   
7  -3.243729  2.468354 -0.105063  0.396458 -0.980292  0.162312  0.019583   
8  -4.051333  3.233979 -0.173979  0.396583 -1.046021  0.195187  0.086667   
9  -3.876896  2.345021 -0.366833  0.317042 -0.394500  0.803375  0.025042   
10 -4.506146  2.688563 -0.284917  0.469563 -0.038792  0.638875  0.139167   
11 -2.990396  1.463875 -0.509812  0.371646 -0.380396  0.725042 -0.083396   

         x.8       x.9      x.10  
y                                 
1   0.037104 -0.624354 -0.161625  
2   0.373813 -0.515958  0.080604  
3   0.204958 -0.478271  0.181875  
4   0.107146 -0.326271 -0.053750  
5   0.424625 -0.200708 -0.280708  
6   0.229250 -0.207500  0.052708  
7   0.762292 -0.030271 -0.122396  
8   0.820771  0.104458  0.021229  
9   0.736146 -0.231833 -0.148104  
10  0.387562 -0.111021 -0.273354  
11  0.507667 -0.327500 -0.226729  
x.1    -3.359563
x.2     0.062938
x.3    -0.294063
x.4     1.203333
x.5     0.387479
x.6     1.221896
x.7     0.096375
x.8     0.037104
x.9    -0.624354
x.10   -0.161625
dtype: float64</code></pre>
<pre class="python"><code>def within_cov(df_grouped: pd.DataFrame,
               df_mean: pd.DataFrame)-&gt;np.ndarray:
    &quot;&quot;&quot;Compute the within-class covariance matrix&quot;&quot;&quot;
    size_class = len(df_grouped)
    dim = df_mean.columns.size
    mat_cov = np.zeros((dim, dim))
    n = 0
    
    for (c, df), (_, mean) in zip(df_grouped, df_mean.iterrows()):
        n += df.shape[0] # df is the grouped dataframe, n is the sum of the lengths of each group
        mat_centered = (df - mean).values # 48 X 10 matrix for each group
        mat_cov += mat_centered.T @ mat_centered # sum of the 10 X 10 within covariance matrix for each group.
    return mat_cov/(n-size_class)</code></pre>
<pre class="python"><code>mat_M = df_mean.values #K X p matrix of class centroids 𝐌
mat_W = within_cov(df_x_grouped, df_mean) # sum of the 10 X 10 within covariance matrix for each group.
#scipy.linalg.eigh() Find eigenvalues array and optionally eigenvectors array of matrix.
vec_D, mat_U = scipy.linalg.eigh(mat_W) #  mat_W = mat_U @ np.diag(vec_D) @ mat_U.T
print(np.allclose(mat_U @ np.diag(vec_D) @ mat_U.T, mat_W))</code></pre>
<pre><code>True</code></pre>
<pre class="python"><code>mat_W_inv_sqrt = (mat_U @ np.diag(np.sqrt(np.reciprocal(vec_D))) @
                  mat_U.T)
mat_Mstar = mat_M @ mat_W_inv_sqrt  # Compute 𝐌∗=𝐌𝐖^{−1/2} using the eigen-decomposition of  𝐖; (K X p)X(p X p)=K X p
vec_Mstar_mean = mat_Mstar.mean(axis=0) # axis=0 mean along the rows, 1 X p dataframe

mat_Mstar_centered = mat_Mstar - vec_Mstar_mean # K X p dataframe
#Compute 𝐁∗ , the covariance matrix of 𝐌∗ ( 𝐁  for between-class covariance)
mat_Bstar = mat_Mstar_centered.T @ mat_Mstar_centered/(mat_Mstar.shape[0]-1)
#and its eigen-decomposition 𝐁∗=𝐕∗𝐃_𝐵𝐕∗𝑇
vec_DBstar, mat_Vstar = scipy.linalg.eigh(mat_Bstar)
#The columns  𝑣∗𝑙  of  𝐕∗  in sequence from first to last define the coordinates of the optimal subspaces.
mat_V = mat_W_inv_sqrt @ mat_Vstar # (p X p)X(p X p)=p X p
mat_x_canonical = mat_x @ mat_V # 528 X p</code></pre>
<pre class="python"><code>fig48 = plt.figure(0, figsize=(10, 10))

ax11 = fig48.add_subplot(2, 2, 1)
ax12 = fig48.add_subplot(2, 2, 2)
ax21 = fig48.add_subplot(2, 2, 3)
ax22 = fig48.add_subplot(2, 2, 4)

for y in range(1, size_class+1):
    mat_x_grouped = mat_x_canonical[df_y == y]
    c = next(ax11._get_lines.prop_cycler)[&#39;color&#39;]
    ax11.plot(mat_x_grouped[:, -1], mat_x_grouped[:, -2], &#39;o&#39;, color=c)
    ax12.plot(mat_x_grouped[:, -2], mat_x_grouped[:, -3], &#39;o&#39;, color=c)
    ax21.plot(mat_x_grouped[:, -1], mat_x_grouped[:, -7], &#39;o&#39;, color=c)
    ax22.plot(mat_x_grouped[:, -9], mat_x_grouped[:, -10], &#39;o&#39;, color=c)
    
    vec_centroid = mat_x_grouped.mean(axis=0)
    ax11.plot(vec_centroid[-1], vec_centroid[-2], &#39;o&#39;, color=c,
              markersize=10, markeredgecolor=&#39;black&#39;, markeredgewidth=3)
    ax12.plot(vec_centroid[-2], vec_centroid[-3], &#39;o&#39;, color=c,
              markersize=10, markeredgecolor=&#39;black&#39;, markeredgewidth=3)
    ax21.plot(vec_centroid[-1], vec_centroid[-7], &#39;o&#39;, color=c,
              markersize=10, markeredgecolor=&#39;black&#39;, markeredgewidth=3)
    ax22.plot(vec_centroid[-9], vec_centroid[-10], &#39;o&#39;, color=c,
              markersize=10, markeredgecolor=&#39;black&#39;, markeredgewidth=3)
    
ax11.set_xlabel(&#39;Coordinate 1&#39;)
ax11.set_ylabel(&#39;Coordinate 2&#39;)
ax12.set_xlabel(&#39;Coordinate 2&#39;)
ax12.set_ylabel(&#39;Coordinate 3&#39;)
ax21.set_xlabel(&#39;Coordinate 1&#39;)
ax21.set_ylabel(&#39;Coordinate 7&#39;)
ax22.set_xlabel(&#39;Coordinate 9&#39;)
ax22.set_ylabel(&#39;Coordinate 10&#39;)
fig48.suptitle(&#39;Linear Discriminant Analysis&#39;)
plt.show()</code></pre>
<div class="figure">
<img src="merged_files/merged_63_0.png" alt="" />
<p class="caption">png</p>
</div>
<pre class="r"><code>%%R
reduced_rank_LDA = function( XTrain, yTrain, XTest, yTest ){
  #
  # R code to implement classification using Regularized Discriminant Analysis
  #
  # See the section with the same name as this function in Chapter 4 from the book ESLII
  #
  # Inputs:
  #   XTrain = training data frame
  #   yTrain = training labels of true classifications with indices 1 - K (where K is the number of classes)
  #   xTest = testing data frame
  #   yTest = testing response

  K = length(unique(yTrain)) # the number of classes (expect the classes to be labeled 1, 2, 3, ..., K-1, K 
  N = dim( XTrain )[1] # the number of samples
  p = dim( XTrain )[2] # the number of features
  
  # Compute the class dependent probabilities and class dependent centroids: 
  #
  PiK = matrix( data=0, nrow=K, ncol=1 )
  M = matrix( data=0, nrow=K, ncol=p )
  ScatterMatrices = list()
  for( ci in 1:K ){
    inds = yTrain == ci
    Nci = sum(inds)
    PiK[ci] = Nci/N
    M[ci,] = t( as.matrix( colMeans( XTrain[ inds, ] ) ) )
  }

  # Compute W:
  #
  W = cov( XTrain ) 

  # Compute M^* = M W^{-1/2} using the eigen-decomposition of W :
  #
  e = eigen(W)
  V = e$vectors # W = V %*% diag(e$values) %*% t(V)
  W_Minus_One_Half = V %*% diag( 1./sqrt(e$values) ) %*% t(V) 
  MStar = M %*% W_Minus_One_Half 

  # Compute B^* the covariance matrix of M^* and its eigen-decomposition:
  #
  BStar = cov( MStar )
  e = eigen(BStar)
  VStar = - e$vectors # note taking the negative to match the results in the book (results are independent of this)

  V = W_Minus_One_Half %*% VStar # the full projection matrix
  
  # Project the data into the invariant subspaces:
  #
  XTrainProjected = t( t(V) %*% t(XTrain) )
  XTestProjected = t( t(V) %*% t(XTest) )
  MProjected = t( t(V) %*% t(M) ) # the centroids projected

  # Classify the training/testing data for each possible projection dimension:
  # 
  TrainClassification = matrix( data=0, nrow=N, ncol=p ) # number of samples x number of projection dimensions 

  discriminant = matrix( data=0, nrow=1, ncol=K )
  for( si in 1:N ){ # for each sample
      for( pi in 1:p ){ # for each projection dimension 
      for( ci in 1:K ){ # for each class centroid 
          discriminant[ci] = 0.5 * sum( ( XTrainProjected[si,1:pi] - MProjected[ci,1:pi] )^2 ) - log( PiK[ci] )
      }
          TrainClassification[si,pi] = which.min( discriminant ) # return the index of the first minimum 
      }
  } 

  N = dim(XTest)[1]
  TestClassification = matrix( data=0, nrow=N, ncol=p ) # number of samples x number of projection dimensions 

  discriminant = matrix( data=0, nrow=1, ncol=K )
  for( si in 1:N ){ # for each sample 
      for( pi in 1:p ){ # for each projection dimension 
      for( ci in 1:K ){ # for each class centroid
          discriminant[ci] = 0.5 * sum( ( XTestProjected[si,1:pi] - MProjected[ci,1:pi] )^2 ) - log( PiK[ci] )
      }
          TestClassification[si,pi] = which.min( discriminant )
      }
  } 
  
  return( list(XTrainProjected,XTestProjected,MProjected,TrainClassification,TestClassification) )

}</code></pre>
<pre class="r"><code>%%R                                                        

out    = load_vowel_data( FALSE, FALSE )
XTrain = out[[1]]
yTrain = out[[2]] 
XTest  = out[[3]]
yTest  = out[[4]] 

out = reduced_rank_LDA( XTrain, yTrain, XTest, yTest )

K = length(unique(yTrain)) # the number of classes (expect the classes to be labeled 1, 2, 3, ..., K-1, K 

XTProj = out[[1]]
MSProj = out[[3]]


x_plot_coordinate = 1
y_plot_coordinate = 7

plot_colors = c(&quot;black&quot;,&quot;blue&quot;,&quot;brown&quot;,&quot;purple&quot;,&quot;orange&quot;,&quot;cyan&quot;,&quot;gray&quot;,&quot;yellow&quot;,&quot;black&quot;,&quot;red&quot;,&quot;green&quot;)
for( ci in 1:K ){
    inds = yTrain == ci
    if( ci==1 ){
        plot( XTProj[inds,x_plot_coordinate], XTProj[inds,y_plot_coordinate], xlab=&quot;Coordinate 1 for Training Data&quot;, 
             ylab=&quot;Coordinate 2 for Training Data&quot;, col=plot_colors[ci], type=&quot;p&quot;, xlim=range(XTProj[,x_plot_coordinate]), 
             ylim=range(XTProj[,y_plot_coordinate]))  
    lines( MSProj[ci,x_plot_coordinate], MSProj[ci,y_plot_coordinate], col=plot_colors[ci], type=&quot;p&quot;, cex=10, pch=&quot;.&quot; )
    }else{
        lines( XTProj[inds,x_plot_coordinate], XTProj[inds,y_plot_coordinate], xlab=&quot;Coordinate 1 for Training Data&quot;, 
              ylab=&quot;Coordinate 2 for Training Data&quot;, col=plot_colors[ci], type=&quot;p&quot; )
    lines( MSProj[ci,x_plot_coordinate], MSProj[ci,y_plot_coordinate], col=plot_colors[ci], type=&quot;p&quot;, cex=10, pch=&quot;.&quot; )    
    }
}</code></pre>
<div class="figure">
<img src="merged_files/merged_65_0.png" alt="" />
<p class="caption">png</p>
</div>
<p>FIGURE 4.8. Four projections onto pairs of canonical variates. Notice that as
the rank of the canonical variates increases, the centroids become less spread out.
In the lower right panel they appear to be superimposed, and the classes most
confused.</p>
</div>
<div id="maximize-between-class-variance-relative-to-within-class" class="section level3">
<h3>Maximize between-class variance relative to within-class</h3>
<p>Fisher’s approach to Classification with <span class="math inline">\(g\)</span> Populations with the same covariance matrix <span class="math inline">\(\mathbf\Sigma\)</span> which is full rank: A fixed linear combination of the <span class="math inline">\(n_i\)</span> observations of the multivariate random variable from <span class="math inline">\(\pi_i, i=1,2,\cdots,g\)</span> is <span class="math display">\[\underset{(n_i\times p)}{\mathbf X_i}\underset{(p\times 1)}{\mathbf a}=\begin{bmatrix}
\mathbf x_{i1}^T\\
\mathbf x_{i2}^T\\
\vdots\\
\mathbf x_{in_i}^T
\end{bmatrix}\mathbf a=\begin{bmatrix}
y_{i1}\\
y_{i2}\\
\vdots\\
y_{in_i}\\
\end{bmatrix}=Y_i\]</span> <span class="math inline">\(E(\mathbf X_i)=\boldsymbol\mu_i\)</span> and <span class="math inline">\(Cov(\mathbf X_i)=\mathbf\Sigma\)</span> then <span class="math inline">\(E(Y_i)=\mu_{iY}=\mathbf a^T\boldsymbol\mu_i\)</span> and <span class="math inline">\(Cov(Y_i)=\mathbf a^T\mathbf\Sigma\mathbf a\)</span> which is the same for all populations. The the overall mean of all populations is <span class="math display">\[\bar{\boldsymbol\mu}=\frac{1}{g}\sum_{i=1}^{g}\boldsymbol\mu_i\]</span> and the overall mean of all <span class="math inline">\(Y_i\)</span> is <span class="math display">\[\bar{\mu}_Y=\mathbf a^T\bar{\boldsymbol\mu}\]</span> Then for the squared separation <span class="math display">\[\text{separation}^2=\frac{\displaystyle\sum_{i=1}^{g}(\mu_{iY}-\bar{\mu}_Y)^2}{\sigma_Y^2}=\frac{\displaystyle\sum_{i=1}^{g}(\mathbf a^T\boldsymbol\mu_i-\mathbf a^T\bar{\boldsymbol\mu})^2}{\mathbf a^T\mathbf \Sigma\mathbf a}=\frac{\mathbf a^T\Biggl(\displaystyle\sum_{i=1}^{g}(\boldsymbol\mu_i-\bar{\boldsymbol\mu})(\boldsymbol\mu_i-\bar{\boldsymbol\mu})^T\Biggr)\mathbf a}{\mathbf a^T\mathbf \Sigma\mathbf a}\]</span> The squared separation measures the variability between the groups of <span class="math inline">\(Y\)</span>-values relative to the common variability within groups. We can then select <span class="math inline">\(\mathbf a\)</span> to maximize this ratio. For the sample mean vectors <span class="math display">\[\bar{\mathbf x}_i=\frac{1}{n_i}\sum_{j=1}^{n_i}\mathbf x_{ij}\]</span> the mean vector is <span class="math display">\[\bar{\mathbf x}=\frac{1}{g}\sum_{i=1}^{g}\bar{\mathbf x}_i\]</span> and <span class="math display">\[\mathbf S=\frac{\displaystyle\sum_{i=1}^{g}\sum_{j=1}^{n_i}(\mathbf x_{ij}-\bar{\mathbf x}_i)(\mathbf x_{ij}-\bar{\mathbf x}_i)^T}{n_1+n_2+\cdots+n_g-g}\]</span> is the estimate of <span class="math inline">\(\mathbf\Sigma\)</span> Then for the squared separation <span class="math display">\[\text{separation}^2=\frac{\mathbf a^T\Biggl(\displaystyle\sum_{i=1}^{g}(\bar{\mathbf x}_i-\bar{\mathbf x})(\bar{\mathbf x}_i-\bar{\mathbf x})^T\Biggr)\mathbf a}{\mathbf a^T\mathbf S\mathbf a}\]</span> Or <span class="math display">\[\frac{\text{separation}^2}{n_1+n_2+\cdots+n_g-g}=\frac{\mathbf a^T\Biggl(\displaystyle\sum_{i=1}^{g}(\bar{\mathbf x}_i-\bar{\mathbf x})(\bar{\mathbf x}_i-\bar{\mathbf x})^T\Biggr)\mathbf a}{\mathbf a^T\Biggl(\displaystyle\sum_{i=1}^{g}\sum_{j=1}^{n_i}(\mathbf x_{ij}-\bar{\mathbf x}_i)(\mathbf x_{ij}-\bar{\mathbf x}_i)^T\Biggr)\mathbf a}\]</span> Let <span class="math inline">\((\lambda_1, \mathbf e_1),(\lambda_2, \mathbf e_2),\cdots,(\lambda_s, \mathbf e_s), s\le \text{min}(g-1,p)\)</span> are the eigenvalue-eigenvector pairs of matrix <span class="math display">\[\Biggl(\displaystyle\sum_{i=1}^{g}\sum_{j=1}^{n_i}(\mathbf x_{ij}-\bar{\mathbf x}_i)(\mathbf x_{ij}-\bar{\mathbf x}_i)^T\Biggr)^{-1}\Biggl(\displaystyle\sum_{i=1}^{g}(\bar{\mathbf x}_i-\bar{\mathbf x})(\bar{\mathbf x}_i-\bar{\mathbf x})^T\Biggr)\]</span> Then the vector of coefficients <span class="math inline">\(\mathbf a\)</span> that maximizes the ratio <span class="math display">\[\frac{\mathbf a^T\Biggl(\displaystyle\sum_{i=1}^{g}(\bar{\mathbf x}_i-\bar{\mathbf x})(\bar{\mathbf x}_i-\bar{\mathbf x})^T\Biggr)\mathbf a}{\mathbf a^T\Biggl(\displaystyle\sum_{i=1}^{g}\sum_{j=1}^{n_i}(\mathbf x_{ij}-\bar{\mathbf x}_i)(\mathbf x_{ij}-\bar{\mathbf x}_i)^T\Biggr)\mathbf a}\]</span> is given by <span class="math inline">\(\mathbf e_1\)</span> and the linear combination <span class="math inline">\(\mathbf e_1^T\mathbf x\)</span> is called the <span style="color: red;"><strong>sample first discriminant</strong></span>, and the linear combination <span class="math inline">\(\mathbf e_k^T\mathbf x\)</span> is called the <span style="color: red;"><strong>sample <span class="math inline">\(k^{th}\)</span> discriminant</strong></span>, <span class="math inline">\(k\le s\)</span>. <span class="math inline">\(\mathbf W=\displaystyle\sum_{i=1}^{g}\sum_{j=1}^{n_i}(\mathbf x_{ij}-\bar{\mathbf x}_i)(\mathbf x_{ij}-\bar{\mathbf x}_i)^T\)</span> is the <span style="color: red;">sample <strong>Within</strong> groups matrix</span>, and <span class="math inline">\(\mathbf B=\displaystyle\sum_{i=1}^{g}(\bar{\mathbf x}_i-\bar{\mathbf x})(\bar{\mathbf x}_i-\bar{\mathbf x})^T\)</span> is the <span style="color: red;">sample <strong>Between</strong> groups matrix</span>. Let <span class="math display">\[\mathbf Y=\begin{bmatrix}
\mathbf e_1^T\mathbf x\\
\mathbf e_2^T\mathbf x\\
\vdots\\
\mathbf e_s^T\mathbf x\\
\end{bmatrix}(s\le \text{min}(g-1,p))\]</span> contains all the <strong>sample discriminants</strong>, then population <span class="math inline">\(\mathbf X_i\)</span> with <span class="math inline">\(n_i\)</span> observations have <strong>sample discriminants</strong> <span class="math display">\[\mathbf Y_i=\underset{(s\times p)}{\begin{bmatrix}
\mathbf e_1^T\\
\mathbf e_2^T\\
\vdots\\
\mathbf e_s^T\\
\end{bmatrix}}\underset{(p\times n_i)}{\mathbf X_i}=\underset{(s\times n_i)}{\begin{bmatrix}
\mathbf Y_1\\
\mathbf Y_2\\
\vdots\\
\mathbf Y_s\\
\end{bmatrix}}(s\le \text{min}(g-1,p))\]</span> and it has mean vector <span class="math display">\[\boldsymbol\mu_{iY}=\begin{bmatrix}
\mu_{iY_1}\\
\mu_{iY_2}\\
\vdots\\
\mu_{iY_s}\\
\end{bmatrix}=\begin{bmatrix}
\mathbf e_1^T\\
\mathbf e_2^T\\
\vdots\\
\mathbf e_s^T\\
\end{bmatrix}\boldsymbol\mu_i=\begin{bmatrix}
\mathbf e_1^T\boldsymbol\mu_i\\
\mathbf e_2^T\boldsymbol\mu_i\\
\vdots\\
\mathbf e_s^T\boldsymbol\mu_i\\
\end{bmatrix}\]</span>. The squared distance from column components of <span class="math inline">\(\mathbf Y_i\)</span> to its column mean <span class="math inline">\(\boldsymbol\mu_{iY}\)</span> is <span class="math display">\[(\mathbf y-\boldsymbol\mu_{iY})^T(\mathbf y-\boldsymbol\mu_{iY})=\sum_{j=1}^{s}[\mathbf a^T(\mathbf x-\boldsymbol\mu_i)]^2\]</span>, we can assigns <span class="math inline">\(\mathbf y\)</span> to population <span class="math inline">\(\pi_k\)</span> if the square of the distance from <span class="math inline">\(\mathbf y\)</span> to <span class="math inline">\(\boldsymbol\mu_{kY}\)</span> is smaller than the square of the distance from <span class="math inline">\(\mathbf y\)</span> to <span class="math inline">\(\boldsymbol\mu_{iY}\)</span> for <span class="math inline">\(i\ne k\)</span> <span class="math display">\[(\mathbf y-\boldsymbol\mu_{kY})^T(\mathbf y-\boldsymbol\mu_{kY})=\sum_{j=1}^{s}(y_j-\mu_{iY_j})^2=\sum_{j=1}^{s}[\mathbf a_j^T(\mathbf x-\boldsymbol\mu_k)]^2\le \sum_{j=1}^{s}[\mathbf a_j^T(\mathbf x-\boldsymbol\mu_i)]^2\]</span> If we only use the first <span class="math inline">\(r, r\le s\)</span> discriminants then <strong>Fisher’s Classification Procedure</strong> based on sample discriminants is allocate <span class="math inline">\(\mathbf x\)</span> to <span class="math inline">\(\pi_k\)</span> if <span class="math display">\[\sum_{j=1}^{r}(\hat{y}_j-\bar{y}_{kj})^2=\sum_{j=1}^{r}[\hat{\mathbf a}_j^T(\mathbf x-\bar{\mathbf x}_k)]^2\le \sum_{j=1}^{r}[\mathbf a_j^T(\mathbf x-\bar{\mathbf x}_i)]^2\]</span> where <span class="math inline">\(\hat{\mathbf a}_j\)</span> is the eigenvectors of <span class="math inline">\((p\times p)\)</span> matrix <span class="math display">\[\mathbf W^{-1}\mathbf B=\Biggl(\displaystyle\sum_{i=1}^{g}\sum_{j=1}^{n_i}(\mathbf x_{ij}-\bar{\mathbf x}_i)(\mathbf x_{ij}-\bar{\mathbf x}_i)^T\Biggr)^{-1}\Biggl(\displaystyle\sum_{i=1}^{g}(\bar{\mathbf x}_i-\bar{\mathbf x})(\bar{\mathbf x}_i-\bar{\mathbf x})^T\Biggr)\]</span></p>
<p>Fisher arrived at this decomposition via a different route, without referencing to Gaussian distributions at all. He posed the problem:</p>
<blockquote>
<p>Find the linear combination <span class="math inline">\(Z=a^TX\)</span> such that the between-class variance is maximized relative to the within-class variance.</p>
</blockquote>
<p>FIGURE 4.9 shows why this criterion makes sense. Although the direction joining the centroids separates the means as much as possible (i.e., maximizes the between-class variance), there is considerable overlap between the projected classes due to the nature of the covariances. By taking the covariance into account as well, a direction with minimum overlap can be found.</p>
<p>The between-class variance of Z is <span class="math inline">\(a^T\mathbf{B}a\)</span> and the within-class variance <span class="math inline">\(a^T\mathbf{W}a\)</span> and the <em>total</em> covariance <span class="math inline">\(\mathbf{T} = \mathbf{B} + \mathbf{W}\)</span>, ignoring class information. Then Fisher’s problem amounts to maximizing the <em>Rayleigh quotient</em>,</p>
<p><span class="math display">\[\begin{equation}
\max_a \frac{a^T\mathbf{B}a}{a^T\mathbf{W}a},
\end{equation}\]</span></p>
<p>or equivalently</p>
<p><span class="math display">\[\begin{equation}
\max_a a^T\mathbf{B}a \text{ subject to } a^T\mathbf{W}a = 1.
\end{equation}\]</span></p>
<p>This is a generalized eigenvalue problem, with <span class="math inline">\(a\)</span> given by the largest eigenvalue of <span class="math inline">\(\mathbf{W}^{-1}\mathbf{B}\)</span>.</p>
</div>
<div id="algorithm-for-the-generalized-eigenvalue-problem" class="section level3">
<h3>Algorithm for the generalized eigenvalue problem</h3>
<p>It is not hard to show (Exercise 4.1) the followings.
1. The optimal <span class="math inline">\(a_1\)</span> is identical to <span class="math inline">\(v_1\)</span> defined above.
2. Similarly one can find the next direction <span class="math inline">\(a_2\)</span>, orthogonal in <span class="math inline">\(\mathbf{W}\)</span> to <span class="math inline">\(a_1\)</span>, such that <span class="math inline">\(a_2^T\mathbf{B}a_2/a_2^T\mathbf{W}a_2\)</span> is maximized; the solution is <span class="math inline">\(a_2 = v_2\)</span>, and so on.</p>
<p>The <span class="math inline">\(a_l\)</span> are referred to as <em>discriminant coordinates</em> or <em>canonical variates</em>, since an alternative derivation of these results is through a canonical correlation analysis of the indicator response matrix <span class="math inline">\(\mathbf{Y}\)</span> on the predictor matrix <span class="math inline">\(\mathbf{X}\)</span> (<span class="math inline">\(\S\)</span> 12.5).</p>
</div>
<div id="summary" class="section level3">
<h3>Summary</h3>
<ul>
<li>Gaussian classification with common covariance leads to linear decision boundaries. Classficiation can be achieved by sphering the data w.r.t. <span class="math inline">\(\mathbf{W}\)</span>, and classifying to the closest centroid (modulo <span class="math inline">\(\log\pi_k\)</span>) in the sphered space.</li>
<li>Since only the relative distances to the centroids count, one can confine the data to the subspace spanned by the centroids in the sphered space.</li>
<li>This subspace can be further decomposed into successively optimal subspaces in terms of centroid separation. This decomposition is identical to the decomposition due to Fisher.</li>
</ul>
</div>
<div id="dimension-reduction-for-classification" class="section level3">
<h3>Dimension reduction for classification</h3>
<p>The reduced subspaces have been motivated as a data reduction (for viewing) tool. Can they also be used for classification, and what is the rationale?</p>
<p>Clearly they can, as in our original derivation; we simply limit the distance-to-centroid calculations to the chosen subspace. One can show that this is a Gaussian classfication rule with the additional restriction that the centroids of the Gaussian lie in a <span class="math inline">\(L\)</span>-dimensional subspace of <span class="math inline">\(\mathbb{R}^p\)</span>. Fitting such a model by maximum likelihood, and then constructing the posterior probabilities using Bayes’ theorem amounts to the classification rule described above (Exercise 4.8).</p>
<pre class="r"><code>%%R                                                        

out    = load_vowel_data( FALSE, FALSE )
XTrain = out[[1]]
yTrain = out[[2]] 
XTest  = out[[3]]
yTest  = out[[4]] 

out2 = reduced_rank_LDA( XTrain, yTrain, XTest, yTest )

K = length(unique(yTrain)) # the number of classes (expect the classes to be labeled 1, 2, 3, ..., K-1, K 
p = dim( XTrain )[2] # the number of features

TrainClassification = out2[[4]]
TestClassification = out2[[5]]


train_error_rate = matrix( data=0, nrow=1, ncol=p )
test_error_rate = matrix( data=0, nrow=1, ncol=p )

NTrain = dim(XTrain)[1]
NTest = dim(XTest)[1]

for( pi in 1:p ){
   train_error_rate[pi] = sum( TrainClassification[,pi] != yTrain )/NTrain
   test_error_rate[pi] = sum( TestClassification[,pi] != yTest )/NTest
}

plot( 1:p, train_error_rate, col=&quot;red&quot;, ylim=c( 0.3, 0.7 ), type=&quot;b&quot;, xlab=&quot;Dimension&quot;, ylab=&quot;Misclassification rate&quot; ) # range( c(train_error_rate,test_error_rate) )
lines( 1:p, test_error_rate, col=&quot;blue&quot;, type=&quot;b&quot; )</code></pre>
<div class="figure">
<img src="merged_files/merged_72_0.png" alt="" />
<p class="caption">png</p>
</div>
<p>FIGURE 4.10. Training and test error rates for the vowel data, as a function
of the dimension of the discriminant subspace. In this case the best error rate is
for dimension 2. Figure 4.11 shows the decision boundaries in this space.</p>
</div>
<div id="impact-of-prior-information-pi_k" class="section level3">
<h3>Impact of prior information <span class="math inline">\(\pi_k\)</span></h3>
<p>Gaussian classification dictates the <span class="math inline">\(\log\pi_k\)</span> correction factor in the distance calculation. The reason for this correction can be seen in FIGURE 4.9. The misclassfication rate is based on the area of overlap between the two densities. If the <span class="math inline">\(\pi_k\)</span> are equal, then the optimal cut-point is midway between the projected means. If not equal, moving the cut-point toward the <em>smaller</em> class will improve the error rate. One can derive the linear rule using LDA (or any other method), and then choose the cut-point to minimize misclassification error over the training data.</p>
<pre class="python"><code>&quot;&quot;&quot;FIGURE 4.11. The decision boundaries for the classifier based on the 2D
LDA solution.

As an example of the benefit of the reduced-rank restriction, we return to
the vowel data with 11 classes and 10 variables, and hence 10 possible
dimensions for the classifier.
&quot;&quot;&quot;
fig411 = plt.figure(1, figsize=(10, 5))

ax1 = fig411.add_subplot(1, 2, 1)
ax2 = fig411.add_subplot(1, 2, 2)

mat_centroid2d = []
for y in range(1, size_class+1):
    mat_x2d_grouped = mat_x_canonical[df_y == y][:, -1:-3:-1]
    c = next(ax1._get_lines.prop_cycler)[&#39;color&#39;]
    ax1.plot(mat_x2d_grouped[:, 0], mat_x2d_grouped[:, 1], &#39;o&#39;, color=c)
    
    vec_centroid2d = mat_x2d_grouped.mean(axis=0)
    mat_centroid2d.append(vec_centroid2d)
    ax1.plot(vec_centroid2d[0], vec_centroid2d[1], &#39;o&#39;, color=c,
             markersize=10, markeredgecolor=&#39;black&#39;, markeredgewidth=3)

mat_centroid2d = np.array(mat_centroid2d)
vec_classified = np.array([
    ((mat_centroid2d - vec_x)**2).sum(axis=1).argmin()
    for vec_x in mat_x_canonical[:, -1:-3:-1]
])
for y, centroid in enumerate(mat_centroid2d):
    mat_x2d_classified = mat_x_canonical[vec_classified == y][:, -1:-3:-1]
    c = next(ax2._get_lines.prop_cycler)[&#39;color&#39;]
    ax2.plot(mat_x2d_classified[:, 0], mat_x2d_classified[:, 1], &#39;o&#39;,
             color=c)
    ax2.plot(centroid[0], centroid[1], &#39;o&#39;, color=c,
             markersize=10, markeredgecolor=&#39;black&#39;, markeredgewidth=3)

ax1.set_xlabel(&#39;Canonical Coordinate 1&#39;)
ax1.set_ylabel(&#39;Canonical Coordinate 2&#39;)
ax1.set_title(&#39;Projected Data&#39;)
ax2.set_xlabel(&#39;Canonical Coordinate 1&#39;)
ax2.set_ylabel(&#39;Canonical Coordinate 2&#39;)
ax2.set_title(&#39;Classification in Reduced Subspace&#39;)
plt.show()</code></pre>
<div class="figure">
<img src="merged_files/merged_75_0.png" alt="" />
<p class="caption">png</p>
</div>
</div>
<div id="connection-between-fishers-reduced-rank-discriminant-analysis-and-regression-of-an-indicator-response-matrix" class="section level3">
<h3>Connection between Fisher’s reduced-rank discriminant analysis and regression of an indicator response matrix</h3>
<p>It turns out that LDA amounts to the regression followed by an eigen-decomposition of <span class="math inline">\(\hat{\mathbf{Y}}^T\mathbf{Y}\)</span>. In the case of two classes, there is a single discriminant variable that is identical up to a scalar multiplication to either of the columns of <span class="math inline">\(\hat{\mathbf{Y}}\)</span>. A related fact is that if one transforms the original predictors <span class="math inline">\(\mathbf{X}\)</span> to <span class="math inline">\(\hat{\mathbf{Y}}\)</span>, then LDA using <span class="math inline">\(\hat{\mathbf{Y}}\)</span> is identical to LDA in the original space (Exercise 4.3).</p>
<pre class="python"><code>df = pd.read_csv(&quot;../../data/South African Heart Disease.txt&quot;)
names = [&#39;sbp&#39;, &#39;tobacco&#39;, &#39;ldl&#39;, &#39;famhist&#39;, &#39;obesity&#39;, &#39;alcohol&#39;, &#39;age&#39;]
df[&#39;famhist&#39;] = pd.get_dummies(df[&#39;famhist&#39;])[&#39;Present&#39;]
X, y = df[names].values, df[[&#39;chd&#39;]].values
X = np.insert(X, 0, values=1, axis=1)
N, p = X.shape
X</code></pre>
<pre><code>array([[  1.  , 160.  ,  12.  , ...,  25.3 ,  97.2 ,  52.  ],
       [  1.  , 144.  ,   0.01, ...,  28.87,   2.06,  63.  ],
       [  1.  , 118.  ,   0.08, ...,  29.14,   3.81,  46.  ],
       ...,
       [  1.  , 108.  ,   3.  , ...,  20.09,  26.64,  55.  ],
       [  1.  , 118.  ,   5.4 , ...,  27.35,  23.97,  40.  ],
       [  1.  , 132.  ,   0.  , ...,  14.7 ,   0.  ,  46.  ]])</code></pre>
<pre class="python"><code>b_hat = np.zeros(shape=(p))
b_hat @ X[0]
df[[&#39;chd&#39;]].values[461, 0]</code></pre>
<pre><code>1</code></pre>
<pre class="python"><code>b_hat = np.zeros(shape=(p))
delta = np.inf
while delta &gt; 0.000000001:
    grad = np.zeros(shape=(1, p))
    hess = np.zeros(shape=(p, p))
    loss = 0.
    for i in range(N):
        bt_xi = b_hat @ X[i]
        ebx = np.exp(bt_xi)
        pxi = ebx/(1+ebx)
        grad += X[i] * (y[i, 0] - pxi)
        xi = np.reshape(X[i], (1, p))
        hess += -(xi.T @ xi) * pxi * (1 - pxi)
        loss += y[i][0] * bt_xi - np.log(1+np.exp(bt_xi))
    delta = np.squeeze(np.linalg.inv(hess) @ grad.T)
    b_hat -= delta
    delta = delta @ delta.T
    print(loss, b_hat)</code></pre>
<pre><code>-320.2339974186954 [-2.8943  0.005   0.0681  0.1436  0.7224 -0.0297 -0.0004  0.0267]
-245.79726362664837 [-3.8984  0.0057  0.0781  0.1792  0.9061 -0.0348  0.0004  0.0395]
-241.70241294159865 [-4.1209  0.0058  0.0795  0.1846  0.9381 -0.0346  0.0006  0.0424]
-241.58716354419536 [-4.1296  0.0058  0.0795  0.1848  0.9392 -0.0345  0.0006  0.0425]
-241.58701618263942 [-4.1296  0.0058  0.0795  0.1848  0.9392 -0.0345  0.0006  0.0425]</code></pre>
<pre class="python"><code>y_hat = np.zeros(shape=y.shape)
for i in range(N):
    e = np.exp(b_hat @ X[i])
    ps = [1 / (1 + e), e / (1 + e)]
    y_hat[i,0] = np.argmax(ps)
np.sum(y == y_hat)</code></pre>
<pre><code>337</code></pre>
</div>
</div>
</div>
<div id="s-4.4.-logistic-regression" class="section level1">
<h1><span class="math inline">\(\S\)</span> 4.4. Logistic Regression</h1>
<p>The logistic regression model arises from the desire to model the posterior probabilities of the <span class="math inline">\(K\)</span> classes via linear functions in <span class="math inline">\(x\)</span>, ensuring the natural properties of the probability: They sum to one and remain in <span class="math inline">\([0,1]\)</span>.</p>
<p>The model has the form</p>
<p><span class="math display">\[\begin{align}
\log\frac{\text{Pr}(G=1|X=x)}{\text{Pr}(G=K|X=x)} &amp;= \beta_{10} + \beta_1^Tx \\
\log\frac{\text{Pr}(G=2|X=x)}{\text{Pr}(G=K|X=x)} &amp;= \beta_{20} + \beta_2^Tx \\
&amp;\vdots \\
\log\frac{\text{Pr}(G=K-1|X=x)}{\text{Pr}(G=K|X=x)} &amp;= \beta_{(K-1)0} + \beta_{K-1}^Tx \\
\end{align}\]</span></p>
<p>The model is specified in terms of <span class="math inline">\(K-1\)</span> log-odds or logit transformations, reflecting the constraint that the probabilities sum to one. The choice of denominator (<span class="math inline">\(K\)</span> in this case) is arbitrary in that the estimates are equivalent under this choice.</p>
<div id="sum-to-one" class="section level3">
<h3>Sum to one</h3>
<p>To emphasize the dependence on the entire parameter set <span class="math inline">\(\theta = \left\lbrace \beta_{10}, \beta_1^T, \cdots, \beta_{(K-1)0}, \beta_{K-1}^T\right\rbrace\)</span>, we denote the probabilities</p>
<p><span class="math display">\[\begin{equation}
\text{Pr}(G=k|X=x) = p_k(x;\theta)
\end{equation}\]</span></p>
<p>A simple calculation shows that</p>
<p><span class="math display">\[\begin{align}
\text{Pr}(G=k|X=x) &amp;= \frac{\exp(\beta_{k0}+\beta_k^Tx)}{1+\sum_{l=1}^{K-1}\exp(\beta_{l0}+\beta_l^Tx)}, \text{ for } k=1,\cdots,K-1, \\
\text{Pr}(G=K|X=x) &amp;= \frac{1}{1+\sum_{l=1}^{K-1}\exp(\beta_{l0}+\beta_l^Tx)},
\end{align}\]</span></p>
<p>and they clearly sum to one.</p>
<p>When <span class="math inline">\(K=2\)</span>, this model is especially simple, since there is only a single linear function.</p>
</div>
</div>
<div id="s-4.4.1.-fitting-logistic-regression-models" class="section level1">
<h1><span class="math inline">\(\S\)</span> 4.4.1. Fitting Logistic Regression Models</h1>
<div id="maximum-likelihood" class="section level3">
<h3>Maximum likelihood</h3>
<p>Logistic regression models are usually fit by maximum likelihood, using the conditional likelihood of <span class="math inline">\(G\)</span> given <span class="math inline">\(X\)</span>. Since <span class="math inline">\(\text{Pr}(G|X)\)</span> completely specifies the conditional distribution, the <em>multinomial</em> distribution is appropriate.</p>
<p>The log-likelihood for <span class="math inline">\(N\)</span> observation is</p>
<p><span class="math display">\[\begin{equation}
l(\theta) = \sum_{i=1}^N \log p_{g_i}(x_i;\theta),
\end{equation}\]</span></p>
<p>where <span class="math inline">\(p_k(x_i;\theta) = \text{Pr}(G=k|X=x_i;\theta)\)</span></p>
</div>
<div id="maximum-likelihood-for-k2-case" class="section level3">
<h3>Maximum likelihood for <span class="math inline">\(K=2\)</span> case</h3>
<p>We discuss in detail the two-class case, sine the algorithms simplify considerably. It is convenient to code the two-class <span class="math inline">\(g_i\)</span> via a <span class="math inline">\(0/1\)</span> response <span class="math inline">\(y_i\)</span>, where <span class="math inline">\(y_i=1\)</span> when <span class="math inline">\(g_i=1\)</span>, and <span class="math inline">\(0\)</span> otherwise. Then we can let</p>
<p><span class="math display">\[\begin{align}
p_1(x;\theta) &amp;= p(x;\theta), \\
p_2(x;\theta) &amp;= 1- p(x;\theta). \\
\end{align}\]</span></p>
<p>The likelihood of the data is <span class="math display">\[L= \prod_{i=1}^{N}p_{g_i}(x_i)=\prod_{i=1}^{N}\text{Pr }(G=1|X=x_i)^{y_i}\text{Pr }(G=2|X=x_i)^{1-y_i}\]</span></p>
<p>The log-likelihood can be written</p>
<p><span class="math display">\[\begin{align}
l(\beta) &amp;= \sum_{i=1}^N \left\lbrace y_i\log p(x_i;\beta) + (1-y_i)\log(1-p(x_i;\beta)) \right\rbrace\\
&amp;= \sum_{i=1}^N \left\lbrace y_i\log \left(\frac{\exp(\beta_k^Tx_i)}{1+\sum_{l=1}^{K-1}\exp(\beta_l^Tx_i)}\right) + (1-y_i)\log\left(1-\left(\frac{\exp(\beta_k^Tx_i)}{1+\sum_{l=1}^{K-1}\exp(\beta_l^Tx_i)}\right)\right) \right\rbrace\\
&amp;= \sum_{i=1}^N \left\lbrace y_i\log \left(\frac{\exp(\beta^Tx_i)}{1+\exp(\beta^Tx_i)}\right) + (1-y_i)\log\left(1-\left(\frac{\exp(\beta^Tx_i)}{1+\exp(\beta^Tx_i)}\right)\right) \right\rbrace\\
&amp;= \sum_{i=1}^N \left\lbrace y_i\log \left(\frac{\exp(\beta^Tx_i)}{1+\exp(\beta^Tx_i)}\right) + (1-y_i)\log\left(\frac{1}{1+\exp(\beta^Tx_i)}\right) \right\rbrace\\
&amp;= \sum_{i=1}^N \left\lbrace y_i\left(\beta^Tx_i-\log(1+\exp(\beta^Tx_i))\right) - (1-y_i)\log\left(1+\exp(\beta^Tx_i)\right) \right\rbrace\\
&amp;= \sum_{i=1}^N \left\lbrace y_i\beta^Tx_i - \log(1+\exp(\beta^Tx_i)) \right\rbrace,
\end{align}\]</span></p>
<p>where <span class="math inline">\(\beta^T = \lbrace \beta_{10}, \beta_1^T \rbrace\)</span>, and we assume that the vector of inputs <span class="math inline">\(x_i\)</span> includes the constant term 1 to acommodate the intercept.</p>
<p>To maximize the log-likelihood, we set its derivatives to zero. These <em>score</em> equations are</p>
<p><span class="math display">\[\begin{equation}
\frac{\partial l(\beta)}{\partial\beta} = \sum_{i=1}^N x_i(y_i-p(x_i;\beta)) = 0,
\end{equation}\]</span></p>
<p>which are <span class="math inline">\(p+1\)</span> equations <em>nonlinear</em> in <span class="math inline">\(\beta\)</span>. Notice that since <span class="math inline">\(x_{i1} =1\)</span>, the first score equation specifies that</p>
<p><span class="math display">\[\begin{equation}
\sum_{i=1}^N y_i = \sum_{i=1}^N p(x_i;\beta),
\end{equation}\]</span></p>
<p>implying that the <em>expected</em> number of class ones matches the observed number (and hence also class twos).</p>
</div>
<div id="newton-raphson-algorithm" class="section level3">
<h3>Newton-Raphson algorithm</h3>
<p>To solve the score equation, we use the Newton-Raphson algorithm, which requires the second-derivative or Hessian matrix</p>
<p><span class="math display">\[\begin{align}
\frac{\partial^2 l(\beta)}{\partial\beta\partial\beta^T} &amp;= \frac{\partial \left(\sum_{i=1}^N x_i(y_i-p(x_i;\beta))\right)}{\partial\beta}\\
&amp;= -\sum_{i=1}^Nx_i\frac{\partial \left(p(x_i;\beta)\right)}{\partial\beta}\\
&amp;= -\sum_{i=1}^Nx_i\frac{\partial \left(\frac{\exp(\beta^Tx_i)}{1+\exp(\beta^Tx_i)}\right)}{\partial\beta}\\
&amp;= -\sum_{i=1}^Nx_i\left(\frac{x_i^T\exp(\beta^Tx_i)}{1+\exp(\beta^Tx_i)}-\frac{x_i^T\exp(\beta^Tx_i)\exp(\beta^Tx_i)}{\left(1+\exp(\beta^Tx_i)\right)^2}\right)\\
&amp;= -\sum_{i=1}^Nx_ix_i^T\left(\frac{\exp(\beta^Tx_i)(1+\exp(\beta^Tx_i))}{\left(1+\exp(\beta^Tx_i)\right)^2}-\frac{\exp(\beta^Tx_i)\exp(\beta^Tx_i)}{\left(1+\exp(\beta^Tx_i)\right)^2}\right)\\
&amp;= -\sum_{i=1}^Nx_ix_i^T\left(\frac{\exp(\beta^Tx_i)}{\left(1+\exp(\beta^Tx_i)\right)^2}\right)\\
&amp;= -\sum_{i=1}^N x_ix_i^T p(x_i;\beta)(1-p(x_i;\beta)).
\end{align}\]</span></p>
<p>Starting with <span class="math inline">\(\beta^{\text{old}}\)</span>, a single Newton update is</p>
<p><span class="math display">\[\begin{equation}
\beta^{\text{new}} = \beta^{\text{old}} - \left( \frac{\partial^2 l(\beta)}{\partial\beta\partial\beta^T} \right)^{-1} \frac{\partial l(\beta)}{\partial\beta},
\end{equation}\]</span></p>
<p>where the derivatives are evaluated at <span class="math inline">\(\beta^{\text{old}}\)</span>.</p>
</div>
<div id="the-same-thing-in-matrix-notation" class="section level3">
<h3>The same thing in matrix notation</h3>
<p>Let</p>
<ul>
<li><span class="math inline">\(\mathbf{y}\)</span> be the vector of <span class="math inline">\(y_i\)</span> values,</li>
<li><span class="math inline">\(\mathbf{X}\)</span> the <span class="math inline">\(N\times (p+1)\)</span> matrix of <span class="math inline">\(x_i\)</span> values,</li>
<li><span class="math inline">\(\mathbf{p}\)</span> the vector of fitted probabilities with <span class="math inline">\(i\)</span>th element <span class="math inline">\(p(x_i;\beta^{\text{old}})\)</span>, and</li>
<li><span class="math inline">\(\mathbf{W}\)</span> <span class="math inline">\(N\times N\)</span> diagonal matrix of weights with <span class="math inline">\(i\)</span>th diagonal elements <span class="math inline">\(p(x_i;\beta^{\text{old}})(1-p(x_i;\beta^{\text{old}}))\)</span>.</li>
</ul>
<p>Then we have</p>
<p><span class="math display">\[\begin{align}
\frac{\partial l(\beta)}{\partial\beta} &amp;= \mathbf{X}^T(\mathbf{y}-\mathbf{p}) \\
\frac{\partial^2l(\beta)}{\partial\beta\partial\beta^T} &amp;= -\mathbf{X}^T\mathbf{WX},
\end{align}\]</span></p>
<p>and thus the Newton step is</p>
<p><span class="math display">\[\begin{align}
\beta^{\text{new}} &amp;= \beta^{\text{old}} + (\mathbf{X}^T\mathbf{WX})^{-1}\mathbf{X}^T(\mathbf{y}-\mathbf{p}) \\
&amp;= (\mathbf{X}^T\mathbf{WX})^{-1} \mathbf{X}^T\mathbf{W}\left( \mathbf{X}\beta^{\text{old}} + \mathbf{W}^{-1}(\mathbf{y}-\mathbf{p}) \right) \\
&amp;= (\mathbf{X}^T\mathbf{WX})^{-1}\mathbf{X}^T\mathbf{W}\mathbf{z},
\end{align}\]</span></p>
<p>where we have re-expressed the Newton step as weighted least squares step, with the response</p>
<p><span class="math display">\[\begin{equation}
\mathbf{z} = \mathbf{X}\beta^{\text{old}} + \mathbf{W}^{-1}(\mathbf{y}-\mathbf{p}),
\end{equation}\]</span></p>
<p>sometimes known as the <em>adjusted response</em>.</p>
</div>
<div id="iteratively-reweighted-least-squares" class="section level3">
<h3>Iteratively reweighted least squares</h3>
<p>These equations for the Newton step get solved repeatedly, since at each iteration <span class="math inline">\(p\)</span> changes, and hence so does <span class="math inline">\(\mathbf{W}\)</span> and <span class="math inline">\(\mathbf{z}\)</span>. This algorithm is referred to as <em>iteratively reweighted least squares</em> or IRLS, since each iteration solves the weighted least squares problem:</p>
<p><span class="math display">\[\begin{equation}
\beta^{\text{new}} \leftarrow \arg\min_\beta (\mathbf{z}-\mathbf{X}\beta)^T\mathbf{W}(\mathbf{z}-\mathbf{X}\beta)
\end{equation}\]</span></p>
<p>It seems that <span class="math inline">\(\beta=0\)</span> is a good starting value, although convergence is never guaranteed. Typically the algorithm does converge, since the log-likelihood is concave, but overshooting can occur. In the rare cases that the log-likelihood decreases, step size halving will guarantee convergence.</p>
</div>
<div id="multiclass-case-with-kge-3" class="section level3">
<h3>Multiclass case with <span class="math inline">\(K\ge 3\)</span></h3>
<p>The Newton step can also be expressed as an IRLS, but with a <em>vector</em> of <span class="math inline">\(K-1\)</span> responses and a nondiagonal weight matrix per observation. (Exercise 4.4)</p>
<p>Alternatively coordinate-descent methods (<span class="math inline">\(\S\)</span> 3.8.6) can be used to maximize the log-likelihood efficiently.</p>
<p>The <span class="math inline">\(\textsf{R}\)</span> package <span class="math inline">\(\textsf{glmnet}\)</span> (Friedman et al., 2010) can fit very large logistic regression problems efficiently, both in <span class="math inline">\(N\)</span> and <span class="math inline">\(p\)</span>.</p>
</div>
<div id="goal-of-logistic-regression" class="section level3">
<h3>Goal of logistic regression</h3>
<p>Logistic regression models are used mostly as a data analysis and inference tool, where the goal is to understand the role of the input variables in <em>explaning</em> the outcome. Typically many models are fit in a search for a parsimonious model involving a subset of the variables, possibly with some interactions terms. The following example illustrates some of the issues involved.</p>
</div>
<div id="s-4.4.2.-example-south-african-heart-disease" class="section level2">
<h2><span class="math inline">\(\S\)</span> 4.4.2. Example: South African Heart Disease</h2>
<p>The data in FIGURE 4.12 are a subset of the Coronary Risk-Factor Study (CORIS) baseline survey, carried out in three rural areas of the Western Cape, South Africa (Rousseauw et al., 1983). These data are described in more detail in Hastie and Tibshirani (1987).</p>
<pre class="python"><code>&quot;&quot;&quot;FIGURE 4.12. A scatterplot matrix of the South African heart desease
data.

&quot;&quot;&quot;
import pandas as pd
import scipy
import scipy.linalg
import matplotlib.pyplot as plt</code></pre>
<pre class="python"><code>df_saheart = pd.read_csv(&#39;../../data/SAheart/SAheart.data.txt&#39;, index_col=0)
print(&#39;A pandas DataFrame of size {} x {} &#39;
      &#39;has been loaded.&#39;.format(*df_saheart.shape))
df_saheart</code></pre>
<pre><code>A pandas DataFrame of size 462 x 10 has been loaded.</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
sbp
</th>
<th>
tobacco
</th>
<th>
ldl
</th>
<th>
adiposity
</th>
<th>
famhist
</th>
<th>
typea
</th>
<th>
obesity
</th>
<th>
alcohol
</th>
<th>
age
</th>
<th>
chd
</th>
</tr>
<tr>
<th>
row.names
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
1
</th>
<td>
160
</td>
<td>
12.00
</td>
<td>
5.73
</td>
<td>
23.11
</td>
<td>
Present
</td>
<td>
49
</td>
<td>
25.30
</td>
<td>
97.20
</td>
<td>
52
</td>
<td>
1
</td>
</tr>
<tr>
<th>
2
</th>
<td>
144
</td>
<td>
0.01
</td>
<td>
4.41
</td>
<td>
28.61
</td>
<td>
Absent
</td>
<td>
55
</td>
<td>
28.87
</td>
<td>
2.06
</td>
<td>
63
</td>
<td>
1
</td>
</tr>
<tr>
<th>
3
</th>
<td>
118
</td>
<td>
0.08
</td>
<td>
3.48
</td>
<td>
32.28
</td>
<td>
Present
</td>
<td>
52
</td>
<td>
29.14
</td>
<td>
3.81
</td>
<td>
46
</td>
<td>
0
</td>
</tr>
<tr>
<th>
4
</th>
<td>
170
</td>
<td>
7.50
</td>
<td>
6.41
</td>
<td>
38.03
</td>
<td>
Present
</td>
<td>
51
</td>
<td>
31.99
</td>
<td>
24.26
</td>
<td>
58
</td>
<td>
1
</td>
</tr>
<tr>
<th>
5
</th>
<td>
134
</td>
<td>
13.60
</td>
<td>
3.50
</td>
<td>
27.78
</td>
<td>
Present
</td>
<td>
60
</td>
<td>
25.99
</td>
<td>
57.34
</td>
<td>
49
</td>
<td>
1
</td>
</tr>
<tr>
<th>
…
</th>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
</tr>
<tr>
<th>
459
</th>
<td>
214
</td>
<td>
0.40
</td>
<td>
5.98
</td>
<td>
31.72
</td>
<td>
Absent
</td>
<td>
64
</td>
<td>
28.45
</td>
<td>
0.00
</td>
<td>
58
</td>
<td>
0
</td>
</tr>
<tr>
<th>
460
</th>
<td>
182
</td>
<td>
4.20
</td>
<td>
4.41
</td>
<td>
32.10
</td>
<td>
Absent
</td>
<td>
52
</td>
<td>
28.61
</td>
<td>
18.72
</td>
<td>
52
</td>
<td>
1
</td>
</tr>
<tr>
<th>
461
</th>
<td>
108
</td>
<td>
3.00
</td>
<td>
1.59
</td>
<td>
15.23
</td>
<td>
Absent
</td>
<td>
40
</td>
<td>
20.09
</td>
<td>
26.64
</td>
<td>
55
</td>
<td>
0
</td>
</tr>
<tr>
<th>
462
</th>
<td>
118
</td>
<td>
5.40
</td>
<td>
11.61
</td>
<td>
30.79
</td>
<td>
Absent
</td>
<td>
64
</td>
<td>
27.35
</td>
<td>
23.97
</td>
<td>
40
</td>
<td>
0
</td>
</tr>
<tr>
<th>
463
</th>
<td>
132
</td>
<td>
0.00
</td>
<td>
4.82
</td>
<td>
33.41
</td>
<td>
Present
</td>
<td>
62
</td>
<td>
14.70
</td>
<td>
0.00
</td>
<td>
46
</td>
<td>
1
</td>
</tr>
</tbody>
</table>
<p>
462 rows × 10 columns
</p>
</div>
<pre class="python"><code>df_saheart.pop(&#39;adiposity&#39;)
df_saheart.pop(&#39;typea&#39;) 
df_y = df_saheart.pop(&#39;chd&#39;)
df_saheart[&#39;famhist&#39;] = df_saheart[&#39;famhist&#39;].map({&#39;Present&#39;: 1,
                                                   &#39;Absent&#39;: 0})
df_saheart.describe()</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
sbp
</th>
<th>
tobacco
</th>
<th>
ldl
</th>
<th>
famhist
</th>
<th>
obesity
</th>
<th>
alcohol
</th>
<th>
age
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
count
</th>
<td>
462.000000
</td>
<td>
462.000000
</td>
<td>
462.000000
</td>
<td>
462.000000
</td>
<td>
462.000000
</td>
<td>
462.000000
</td>
<td>
462.000000
</td>
</tr>
<tr>
<th>
mean
</th>
<td>
138.326840
</td>
<td>
3.635649
</td>
<td>
4.740325
</td>
<td>
0.415584
</td>
<td>
26.044113
</td>
<td>
17.044394
</td>
<td>
42.816017
</td>
</tr>
<tr>
<th>
std
</th>
<td>
20.496317
</td>
<td>
4.593024
</td>
<td>
2.070909
</td>
<td>
0.493357
</td>
<td>
4.213680
</td>
<td>
24.481059
</td>
<td>
14.608956
</td>
</tr>
<tr>
<th>
min
</th>
<td>
101.000000
</td>
<td>
0.000000
</td>
<td>
0.980000
</td>
<td>
0.000000
</td>
<td>
14.700000
</td>
<td>
0.000000
</td>
<td>
15.000000
</td>
</tr>
<tr>
<th>
25%
</th>
<td>
124.000000
</td>
<td>
0.052500
</td>
<td>
3.282500
</td>
<td>
0.000000
</td>
<td>
22.985000
</td>
<td>
0.510000
</td>
<td>
31.000000
</td>
</tr>
<tr>
<th>
50%
</th>
<td>
134.000000
</td>
<td>
2.000000
</td>
<td>
4.340000
</td>
<td>
0.000000
</td>
<td>
25.805000
</td>
<td>
7.510000
</td>
<td>
45.000000
</td>
</tr>
<tr>
<th>
75%
</th>
<td>
148.000000
</td>
<td>
5.500000
</td>
<td>
5.790000
</td>
<td>
1.000000
</td>
<td>
28.497500
</td>
<td>
23.892500
</td>
<td>
55.000000
</td>
</tr>
<tr>
<th>
max
</th>
<td>
218.000000
</td>
<td>
31.200000
</td>
<td>
15.330000
</td>
<td>
1.000000
</td>
<td>
46.580000
</td>
<td>
147.190000
</td>
<td>
64.000000
</td>
</tr>
</tbody>
</table>
</div>
<pre class="python"><code>colors = df_y.apply(lambda y: &#39;C3&#39; if y else &#39;C2&#39;)
pd.plotting.scatter_matrix(df_saheart, c=colors, figsize=(10, 10), marker = &#39;o&#39;)
plt.show()</code></pre>
<div class="figure">
<img src="merged_files/merged_94_0.png" alt="" />
<p class="caption">png</p>
</div>
<p>FIGURE 4.12. A scatterplot matrix of the South African heart disease data.
Each plot shows a pair of risk factors, and the cases and controls are color coded
(red is a case). The variable family history of heart disease (famhist) is binary
(yes or no).</p>
<p>We fit a logistic-regression model by maximum likelihood, giving the results shown in TABLE 4.2.</p>
<pre class="python"><code>&quot;&quot;&quot;TABLE 4.2. Results from a logistic regression fit to the South African
heart disease data
&quot;&quot;&quot;
mat_X = df_saheart.values
size_training, size_predictor = mat_X.shape
size_beta = size_predictor + 1

vec_y = df_y.values
mat_1X = np.hstack((np.ones((size_training, 1)), mat_X))


def fvec_p(mat_x:np.ndarray, vec_beta:np.ndarray)-&gt;np.ndarray:
    num = np.exp(mat_x@vec_beta)
    return num/(num+1)


def fdiag_W(mat_x:np.ndarray, vec_beta:np.ndarray)-&gt;np.ndarray:
    vec_p = fvec_p(mat_x, vec_beta)
    return vec_p*(1-vec_p)</code></pre>
<pre class="python"><code>vec_beta_old = np.zeros(size_beta)
vec_increment = np.ones(size_beta)
while (vec_increment**2).sum() &gt; 1e-8:
    vec_p = fvec_p(mat_1X, vec_beta_old)
    gradient = mat_1X.T @ (vec_y-vec_p)
    hessian = mat_1X.T @ np.diag(fdiag_W(mat_1X, vec_beta_old)) @ mat_1X
    vec_increment = scipy.linalg.solve(hessian, gradient)
    vec_beta_new = vec_beta_old + vec_increment
    vec_beta_old = vec_beta_new.copy()</code></pre>
<pre class="python"><code>import math
mat_xWx_inv = scipy.linalg.inv(mat_1X.T @ np.diag(fdiag_W(mat_1X, vec_beta_new)) @ mat_1X)
vec_y_fitted = []
for i in 1-fvec_p(mat_1X, vec_beta_new):
    value = 1 if i &gt; 0.5 else 0
    vec_y_fitted.append(value)
est_sigma2 = sum((vec_y-vec_y_fitted)**2)/(size_training-size_predictor-1)
table_stderr = [math.sqrt(mat_xWx_inv[j, j]*est_sigma2)
                for j in range(size_predictor+1)]
Z_Score = vec_beta_new/table_stderr
table_stderr</code></pre>
<pre><code>[0.8307079712754366,
 0.004852899700883536,
 0.022586133885393725,
 0.049464390940702214,
 0.1937428632282466,
 0.025076456406714104,
 0.003838312168845923,
 0.008766703653259727]</code></pre>
<pre class="python"><code>print(&#39;{0:&gt;15} {1:&gt;15} {2:&gt;15} {3:&gt;15}&#39;.format(&#39;Term&#39;, &#39;Coefficient&#39;,
                                               &#39;Std. Error&#39;, &#39;Z Score&#39;))
print(&#39;-&#39;*64)
table_term = [&#39;intercept&#39;] + list(df_saheart.columns)
for term, coeff, stderr, Z_Score in zip(table_term, vec_beta_new, table_stderr, Z_Score):
    print(&#39;{0:&gt;15} {1:&gt;15f} {2:&gt;15f} {3:&gt;15f}&#39;.format(term, coeff, stderr, Z_Score))</code></pre>
<pre><code>           Term     Coefficient      Std. Error         Z Score
----------------------------------------------------------------
      intercept       -4.129600        0.830708       -4.971181
            sbp        0.005761        0.004853        1.187059
        tobacco        0.079526        0.022586        3.520994
            ldl        0.184779        0.049464        3.735603
        famhist        0.939185        0.193743        4.847588
        obesity       -0.034543        0.025076       -1.377525
        alcohol        0.000607        0.003838        0.158013
            age        0.042541        0.008767        4.852589</code></pre>
<p>This summary includes <span class="math inline">\(Z\)</span> scores (<span class="math inline">\(\frac{\text{coefficients}}{\text{stderr}}\)</span>); a nonsignificant <span class="math inline">\(Z\)</span> score suggests a coefficient can be dropped from the model. Each of these correspond formally to a test of the null hypothesis that the coefficient in question is zero, while all the others are not (a.k.a. the Wald test).</p>
<div id="correlations-between-predictors" class="section level3">
<h3>Correlations between predictors</h3>
<p>There are some surprises in this table, which must be interpreted with caution. Systolic blood pressure (<span class="math inline">\(\textsf{sbp}\)</span>) is not significant! Nor is <span class="math inline">\(\textsf{obesity}\)</span>, and its sign is negative.</p>
<p>This confusion is a result of the correlation between the set of predictors. On their own, both <span class="math inline">\(\textsf{sbp}\)</span> and <span class="math inline">\(\textsf{obesity}\)</span> are significant, However, in the presense of many other correlated variables, thery are no longer needed (and can even get a negative sign).</p>
</div>
<div id="model-selection" class="section level3">
<h3>Model selection</h3>
<p>At this stage the analyst might do some model selection; find a subset of the variables that are sufficient for explaining their joint effect on the prevalence of the response (<span class="math inline">\(\textsf{chd}\)</span>).</p>
<p>One way to proceed by is to drop the least significant coefficient, and refit the model. This is done repeatedly until no further terms can be dropped. This gave the model shown in TABLE 4.3.</p>
<p>A better but time-consuming strategy is to refit each of the models with one variable removed, and then perform an <em>analysis of deviance</em> to decide which variable to exclude.</p>
<p>The residual deviance of a fitted model is</p>
<p><span class="math display">\[\begin{equation}
\text{residual deviance}(\beta) = -2\text{ log-likelihood}(\beta),
\end{equation}\]</span></p>
<p>and the deviance between two models is the difference of their residual deviance, as</p>
<p><span class="math display">\[\begin{equation}
\text{deviance}(\beta^{(1)}, \beta^{(2)}) = \text{residual deviance}(\beta^{(1)}) - \text{residual deviance}(\beta^{(2)}).
\end{equation}\]</span></p>
<p>This strategy gave the same final model as TABLE 4.3.</p>
<p>TABLE 4.3. Results from stepwise logistic regression fit to South African heart
disease data.</p>
<pre class="r"><code>%%R

SAheart = read.csv(&#39;../../data/SAheart/SAheart.data.txt&#39;, header=TRUE)

m_Largest = glm( chd ~ sbp + tobacco + ldl + famhist + obesity + alcohol + age, family=binomial(), data=SAheart )
m_Smallest = glm( chd ~ 1.0, family=binomial(), data=SAheart )

stepped_model = step( m_Largest, scope=list(lower=~+1, upper=~sbp + tobacco + ldl + famhist + obesity + alcohol + age), direction=&quot;backward&quot;, data=SAheart )

print( stepped_model )
summary(stepped_model)</code></pre>
<pre><code>Start:  AIC=499.17
chd ~ sbp + tobacco + ldl + famhist + obesity + alcohol + age

          Df Deviance    AIC
- alcohol  1   483.19 497.19
- sbp      1   484.22 498.22
- obesity  1   484.61 498.61
&lt;none&gt;         483.17 499.17
- tobacco  1   493.05 507.05
- ldl      1   494.09 508.09
- famhist  1   500.89 514.89
- age      1   501.51 515.51

Step:  AIC=497.19
chd ~ sbp + tobacco + ldl + famhist + obesity + age

          Df Deviance    AIC
- sbp      1   484.30 496.30
- obesity  1   484.63 496.63
&lt;none&gt;         483.19 497.19
- tobacco  1   493.62 505.62
- ldl      1   494.12 506.12
- famhist  1   501.07 513.07
- age      1   501.54 513.54

Step:  AIC=496.3
chd ~ tobacco + ldl + famhist + obesity + age

          Df Deviance    AIC
- obesity  1   485.44 495.44
&lt;none&gt;         484.30 496.30
- tobacco  1   494.99 504.99
- ldl      1   495.36 505.36
- famhist  1   501.93 511.93
- age      1   507.07 517.07

Step:  AIC=495.44
chd ~ tobacco + ldl + famhist + age

          Df Deviance    AIC
&lt;none&gt;         485.44 495.44
- ldl      1   495.39 503.39
- tobacco  1   496.18 504.18
- famhist  1   502.82 510.82
- age      1   507.24 515.24

Call:  glm(formula = chd ~ tobacco + ldl + famhist + age, family = binomial(), 
    data = SAheart)

Coefficients:
   (Intercept)         tobacco             ldl  famhistPresent             age  
      -4.20428         0.08070         0.16758         0.92412         0.04404  

Degrees of Freedom: 461 Total (i.e. Null);  457 Residual
Null Deviance:      596.1 
Residual Deviance: 485.4    AIC: 495.4

Call:
glm(formula = chd ~ tobacco + ldl + famhist + age, family = binomial(), 
    data = SAheart)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.7559  -0.8632  -0.4545   0.9457   2.4904  

Coefficients:
                Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)    -4.204275   0.498315  -8.437  &lt; 2e-16 ***
tobacco         0.080701   0.025514   3.163  0.00156 ** 
ldl             0.167584   0.054189   3.093  0.00198 ** 
famhistPresent  0.924117   0.223178   4.141 3.46e-05 ***
age             0.044042   0.009743   4.521 6.17e-06 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 596.11  on 461  degrees of freedom
Residual deviance: 485.44  on 457  degrees of freedom
AIC: 495.44

Number of Fisher Scoring iterations: 4</code></pre>
</div>
<div id="interpretation-of-a-coefficient" class="section level3">
<h3>Interpretation of a coefficient</h3>
<p>How does one interpret <span class="math inline">\(\textsf{tobacco}\)</span> coefficient of <span class="math inline">\(0.081\)</span> (<span class="math inline">\(\text{Std. Error} = 0.026\)</span>), for example?</p>
<p>An increase of <span class="math inline">\(1\text{kg}\)</span> in lifetime tobacco usage accounts for an increase in the odds of coronary heart disease of <span class="math inline">\(\exp(0.081)=1.084\)</span> or <span class="math inline">\(8.4\%\)</span>.</p>
<p>Incorporating the standard error we get an approximate <span class="math inline">\(95\%\)</span> confidence interval of</p>
<p><span class="math display">\[\begin{equation}
\exp(0.081 \pm 2\times 0.026) = (1.03, 1.14).
\end{equation}\]</span></p>
<p>We see that some of the variables have nonlinear effects, and when modeled appropriately, are not excluded from the model.</p>
</div>
</div>
<div id="s-4.4.3.-quadratic-approximations-and-inference" class="section level2">
<h2><span class="math inline">\(\S\)</span> 4.4.3. Quadratic Approximations and Inference</h2>
<p>The maximum-likelihood parameter estimates <span class="math inline">\(\hat\beta\)</span> satisfy a self-consistency relationship: they are the coefficients of a weighted least squares fit, where the responses are:
<span class="math display">\[z_i=x_i^T\hat\beta+\frac{(y_i-\hat p_i)}{\hat p_i(1-\hat p_i)}\]</span> and the weights are <span class="math inline">\(w_i = \hat p_i(1− \hat p_i)\)</span>, both depending on <span class="math inline">\(\hat\beta\)</span> itself. Apart from providing a convenient algorithm, this connection with least squares has more to offer:</p>
<ul>
<li><p>The weighted residual sum-of-squares is the familiar Pearson chi-square
statistic <span class="math display">\[\sum_{i=1}^{N}\frac{(y_i-\hat p_i)^2}{\hat p_i(1-\hat p_i)}\]</span> a quadratic approximation to the deviance.</p></li>
<li><p>Asymptotic likelihood theory says that if the model is correct, then
<span class="math inline">\(\hat\beta\)</span> is consistent (i.e., converges to the true <span class="math inline">\(\beta\)</span>).</p></li>
<li><p>A central limit theorem then shows that the distribution of <span class="math inline">\(\hat\beta\)</span> converges to <span class="math inline">\(N(\beta, (X^TWX)^{−1})\)</span>. This and other asymptotics can be derived
directly from the weighted least squares fit by mimicking normal theory inference.</p></li>
<li><p>Model building can be costly for logistic regression models, because
each model fitted requires iteration. Popular shortcuts are the Rao
score test which tests for inclusion of a term, and the Wald test which
can be used to test for exclusion of a term. Neither of these require
iterative fitting, and are based on the maximum-likelihood fit of the
current model. It turns out that both of these amount to adding
or dropping a term from the weighted least squares fit, using the
same weights. Such computations can be done efficiently, without
recomputing the entire weighted least squares fit.</p></li>
</ul>
</div>
<div id="s-4.4.4-l_1-regularized-logistic-regression" class="section level2">
<h2><span class="math inline">\(\S\)</span> 4.4.4 <span class="math inline">\(L_1\)</span> Regularized Logistic Regression</h2>
<p>The L1 penalty used in the lasso (Section 3.4.2) can be used for variable
selection and shrinkage with any linear regression model. For logistic regression,
we would maximize a penalized version of
<span class="math display">\[\begin{align}
l(\beta) &amp;= \sum_{i=1}^N \left\lbrace y_i\log p(x_i;\beta) + (1-y_i)\log(1-p(x_i;\beta)) \right\rbrace \\
&amp;= \sum_{i=1}^N \left\lbrace y_i\beta^Tx_i - \log(1+\exp(\beta^Tx)) \right\rbrace,\quad (4.20)
\end{align}
\]</span></p>
<p><span class="math display">\[\underset{\beta_0,\beta}{\text{max}}\left\lbrace\sum_{i=1}^N \left[y_i\left(\beta_0+\beta^Tx_i\right)- \log\left(1+\exp(\beta_0+\beta^Tx_i)\right)\right]-\lambda\sum_{j=1}^{p}|\beta_j|\right\rbrace, \quad (4.31)\]</span>
As with the lasso, we typically do not penalize the intercept term, and standardize the predictors for the penalty to be meaningful. Criterion (4.31) is concave, and a solution can be found using nonlinear programming methods
(Koh et al., 2007, for example). Alternatively, using the same quadratic
approximations that were used in the Newton algorithm in Section 4.4.1,
we can solve (4.31) by repeated application of a weighted lasso algorithm.
Interestingly, the score equations [see (4.24)] for the variables with non-zero
coefficients have the form <span class="math display">\[x_j^T(y-p)=\lambda\cdot\text{sign}(\beta_j), \quad(4.32)\]</span> which generalizes <span class="math display">\[x_j^T(y-X\beta)=\lambda\cdot\text{sign}(\beta_j), \quad(3.58)\]</span> in Section 3.4.4; the active variables are tied in their generalized correlation with the residuals.
Path algorithms such as LAR for lasso are more difficult, because the
coefficient profiles are piecewise smooth rather than linear. Nevertheless,
progress can be made using quadratic approximations.</p>
<pre class="r"><code>%%R
library(glmpath)
data(heart.data)
attach(heart.data)
fit &lt;- glmpath(x, y, family=binomial)
par(mfrow=c(1, 1))
plot(fit)</code></pre>
<pre><code>R[write to console]: The following objects are masked from heart.data (pos = 3):

    x, y


R[write to console]: The following objects are masked from heart.data (pos = 4):

    x, y</code></pre>
<div class="figure">
<img src="merged_files/merged_109_1.png" alt="" />
<p class="caption">png</p>
</div>
<p>FIGURE 4.13. L1 regularized logistic regression coefficients for the South
African heart disease data, plotted as a function of the L1 norm. The variables
were all standardized to have unit variance. The profiles are computed exactly at
each of the plotted points.</p>
<pre class="r"><code>%%R
library(glmpath)
data(heart.data)
attach(heart.data)
fit &lt;- glmpath(x, y, family=binomial)
plot(fit, xvar=&quot;lambda&quot;)
plot(fit, xvar=&quot;step&quot;)
plot(fit, xvar=&quot;step&quot;, xlimit=8)
plot(fit, type=&quot;aic&quot;)
plot(fit, type=&quot;bic&quot;)</code></pre>
<pre><code>R[write to console]: The following objects are masked from heart.data (pos = 3):

    x, y


R[write to console]: The following objects are masked from heart.data (pos = 4):

    x, y


R[write to console]: The following objects are masked from heart.data (pos = 5):

    x, y


R[write to console]: The following objects are masked from heart.data (pos = 6):

    x, y


R[write to console]: The following objects are masked from heart.data (pos = 7):

    x, y</code></pre>
<div class="figure">
<img src="merged_files/merged_111_1.png" alt="" />
<p class="caption">png</p>
</div>
<div class="figure">
<img src="merged_files/merged_111_2.png" alt="" />
<p class="caption">png</p>
</div>
<div class="figure">
<img src="merged_files/merged_111_3.png" alt="" />
<p class="caption">png</p>
</div>
<div class="figure">
<img src="merged_files/merged_111_4.png" alt="" />
<p class="caption">png</p>
</div>
<div class="figure">
<img src="merged_files/merged_111_5.png" alt="" />
<p class="caption">png</p>
</div>
</div>
<div id="s-4.4.5-logistic-regression-or-lda" class="section level2">
<h2><span class="math inline">\(\S\)</span> 4.4.5 Logistic Regression or LDA?</h2>
<div id="common-linearity" class="section level3">
<h3>Common linearity</h3>
<p>LDA has the log-posterior odds which are linear functions of x:</p>
<p><span class="math display">\[\begin{align}
\log\frac{\text{Pr}(G=k|X=x)}{\text{Pr}(G=K|X=x)} &amp;= \log\frac{\pi_k}{\pi_K} - \frac{1}{2}(\mu_k-\mu_K)^T\Sigma^{-1}(\mu_k-\mu_K) + x^T\Sigma^{-1}(\mu_k-\mu_K) \\
&amp;= \alpha_{k0} + \alpha_k^Tx,
\end{align}\]</span></p>
<p>and this linearity is a consequence of the Gaussian assumption for the class densities with a common covariance matrix.</p>
<p>The linear logistic model by construction has linear logits:</p>
<p><span class="math display">\[\begin{equation}
\log\frac{\text{Pr}(G=k|X=x)}{\text{Pr}(G=K|X=x)} = \beta_{k0} + \beta_k^Tx
\end{equation}\]</span></p>
<p>It seems that the models are the same. Although they have exactly the same
form, the difference lies in the way the linear coefficients are estimated. The
logistic regression model is more general, in that it makes less assumptions.
We can write the <strong>joint density</strong> of <span class="math inline">\(X\)</span> and <span class="math inline">\(G\)</span> as <span class="math display">\[\text{Pr}(X,G=k)=\text{Pr}(G=k|X)\text{Pr}(X), \quad(4.35)\]</span></p>
<p>where <span class="math inline">\(Pr(X)\)</span> denotes the marginal density of the inputs <span class="math inline">\(X\)</span>. For both LDA
and logistic regression, the first term on the right has the logit-linear
form:</p>
<p><span class="math display">\[\begin{equation}
\text{Pr}(G=k|X=x) = \frac{\exp(\beta_{k0}+\beta_k^Tx)}{1+\sum_{l=1}^{K-1}\exp(\beta_{l0}+\beta_l^Tx)} \quad(4.36)
\end{equation}\]</span></p>
<p>where we have again arbitrarily chosen the last class as the reference.</p>
</div>
<div id="different-assumptions" class="section level3">
<h3>Different assumptions</h3>
<p>Although they have exactly the same form, the difference lies in the way the linear coefficients are estimated: The logistic regression model is more general, in that it makes less assumptions.</p>
<p>Note the <em>joint density</em> of <span class="math inline">\(X\)</span> and <span class="math inline">\(G\)</span> as</p>
<p><span class="math display">\[\begin{equation}
\text{Pr}(X,G=k) = \text{Pr}(X)\text{Pr}(G=k|X),
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\text{Pr}(X)\)</span> denotes the marginal density of the inputs <span class="math inline">\(X\)</span>.</p>
<p>The logistic regression model leaves <span class="math inline">\(\text{Pr}(X)\)</span> as an arbitrary density function, and fits the parameters of <span class="math inline">\(\text{Pr}(G|X)\)</span> by maximizing the <em>conditional likelihood</em> – the multinomial likelihood with probabilities the <span class="math inline">\(\text{Pr}(G=k|X)\)</span>. Although <span class="math inline">\(\text{Pr}(X)\)</span> is totally ignored, we can think of this marginal density as being estimated in a fully nonparametric and unrestricted fashion, using empirical distribution function which places mass <span class="math inline">\(1/N\)</span> at each observation.</p>
<p>LDA fits the parameters by maximizing the full log-likelihood, based on the joint density</p>
<p><span class="math display">\[\begin{equation}
\text{Pr}(X,G=k) = \phi(X;\mu_k,\Sigma)\pi_k,
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\phi\)</span> is the Gaussian density function. Standard normal theory leads easily to the estimates <span class="math inline">\(\hat\mu_k\)</span>, <span class="math inline">\(\hat\Sigma\)</span>, and <span class="math inline">\(\hat\pi_k\)</span> given in Section 4.3. Since the linear parameters of the logistic form</p>
<p><span class="math display">\[\begin{equation}
\log\frac{\text{Pr}(G=k|X=x)}{\text{Pr}(G=K|X=x)} = \log\frac{\pi_k}{\pi_K} - \frac{1}{2}(\mu_k-\mu_K)^T\Sigma^{-1}(\mu_k-\mu_K) + x^T\Sigma^{-1}(\mu_k-\mu_K)
\end{equation}\]</span></p>
<p>are functions of the Gaussian parameters, we get their maximum-likelihood estimates by plugging in the corresponding estimates.</p>
</div>
<div id="role-of-the-marginal-density-textprx-in-lda" class="section level3">
<h3>Role of the marginal density <span class="math inline">\(\text{Pr}(X)\)</span> in LDA</h3>
<p>However, unlike in the conditional case, the marginal density <span class="math inline">\(\text{Pr}(X)\)</span> does play a role here. It is a mixture density</p>
<p><span class="math display">\[\begin{equation}
\text{Pr}(X) = \sum_{k=1}^K \pi_k\phi(X;\mu_k,\Sigma),
\end{equation}\]</span></p>
<p>which also involves the parameters. What role can this additional component or restriction play?</p>
<p>By relying on the additional model assumptions, we have more information about the parameters, and hence can estimate them more efficiently (lower variance). If in fact the true <span class="math inline">\(f_k(x)\)</span> are Gaussian, then in the worst case ignoring this marginal part of the likelihood constitutes a loss of efficiency of about <span class="math inline">\(30\%\)</span> asymptotically in the error rate (Efron, 1975). Paraphrasing: With <span class="math inline">\(30\%\)</span> more data, the conditional likelihood will do as well.</p>
<p>For example, observations far from the decision boundary (which are down-weighted by logistic regression) play a role in estimating the common covariance matrix. This is not a good news, because it also means that LDA is not robust to gross outliers.</p>
</div>
<div id="marginal-likelihood-as-a-regularizer" class="section level3">
<h3>Marginal likelihood as a regularizer</h3>
<p>The marginal likelihood can be thought of as a regularizer, requiring in some sense that class densities be <em>visible</em> from this marginal view.</p>
<p>For example, if the data in a two-class logistic regression model can be perfectly separated by a hyperplane, the maximum likelihood estimates of the parameters are undefined (i.e., infinite; see Exercise 4.5).</p>
<p>The LDA coefficients for the same data will be well defined, since the marginal likelihood will not permit these degeneracies.</p>
</div>
<div id="in-real-world" class="section level3">
<h3>In real world</h3>
<blockquote>
<p>It is generally felt that logistic regression is a safer and more robust bet than the LDA model, relying on fewer assumptions.</p>
</blockquote>
<p>In practice these assumptions are never correct, and often some of the components of <span class="math inline">\(X\)</span> are qualitative variables. It is our experience that the models give very similar results, even when LDA is used inappropriately, such as with qualitative predictors.</p>
<pre class="python"><code>import numpy as np
from numpy.linalg import norm
from sklearn.linear_model import LinearRegression
from matplotlib import pyplot as plt
%matplotlib inline</code></pre>
<pre class="python"><code># |Figure 4.14 shows 20 data points in two classes R^2. 
# |These data can be separated by a linear boundary.
points = np.array([[ 65, 410,   1], [331, 479,   1], [483, 483,   1], 
                   [318, 410,   1], [187, 348,   1], [192, 310,   1],
                   [223, 234,   1], [270, 241,   1], [339, 254,   1],
                   [303, 209,   1],
                   [168, 244,  -1], [222, 192,  -1], [168, 213,  -1],
                   [ 16, 214,  -1], [197, 180,  -1], [ 48, 120,  -1],
                   [ 91, 110,  -1], [192,  79,  -1], [219, 107,  -1],
                   [223,  66,  -1]])
X, Y = points[:,:2], points[:,2]
colors = np.array([&#39;red&#39;, &#39;&#39;, &#39;green&#39;])[Y+1]</code></pre>
<pre class="python"><code>def plot_separating_line(b0, b1, b2, c):
    fig, ax1 = plt.subplots(figsize=(3.35, 3.1), dpi=110)
    ax1.tick_params(bottom=False, left=False, labelleft=False, labelbottom=False)
    fig.subplots_adjust(left=0, right=1, top=1, bottom=0)
    ax1.scatter(points[:,0], points[:,1], color=colors, s=8)
    ax1.plot([-10, 510], [-(b0-10*b1)/b2, -(b0+510*b1)/b2], color=c, lw=1)
    ax1.set_xlim(-10, 510)
    ax1.set_ylim(50, 510)</code></pre>
<pre class="python"><code># |The orange line is the least squares solution to the problem, 
# |obtained by regressing the -1/1 response Y on X (with intercept); 
# |the line is given by {x: b0 + b1*x1 + b2*x2 = 0}.
reg = LinearRegression().fit(X, Y)
b0, b1, b2 = reg.intercept_, *reg.coef_
# |This least squares solution does not do a perfect job 
# |in separating the points, and makes one error.
plot_separating_line(b0, b1, b2, &#39;#E69E00&#39;)</code></pre>
<div class="figure">
<img src="merged_files/merged_120_0.png" alt="" />
<p class="caption">png</p>
</div>
<pre class="python"><code># The perceptron learning algorithm tries to find separating hyperplane
# by minimizing the distance of missclassified points to the decision bounary.
p = 0.000001
for k in range(1000):
    has_misclassifications = False
    for i in range(points.shape[0]):
        if np.sign(X[i] @ np.array([b1, b2]) + b0) != Y[i]:            
            # The algorithm in fact uses stochastic gradient descent to minimize this 
            # piecewise linear criterion. This means that rather that computing the 
            # sum of the dradient contributions of each observation followed by a step
            # in the negative gradient direction, a step is taken after each observation
            # is visited
            db = p * Y[i] * points[i] # delta b
            b0 += p* Y[i]
            b1 += db[0]
            b2 += db[1]
            has_misclassifications = True
    if not has_misclassifications:
        break
print(&#39;epochs&#39;, k)</code></pre>
<pre><code>epochs 1</code></pre>
<pre class="python"><code>plot_separating_line(b0, b1, b2, &#39;b&#39;)</code></pre>
<div class="figure">
<img src="merged_files/merged_122_0.png" alt="" />
<p class="caption">png</p>
</div>
<pre class="python"><code># CVXPY is a Python-embedded modeling language for convex optimization problems. 
# It automatically transforms the problem into standard form, calls a solver, and unpacks the results.
b0, b1, b2 = cp.Variable(), cp.Variable(), cp.Variable()
objective = cp.Minimize(0.5*(b1**2 + b2**2))
constraints = []
for i in range(X.shape[0]):
    constraints.append(Y[i]*(X[i,0]*b1+X[i,1]*b2+b0) &gt;= 1)
prob = cp.Problem(objective, constraints)
result = prob.solve()
b0, b1, b2 = b0.value, b1.value, b2.value</code></pre>
<pre class="python"><code>plot_separating_line(b0, b1, b2, &#39;cyan&#39;)</code></pre>
<div class="figure">
<img src="merged_files/merged_124_0.png" alt="" />
<p class="caption">png</p>
</div>
<p>FIGURE 4.14. A toy example with two classes separable by a hyperplane. The
orange line is the least squares solution, which misclassifies one of the training
points. Also shown are blue and cyan separating hyperplanes found by the perceptron
learning algorithm with different random starts.</p>
</div>
</div>
</div>
<div id="s-4.5.-separating-hyperplanes" class="section level1">
<h1><span class="math inline">\(\S\)</span> 4.5. Separating Hyperplanes</h1>
<p>We describe separating hyperplane classifiers, constructing linear decision boundaries that explicitly try to separate the data into different classes as well as possible. They provide the basis for support vector classifiers, discussed in Chapter 12.</p>
<p>FIGURE 4.14 shows 20 data points of two classes in <span class="math inline">\(\mathbb{R}^2\)</span>, which can be separated by a linear boundary but there are infinitely many possible <em>separating hyperplanes</em>.</p>
<p>The orange line is the least squares solution to the problem, obtained by regressing the <span class="math inline">\(-1/1\)</span> response <span class="math inline">\(Y\)</span> on <span class="math inline">\(X\)</span> with intercept; the line is given by</p>
<p><span class="math display">\[\begin{equation}
\left\lbrace x: \hat\beta_0 + \hat\beta_1x_1 + \hat\beta_2x_2=0 \right\rbrace.
\end{equation}\]</span></p>
<p>This least squares solution does not do a perfect job in separating the points, and makes one error. This is the same boundary found by LDA, in light of its equivalence with linear regression in the two-class case (<span class="math inline">\(\S\)</span> 4.3 and Exercise 4.2).</p>
<div id="perceptrons" class="section level3">
<h3>Perceptrons</h3>
<p>Classifiers such as above, that compute a linear combination of the input features and return the sign, were called <em>perceptrons</em> in the engineering literatur in the late 1950s (Rosenblatt, 1958). Perceptrons set he foundations for the neural network models of the 1980s and 1990s.</p>
</div>
<div id="review-on-vector-algebra" class="section level3">
<h3>Review on vector algebra</h3>
<p>FIGURE 4.15 depicts a hyperplane or <em>affine set</em> <span class="math inline">\(L\)</span> defined by the equation</p>
<p><span class="math display">\[\begin{equation}
f(x) = \beta_0 + \beta^T x = 0,
\end{equation}\]</span></p>
<p>since we are in <span class="math inline">\(\mathbb{R}^2\)</span> this is a line.</p>
<p>Here we list some properties:
1. For any two points <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> lying in <span class="math inline">\(L\)</span>,<br />
<span class="math display">\[\begin{equation}
\beta^T(x_1-x_2)=0,
\end{equation}\]</span>
and hence the unit vector <span class="math inline">\(\beta^* = \beta/\|\beta\|\)</span> is normal to the surface of <span class="math inline">\(L\)</span>.
2. For any point <span class="math inline">\(x_0\)</span> in <span class="math inline">\(L\)</span>,<br />
<span class="math display">\[\begin{equation}
\beta^Tx_0 = -\beta_0.
\end{equation}\]</span>
3. The signed distance of any point <span class="math inline">\(x\)</span> to <span class="math inline">\(L\)</span> is given by<br />
<span class="math display">\[\begin{align}
\beta^{*T}(x-x_0) &amp;= \frac{1}{\|\beta\|}(\beta^Tx+\beta_0) \\
&amp;= \frac{1}{\|f&#39;(x)\|}f(x).
\end{align}\]</span>
Hence <span class="math inline">\(f(x)\)</span> is proportional to the signed distance from <span class="math inline">\(x\)</span> to the hyperplane defined by <span class="math inline">\(f(x)=0\)</span>.</p>
<pre class="python"><code>from scipy import stats
a = (0, 1)
b = (0, 2)
c = (-1, 3)

slope, intercept, r_value, p_value, std_err = stats.linregress(a,b)

# (Note that although math.sqrt returns only one number, in general there is a positive and negative square root. 
# C corresponds to the positive square root, and D to the negative).


fig35 = plt.figure(figsize=(8, 8), dpi=110)
ax1 = fig35.add_subplot(1, 1, 1)
ax1.plot([c[0],3.0], [c[1], c[1]+c[0]/slope-3.0/slope], color=&#39;g&#39;, lw=1)
ax1.axis(&#39;off&#39;)
ax1.set_xlim(-2, 6)
ax1.set_ylim(-2, 6)
ax1.arrow(0, 0, dx=0.5, dy=1, color=&#39;r&#39;, head_width=0.1,width=.01, linewidth=0.5)
ax1.arrow(-2, 0, dx=6, dy=0, color=&#39;0&#39;, head_width=0.2,width=.01, linewidth=0.5)
ax1.arrow(0, -2, dx=0, dy=6, color=&#39;0&#39;, head_width=0.2,width=.01, linewidth=0.5)

ax1.plot([0.5,2], [1, 4], &#39;r--&#39;, lw=0.5)
ax1.arrow(1, 2, dx=2, dy=0.2, color=&#39;r&#39;, head_width=0.1,width=.01, linewidth=0.5)
ax1.plot([(2.2+3/slope)/(2+1/slope),3], [2*(2.2+3/slope)/(2+1/slope), 2.2], &#39;r--&#39;, lw=0.5)
ax1.text(1.1, 2.1, r&#39;$x_0$&#39;)
ax1.text(.2, .2, r&#39;$\beta^*$&#39;)
ax1.text(2.2, 1.5, r&#39;$\beta_0+\beta^Tx=0$&#39;)</code></pre>
<pre><code>Text(2.2, 1.5, &#39;$\\beta_0+\\beta^Tx=0$&#39;)</code></pre>
<div class="figure">
<img src="merged_files/merged_129_1.png" alt="" />
<p class="caption">png</p>
</div>
<p>FIGURE 4.15. The linear algebra of a hyperplane (affine set).</p>
</div>
<div id="s-4.5.1.-rosenblatts-perceptron-learning-algorithm" class="section level2">
<h2><span class="math inline">\(\S\)</span> 4.5.1. Rosenblatt’s Perceptron Learning Algorithm</h2>
<blockquote>
<p>The <em>perceptron learning algorithm</em> tries to find a separating hyperplane by minimizing the distance of misclassified points to the decision boundary.</p>
</blockquote>
<p>If a response <span class="math inline">\(y_i=1\)</span> is misclassified, then <span class="math inline">\(x_i^T\beta + \beta_0 \lt 0\)</span>, and the opposite for a misclassified response with <span class="math inline">\(y_i=-1\)</span>. The goal is to minimize</p>
<p><span class="math display">\[\begin{equation}
D(\beta,\beta_0) = -\sum_{i\in\mathcal{M}} y_i(x_i^T\beta + \beta_0),
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\mathcal{M}\)</span> indexes the set of misclassified points. The quantity is non-negative and proportional to the distance of the misclassified points to the decision boundary defined by <span class="math inline">\(\beta^Tx+\beta_0=0\)</span>.</p>
<p>Assuming <span class="math inline">\(\mathcal{M}\)</span> is fixed, the gradient is given by</p>
<p><span class="math display">\[\begin{align}
\partial\frac{D(\beta,\beta_0)}{\partial\beta} &amp;= -\sum_{i\in\mathcal{M}} y_ix_i, \\
\partial\frac{D(\beta,\beta_0)}{\partial\beta_0} &amp;= -\sum_{i\in\mathcal{M}} y_i.
\end{align}\]</span></p>
<div id="stochastic-gradient-descent" class="section level3">
<h3>Stochastic gradient descent</h3>
<p>The algorithm in face uses <em>stochastic gradient descent</em> to minimize this piecewise linear criterion. This means that rather than computing the sum of the gradient contributions of each observation followed by a step in the negative gradient direction, a step in taken after each observation is visited.</p>
<p>Hence the misclassified observations are visited in some sequence, and the parameters <span class="math inline">\(\beta\)</span> are updated via</p>
<p><span class="math display">\[\begin{equation}
\begin{pmatrix}\beta \\ \beta_0\end{pmatrix}
\leftarrow
\begin{pmatrix}\beta \\ \beta_0\end{pmatrix}
+
\rho \begin{pmatrix}y_ix_i \\ y_i\end{pmatrix},
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\rho\)</span> is the learning rate, which in this case can be taken to be <span class="math inline">\(1\)</span> without loss in generality.</p>
<p>If the classes are linearly separable, it can be shown that the algorithm converges to a separating hyperplane in a finite number of steps (Exercise 4.6). FIGURE 4.14 shows two solutions to a toy problem, each started at a different random guess.</p>
</div>
<div id="issues" class="section level3">
<h3>Issues</h3>
<p>There are a number of problems with this algorithm, summarized in Ripley (1996):</p>
<ul>
<li>When the data are separable, there are many solutions, and which one is found depends on the starting values.</li>
<li>The “finite” number of steps can be very large. The smaller the gap, the longer the time to find it.</li>
<li>When the data are not separable, the algorithm will not converge, and cycles develop. The cycles can be long and therefore hard to detect.</li>
</ul>
<p>The second problem can often be eliminated by seeking a hyperplane not in the orignal space, but in a much enlarged space obtained by creating many basis-function transformations of the original variables. This is analogous to driving the residuals in a ploynomial regression problem down to zero by making the degree sufficiently large.</p>
<p>Perfect separation cannot always be achieved: For example, if observations from two different classes share the same input. It may not be desirable either, since the resulting model is likely to be overfit and will not generalizes well.</p>
<p>A rather elegant solution to the first problem is to add additional constraints to the separating hyperplane.</p>
</div>
</div>
<div id="s-4.5.2.-optimal-separating-hyperplanes" class="section level2">
<h2><span class="math inline">\(\S\)</span> 4.5.2. Optimal Separating Hyperplanes</h2>
<p>The <em>optimal separating hyperplanes</em> separates the two classes and maximizes the distance to the closest point from either class (Vapnik, 1996). Not only does this provide a unique solution to the separating hyperplane problem, but by maximizing the margin between two classes on the training data, this leads to better classification performance on test data.</p>
<div id="formulation" class="section level3">
<h3>Formulation</h3>
<p>We need to generalize the perceptron criterion</p>
<p><span class="math display">\[\begin{equation}
D(\beta,\beta_0) = -\sum_{i\in\mathcal{M}} y_i(x_i^T\beta + \beta_0).
\end{equation}\]</span></p>
<p>Consider the optimization problem</p>
<p><span class="math display">\[\begin{equation}
\max_{\beta,\beta_0,\|\beta\|=1} M \\
\text{subject to } y_i(x_i^T\beta + \beta_0) \ge M, \text{ for } i = 1,\cdots,N.
\end{equation}\]</span></p>
<p>The set of conditions ensure that all the points are at least a signed distance <span class="math inline">\(M\)</span> from the decision boundary defined by <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\beta_0\)</span>, and we seek the largest such <span class="math inline">\(M\)</span> and associated parameters.</p>
<p>We can get rid of the <span class="math inline">\(\|\beta\| = 1\)</span> constraint by replacing the conditions with</p>
<p><span class="math display">\[\begin{equation}
\frac{1}{\|\beta\|} y_i(x_i^T\beta + \beta_0) \ge M, \\
\text{or equivalently} \\
y_i(x_i^T\beta + \beta_0) \ge M\|\beta\|,
\end{equation}\]</span></p>
<p>which redefines <span class="math inline">\(\beta_0\)</span>.</p>
<p>Since for any <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\beta_0\)</span> satisfying these inequalities, any positively scaled multiple satisfies them too, we can arbitrarily set</p>
<p><span class="math display">\[\begin{equation}
\|\beta\| = \frac{1}{M},
\end{equation}\]</span></p>
<p>which leads to the equivalent formulation as</p>
<p><span class="math display">\[\begin{equation}
\min_{\beta,\beta_0} \frac{1}{2}\|\beta\|^2 \\
\text{subject to } y_i(x_i^T\beta + \beta_0) \ge 1, \text{ for }i=1,\cdots,N.
\end{equation}\]</span></p>
<p>In light of <span class="math inline">\((4.40)\)</span>, the constraints define an empty slab or margin around the linear decision boundary of thickness <span class="math inline">\(1/\|\beta\|\)</span>. Hence we choose <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\beta_0\)</span> to maximize its thickness.</p>
</div>
<div id="convex-optimization" class="section level3">
<h3>Convex optimization</h3>
<p>This is a convex optimization problem (quadratic criterion with linear inequality constraints). The Lagrange (primal) function, to be minimized w.r.t. <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\beta_0\)</span>, is</p>
<p><span class="math display">\[\begin{equation}
L_P = \frac{1}{2}\|\beta\|^2 - \sum_{i=1}^N \alpha_i \left[ y_i(x_i^T\beta + \beta_0) -1 \right].
\end{equation}\]</span></p>
<p>Setting the derivatives to zero, we obtain:</p>
<p><span class="math display">\[\begin{align}
\beta &amp;= \sum_{i=1}^N \alpha_i y_i x_i, \\
0 &amp;= \sum_{i=1}^N \alpha_i y_i,
\end{align}\]</span></p>
<p>and substitutig these in <span class="math inline">\(L_P\)</span> we obtain the so-called Wolfe dual</p>
<p><span class="math display">\[\begin{align}
L_D &amp;= \sum_{i=1}^N \alpha_i - \frac{1}{2}\|\beta\|^2\\
&amp;= \sum_{i=1}^N \alpha_i - \frac{1}{2} \sum_{i=1}^N \sum_{k=1}^N \alpha_i \alpha_k y_i y_k x_i^T x_k \\
&amp;\text{subject to } \alpha_i \ge 0 \text{ and } \sum_{i=1}^N \alpha_i y_i = 0.
\end{align}\]</span></p>
<p>The solution is obtained by maximizing <span class="math inline">\(L_D\)</span> in the positive orthant, a simpler convex optimization problem, for which standard software can be used. In addition the solution must satisfy the Karush-Kuhn-Tucker (KKT) conditions, which includes the above three conditions and</p>
<p><span class="math display">\[\begin{equation}
\alpha_i \left[ y_i (x_i^T\beta + \beta_0)-1 \right] = 0, \forall i.
\end{equation}\]</span></p>
</div>
<div id="implications-of-the-algorithm" class="section level3">
<h3>Implications of the algorithm</h3>
<p>From these we can see that</p>
<ul>
<li>if <span class="math inline">\(\alpha_i \gt 0\)</span>, then <span class="math inline">\(y_i(x_i^T\beta + \beta_0) = 1\)</span>, or in other words, <span class="math inline">\(x_i\)</span> is on the boundary of the slab;</li>
<li>if <span class="math inline">\(y_i(x_i^T\beta + \beta_0) \gt 1\)</span>, <span class="math inline">\(x_i\)</span> is not on the boundary of the slab, and <span class="math inline">\(\alpha_i = 0\)</span>.</li>
</ul>
<p>From the above condition of the primal Lagrange function</p>
<p><span class="math display">\[\begin{equation}
\beta = \sum_{i=1}^N \alpha_i y_i x_i,
\end{equation}\]</span></p>
<p>we see that the solution vector <span class="math inline">\(\beta\)</span> is defined in terms of a linear combination of the <em>support points</em> <span class="math inline">\(x_i\)</span> – those points defined to be on the boundary of slab via <span class="math inline">\(\alpha_i \gt 0\)</span>.</p>
<p>FIGURE 4.16 shows the optimal separating hyperplane for our toy example; these are three support points. Likewise, <span class="math inline">\(\beta_0\)</span> is obtained by solving the last KKT condition</p>
<p><span class="math display">\[\begin{equation}
\alpha_i \left[ y_i (x_i^T\beta + \beta_o) \right] = 0,
\end{equation}\]</span></p>
<p>for any of the support points.</p>
<pre class="python"><code>&quot;&quot;&quot;FIGURE 4.16. Optimal separating hyperplane

The same data aas in FIGURE 4.14. The shaded region delineates the maximum
margin separating the two classes. There are three support points
indicated, which lie on the boundary of the margin, and the optimal
separating hyperplane (blue line) bisects the slab. Included in the figure
is the boundary found using logistic regreesion (red line), which is very
close to the optimal separating hyperplane (see Section 12.3.3).

https://docs.scipy.org/doc/scipy/reference/optimize.html&quot;&quot;&quot;

b0, b1, b2 = cp.Variable(), cp.Variable(), cp.Variable()
objective = cp.Minimize(0.5*(b1**2 + b2**2))
constraints = []
for i in range(X.shape[0]):
    constraints.append(Y[i]*(X[i,0]*b1+X[i,1]*b2+b0) &gt;= 1)
prob = cp.Problem(objective, constraints)
result = prob.solve()
b0, b1, b2 = b0.value, b1.value, b2.value
plot_separating_line(b0, b1, b2, &#39;b&#39;)

# the distances of the points
distances = (b0+b1*X[:,0]+b2*X[:,1])/math.sqrt(b1**2+b2**2)
distances[np.argsort(abs(distances))]
nearpoints = X[np.argsort(abs(distances))][0:3,:]

# the slab margin
dist = abs(distances[np.argsort(abs(distances))][0])
slope = (-(b0-10*b1)/b2+(b0+510*b1)/b2)/(-10-510)
slope2 = -1.0/slope
margin = math.sqrt(dist**2+(dist/slope2)**2)

plt.scatter(nearpoints[:,0], nearpoints[:,1], color=&quot;b&quot;, s=15, marker = &quot;s&quot;)
plt.fill_between([-10, 510], [-(b0-10*b1)/b2+margin, -(b0+510*b1)/b2+margin], 
                 [-(b0-10*b1)/b2-margin, -(b0+510*b1)/b2-margin], color=&#39;yellow&#39;,
                alpha = 0.5)</code></pre>
<pre><code>&lt;matplotlib.collections.PolyCollection at 0x7f0e4eb78280&gt;</code></pre>
<div class="figure">
<img src="merged_files/merged_138_1.png" alt="" />
<p class="caption">png</p>
</div>
<p>FIGURE 4.16. The same data as in Figure 4.14. The shaded region delineates
the maximum margin separating the two classes. There are three support points
indicated, which lie on the boundary of the margin, and the optimal separating
hyperplane (blue line) bisects the slab. Included in the figure is the boundary found
using logistic regression (red line), which is very close to the optimal separating
hyperplane (see Section 12.3.3).</p>
</div>
<div id="classification" class="section level3">
<h3>Classification</h3>
<p>The optimal separating hyperplane produces a function <span class="math inline">\(\hat{f}(x) = x^T\hat\beta + \hat\beta_0\)</span> for classifying new observations:</p>
<p><span class="math display">\[\begin{equation}
\hat{G}(x) = \text{sign } \hat{f}(x).
\end{equation}\]</span></p>
<p>Although none of the training observations fall in the margin (by construction), this will not necessarily be the case for test observations. The intuition is that a large margin on the training data will lead to good separation on the test data.</p>
</div>
<div id="dependency-on-model-assumption" class="section level3">
<h3>Dependency on model assumption</h3>
<p>The description of the solution in terms of support points seems to suggest that the optimal hyperplane focuses more on the points that count, and is more robust to model misspecification.</p>
<p>The LDA solution, on the other hand, depends on all of the data, even points far away from the decision boundary. Note, however, that the identification of these support points required the use of all the data. Of course, if the classes are really Gaussian, then LDA is optimal, and separating hyperplane will pay a price for focusing on the (noisier) data at the boundaries if the classes.</p>
<p>Included in FIGURE 4.16 is the logistic regression solution to this problem, fit by maximum likelihood. Both solutions are similar in this case. When a separating hyperplane exists, logistic regression will always find it, since the log-likelihood can be driven to <span class="math inline">\(0\)</span> in this case (Exercise 4.5).</p>
<p>The logistic regression solution shares some other qualitative features with
the separating hyperplane solution. The coefficient vector is defined by a
weighted least squares fit of a zero-mean linearized response on the input
features, and the weights are larger for points near the decision boundary
than for those further away.</p>
</div>
<div id="when-the-data-are-not-separable" class="section level3">
<h3>When the data are not separable</h3>
<p>There will be no feasible solution to this problem, and an alternative formulation is needed.</p>
<p>Again one can enlarge the space using basis transformations, but this can lead to artificial separation through over-fitting. In Chapter 12 we discuss a more attractive alternative known as the <em>support vector machine</em>, which allows for overlap, but minimizes a measure of the extent of this overlap.</p>
</div>
</div>
</div>
<div id="exercises" class="section level1">
<h1>Exercises</h1>
<div id="ex.-4.1" class="section level2">
<h2>Ex. 4.1</h2>
<p>Show how to solve the generalized eigenvalue problem <span class="math inline">\(\text{max  }a^T\mathbf Ba\)</span> subject to <span class="math inline">\(a^TWa = 1\)</span> by transforming to a standard eigenvalue problem, where <span class="math inline">\(\mathbf B\)</span> is the between-class covariance matrix and <span class="math inline">\(\mathbf W\)</span> is the within-class covariance matrix.</p>
<p>Since this is an equality constraint, we can set it up in Lagrangian form and solve using lagrangian multipliers. The problem is of the form
<span class="math display">\[\text{max  }a^T\mathbf Ba\]</span>
<span class="math display">\[\text{subject to }a^TWa = 1\]</span>
Then, in Lagrangian form, this is <span class="math display">\[L(a,\lambda)=a^T\mathbf Ba+\lambda(a^T\mathbf Wa-1)\]</span> We can take partials with respect to <span class="math inline">\(a\)</span> and <span class="math inline">\(\lambda\)</span> so that <span class="math display">\[\frac{\partial(a,\lambda)}{\partial a}=2\mathbf Ba+2\lambda\mathbf Wa=0\]</span>
<span class="math display">\[\frac{\partial(a,\lambda)}{\partial \lambda}=a^T\mathbf Wa-1=0\]</span>
And so for the first equation, <span class="math display">\[-\mathbf Ba=\lambda\mathbf Wa\]</span>
<span class="math display">\[-\mathbf W^{-1}\mathbf Ba=\lambda a\]</span>
Notice that this is in eigen decomposition form and since we want to maximize the original quantity, we know that <span class="math inline">\(a\)</span> must be the first eigenvector and <span class="math inline">\(\lambda\)</span> the corresponding eigenvalue to the matrix <span class="math inline">\(-\mathbf W^{-1}\mathbf B\)</span>.</p>
</div>
<div id="ex.-4.2" class="section level2">
<h2>Ex. 4.2</h2>
<p>Suppose we have features <span class="math inline">\(x \in \mathbb R^p\)</span>, a two-class response, with class sizes <span class="math inline">\(N_1,N_2\)</span>, and the target coded as <span class="math inline">\(−N/N_1,N/N_2\)</span>.</p>
<ol style="list-style-type: lower-alpha">
<li><p>Show that the LDA rule classifies to class 2 if
<span class="math display">\[x^T\hat\Sigma^{-1}(\hat\mu_2-\hat\mu_1)&gt;\frac{1}{2}(\hat\mu_2+\hat\mu_1)^T\hat\Sigma^{-1}(\hat\mu_2-\hat\mu_1)-\log(N_2/N_1)\]</span>
and class 1 otherwise.</p></li>
<li><p>Consider minimization of the least squares criterion
<span class="math display">\[\sum_{i=1}^{N}(y_i-\beta_0-x_i^T\beta)^2\]</span>
Show that the solution <span class="math inline">\(\hat\beta\)</span> satisfies
<span class="math display">\[\left[(N-2)\hat\Sigma+N\hat\Sigma_{\mathcal B}\right]\beta=N(\hat\mu_2-\hat\mu_1)\]</span> (after simplification), where <span class="math display">\[\hat\Sigma_{\mathcal B}=\frac{N_1N_2}{N_2}(\hat\mu_2-\hat\mu_1)(\hat\mu_2-\hat\mu_1)^T\]</span></p></li>
<li><p>Hence show that <span class="math inline">\(\hat\Sigma_{\mathcal B}\beta\)</span> is in the direction <span class="math inline">\((\hat\mu_2-\hat\mu_1)\)</span> and thus <span class="math display">\[\hat\beta \propto\hat\Sigma^{-1}(\hat\mu_2-\hat\mu_1)\]</span> Therefore the least-squares regression coefficient is identical to the LDA coefficient, up to a scalar multiple.</p></li>
<li><p>Show that this result holds for any (distinct) coding of the two classes.</p></li>
<li><p>Find the solution <span class="math inline">\(\hat\beta_0\)</span> (up to the same scalar multiple as in (c), and hence the predicted value <span class="math inline">\(\hat f(x) = \hat\beta_0 + x^T \hat\beta\)</span>. Consider the following rule: classify to class 2 if <span class="math inline">\(\hat f(x) &gt; 0\)</span> and class 1 otherwise. Show this is not the same as the LDA rule unless the classes have equal numbers of observations.</p></li>
<li><p>Under zero-one classification loss, for each class <span class="math inline">\(\mathcal G_k\)</span> the Bayes’ discriminant functions <span class="math inline">\(\delta_k(x)\)</span> take the following form <span class="math display">\[\delta_k(x)=\log(p(x|\mathcal G_k))+\log(\pi_k)\]</span>
If our conditional density <span class="math inline">\(p(x|\mathcal G_k)\)</span> is given by a multidimensional normal then its function is given by</p></li>
</ol>
<p><span class="math display">\[\begin{equation}
p(x|\mathcal G_k)= f_k(x;\mu_k, \Sigma_k) = \frac{1}{(2\pi)^{p/2}|\Sigma_k|^{1/2}}\exp\left\lbrace -\frac{1}{2}(x-\mu_k)^T\Sigma_k^{-1}(x-\mu_k) \right\rbrace
\end{equation}\]</span></p>
<p>Linear discriminant analysis (LDA) arises in the special case when we assume that the classes have a common covariance matrix <span class="math inline">\(\Sigma_k=\Sigma,\forall k\)</span>.</p>
<p>In comparing two classes <span class="math inline">\(k\)</span> and <span class="math inline">\(l\)</span>, it is sufficient to look at the log-ratio, and we see that as an equation linear in <span class="math inline">\(x\)</span>,
<span class="math display">\[
\begin{align}
\log\frac{\text{Pr}(G=k|X=x)}{\text{Pr}(G=l|X=x)} &amp;= \log\frac{f_k(x;\mu_k, \Sigma_k)}{f_l(x;\mu_l, \Sigma_l)} + \log\frac{\pi_k}{\pi_l}= \delta_k(x) - \delta_l(x),
\end{align}
\]</span>
where <span class="math inline">\(\delta_k\)</span> is the <em>linear discriminant function</em></p>
<p><span class="math display">\[\begin{align}
\delta_k(x) &amp;= -\frac{p}{2}\log(2\pi)-\frac{1}{2}\log|\Sigma_k| -\frac{1}{2}(x-\mu_k)^T\Sigma_k^{-1}(x-\mu_k) + \log\pi_k\\
&amp;= -\frac{p}{2}\log(2\pi)-\frac{1}{2}\log|\Sigma_k| -\frac{1}{2}\left(x^T\Sigma_k^{-1}x-x^T\Sigma_k^{-1}\mu_k-\mu_k^T\Sigma_k^{-1}x+\mu_k^T\Sigma_k^{-1}\mu_k\right) + \log\pi_k\\
&amp;= -\frac{p}{2}\log(2\pi)-\frac{1}{2}\log|\Sigma_k| -\frac{1}{2}\left(x^T\Sigma_k^{-1}x-2x^T\Sigma_k^{-1}\mu_k+\mu_k^T\Sigma_k^{-1}\mu_k\right) + \log\pi_k\\
&amp;= -\frac{p}{2}\log(2\pi)-\frac{1}{2}\log|\Sigma_k| -\frac{1}{2}\left(x^T\Sigma_k^{-1}x-2x^T\Sigma_k^{-1}\mu_k+\mu_k^T\Sigma_k^{-1}\mu_k\right) + \log\left(\frac{N_k}{N}\right)\\
\end{align}\]</span></p>
<p>The decision boundary between each pair of classes <span class="math inline">\(k\)</span> and <span class="math inline">\(l\)</span> is described by a quadratic equation <span class="math inline">\(\left\lbrace x: \delta_k(x) = \delta_l(x) \right\rbrace\)</span>. Since linear discriminant analysis (LDA) corresponds to the case of equal covariance matrices our decision boundaries and if there are only 2 classes, if
<span class="math inline">\(\delta_2(x)&gt;\delta_1(x)\)</span> and we pick class 2 as the classification outcome (and class 1 otherwise).<br />
<span class="math display">\[x^T\Sigma^{-1}\mu_2-\frac{1}{2}\mu_2^T\Sigma^{-1}\mu_2 + \log\left(\frac{N_2}{N}\right) &gt; x^T\Sigma^{-1}\mu_1-\frac{1}{2}\mu_1^T\Sigma^{-1}\mu_1 + \log\left(\frac{N_1}{N}\right)\]</span></p>
<ol start="2" style="list-style-type: lower-alpha">
<li>To minimize the expression <span class="math display">\[\sum_{i=1}^{N}(y_i-\beta_0-x_i^T\beta)^2\]</span> over <span class="math inline">\((\beta_0,\beta)&#39;\)</span> we know that the solution <span class="math inline">\((\hat\beta_0,\hat\beta)&#39;\)</span> must satisfy the normal equations which in this case is given by <span class="math display">\[X^TX\begin{bmatrix}
\beta_0\\
\beta
\end{bmatrix}=X^Ty\]</span> since
<span class="math display">\[X^TX=\begin{bmatrix}
1&amp;\cdots&amp;1\\
x_1&amp;\cdots&amp;x_N\\
\end{bmatrix}\begin{bmatrix}
1&amp;x_1^T\\
\vdots&amp;\vdots\\
1&amp;x_N^T\\
\end{bmatrix}=\begin{bmatrix}
N&amp;\sum_{i=1}^{N}x_i^T\\
\sum_{i=1}^{N}x_i&amp;\sum_{i=1}^{N}x_ix_i^T\\
\end{bmatrix}\\
=\begin{bmatrix}
N&amp;N_1\mu_1^T+N_2\mu_2^T\\
N_1\mu_1+N_2\mu_2&amp;\sum_{i=1}^{N}x_ix_i^T\\
\end{bmatrix}\]</span>
Also if we pool all of the samples for this two class problem (<span class="math inline">\(K = 2\)</span>) together we can estimate the pooled covariance matrix <span class="math inline">\(\hat\Sigma\)</span> as
<span class="math display">\[\hat\Sigma=\frac{1}{N-K}\sum_{k=1}^{K}\sum_{i:\mathcal G_i=k}(x_i-\mu_k)(x_i-\mu_k)^T\]</span> When <span class="math inline">\(K=2\)</span> this is <span class="math display">\[\begin{align}\hat\Sigma &amp;=\frac{1}{N-2}\left[\sum_{i:\mathcal G_i=1}(x_i-\mu_1)(x_i-\mu_1)^T+\sum_{i:\mathcal G_i=2}(x_i-\mu_2)(x_i-\mu_2)^T\right]\\
&amp;=\frac{1}{N-2}\left[\sum_{i:\mathcal G_i=1}x_ix_i^T-N_1\mu_1\mu_1^T+\sum_{i:\mathcal G_i=2}x_ix_i^T-N_2\mu_2\mu_2^T\right]\\
\end{align}\]</span></li>
</ol>
<p>Then <span class="math display">\[X^TX=\begin{bmatrix}
N&amp;N_1\mu_1^T+N_2\mu_2^T\\
N_1\mu_1+N_2\mu_2&amp;\sum_{i=1}^{N}x_ix_i^T\\
\end{bmatrix}=\begin{bmatrix}
N&amp;N_1\mu_1^T+N_2\mu_2^T\\
N_1\mu_1+N_2\mu_2&amp;(N-2)\hat\Sigma+N_1\mu_1\mu_1^T+N_2\mu_2\mu_2^T\\
\end{bmatrix}\]</span></p>
<p>and if we code our response as <span class="math inline">\(-\frac{N}{N_1}\)</span> for the first class and <span class="math inline">\(\frac{N}{N_2}\)</span> class (where <span class="math inline">\(N = N_1 + N_2\)</span>), then <span class="math display">\[X^Ty=\begin{bmatrix}
1&amp;\cdots&amp;1\\
x_1&amp;\cdots&amp;x_N\\
\end{bmatrix}\begin{bmatrix}
-\frac{N}{N_1}\\
\vdots&amp;\vdots\\
\frac{N}{N_2}\\
\end{bmatrix}=\begin{bmatrix}
-N_1\frac{N}{N_1}+N_2\frac{N}{N_2}\\
\sum_{i=1}^{N_1}x_i(-\frac{N}{N_1})+\sum_{i=N_1+1}^{N}x_i(\frac{N}{N_2})\\
\end{bmatrix}=\begin{bmatrix}
0\\
-N\mu_1+N\mu_2\\
\end{bmatrix}\]</span>
then
<span class="math display">\[X^TX\begin{bmatrix}
\beta_0\\
\beta
\end{bmatrix}=\begin{bmatrix}
N&amp;N_1\mu_1^T+N_2\mu_2^T\\
N_1\mu_1+N_2\mu_2&amp;(N-2)\hat\Sigma+N_1\mu_1\mu_1^T+N_2\mu_2\mu_2^T\\
\end{bmatrix}\begin{bmatrix}
\beta_0\\
\beta
\end{bmatrix}=\begin{bmatrix}
0\\
-N\mu_1+N\mu_2\\
\end{bmatrix}\]</span></p>
<p>Then <span class="math display">\[N\beta_0+(N_1\mu_1^T+N_2\mu_2^T)\beta=0\]</span> then <span class="math display">\[\beta_0=-\frac{1}{N}(N_1\mu_1^T+N_2\mu_2^T)\beta\]</span></p>
<p>And <span class="math display">\[(N_1\mu_1+N_2\mu_2)\beta_0+\left((N-2)\hat\Sigma+N_1\mu_1\mu_1^T+N_2\mu_2\mu_2^T\right)\beta=N(\mu_2-\mu_1)\]</span>
then
<span class="math display">\[\begin{align}
(N_1\mu_1+N_2\mu_2)\left(-\frac{1}{N}(N_1\mu_1^T+N_2\mu_2^T)\beta\right)+\left((N-2)\hat\Sigma+N_1\mu_1\mu_1^T+N_2\mu_2\mu_2^T\right)\beta\\
=-\frac{1}{N}(N_1\mu_1+N_2\mu_2)(N_1\mu_1^T+N_2\mu_2^T)\beta+\left((N-2)\hat\Sigma+N_1\mu_1\mu_1^T+N_2\mu_2\mu_2^T\right)\beta\\
=\left((N-2)\hat\Sigma-\frac{N_1^2}{N}\mu_1\mu_1^T-\frac{2N_1N_2}{N}\mu_1\mu_2^T-\frac{N_2^2}{N}\mu_2\mu_2^T+N_1\mu_1\mu_1^T+N_2\mu_2\mu_2^T\right)\beta\\
=\left((N-2)\hat\Sigma+\left(-\frac{N_1^2}{N}+N_1\right)\mu_1\mu_1^T-\frac{2N_1N_2}{N}\mu_1\mu_2^T+\left(-\frac{N_2^2}{N}+N_2\right)\mu_2\mu_2^T\right)\beta\\
=\left((N-2)\hat\Sigma+\frac{N_1N_2}{N}\mu_1\mu_1^T-\frac{2N_1N_2}{N}\mu_1\mu_2^T+\frac{N_2N_1}{N}\mu_2\mu_2^T\right)\beta\\
=\left((N-2)\hat\Sigma+\frac{N_1N_2}{N}(\mu_1\mu_1^T-2\mu_1\mu_2^T+\mu_2\mu_2^T)\right)\beta\\
=\left((N-2)\hat\Sigma+\frac{N_1N_2}{N}(\mu_1-\mu_2)(\mu_1-\mu_2)^T\right)\beta\\
=\left((N-2)\hat\Sigma+\frac{N_1N_2}{N}(\mu_2-\mu_1)(\mu_2-\mu_1)^T\right)\beta\\
=N(\mu_2-\mu_1)
\end{align}\]</span></p>
<p>then the solution <span class="math inline">\(\hat\beta\)</span> satisfies
<span class="math display">\[\left[(N-2)\hat\Sigma+N\hat\Sigma_{\mathcal B}\right]\beta=N(\hat\mu_2-\hat\mu_1)\]</span> (after simplification), where <span class="math display">\[\hat\Sigma_{\mathcal B}=\frac{N_1N_2}{N_2}(\hat\mu_2-\hat\mu_1)(\hat\mu_2-\hat\mu_1)^T\]</span></p>
<ol start="3" style="list-style-type: lower-alpha">
<li>Since that <span class="math display">\[\hat\Sigma_{\mathcal B}\beta=\frac{N_1N_2}{N_2}(\hat\mu_2-\hat\mu_1)(\hat\mu_2-\hat\mu_1)^T\beta\]</span> and <span class="math inline">\((\hat\mu_2-\hat\mu_1)^T\beta\)</span> is scalar. Then the direction is the same with <span class="math inline">\((\hat\mu_2-\hat\mu_1)\)</span> and thus <span class="math display">\[\hat\beta \propto\hat\Sigma^{-1}(\hat\mu_2-\hat\mu_1)\]</span> Therefore the least-squares regression coefficient is identical to the LDA coefficient, up to a scalar multiple.</li>
</ol>
</div>
<div id="ex.-4.3" class="section level2">
<h2>Ex. 4.3</h2>
<p>Suppose we transform the original predictors <span class="math inline">\(\mathbf X\)</span> to <span class="math inline">\(\hat{\mathbf Y}\)</span> via linear regression. In detail, let <span class="math inline">\(\hat{\mathbf Y} = \mathbf X(\mathbf X^T\mathbf X)^{−1}\mathbf X^T\mathbf Y = \mathbf X\hat{\mathbf B}\)</span>, where <span class="math inline">\(\mathbf Y\)</span> is the indicator response matrix. Similarly for any input <span class="math inline">\(x \in \mathbb R^p\)</span>, we get a transformed vector <span class="math inline">\(\hat y = \hat{\mathbf B}^Tx \in \mathbb R^K\)</span>. Show that LDA using <span class="math inline">\(\hat{\mathbf Y}\)</span> is identical to LDA in the original space.</p>
</div>
<div id="ex.-4.4-multidimensional-logistic-regression" class="section level2">
<h2>Ex. 4.4 (multidimensional logistic regression)</h2>
<p>Consider the multilogit model with <span class="math inline">\(K\)</span> classes (4.17). Let <span class="math inline">\(\beta\)</span> be the
<span class="math inline">\((p + 1)(K − 1)\)</span>-vector consisting of all the coefficients. Define a suitably
enlarged version of the input vector x to accommodate this vectorized coefficient matrix. Derive the Newton-Raphson algorithm for maximizing the
multinomial log-likelihood, and describe how you would implement this algorithm.</p>
<p>In the case of <span class="math inline">\(K &gt; 2\)</span> classes, in the same way as discussed in the section on fitting a logistic regression model, for each sample point with a given measurement vector <span class="math inline">\(x\)</span> (here we are implicitly considering one of the samples from our training set) we will associate a position coded response vector variable <span class="math inline">\(y\)</span> of size <span class="math inline">\(K-1\)</span> where the <span class="math inline">\(l\)</span>-th component of <span class="math inline">\(y\)</span> is equal to one if this sample is drawn from the <span class="math inline">\(l\)</span>-th class and zero otherwise i.e. if the sample <span class="math inline">\(x_i\)</span> came from class <span class="math inline">\(l\)</span> when <span class="math inline">\(1 \le l \le K − 1\)</span> the <span class="math inline">\(l\)</span>th element of <span class="math inline">\(y_i\)</span> is one and all the other elements are zero. If <span class="math inline">\(x_i\)</span> is a sample from class <span class="math inline">\(K\)</span> then all elements of the vector <span class="math inline">\(y_i\)</span> are zero.</p>
<p>the likelihood that this particular measured vector <span class="math inline">\(x\)</span> is from its known
class can be written as <span class="math display">\[p_y(x) = Pr(G = 1|X = x)^{y_1}Pr(G = 2|X = x)^{y_2} \cdots Pr(G = K − 1|X = x)y^{K-1}
\times \left[1 − Pr(G = 1|X = x) − Pr(G = 2|X = x) − \cdots − Pr(G = K − 1|X = x)\right]^{1-\sum_{l=1}^{K-1}y_l}\]</span></p>
<p>Since this expression is for one data point the log-likelihood for an entire data set will be given by <span class="math display">\[l=\sum_{i=1}^{N}\log(p_{y_i}(x_i))\]</span> Using the Equation in the above expression we find <span class="math inline">\(\log(p_y(x))\)</span> for any given training pair <span class="math inline">\((x_i, y_i)\)</span> is given by <span class="math display">\[\begin{align}
\log(p_{\mathbf y}(\mathbf x))=y_1 \log(Pr(G = 1|X = x)) + y_2 \log(Pr(G = 2|X = x)) + \cdots + y_{K-1} \log(Pr(G = K − 1|X = x)) \\
+ (1 − y_1 − y_2 − \cdots − y_{K−1}) \log(Pr(G = K|X = x))\\
=\log(Pr(G = K|X = x))+y_1\log\left(\frac{Pr(G = 1|X = x)}{Pr(G = K|X = x)}\right)+y_2\log\left(\frac{Pr(G = 2|X = x)}{Pr(G = K|X = x)}\right)+\cdots\\
+y_{K-1}\log\left(\frac{Pr(G = K-1|X = x)}{Pr(G = K|X = x)}\right)\\
=\log(Pr(G = K|X = x))+y_1(\beta_{01}+\beta_1^Tx)+y_2(\beta_{02}+\beta_2^Tx)+\cdots\\
+y_{K-1}(\beta_{0(k-1)}+\beta_{K-1}^Tx)
\end{align}\]</span> The total log-likelihood is then given by summing the above expression over all data points <span class="math display">\[l(\theta)=\sum_{i=1}^{N}\left[\log(Pr(G = k|X = x_i))+\sum_{l=1}^{K-1}y_{il}\beta_l^Tx_i\right]\]</span> where</p>
<ul>
<li><p><span class="math inline">\(x_i\)</span> is the <span class="math inline">\(i\)</span>th vector sample <span class="math inline">\(1 \le i \le N\)</span> with a leading one and so is of length <span class="math inline">\(p + 1\)</span>.</p></li>
<li><p><span class="math inline">\(y_{il}\)</span> is the lth component of the ith response vector, i.e. if the sample <span class="math inline">\(x_i\)</span> came from class <span class="math inline">\(l\)</span> when <span class="math inline">\(1 \le l \le K − 1\)</span> the <span class="math inline">\(l\)</span>th element of <span class="math inline">\(y_i\)</span> is one and all the other elements are zero. If <span class="math inline">\(x_i\)</span> is a sample from class <span class="math inline">\(K\)</span> then all elements of the vector <span class="math inline">\(y_i\)</span> are zero.</p></li>
<li><p><span class="math inline">\(\beta_l\)</span> is a vector of coefficients for the <span class="math inline">\(l\)</span>th class <span class="math inline">\(1 \le l \le K − 1\)</span> with the leading <span class="math inline">\(\beta_{0l}\)</span> prepended and thus is of length <span class="math inline">\(p + 1\)</span>.</p></li>
<li><p><span class="math inline">\(Pr(G = k|X = x_i)\)</span> is the a posterior probability that <span class="math inline">\(x_i\)</span> comes from class <span class="math inline">\(G = K\)</span> and is given in terms of the parameters <span class="math inline">\(\{\beta_l\}_{l=1}^{K−1}\)</span> as <span class="math display">\[Pr(G = k|X = x_i)=\frac{1}{1+\sum_{l=1}^{K-1}\exp(\beta_l^Tx_i)}\]</span></p></li>
<li><p>The total parameter set that we need to solve for of <span class="math inline">\(\theta\)</span> can be thought of as the “stacked” vector of <span class="math inline">\(\beta\)</span>’s or <span class="math display">\[\theta=\begin{bmatrix}
\beta_1\\
\beta_2\\
\vdots\\
\beta_{K-2}\\
\beta_{K-1}\\
\end{bmatrix}\]</span> this is a vector of size <span class="math inline">\((K − 1)(p + 1)\)</span>. Since each sub vector <span class="math inline">\(\beta_l\)</span> has <span class="math inline">\(p + 1\)</span> components.</p></li>
</ul>
<p>Then <span class="math display">\[\begin{align}
l(\theta)&amp;=\sum_{i=1}^{N}\left[\log(Pr(G = k|X = x_i))+\sum_{l=1}^{K-1}y_{il}\beta_l^Tx_i\right]\\
&amp;=\sum_{i=1}^{N}\left[\log\left(\frac{1}{1+\sum_{l=1}^{K-1}\exp(\beta_l^Tx_i)}\right)+\sum_{l=1}^{K-1}y_{il}\beta_l^Tx_i\right]\\
&amp;=\sum_{i=1}^{N}\left[-\log\left(1+\sum_{l=1}^{K-1}\exp(\beta_l^Tx_i)\right)+\sum_{l=1}^{K-1}y_{il}\beta_l^Tx_i\right]\\
\end{align}\]</span> Once we have the objective function <span class="math inline">\(l(\theta)\)</span> defined we can develop an algorithm to maximize it. To develop a procedure for this maximization we will use the Newton-Raphson algorithm in terms of <span class="math inline">\(\theta\)</span> (which is a block column vector in <span class="math inline">\(\beta\)</span>) as <span class="math display">\[\theta^{new}=\theta^{old}-\left(\frac{\partial^2l(\theta)}{\partial\theta\partial\theta^T}\right)^{-1}\frac{\partial l(\theta)}{\partial \theta}\]</span> We need to evaluate the derivatives in the Newton-Raphson method. We will do this in
block form (which is the same way that <span class="math inline">\(\theta\)</span> is constructed). The expression <span class="math inline">\(\frac{\partial l(\theta)}{\partial \theta}\)</span> is a vector with blocks given by the derivatives of <span class="math inline">\(l(\theta)\)</span> with respect to <span class="math inline">\(\beta_l\)</span> or <span class="math display">\[\begin{align}
\frac{\partial l(\theta)}{\partial \beta_l}&amp;=\sum_{i=1}^{N}y_{il}x_i-\frac{\exp(\beta_l^Tx_i)}{1+\sum_{l&#39;=1}^{K-1}\exp(\beta_{l&#39;}^Tx_i)}x_i\\
&amp;=\sum_{i=1}^{N}\left(y_{il}-Pr(G=l|X=x_i)\right)x_i\\
\end{align}\]</span> The argument of the summation above are each column vectors of dimension <span class="math inline">\(p + 1\)</span> (since the vectors x_i are <span class="math inline">\(p + 1\)</span>) and we to create the full vector <span class="math inline">\(\partial l(\theta)/\partial \theta\)</span> we would stack the <span class="math inline">\(K − 1\)</span> vectors
above (one for each of <span class="math inline">\(l\)</span> in <span class="math inline">\(1 \le l \le K − 1\)</span>) on top of each other. That is we form the full gradient vector as
<span class="math display">\[\frac{\partial l(\theta)}{\partial \theta}=\begin{bmatrix}
    \partial l/\partial \beta_1\\
   \partial l/\partial \beta_2\\
    \vdots\\
   \partial l/\partial \beta_{K-1}\\
    \end{bmatrix}\]</span></p>
<p>If we write the above <span class="math inline">\(\beta_l\)</span> derivative as two terms as <span class="math display">\[\begin{align}
\frac{\partial l(\theta)}{\partial \beta_l}&amp;=\sum_{i=1}^{N}y_{il}x_i-\frac{\exp(\beta_l^Tx_i)}{1+\sum_{l&#39;=1}^{K-1}\exp(\beta_{l&#39;}^Tx_i)}x_i\\
&amp;=\sum_{i=1}^{N}y_{il}x_i-\sum_{i=1}^{N}Pr(G=l|X=x_i)x_i\\
&amp;=X^T\begin{bmatrix}
y_{1,l}\\
y_{2,l}\\
\vdots\\
y_{N,l}\\
\end{bmatrix}-X^T\begin{bmatrix}
Pr(G=l|X=x_1)\\
Pr(G=l|X=x_2)\\
\vdots\\
Pr(G=l|X=x_N)\\
\end{bmatrix}\\
&amp;=X^T\begin{bmatrix}
y_{1,l}-Pr(G=l|X=x_1)\\
y_{2,l}-Pr(G=l|X=x_2)\\
\vdots\\
y_{N,l}-Pr(G=l|X=x_N)\\
\end{bmatrix}\\
&amp;=X^T\begin{bmatrix}
\mathbf t_{l}-\mathbf P_l\\
\end{bmatrix}\\
\end{align}\]</span> and then
<span class="math display">\[\begin{align}
\frac{\partial l(\theta)}{\partial \theta}&amp;=\begin{bmatrix}
\partial l/\partial \beta_1\\
\partial l/\partial \beta_2\\
\vdots\\
\partial l/\partial \beta_{K-1}\\
\end{bmatrix}\\
&amp;=\begin{bmatrix}
X^T\mathbf t_{1}-\mathbf P_1\\
X^T\mathbf t_{2}-\mathbf P_2\\
\vdots\\
X^T\mathbf t_{K-1}-\mathbf P_{K-1}\\
\end{bmatrix}\\
\end{align}\]</span> which is a <span class="math inline">\((K − 1)(p+ 1)\times(K − 1)N\)</span> dimensioned matrix.</p>
<p>Next we have to evaluate the second derivative of <span class="math inline">\(l(\theta)\)</span>. As we did when we evaluated the first derivative we will evaluate this expression in block form.
<span class="math display">\[\begin{align}
\frac{\partial^2 l(\theta)}{\partial \beta_l\partial \beta_{l&#39;}^T}
&amp;=\frac{\partial\left\{\sum_{i=1}^{N}\left(y_{il}-Pr(G=l|X=x_i)\right)x_i\right\}}{\partial \beta_{l&#39;}^T}\\
&amp;=-\sum_{i=1}^{N}\frac{\partial Pr(G=l|X=x_i)}{\partial \beta_{l&#39;}^T}x_i
\end{align}\]</span> The case where <span class="math inline">\(l \ne l′\)</span> is slightly easier to compute and the derivative of <span class="math inline">\(Pr(G = l|X = x_i)\)</span> with respect to <span class="math inline">\(\beta_{l&#39;}^T\)</span> in that case is <span class="math display">\[\begin{align}
\frac{\partial^2 l(\theta)}{\partial \beta_l\partial \beta_{l&#39;}^T}
&amp;=\frac{\partial\left\{\sum_{i=1}^{N}\left(y_{il}-Pr(G=l|X=x_i)\right)x_i\right\}}{\partial \beta_{l&#39;}^T}\\
&amp;=-\sum_{i=1}^{N}\frac{\partial Pr(G=l|X=x_i)}{\partial \beta_{l&#39;}^T}x_i\\
&amp;=-\sum_{i=1}^{N}Pr(G=l|X=x_i)Pr(G=l&#39;|X=x_i)x_i^Tx_i\quad(l\ne l&#39;)\\
\end{align}\]</span></p>
<p>If <span class="math inline">\(l = l&#39;\)</span> we find the derivative of <span class="math inline">\(Pr(G = l|X = x_i)\)</span> with respect to <span class="math inline">\(\beta_{l&#39;}^T\)</span> given by <span class="math display">\[\begin{align}
\frac{\partial Pr(G=l|X=x_i)}{\partial\beta_{l&#39;}^T}&amp;=\frac{\partial}{\partial\beta_{l&#39;}^T}\left(\frac{\exp(\beta_{l&#39;}x_i)}{1+\sum_{l&#39;&#39;=1}^{K-1}\exp(\beta_{l&#39;&#39;}x_i)}\right)\\
&amp;=\frac{\exp(\beta_{l&#39;}x_i)}{1+\sum_{l&#39;&#39;=1}^{K-1}\exp(\beta_{l&#39;&#39;}x_i)}x_i-\frac{\exp(\beta_{l&#39;}x_i)}{\left(1+\sum_{l&#39;&#39;=1}^{K-1}\exp(\beta_{l&#39;&#39;}x_i)\right)^2}(\exp(\beta_{l&#39;}x_i))x_i\\
&amp;=Pr(G=l&#39;|X=x_i)x_i-Pr(G=l&#39;|X=x_i)^2x_i\\
&amp;=Pr(G=l&#39;|X=x_i)[1-Pr(G=l&#39;|X=x_i)]x_i\\
\end{align}\]</span> From this we have that the block diagonal second derivative terms are given by <span class="math display">\[\begin{align}
\frac{\partial^2 l(\theta)}{\partial \beta_l\partial \beta_{l&#39;}^T}
&amp;=\frac{\partial\left\{\sum_{i=1}^{N}\left(y_{il}-Pr(G=l|X=x_i)\right)x_i\right\}}{\partial \beta_{l&#39;}^T}\\
&amp;=-\sum_{i=1}^{N}\frac{\partial Pr(G=l|X=x_i)}{\partial \beta_{l&#39;}^T}x_i\\
&amp;=-\sum_{i=1}^{N}Pr(G=l&#39;|X=x_i)[1-Pr(G=l&#39;|X=x_i)]x_i^Tx_i\\
&amp;=-\sum_{i=1}^{N}Pr(G=l|X=x_i)[1-Pr(G=l|X=x_i)]x_i^Tx_i\\
\end{align}\]</span> which is a <span class="math inline">\((p + 1) \times (p + 1)\)</span> matrix.</p>
<p>To compute the full Hessian we will assemble the block pieces (computed above) and form the full matrix as <span class="math display">\[\begin{align}
\frac{\partial^2 l(\theta)}{\partial \theta\partial \theta^T}&amp;=\frac{\partial}{\partial \theta^T}\left(\frac{\partial l(\theta)}{\partial \theta}\right)\\
&amp;=\frac{\partial}{\partial \theta^T}\begin{bmatrix}
\partial l/\partial \beta_1\\
\partial l/\partial \beta_2\\
\vdots\\
\partial l/\partial \beta_{K-1}\\
\end{bmatrix}\\
&amp;=\begin{bmatrix}
\frac{\partial^2 l}{\partial\beta_1\partial\beta_1^T}&amp;\frac{\partial^2 l}{\partial\beta_1\partial\beta_2^T}&amp;\cdots&amp;\frac{\partial^2 l}{\partial\beta_1\partial\beta_{K-1}^T}\\
\frac{\partial^2 l}{\partial\beta_2\partial\beta_1^T}&amp;\frac{\partial^2 l}{\partial\beta_2\partial\beta_2^T}&amp;\cdots&amp;\frac{\partial^2 l}{\partial\beta_2\partial\beta_{K-1}^T}\\
\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\
\frac{\partial^2 l}{\partial\beta_{K-1}\partial\beta_1^T}&amp;\frac{\partial^2 l}{\partial\beta_{K-1}\partial\beta_2^T}&amp;\cdots&amp;\frac{\partial^2 l}{\partial\beta_{K-1}\partial\beta_{K-1}^T}\\
\end{bmatrix}\\
&amp;=\begin{bmatrix}
-\sum_{i=1}^{N}Pr(G=1|X=x_i)[1-Pr(G=1|X=x_i)]x_i^Tx_i&amp;-\sum_{i=1}^{N}Pr(G=1|X=x_i)[1-Pr(G=2|X=x_i)]x_i^Tx_i&amp;\cdots&amp;-\sum_{i=1}^{N}Pr(G=1|X=x_i)[1-Pr(G=K-1|X=x_i)]x_i^Tx_i\\
-\sum_{i=1}^{N}Pr(G=2|X=x_i)[1-Pr(G=1|X=x_i)]x_i^Tx_i&amp;-\sum_{i=1}^{N}Pr(G=2|X=x_i)[1-Pr(G=2|X=x_i)]x_i^Tx_i&amp;\cdots&amp;-\sum_{i=1}^{N}Pr(G=2|X=x_i)[1-Pr(G=K-1|X=x_i)]x_i^Tx_i\\
\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\
-\sum_{i=1}^{N}Pr(G=K-1|X=x_i)[1-Pr(G=1|X=x_i)]x_i^Tx_i&amp;-\sum_{i=1}^{N}Pr(G=K-1|X=x_i)[1-Pr(G=2|X=x_i)]x_i^Tx_i&amp;\cdots&amp;-\sum_{i=1}^{N}Pr(G=K-1|X=x_i)[1-Pr(G=K-1|X=x_i)]x_i^Tx_i\\
\end{bmatrix}\\
&amp;=-\begin{bmatrix}
X^T&amp;0&amp;\cdots&amp;0\\
0&amp;X^T&amp;\cdots&amp;0\\
\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\
0&amp;0&amp;\cdots&amp;X^T\\
\end{bmatrix}
\begin{bmatrix}
\sum_{i=1}^{N}Pr(G=1|X=x_i)[1-Pr(G=1|X=x_i)]&amp;\sum_{i=1}^{N}Pr(G=1|X=x_i)[1-Pr(G=2|X=x_i)]&amp;\cdots&amp;\sum_{i=1}^{N}Pr(G=1|X=x_i)[1-Pr(G=K-1|X=x_i)]\\
\sum_{i=1}^{N}Pr(G=2|X=x_i)[1-Pr(G=1|X=x_i)]&amp;\sum_{i=1}^{N}Pr(G=2|X=x_i)[1-Pr(G=2|X=x_i)]&amp;\cdots&amp;\sum_{i=1}^{N}Pr(G=2|X=x_i)[1-Pr(G=K-1|X=x_i)]\\
\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\
\sum_{i=1}^{N}Pr(G=K-1|X=x_i)[1-Pr(G=1|X=x_i)]&amp;\sum_{i=1}^{N}Pr(G=K-1|X=x_i)[1-Pr(G=2|X=x_i)]&amp;\cdots&amp;\sum_{i=1}^{N}Pr(G=K-1|X=x_i)[1-Pr(G=K-1|X=x_i)]\\
\end{bmatrix}\begin{bmatrix}
X&amp;0&amp;\cdots&amp;0\\
0&amp;X&amp;\cdots&amp;0\\
\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\
0&amp;0&amp;\cdots&amp;X\\
\end{bmatrix}\\
&amp;=-\mathbf X^T\mathbf W\mathbf X
\end{align}\]</span></p>
<p>Then <span class="math display">\[\begin{align}
\theta^{new}&amp;=\theta^{old}-\left(\frac{\partial^2l(\theta)}{\partial\theta\partial\theta^T}\right)^{-1}\frac{\partial l(\theta)}{\partial \theta}\\
&amp;=\theta^{old}+(\mathbf X^T\mathbf W\mathbf X)^{-1}\begin{bmatrix}
X^T\mathbf t_{1}-\mathbf P_1\\
X^T\mathbf t_{2}-\mathbf P_2\\
\vdots\\
X^T\mathbf t_{K-1}-\mathbf P_{K-1}\\
\end{bmatrix}\\
&amp;=\theta^{old}+(\mathbf X^T\mathbf W\mathbf X)^{-1}\mathbf X^T\begin{bmatrix}
\mathbf t_{1}-\mathbf P_1\\
\mathbf t_{2}-\mathbf P_2\\
\vdots\\
\mathbf t_{K-1}-\mathbf P_{K-1}\\
\end{bmatrix}
\end{align}\]</span></p>
</div>
<div id="ex.-4.5" class="section level2">
<h2>Ex. 4.5</h2>
<p>Consider a two-class logistic regression problem with <span class="math inline">\(x \in \mathbb R\)</span>. Characterize the maximum-likelihood estimates of the slope and intercept parameter if the sample <span class="math inline">\(x_i\)</span> for the two classes are separated by a point <span class="math inline">\(x_0 \in \mathbb R\)</span>. Generalize this result to</p>
<ol style="list-style-type: lower-alpha">
<li><span class="math inline">\(x \in \mathbb R\)</span> (see Figure 4.16), and (b) more than two classes.</li>
</ol>
</div>
<div id="ex.-4.6" class="section level2">
<h2>Ex. 4.6</h2>
<p>Suppose we have <span class="math inline">\(N\)</span> points <span class="math inline">\(x_i\)</span> in <span class="math inline">\(\mathbb R^p\)</span> in general position, with class labels <span class="math inline">\(y_i \in {−1, 1}\)</span>. Prove that the perceptron learning algorithm converges to a separating hyperplane in a finite number of steps:</p>
<ol style="list-style-type: lower-alpha">
<li>Denote a hyperplane by <span class="math inline">\(f(x) = \beta_1^T x + \beta_0 = 0\)</span>, or in more compact notation <span class="math inline">\(\beta^T x^∗ = 0\)</span>, where <span class="math inline">\(x^∗ = (x, 1)\)</span> and <span class="math inline">\(\beta = (\beta_1, \beta_0)\)</span>. Let <span class="math inline">\(z_i = x_i^∗ /\lVert x_i^∗ \rVert\)</span>. Show that separability implies the existence of a <span class="math inline">\(\beta_{sep}\)</span> such that <span class="math inline">\(y_i\beta_{sep}^Tz_i \ge 1\;\; \forall i\)</span></li>
</ol>
<p>By definition, if the points are separable then there exists a vector <span class="math inline">\(\beta\)</span> such that
<span class="math display">\[\beta^T x_i^∗ &gt; 0 \text{  when   } y_i = +1\]</span>
<span class="math display">\[\beta^T x_i^∗ &lt; 0 \text{  when   } y_i = -1\]</span>
<span class="math display">\[ \forall i = 1, 2, \cdots ,N\]</span>
This is equivalent to the expression that <span class="math inline">\(y_i\beta^Tx_i^∗ &gt; 0\quad \forall i\)</span>.
Equivalently we can divide this expression by <span class="math inline">\(\lVert x_i^∗ \rVert\)</span> (a positive number) to get <span class="math inline">\(y_i\beta^Tz_i &gt; 0\quad \forall i\)</span>. Since each one of these <span class="math inline">\(N\)</span> values of <span class="math inline">\(y_i\beta^Tz_i\)</span> is positive, let <span class="math inline">\(m &gt; 0\)</span> be the smallest value of this product observed over all our training set. Thus by definition of <span class="math inline">\(m\)</span> we have
<span class="math inline">\(y_i\beta^Tz_i\ge m\quad \forall i\)</span>. When we divide both sides of this inequality by this positive value of <span class="math inline">\(m\)</span> we get
<span class="math display">\[\frac{1}{m}y_i\beta^Tz_i\ge 1\quad \forall i\]</span>.
If we define <span class="math inline">\(\beta^{sep} ≡ \frac{1}{m}\beta\)</span> we have shown that <span class="math display">\[y_i(\beta^{sep})^Tz_i\ge 1\quad \forall i\]</span></p>
<ol start="2" style="list-style-type: lower-alpha">
<li>Given a current <span class="math inline">\(\beta_{old}\)</span>, the perceptron algorithm identifies a point <span class="math inline">\(z_i\)</span> that is misclassified, and produces the update <span class="math inline">\(\beta_{new} ← \beta_{old} + y_iz_i\)</span>. Show that <span class="math display">\[\lVert\beta_{new}−\beta_{sep}\rVert^2 ≤ \lVert\beta_{old}−\beta_{sep}\rVert^2 −1\]</span> and hence that the algorithm converges to a separating hyperplane in no more than <span class="math inline">\(\lVert\beta_{start}−\beta_{sep}\rVert^2\)</span> steps (Ripley, 1996).</li>
</ol>
<p>From <span class="math inline">\(\beta_{new} = \beta_{old} + y_iz_i\)</span> we have that
<span class="math display">\[\beta_{new}-\beta_{sep} = \beta_{old} -\beta_{sep} + y_iz_i\]</span>
When we square this result we get
<span class="math display">\[\lVert \beta_{new}-\beta_{sep}\rVert^2 = \lVert\beta_{old} -\beta_{sep}\rVert^2 + \lVert y_iz_i\rVert^2+ 2y_i(\beta_{old} -\beta_{sep})^Tz_i\]</span>
Since <span class="math inline">\(y_i = \pm 1\)</span> and <span class="math inline">\(\lVert z_i\rVert^2 = 1\)</span> we have that
<span class="math display">\[\lVert y_iz_i\rVert^2 = 1\]</span>
Since the “point” <span class="math inline">\(y_i\)</span>, <span class="math inline">\(z_i\)</span> was misclassified by the vector <span class="math inline">\(\beta_{old}\)</span> we have <span class="math inline">\(y_i\beta_{old}z_i &lt; 0\)</span> (if it was positive we would have classified it correctly). Since <span class="math inline">\(\beta_{sep}\)</span> is the vector that can correctly classify all points we have <span class="math display">\[y_i\beta_{sep}z_i &gt; 1\]</span> With these two facts we can write <span class="math display">\[2y_i(\beta_{old} -\beta_{sep})^Tz_i\le 2(0 − 1) = −2\]</span>
Thus we have just shown that
<span class="math display">\[\lVert \beta_{new}-\beta_{sep}\rVert^2 = \lVert\beta_{old} -\beta_{sep}\rVert^2 + \lVert y_iz_i\rVert^2+ 2y_i(\beta_{old} -\beta_{sep})^Tz_i\le \lVert\beta_{old} -\beta_{sep}\rVert^2 -1\]</span></p>
</div>
<div id="ex.-4.7" class="section level2">
<h2>Ex. 4.7</h2>
<p>Consider the criterion
<span class="math display">\[D^*(\beta,\beta_0)=-\sum_{i=1}^{N}y_i(x_i^T\beta+\beta_0)\]</span> a generalization of (4.41) where we sum over all the observations. Consider minimizing <span class="math inline">\(D^∗\)</span> subject to <span class="math inline">\(\lVert \beta\rVert = 1\)</span>. Describe this criterion in words. Does
it solve the optimal separating hyperplane problem?</p>
</div>
<div id="ex.-4.8" class="section level2">
<h2>Ex. 4.8</h2>
<p>Consider the multivariate Gaussian model <span class="math inline">\(X|G = k \sim N(\mu_k,\Sigma)\)</span>,
with the additional restriction that <span class="math inline">\(\text{rank }\left\lbrace\mu_k\right\rbrace_1^K= L &lt; \text{max}(K − 1, p)\)</span>. Derive the constrained MLEs for the <span class="math inline">\(\mu_k\)</span> and <span class="math inline">\(\Sigma\)</span>. Show that the Bayes classification rule is equivalent to classifying in the reduced subspace computed by LDA (Hastie and Tibshirani, 1996b).</p>
</div>
<div id="ex.-4.9" class="section level2">
<h2>Ex. 4.9</h2>
<p>Write a computer program to perform a quadratic discriminant
analysis by fitting a separate Gaussian model per class. Try it out on the
vowel data, and compute the misclassification error for the test data. The
data can be found in the book website www-stat.stanford.edu/ElemStatLearn.</p>
</div>
</div>
<div id="references" class="section level1 unnumbered">
<h1>References</h1>
<div id="refs" class="references csl-bib-body">
<div id="ref-friedman2001elements" class="csl-entry">
1. Friedman J, Hastie T, Tibshirani R, others. The elements of statistical learning. Springer series in statistics New York; 2009.
</div>
</div>
</div>

    </div>
  </article>

  


</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
          </li>
          <li>
          </li>
        </ul>
      </footer>

    </div>
    



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/r.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/yaml.min.js"></script>
<script>hljs.configure({languages: []}); hljs.initHighlightingOnLoad();</script>



    
<script src="../../../../js/math-code.js"></script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
  </body>
</html>

