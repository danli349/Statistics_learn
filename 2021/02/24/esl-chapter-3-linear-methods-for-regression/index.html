<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.80.0" />


<title>ESL chapter 3 Linear Methods for Regression - A Hugo website</title>
<meta property="og:title" content="ESL chapter 3 Linear Methods for Regression - A Hugo website">


  <link href='../../../../favicon.ico' rel='icon' type='image/x-icon'/>



  








<link href='//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css' rel='stylesheet' type='text/css' />



<link rel="stylesheet" href="../../../../css/fonts.css" media="all">
<link rel="stylesheet" href="../../../../css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="../../../../" class="nav-logo">
    <img src="../../../../images/logo.png"
         width="50"
         height="50"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="../../../../about/">about</a></li>
    
    <li><a href="../../../../vitae/">CV</a></li>
    
    <li><a href="https://github.com/danli349">GitHub</a></li>
    
    <li><a href="https://scholar.google.com/citations?user=mNjLK8EAAAAJ&amp;hl=en">Googlr Scholar</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">117 min read</span>
    

    <h1 class="article-title">ESL chapter 3 Linear Methods for Regression</h1>

    
    <span class="article-date">2021-02-24</span>
    

    <div class="article-content">
      
<script src="../../../../2021/02/24/esl-chapter-3-linear-methods-for-regression/index_files/header-attrs/header-attrs.js"></script>

<div id="TOC">
<ul>
<li><a href="#chapter-3.-linear-methods-for-regression">Chapter 3. Linear Methods for Regression</a></li>
<li><a href="#s-3.1.-introduction"><span class="math inline">\(\S\)</span> 3.1. Introduction</a></li>
<li><a href="#s-3.2.-linear-regression-models-and-least-squares"><span class="math inline">\(\S\)</span> 3.2. Linear Regression Models and Least Squares</a>
<ul>
<li><a href="#the-linear-model">The linear model</a></li>
<li><a href="#least-squares-fit">Least squares fit</a></li>
<li><a href="#solution-of-least-squares">Solution of least squares</a></li>
<li><a href="#geometrical-representation-of-the-least-squares-estimate">Geometrical representation of the least squares estimate</a></li>
<li><a href="#sampling-properties-of-hatbeta">Sampling properties of <span class="math inline">\(\hat{\beta}\)</span></a></li>
<li><a href="#inference-and-hypothesis-testing">Inference and hypothesis testing</a></li>
<li><a href="#confidence-intervals">Confidence intervals</a></li>
<li><a href="#s-3.2.1.-example-prostate-cancer"><span class="math inline">\(\S\)</span> 3.2.1. Example: Prostate Cancer</a></li>
<li><a href="#s-3.2.2.-the-gauss-markov-theorem"><span class="math inline">\(\S\)</span> 3.2.2. The Gauss-Markov Theorem</a>
<ul>
<li><a href="#the-statement-of-the-theorem">The statement of the theorem</a></li>
<li><a href="#implications-of-the-gauss-markov-theorem">Implications of the Gauss-Markov theorem</a></li>
<li><a href="#relation-between-prediction-accuracy-and-mse">Relation between prediction accuracy and MSE</a></li>
</ul></li>
<li><a href="#s-3.2.3.-multiple-regression-from-simple-univariate-regression"><span class="math inline">\(\S\)</span> 3.2.3. Multiple Regression from Simple Univariate Regression</a>
<ul>
<li><a href="#building-blocks-for-multiple-linear-regression">Building blocks for multiple linear regression</a></li>
<li><a href="#gram-schmidt-procedure-and-qr-decomposition">Gram-Schmidt procedure and QR decomposition</a></li>
</ul></li>
<li><a href="#s-3.2.4.-multiple-outputs"><span class="math inline">\(\S\)</span> 3.2.4. Multiple Outputs</a>
<ul>
<li><a href="#correlated-errors">Correlated errors</a></li>
</ul></li>
</ul></li>
<li><a href="#s-3.3.-subset-selection"><span class="math inline">\(\S\)</span> 3.3. Subset Selection</a>
<ul>
<li><a href="#s-3.3.1.-best-subset-selection"><span class="math inline">\(\S\)</span> 3.3.1. Best-Subset Selection</a></li>
<li><a href="#s-3.3.2.-forward--and-backward-stepwise-selection"><span class="math inline">\(\S\)</span> 3.3.2. Forward- and Backward-Stepwise Selection</a>
<ul>
<li><a href="#forward-stepwise-selection">Forward-stepwise selection</a></li>
<li><a href="#backward-stepwise-selction">Backward-stepwise selction</a></li>
</ul></li>
<li><a href="#s-3.3.3.-forward-stagewise-regression"><span class="math inline">\(\S\)</span> 3.3.3. Forward-Stagewise Regression</a></li>
<li><a href="#s-3.3.4.-prostate-cancer-data-example"><span class="math inline">\(\S\)</span> 3.3.4. Prostate Cancer Data Example</a>
<ul>
<li><a href="#cross-validation-briefly">Cross-validation, briefly</a></li>
</ul></li>
</ul></li>
<li><a href="#s-3.4.-shrinkage-methods"><span class="math inline">\(\S\)</span> 3.4. Shrinkage Methods</a>
<ul>
<li><a href="#s-3.4.1.-ridge-regression"><span class="math inline">\(\S\)</span> 3.4.1. Ridge Regression</a>
<ul>
<li><a href="#scaling-and-centering">Scaling and centering</a></li>
<li><a href="#from-the-bayesian-point-of-view">From the Bayesian point of view</a></li>
<li><a href="#matrix-form">Matrix form</a></li>
<li><a href="#the-singular-value-decomposition-svd">The singular value decomposition (SVD)</a></li>
<li><a href="#the-svd-and-the-principal-components">The SVD and the principal components</a></li>
<li><a href="#the-effective-degrees-of-freedom">The effective degrees of freedom</a></li>
</ul></li>
<li><a href="#s-3.4.2.-the-lasso"><span class="math inline">\(\S\)</span> 3.4.2. The Lasso</a></li>
<li><a href="#s-3.4.3.-discussion-subset-selection-ridge-regression-and-the-lasso"><span class="math inline">\(\S\)</span> 3.4.3. Discussion: Subset Selection, Ridge Regression, and the Lasso</a>
<ul>
<li><a href="#explicit-solutions-in-the-ideal-situation">Explicit solutions in the ideal situation</a></li>
<li><a href="#back-to-the-reality-the-nonorthonormal-case">Back to the reality; the nonorthonormal case</a></li>
<li><a href="#generalization-to-bayes-estimates">Generalization to Bayes estimates</a></li>
</ul></li>
<li><a href="#s-3.4.4.-least-angle-regression"><span class="math inline">\(\S\)</span> 3.4.4. Least Angle Regression</a>
<ul>
<li><a href="#algorithm-3.2.-least-angle-regression">Algorithm 3.2. Least Angle Regression</a></li>
<li><a href="#algorithm-3.2a.-least-angle-regression-lasso-modification">Algorithm 3.2a. Least Angle Regression: Lasso Modification</a></li>
<li><a href="#computational-efficiency">Computational efficiency</a></li>
<li><a href="#heuristic-argument-for-why-lar-and-lasso-are-so-similar">Heuristic argument for why LAR and lasso are so similar</a></li>
<li><a href="#degrees-of-freedom-formula-for-lar-and-lasso">Degrees-of-freedom formula for LAR and lasso</a></li>
</ul></li>
</ul></li>
<li><a href="#s-3.5.-methods-using-derived-input-directions"><span class="math inline">\(\S\)</span> 3.5. Methods Using Derived Input Directions</a>
<ul>
<li><a href="#s-3.5.1.-principal-components-regression"><span class="math inline">\(\S\)</span> 3.5.1. Principal Components Regression</a></li>
<li><a href="#s-3.5.2.-partial-least-squares"><span class="math inline">\(\S\)</span> 3.5.2. Partial Least Squares</a>
<ul>
<li><a href="#algorithm-3.3.-partial-least-squares.">Algorithm 3.3. Partial least squares.</a></li>
<li><a href="#gist-of-the-pls-algorithm">Gist of the PLS algorithm</a></li>
<li><a href="#relation-to-the-optimization-problem">Relation to the optimization problem</a></li>
</ul></li>
</ul></li>
<li><a href="#s-3.6.-discussion-a-comparison-of-the-selection-and-shrinkage-methods"><span class="math inline">\(\S\)</span> 3.6. Discussion: A Comparison of the Selection and Shrinkage Methods</a></li>
<li><a href="#s-exercises"><span class="math inline">\(\S\)</span> Exercises</a>
<ul>
<li><a href="#s-ex.-3.1-the-f-statistic-is-equivalent-to-the-square-of-the-z-score"><span class="math inline">\(\S\)</span> Ex. 3.1 (the F-statistic is equivalent to the square of the Z-score)</a></li>
<li><a href="#s-ex.-3.2-confidence-intervals-on-a-cubic-equation"><span class="math inline">\(\S\)</span> Ex. 3.2 (confidence intervals on a cubic equation)</a></li>
<li><a href="#s-ex.-3.3-the-gauss-markov-theorem"><span class="math inline">\(\S\)</span> Ex. 3.3 (the Gauss-Markov theorem)</a></li>
<li><a href="#s-ex.-3.4-the-vector-of-least-squares-coefficients-from-gram-schmidt"><span class="math inline">\(\S\)</span> Ex. 3.4 (the vector of least squares coefficients from Gram-Schmidt)</a></li>
<li><a href="#s-ex.-3.5-an-equivalent-problem-to-ridge-regression"><span class="math inline">\(\S\)</span> Ex. 3.5 (an equivalent problem to ridge regression)</a></li>
<li><a href="#s-ex.-3.6-the-ridge-regression-estimate"><span class="math inline">\(\S\)</span> Ex. 3.6 (the ridge regression estimate)</a></li>
<li><a href="#s-ex.-3.7"><span class="math inline">\(\S\)</span> Ex. 3.7</a></li>
<li><a href="#s-ex.-3.8"><span class="math inline">\(\S\)</span> Ex. 3.8</a></li>
<li><a href="#s-ex.-3.9-using-the-qr-decomposition-for-fast-forward-stepwise-selection"><span class="math inline">\(\S\)</span> Ex. 3.9 (using the QR decomposition for fast forward-stepwise selection)</a></li>
</ul></li>
<li><a href="#references">References</a></li>
</ul>
</div>

<h3>
Normal, Student’s Distributions
</h3>
<pre class="python"><code>from scipy.stats import norm, t
import numpy as np
from matplotlib import pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
%matplotlib inline</code></pre>
<pre class="python"><code># create frozen RVs for standard normal and Student&#39;s with 30, 100 degrees of freedom
rv_t30, rv_t100, rv_norm = t(30), t(100), norm()</code></pre>
<pre class="python"><code># calculate tail probabilities Pr(|Z|&gt;z) at some points
Z = np.linspace(1.8, 3, 13)
tail_probs = {rv_t30: [], rv_t100: [], rv_norm: []}
for z in Z:
    for rv in tail_probs:
        tail_probs[rv].append(1 - rv.cdf(z))
# FIGURE 3.3. explains that normal distribution is good for testing significance
for rv in tail_probs:
    plt.plot(Z, tail_probs[rv])</code></pre>
<div class="figure">
<img src="merged_files/merged_3_0.png" alt="" />
<p class="caption">png</p>
</div>
<pre class="python"><code>for rv in tail_probs:
    print(rv)</code></pre>
<pre><code>&lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x7fb068298fd0&gt;
&lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x7fb0913d9730&gt;
&lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x7fb0913d9a60&gt;</code></pre>
<h3>
F-distribution
</h3>
<pre class="python"><code>from scipy.stats import f
rv_f4_58 = f(4, 58)
# 3.16 F statistics calculation
F = ((32.81-29.43)/(9-5)) / (29.43/(67-9))
# tail probability
1 - rv_f4_58.cdf(F)</code></pre>
<pre><code>0.1703876176583532</code></pre>
<h3>
Mean, Variance, Standard Deviation
</h3>
<pre class="python"><code>values = np.array([-14.82381293, -0.29423447, -13.56067979, -1.6288903, -0.31632439,
                   0.53459687, -1.34069996, -1.61042692, -4.03220519, -0.24332097])</code></pre>
<pre class="python"><code>print(&#39;mean:              &#39;, np.mean(values))
print(&#39;variance:          &#39;, np.var(values))
print(&#39;standard deviation:&#39;, np.std(values))
print(&#39;---- unbiased estimates ----&#39;)
# ddof means Delta Degrees of Freedom
print(&#39;variance:          &#39;, np.var(values, ddof=1))
print(&#39;standard deviation:&#39;, np.std(values, ddof=1))</code></pre>
<pre><code>mean:               -3.7315998049999997
variance:           28.822364260579157
standard deviation: 5.36864640860051
---- unbiased estimates ----
variance:           32.024849178421285
standard deviation: 5.659050201086865</code></pre>
<h3>
Variance-Covariance, Correlation Matrix
</h3>
<pre class="python"><code>import pandas as pd</code></pre>
<pre class="python"><code>data = pd.read_csv(&quot;../../data/Prostate Cancer.txt&quot;) 
names = [&#39;lcavol&#39;, &#39;lweight&#39;, &#39;age&#39;, &#39;lbph&#39;, &#39;svi&#39;, &#39;lcp&#39;, &#39;gleason&#39;, &#39;pgg45&#39;]
X = data[names].values
len(X)</code></pre>
<pre><code>97</code></pre>
<pre class="python"><code># correlation matrix using pandas
data[names].corr()</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
lcavol
</th>
<th>
lweight
</th>
<th>
age
</th>
<th>
lbph
</th>
<th>
svi
</th>
<th>
lcp
</th>
<th>
gleason
</th>
<th>
pgg45
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
lcavol
</th>
<td>
1.000000
</td>
<td>
0.280521
</td>
<td>
0.225000
</td>
<td>
0.027350
</td>
<td>
0.538845
</td>
<td>
0.675310
</td>
<td>
0.432417
</td>
<td>
0.433652
</td>
</tr>
<tr>
<th>
lweight
</th>
<td>
0.280521
</td>
<td>
1.000000
</td>
<td>
0.347969
</td>
<td>
0.442264
</td>
<td>
0.155385
</td>
<td>
0.164537
</td>
<td>
0.056882
</td>
<td>
0.107354
</td>
</tr>
<tr>
<th>
age
</th>
<td>
0.225000
</td>
<td>
0.347969
</td>
<td>
1.000000
</td>
<td>
0.350186
</td>
<td>
0.117658
</td>
<td>
0.127668
</td>
<td>
0.268892
</td>
<td>
0.276112
</td>
</tr>
<tr>
<th>
lbph
</th>
<td>
0.027350
</td>
<td>
0.442264
</td>
<td>
0.350186
</td>
<td>
1.000000
</td>
<td>
-0.085843
</td>
<td>
-0.006999
</td>
<td>
0.077820
</td>
<td>
0.078460
</td>
</tr>
<tr>
<th>
svi
</th>
<td>
0.538845
</td>
<td>
0.155385
</td>
<td>
0.117658
</td>
<td>
-0.085843
</td>
<td>
1.000000
</td>
<td>
0.673111
</td>
<td>
0.320412
</td>
<td>
0.457648
</td>
</tr>
<tr>
<th>
lcp
</th>
<td>
0.675310
</td>
<td>
0.164537
</td>
<td>
0.127668
</td>
<td>
-0.006999
</td>
<td>
0.673111
</td>
<td>
1.000000
</td>
<td>
0.514830
</td>
<td>
0.631528
</td>
</tr>
<tr>
<th>
gleason
</th>
<td>
0.432417
</td>
<td>
0.056882
</td>
<td>
0.268892
</td>
<td>
0.077820
</td>
<td>
0.320412
</td>
<td>
0.514830
</td>
<td>
1.000000
</td>
<td>
0.751905
</td>
</tr>
<tr>
<th>
pgg45
</th>
<td>
0.433652
</td>
<td>
0.107354
</td>
<td>
0.276112
</td>
<td>
0.078460
</td>
<td>
0.457648
</td>
<td>
0.631528
</td>
<td>
0.751905
</td>
<td>
1.000000
</td>
</tr>
</tbody>
</table>
</div>
<pre class="python"><code># correlation matrix using numpy
np.set_printoptions(precision=2, suppress=True)
np.corrcoef(X, rowvar=False)</code></pre>
<pre><code>array([[ 1.  ,  0.28,  0.22,  0.03,  0.54,  0.68,  0.43,  0.43],
       [ 0.28,  1.  ,  0.35,  0.44,  0.16,  0.16,  0.06,  0.11],
       [ 0.22,  0.35,  1.  ,  0.35,  0.12,  0.13,  0.27,  0.28],
       [ 0.03,  0.44,  0.35,  1.  , -0.09, -0.01,  0.08,  0.08],
       [ 0.54,  0.16,  0.12, -0.09,  1.  ,  0.67,  0.32,  0.46],
       [ 0.68,  0.16,  0.13, -0.01,  0.67,  1.  ,  0.51,  0.63],
       [ 0.43,  0.06,  0.27,  0.08,  0.32,  0.51,  1.  ,  0.75],
       [ 0.43,  0.11,  0.28,  0.08,  0.46,  0.63,  0.75,  1.  ]])</code></pre>
<pre class="python"><code># variance-covariance
np.cov(X, rowvar=False)</code></pre>
<pre><code>array([[  1.39,   0.14,   1.97,   0.05,   0.26,   1.11,   0.37,  14.42],
       [  0.14,   0.18,   1.11,   0.27,   0.03,   0.1 ,   0.02,   1.3 ],
       [  1.97,   1.11,  55.43,   3.78,   0.36,   1.33,   1.45,  57.98],
       [  0.05,   0.27,   3.78,   2.1 ,  -0.05,  -0.01,   0.08,   3.21],
       [  0.26,   0.03,   0.36,  -0.05,   0.17,   0.39,   0.1 ,   5.34],
       [  1.11,   0.1 ,   1.33,  -0.01,   0.39,   1.96,   0.52,  24.91],
       [  0.37,   0.02,   1.45,   0.08,   0.1 ,   0.52,   0.52,  15.31],
       [ 14.42,   1.3 ,  57.98,   3.21,   5.34,  24.91,  15.31, 795.47]])</code></pre>
<pre class="python"><code># Use StandardScaler() function to standardize features by removing the mean and scaling to unit variance
# The standard score of a sample x is calculated as:
# z = (x - u) / s
# where u is the mean of the training samples or zero if with_mean=False, and s is the standard deviation of the training samples 
# or one if with_std=False.

# Use fit(X[, y, sample_weight]) to compute the mean and std to be used for later scaling.
# transform(X[, copy]) Perform standardization by centering and scaling

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaler.fit(X)
X_transformed = scaler.transform(X)
# transformed variance-covariance matrix (nearly equals to correlation matrix)
np.cov(X_transformed, rowvar=False)</code></pre>
<pre><code>array([[ 1.01,  0.28,  0.23,  0.03,  0.54,  0.68,  0.44,  0.44],
       [ 0.28,  1.01,  0.35,  0.45,  0.16,  0.17,  0.06,  0.11],
       [ 0.23,  0.35,  1.01,  0.35,  0.12,  0.13,  0.27,  0.28],
       [ 0.03,  0.45,  0.35,  1.01, -0.09, -0.01,  0.08,  0.08],
       [ 0.54,  0.16,  0.12, -0.09,  1.01,  0.68,  0.32,  0.46],
       [ 0.68,  0.17,  0.13, -0.01,  0.68,  1.01,  0.52,  0.64],
       [ 0.44,  0.06,  0.27,  0.08,  0.32,  0.52,  1.01,  0.76],
       [ 0.44,  0.11,  0.28,  0.08,  0.46,  0.64,  0.76,  1.01]])</code></pre>
<pre class="python"><code># We can also use StandardScaler().fit_transform()
X_transformed = StandardScaler().fit_transform(X)
# transformed variance-covariance matrix (nearly equals to correlation matrix)
np.cov(X_transformed, rowvar=False)</code></pre>
<pre><code>array([[ 1.01,  0.28,  0.23,  0.03,  0.54,  0.68,  0.44,  0.44],
       [ 0.28,  1.01,  0.35,  0.45,  0.16,  0.17,  0.06,  0.11],
       [ 0.23,  0.35,  1.01,  0.35,  0.12,  0.13,  0.27,  0.28],
       [ 0.03,  0.45,  0.35,  1.01, -0.09, -0.01,  0.08,  0.08],
       [ 0.54,  0.16,  0.12, -0.09,  1.01,  0.68,  0.32,  0.46],
       [ 0.68,  0.17,  0.13, -0.01,  0.68,  1.01,  0.52,  0.64],
       [ 0.44,  0.06,  0.27,  0.08,  0.32,  0.52,  1.01,  0.76],
       [ 0.44,  0.11,  0.28,  0.08,  0.46,  0.64,  0.76,  1.01]])</code></pre>
<h3>
QR Factorization
</h3>
<pre class="python"><code># np.random.randn() Return a sample (or samples) from the “standard normal” distribution.
a = np.random.randn(9, 6)
# a = q@r; where q^T @ q == I, r is upper triangular
# np.linalg.qr() Compute the qr factorization of a matrix.
# Factor the matrix a as qr, where q is orthonormal and r is upper-triangular.
q, r = np.linalg.qr(a)
print(a, &#39;\n\n&#39;, q, &#39;\n\n&#39;, r)</code></pre>
<pre><code>[[ 0.58 -0.31  0.74  1.01  1.53 -0.71]
 [ 0.68  1.46  0.83 -0.44 -1.04 -0.46]
 [-0.87  0.15 -1.26 -0.36 -0.73 -0.18]
 [-0.12  0.67 -0.85 -0.95  0.42 -0.72]
 [-1.26  0.43 -0.29  0.47  0.33 -0.45]
 [ 0.84 -0.02 -0.17 -1.4   0.31  1.66]
 [-0.51 -0.09 -0.58 -0.97 -0.56  0.14]
 [-1.72  1.39  1.41  0.22 -0.33  0.38]
 [-0.65 -0.77  0.92  0.77  0.56 -0.35]] 

 [[-0.21  0.08  0.32  0.35 -0.55  0.12]
 [-0.25 -0.73  0.17  0.04  0.3   0.33]
 [ 0.32  0.03 -0.5   0.12  0.2  -0.06]
 [ 0.04 -0.29 -0.4  -0.07 -0.58  0.49]
 [ 0.46 -0.06 -0.15  0.3  -0.29 -0.15]
 [-0.31 -0.08 -0.07 -0.64 -0.36 -0.45]
 [ 0.19  0.1  -0.21 -0.5   0.11  0.4 ]
 [ 0.63 -0.44  0.42 -0.22 -0.11 -0.23]
 [ 0.24  0.42  0.45 -0.23 -0.03  0.45]] 

 [[-2.74  0.65  0.12  0.53 -0.4  -0.35]
 [ 0.   -2.23 -0.59  0.87  1.01  0.08]
 [ 0.    0.    2.55  1.49  0.67 -0.  ]
 [ 0.    0.    0.    1.66  0.5  -1.51]
 [ 0.    0.    0.    0.   -1.79  0.15]
 [ 0.    0.    0.    0.    0.   -1.44]]</code></pre>
<div id="chapter-3.-linear-methods-for-regression" class="section level1">
<h1>Chapter 3. Linear Methods for Regression</h1>
</div>
<div id="s-3.1.-introduction" class="section level1">
<h1><span class="math inline">\(\S\)</span> 3.1. Introduction</h1>
<p>A linear regression model assumes that the regression function <span class="math inline">\(\text{E}(Y|X)\)</span> is linear in the inputs <span class="math inline">\(X_1,\cdots,X_p\)</span>. Linear models were largely developed in the precomputer age of statistics, but even in today’s computer era there are still good reasons to study and use them. They are simple and often provide an adequate and interpretable description of how the inputs affect the output.</p>
<p>For prediction purposes they can sometimes outperform fancier nonlinear models, especially in situations with small numbers of training cases, low signal-to-noise ratio or sparse data.</p>
<p>Finally, linear methods can be applied to transformations of the inputs and this considerably expands their scope. These generalization are sometimes called basis-function methods (Chapter 5).</p>
<p>In this chapter we describe linear methods for regression, while in the next chapter we discuss linear methods for classification.</p>
<blockquote>
<p>On some topics we go into considerable detail, as it is out firm belief that an understanding of linear methods is essential for understanding nonlinear ones.</p>
</blockquote>
<p>In fact, many nonlinear techniques are direct generalizations of the linear methods discussed here.</p>
</div>
<div id="s-3.2.-linear-regression-models-and-least-squares" class="section level1">
<h1><span class="math inline">\(\S\)</span> 3.2. Linear Regression Models and Least Squares</h1>
<div id="the-linear-model" class="section level3">
<h3>The linear model</h3>
<p>We have</p>
<ul>
<li>an input vector <span class="math inline">\(X^T = (X_1, X_2, \cdots, X_p)\)</span> and</li>
<li>a real-valued output <span class="math inline">\(Y\)</span> to predict.</li>
</ul>
<p>The linear regression model has the form with unknown parameters <span class="math inline">\(\beta_j\)</span>’s,</p>
<p><span class="math display">\[\begin{equation}
f(X) = \beta_0 + \sum_{j=1}^p X_j\beta_j.
\end{equation}\]</span></p>
<p>The linear model either assumes that the regression function <span class="math inline">\(\text{E}(Y|X)\)</span> is linear, or that the linear model is a reasonable approximation.</p>
<p>The variable <span class="math inline">\(X_j\)</span> can come from different sources:</p>
<ul>
<li>Quantitative inputs, and its transformations, e.g., log, squared-root, square,</li>
<li>basis expansions, e.g., <span class="math inline">\(X_2=X_1^2, X_3=X_1^3\)</span>, leading to a polynomial representation,</li>
<li>numeric or “dummy” coding of the levels of qualitative inputs.<br />
For example, if <span class="math inline">\(G\)</span> is a five-level factor input, we might create <span class="math inline">\(X_j=I(G=j),\)</span> for <span class="math inline">\(j = 1,\cdots,5\)</span>.</li>
<li>Interactions between variables, e.g., <span class="math inline">\(X_3=X_1\cdot X_2\)</span>.</li>
</ul>
<p>No matter the source of the <span class="math inline">\(X_j\)</span>, the model is linear in the parameters.</p>
</div>
<div id="least-squares-fit" class="section level3">
<h3>Least squares fit</h3>
<p>Typically we have a set of training data</p>
<ul>
<li><span class="math inline">\((x_1, y_1), \cdots, (x_N, y_N)\)</span> from which to estimate the parameters <span class="math inline">\(\beta\)</span>.</li>
<li>Each <span class="math inline">\(x_i = (x_{i1}, x_{i2}, \cdots, x_{ip})^T\)</span> is a vector of feature measurements for the <span class="math inline">\(i\)</span>th case.</li>
</ul>
<p>The most popular estimation method is <em>least squares</em>, in which we pick coefficients <span class="math inline">\(\beta=(\beta_0,\beta_1,\cdots,\beta_p)^T\)</span> to minimize the residual sum of squares</p>
<p><span class="math display">\[\begin{align}
\text{RSS}(\beta) &amp;= \sum_{i=1}^N\left(y_i - f(x_i)\right)^2 \\
&amp;= \sum_{i=1}^N \left(y_i - \beta_0 - \sum_{j=1}^px_{ij}\beta_j\right)^2.
\end{align}\]</span></p>
<p>From a statistical point of view, this criterion is reasonable if the training observations <span class="math inline">\((x_i,y_i)\)</span> represent independent random draws from their population. Even if the <span class="math inline">\(x_i\)</span>’s were not drawn randomly, the criterion is still valid if the <span class="math inline">\(y_i\)</span>’s are conditionally independent given the inputs <span class="math inline">\(x_i\)</span>.</p>
<p>See FIGURE 3.1 in the textbook for illustration of the geometry of least-squares fitting in <span class="math inline">\(\mathbb{R}^{p+1}\)</span> space occupied by the pairs <span class="math inline">\((X,Y)\)</span>.</p>
<blockquote>
<p>Note that RSS makes no assumptions about the validity of the linear model; it simply finds the best linear fit to the data. Least squares fitting is intuitively satisfying no matter how the data arise; the criterion measures the average lack of fit.</p>
</blockquote>
</div>
<div id="solution-of-least-squares" class="section level3">
<h3>Solution of least squares</h3>
<p>How do we minimize RSS?</p>
<p>Denote</p>
<ul>
<li><span class="math inline">\(\mathbf{X}\)</span> the <span class="math inline">\(N\times(p+1)\)</span> matrix with each row an input vector (with a 1 in the first position),</li>
<li><span class="math inline">\(\mathbf{y}\)</span> the <span class="math inline">\(N\)</span>-vector of outputs in the training set.</li>
</ul>
<p>Then we can write RSS as</p>
<p><span class="math display">\[\begin{equation}
\text{RSS}(\beta) = \left(\mathbf{y}-\mathbf{X}\beta\right)^T\left(\mathbf{y}-\mathbf{X}\beta\right) = \|\mathbf{y}-\mathbf{X}\beta\|^2.
\end{equation}\]</span></p>
<p>This is a quadratic function in the <span class="math inline">\(p+1\)</span> parameters. Differentiating w.r.t. <span class="math inline">\(\beta\)</span> we obtain</p>
<p><span class="math display">\[\begin{align}
\frac{\partial\text{RSS}}{\partial\beta} &amp;= -2\mathbf{X}^T\left(\mathbf{y}-\mathbf{X}\beta\right) \\
\frac{\partial^2\text{RSS}}{\partial\beta\partial\beta^T} &amp;= 2\mathbf{X}^T\mathbf{X}
\end{align}\]</span></p>
<p>Assuming (for the moment) that <span class="math inline">\(\mathbf{X}\)</span> has full column rank, and hence <span class="math inline">\(\mathbf{X}^T\mathbf{X}\)</span> is positive definite (therefore it is guaranteed to the unique minimum exists), we set the first derivative to zero</p>
<p><span class="math display">\[\begin{equation}
\mathbf{X}^T(\mathbf{y}-\mathbf{X}\beta) = 0
\end{equation}\]</span></p>
<p>to obtain the unique solution</p>
<p><span class="math display">\[\begin{equation}
\hat\beta = \left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T\mathbf{y}.
\end{equation}\]</span></p>
<p>The predicted values at an input vector <span class="math inline">\(x_0\)</span> are given by</p>
<p><span class="math display">\[\begin{equation}
\hat{f}(x_0) = (1:x_0)^T\hat\beta,
\end{equation}\]</span></p>
<p>and fitted values at the training samples are</p>
<p><span class="math display">\[\begin{equation}
\hat{y} = \mathbf{X}\hat\beta = \mathbf{X}\left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T\mathbf{y} = \mathbf{H}\mathbf{y},
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\hat{y_i}=\hat{f}(x_i)\)</span>. The matrix</p>
<p><span class="math display">\[\begin{equation}
\mathbf{H} = \mathbf{X}\left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T
\end{equation}\]</span></p>
<p>is sometimes called the “hat” matrix or projection metrix because it puts the hat on <span class="math inline">\(\mathbf{y}\)</span> and project <span class="math inline">\(\mathbf{y}\)</span> to <span class="math inline">\(\hat{\mathbf{y}}\)</span> in <span class="math inline">\(col(\mathbf{X})\)</span>.</p>
</div>
<div id="geometrical-representation-of-the-least-squares-estimate" class="section level3">
<h3>Geometrical representation of the least squares estimate</h3>
<p>FIGURE 3.2 shows a different geometrical representation of the least squares estimate, this time in <span class="math inline">\(\mathbb{R}^N\)</span>.</p>
<p>We denote the column vector of <span class="math inline">\(\mathbf{X}\)</span> by <span class="math inline">\(\mathbf{x}_0\)</span>, <span class="math inline">\(\mathbf{x}_1\)</span>, <span class="math inline">\(\cdots\)</span>, <span class="math inline">\(\mathbf{x}_p\)</span>, with <span class="math inline">\(\mathbf{x}_0 \equiv 1\)</span>. For much of what follows, this first column is treated like any other.</p>
<p>These vectors span a subspace of <span class="math inline">\(\mathbb{R}^N\)</span>, also referred to as the column space of <span class="math inline">\(\mathbf{X}\)</span> denoted by <span class="math inline">\(\text{col}(\mathbf{X})\)</span>. We minimize RSS by choosing <span class="math inline">\(\hat\beta\)</span> so that the residual vector <span class="math inline">\(\mathbf{y}-\hat{\mathbf{y}}\)</span> is orthogonal to this subspace. This orthogonality is expressed in</p>
<p><span class="math display">\[\begin{equation}
\mathbf{X}^T(\mathbf{y}-\mathbf{X}\beta) = 0,
\end{equation}\]</span></p>
<p>and the resulting estimate <span class="math inline">\(\hat{\mathbf{y}}\)</span> is hence the <em>orthogonal projection</em> of <span class="math inline">\(\mathbf{y}\)</span> onto this subspace. The hat matrix <span class="math inline">\(\mathbf{H}\)</span> computes the orthogonal projection, and hence it is also known as a projection matrix.</p>
<div id="rank-deficiency" class="section level4">
<h4>Rank deficiency</h4>
<p>It might happen that the columns of <span class="math inline">\(\mathbf{X}\)</span> are not linearly independent, so that <span class="math inline">\(\mathbf{X}\)</span> is not of full rank. This would occur, for example, if two of the inputs were perfectly correlated, e.g., <span class="math inline">\(\mathbf{x}_2=3\mathbf{x}_1\)</span>.</p>
<p>Then <span class="math inline">\(\mathbf{X}^T\mathbf{X}\)</span> is singular and the least squares coefficients <span class="math inline">\(\hat\beta\)</span> are not uniquely defined. However, the fitted values <span class="math inline">\(\hat{\mathbf{y}}=\mathbf{X}\hat\beta\)</span> are still the projection of <span class="math inline">\(\mathbf{y}\)</span> onto the <span class="math inline">\(\text{col}(\mathbf{X})\)</span>; there are just more than one way to express that projection in terms of the column vectors of <span class="math inline">\(\mathbf{X}\)</span>.</p>
<p>The non-full-rank case occurs most often when one or more qualitative inputs are coded in a redundant fashion.</p>
<p>There is usually a natural way to resolve the non-unique representation, by recording and/or dropping redundant columns in <span class="math inline">\(\mathbf{X}\)</span>. Most regression software packages detect these redundancies and automatically implement some strategy for removing them.</p>
<p>Rank deficientcies can also occur in signal and image analysis, where the number of inputs <span class="math inline">\(p\)</span> can exceed the number of training cases <span class="math inline">\(N\)</span>. In this case, the features are typically reduced by filtering or else the fitting is controlled by regularization (<span class="math inline">\(\S\)</span> 5.2.3 and Chapter 18).</p>
</div>
</div>
<div id="sampling-properties-of-hatbeta" class="section level3">
<h3>Sampling properties of <span class="math inline">\(\hat{\beta}\)</span></h3>
<p>Up to now we have made minimal assumptions about the true distribution of the data. In order to pin down the sampling properties of <span class="math inline">\(\hat\beta\)</span>, we now assume that</p>
<ul>
<li>the observations <span class="math inline">\(y_i\)</span> are uncorrelated and</li>
<li><span class="math inline">\(y_i\)</span> have constant variance <span class="math inline">\(\sigma^2\)</span>,</li>
<li>the <span class="math inline">\(x_i\)</span> are fixed (nonrandom).</li>
</ul>
<p><span class="math display">\[\begin{align}
\text{E}(\hat{\beta})&amp;=\text{E}\Bigl(\left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T\mathbf{y}\Bigr)\\
&amp;=\Bigl(\left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T\Bigr)\text{E}(\mathbf{y})\\
&amp;=(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{X}\beta=\beta
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
\hat{\beta}-\text{E}(\hat{\beta})&amp;=\Bigl(\left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T\mathbf{y}\Bigr)-\Bigl(\left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T\mathbf{X}\beta\Bigr)\\
&amp;=\left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T(\mathbf{y}-\mathbf{X}\beta)\\
&amp;=\left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T\epsilon
\end{align}\]</span>
where <span class="math inline">\(\epsilon\)</span> is a random column vector of dimension <span class="math inline">\(N\)</span>.</p>
<p>The variance-covariance matrix of the least squares estmiates is</p>
<p><span class="math display">\[\begin{align}
\text{Var}(\hat{\beta}) &amp;= \text{Var}\left(\left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T\mathbf{y}\right) \\
&amp;= \left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T\text{Var}\left(\mathbf{y}\right)\mathbf{X}\left(\mathbf{X}^T\mathbf{X}\right)^{-1} \\
&amp;= \left(\mathbf{X}^T\mathbf{X}\right)^{-1}\sigma^2.
\end{align}\]</span></p>
<p>Typically one estimates the variance <span class="math inline">\(\sigma^2\)</span> by</p>
<p><span class="math display">\[\begin{equation}
\hat\sigma^2 = \frac{1}{N-p-1}\sum_{i=1}^N\left(y_i-\hat{y}_i\right)^2\\
=\frac{1}{N-p-1}\sum_{i=1}^N\left(y_i-x_i^T\hat{\beta}\right)^2,
\end{equation}\]</span></p>
<p>where the denominator <span class="math inline">\(N-p-1\)</span> rather than <span class="math inline">\(N\)</span> makes <span class="math inline">\(\hat\sigma^2\)</span> an unbiased estimate of <span class="math inline">\(\sigma^2\)</span>:</p>
<p><span class="math display">\[\begin{equation}
\text{E}(\hat\sigma^2) = \sigma^2.
\end{equation}\]</span></p>
<p>If <span class="math inline">\(x\)</span> is the standard variable in <span class="math inline">\(N(0, \sigma^2)\)</span>, then <span class="math inline">\(E(x^2) = \sigma^2\)</span>. It follows that the distance squared from origin in <span class="math inline">\(V\)</span>, <span class="math inline">\(\sum_{i=1}^k v_i^2\)</span>, has expectation <span class="math inline">\(k\sigma^2\)</span>. We now use the fact that in ordinary least squares <span class="math inline">\(\mathbf{\hat{y}}\)</span> is the orthogonal projection of <span class="math inline">\(\mathbf{y}\)</span> onto the column space of <span class="math inline">\(\mathbf{X}\)</span> as a subspace of <span class="math inline">\(\mathbb R^N\)</span>. Under our assumption of the independence of the columns of <span class="math inline">\(\mathbf{X}\)</span> this space has dimension <span class="math inline">\(p+1\)</span>. In the notation above <span class="math inline">\(\hat{\mathbf{y}}\in V\)</span> with <span class="math inline">\(V\)</span> the column space of <span class="math inline">\(\mathbf{X}\)</span> and <span class="math inline">\(\mathbf{y}−\hat{\mathbf{y}} \in W\)</span>, where <span class="math inline">\(W\)</span> is the orthogonal complement of the column space of <span class="math inline">\(\mathbf{X}\)</span>. Because <span class="math inline">\(\mathbf{y} \in \mathbb R^N\)</span> and <span class="math inline">\(V\)</span> is of dimension <span class="math inline">\(p + 1\)</span>, we know that <span class="math inline">\(W\)</span> has dimension <span class="math inline">\(N − p − 1\)</span> and <span class="math inline">\(\mathbf{y}−\hat{\mathbf{y}}\)</span> is a random vector in <span class="math inline">\(W\)</span> with distribution <span class="math inline">\(N(0, \sigma^2\mathbf{I}_{N−p−1})\)</span>. The sum of squares of the <span class="math inline">\(N\)</span> components of <span class="math inline">\(\mathbf{y}−\hat{\mathbf{y}}\)</span> is the square of the distance in <span class="math inline">\(W\)</span> to the origin. Therefore <span class="math inline">\(\sum_{i=1}^{N}(\mathbf{y}_i−\hat{\mathbf{y}}_i)^2\)</span> has expectation <span class="math inline">\((N − p − 1)\sigma^2\)</span>.</p>
</div>
<div id="inference-and-hypothesis-testing" class="section level3">
<h3>Inference and hypothesis testing</h3>
<p>To draw inferences about the parameters and the model, additional assumptions are needed. We now assume that</p>
<ul>
<li>the linear model $f(X) = <em>0 + </em>{j=1}^p X_j_j $ is the correct model for the mean.
i.e., the conditional expectation of <span class="math inline">\(Y\)</span> is linear in <span class="math inline">\(X\)</span>;</li>
<li>the deviations of <span class="math inline">\(Y\)</span> around its expectation are additive and Gaussian.</li>
</ul>
<p>Hence</p>
<p><span class="math display">\[\begin{align}
Y &amp;= \text{E}\left(Y|X_1,\cdots,X_p\right)+\epsilon \\
&amp;= \beta_0 + \sum_{j=1}^p X_j\beta_j + \epsilon,
\end{align}\]</span></p>
<p>where <span class="math inline">\(\epsilon\sim N(0,\sigma^2)\)</span>. Then it is easy to show that</p>
<p><span class="math display">\[\begin{align}
\hat\beta\sim N\left(\beta, \left(\mathbf{X}^T\mathbf{X}\right)^{-1}\sigma^2\right), \\
(N-p-1)\hat\sigma^2\sim\sigma^2\chi^2_{N-p-1}.
\end{align}\]</span></p>
<p>In addition <span class="math inline">\(\hat\beta\)</span> and <span class="math inline">\(\hat\sigma^2\)</span> are statistically independent.</p>
<p>We use these distributional properties to form tests of hypothesis and confidence intervals for the parameters <span class="math inline">\(\beta_j\)</span>. To test the null hypothesis <span class="math inline">\(\mu_0: \beta_j=0\)</span>, we form the standardized coefficient or <em>Z-score</em></p>
<p><span class="math display">\[\begin{equation}
z_j = \frac{\hat\beta_j}{\hat\sigma\sqrt{v_j}},
\end{equation}\]</span></p>
<p>where <span class="math inline">\(v_j = \left(\mathbf{X}^T\mathbf{X}\right)^{-1}_{jj}\)</span>.</p>
<p>Under the null hypothesis, <span class="math inline">\(z_j\sim t_{N-p-1}\)</span>, and hence a large (absolute) value of <span class="math inline">\(z_j\)</span> will lead to rejection of this null hypothesis.</p>
<p>If <span class="math inline">\(\hat\sigma\)</span> is replaced by a known value <span class="math inline">\(\sigma\)</span>, then <span class="math inline">\(z_j\)</span> would have a standard normal distribution. The difference between the tail quatiles of <span class="math inline">\(t\)</span>-distribution and a standard normal distribution become negligible as the sample size increases, so we typically use the normal quantiles (see FIGURE 3.3).</p>
<pre class="python"><code>&quot;&quot;&quot;FIGURE 3.3. The tail probabilities for three distributions, t30, t100, N(0,1).

Pr(|Z| &gt; z) for three distributions, t(df=30), t(df=100), and standard normal.
The difference between t and the standard normal becomes negligible for N &gt; 100.
&quot;&quot;&quot;
import matplotlib.pyplot as plt
import scipy
import scipy.stats

x = np.linspace(1.9, 3, 1000)
pdf_gaussian = scipy.stats.norm.pdf(x)
pdf_t30 = scipy.stats.t.pdf(x, df=30)
pdf_t100 = scipy.stats.t.pdf(x, df=100)

fig = plt.figure(figsize=(10, 5))
ax = fig.add_subplot(1, 1, 1)
ax.plot(x, pdf_gaussian, label=&#39;normal&#39;)
ax.plot(x, pdf_t30, label=&#39;t(df=30)&#39;)
ax.plot(x, pdf_t100, label=&#39;t(df=100)&#39;)
ax.legend()
ax.plot([1.9, 3], [.01, .01], &#39;--&#39;, color=&#39;gray&#39;)
ax.plot([1.9, 3], [.05, .05], &#39;--&#39;, color=&#39;gray&#39;)
ax.set_xlabel(&#39;Z&#39;)
ax.set_ylabel(&#39;Tail Probabilites&#39;)
plt.show()</code></pre>
<div class="figure">
<img src="merged_files/merged_27_0.png" alt="" />
<p class="caption">png</p>
</div>
<p>Often we need to test for the significance of groups of coefficients simultaneously. For example, to test if a categorical variable with <span class="math inline">\(k\)</span> levels can be excluded from a model, we need to test whether the coefficients of the dummy variables used to represent the levels can all be set to zero.</p>
<p>Here we use the <span class="math inline">\(F\)</span> statistics,</p>
<p><span class="math display">\[\begin{equation}
F = \frac{(\text{RSS}_0-\text{RSS}_1)/(p_1-p_0)}{\text{RSS}_1/(N-p_1-1)},
\end{equation}\]</span></p>
<p>where</p>
<ul>
<li><span class="math inline">\(\text{RSS}_1\)</span> is for the bigger model with <span class="math inline">\(p_1+1\)</span> parameters and</li>
<li><span class="math inline">\(\text{RSS}_0\)</span> for the nested smaller model with <span class="math inline">\(p_0+1\)</span> parameters,</li>
<li>having <span class="math inline">\(p_1-p_0\)</span> parameters constrained to be zero.</li>
</ul>
<p>The <span class="math inline">\(F\)</span> statistic measures the change in residual sum-of-squares per additional parameter in the bigger model, and it is normalized by an estimate of <span class="math inline">\(\sigma^2\)</span>.</p>
<p>Under the Gaussian assumption, and the null hypothesis that the smaller models is correct, the <span class="math inline">\(F\)</span> statistics will have a <span class="math inline">\(F_{p_1-p_0,N-p_1-1}\)</span> distribution. It can be shown (Exercise 3.1) that the <span class="math inline">\(t\)</span>-statistic <span class="math inline">\(z_j = \hat\beta_j \big/ (\hat\sigma \sqrt{v_j})\)</span> are equivalent to the <span class="math inline">\(F\)</span> statistic for dropping the single coefficient <span class="math inline">\(\beta_j\)</span> from the model.</p>
<p>For large <span class="math inline">\(N\)</span>, the quantiles of <span class="math inline">\(F_{p_1-p_0,N-p_1-1}\)</span> approaches those of <span class="math inline">\(\chi^2_{p_1-p_0}/(p_1-p_0)\)</span>.</p>
</div>
<div id="confidence-intervals" class="section level3">
<h3>Confidence intervals</h3>
<p>Similarly, we can isolate <span class="math inline">\(\beta_j\)</span> in <span class="math inline">\(\hat\beta \sim N(\beta, \left(\mathbf{X}^T \mathbf{X}\right)^{-1}\sigma^2)\)</span> to obtain a <span class="math inline">\(1-2\alpha\)</span> confidence interval for <span class="math inline">\(\beta_j\)</span>:</p>
<p><span class="math display">\[\begin{equation}
\left(\hat\beta_j-z^{1-\alpha}v_j^{\frac{1}{2}}\hat\sigma, \hat\beta_j+z^{1-\alpha}v_j^{\frac{1}{2}}\hat\sigma\right),
\end{equation}\]</span></p>
<p>where <span class="math inline">\(z^{(1-\alpha)}\)</span> is the <span class="math inline">\(1-\alpha\)</span> percentile of the normal distribution:</p>
<p><span class="math display">\[\begin{align}
z^{(1-0.025)} &amp;= 1.96, \\
z^{(1-0.05)} &amp;= 1.645, \text{etc}.
\end{align}\]</span></p>
<p>Hence the standard practice of reporting <span class="math inline">\(\hat\beta \pm 2\cdot \text{se}(\hat\beta)\)</span> amounts to an approximate 95% confidence interval.</p>
<p>Even if the Gaussian error assumption does not hold, this interval will be approximately corrent, with its coverage approaching <span class="math inline">\(1-2\alpha\)</span> as the sample size <span class="math inline">\(N \rightarrow \infty\)</span>.</p>
<p>In a similar fashion we can obtain an approximate confidence set for the entire parameter vector <span class="math inline">\(\beta\)</span>, namely</p>
<p><span class="math display">\[\begin{equation}
C_\beta = \left\{ \beta \big| (\hat\beta-\beta)^T\mathbf{X}^T\mathbf{X}(\hat\beta-\beta) \le \hat\sigma^2{\chi^2_{p+1}}^{(1-\alpha)}\right\},
\end{equation}\]</span></p>
<p>where <span class="math inline">\({\chi_l^2}^{(1-\alpha)}\)</span> is the <span class="math inline">\(1-\alpha\)</span> percentile of the chi-squared distribution on <span class="math inline">\(l\)</span> degrees of freedom;</p>
<p><span class="math display">\[\begin{align}
{\chi_5^2}^{(1-0.05)} &amp;= 11.1, \\
{\chi_5^2}^{(1-0.1)} &amp;= 9.2.
\end{align}\]</span></p>
<p>This condifence set for <span class="math inline">\(\beta\)</span> generates a corresponding confidence set for the true function <span class="math inline">\(f(x) = x^T\beta\)</span>, namely</p>
<p><span class="math display">\[\begin{equation}
\left\{ x^T\beta \big| \beta \in C_\beta \right\}
\end{equation}\]</span></p>
<p>(Exercise 3.2; FIGURE 5.4).</p>
<pre class="python"><code>import scipy.stats
scipy.stats.norm(0, 1).pdf(0)</code></pre>
<pre><code>0.3989422804014327</code></pre>
<pre class="python"><code>scipy.stats.norm(0, 1).cdf(0)</code></pre>
<pre><code>0.5</code></pre>
<pre class="python"><code>#To find the probability that the variable has a value LESS than or equal
#let&#39;s say 1.64, you&#39;d use CDF cumulative Density Function
scipy.stats.norm.cdf(1.64,0,1)</code></pre>
<pre><code>0.9494974165258963</code></pre>
<pre class="python"><code>scipy.stats.norm.cdf(2,0,1)</code></pre>
<pre><code>0.9772498680518208</code></pre>
<pre class="python"><code>#To find the probability that the variable has a value GREATER than or
#equal to let&#39;s say 1.64, you&#39;d use SF Survival Function 
scipy.stats.norm.sf(1.64,0,1)</code></pre>
<pre><code>0.05050258347410371</code></pre>
<pre class="python"><code>#To find the variate for which the probability is given, let&#39;s say the 
#value which needed to provide a (1-0.05)% probability, you&#39;d use the 
#PPF Percent Point Function
scipy.stats.norm.ppf(.95,0,1)</code></pre>
<pre><code>1.6448536269514722</code></pre>
<pre class="python"><code>scipy.stats.norm.ppf(.975,0,1)</code></pre>
<pre><code>1.959963984540054</code></pre>
<pre class="python"><code>from scipy.stats import chi2
chi2.ppf(0.95, 5)</code></pre>
<pre><code>11.070497693516351</code></pre>
</div>
<div id="s-3.2.1.-example-prostate-cancer" class="section level2">
<h2><span class="math inline">\(\S\)</span> 3.2.1. Example: Prostate Cancer</h2>
<p>The data for this example come from a study by Stamey et al. (1989). They examined the correlation between the level of prostate-specific antigen and a number of clinical measures in men who were about to receive a radical prostatectomy.</p>
<p>The variables are</p>
<ul>
<li>log cancer volumn (<span class="math inline">\(\textsf{lcavol}\)</span>),</li>
<li>log prostate weight (<span class="math inline">\(\textsf{lweight}\)</span>),</li>
<li><span class="math inline">\(\textsf{age}\)</span>,</li>
<li>log of the amount of benign prostatic prostatic hyperplasia (<span class="math inline">\(\textsf{lbph}\)</span>),</li>
<li>seminal vesicle invasion (<span class="math inline">\(\textsf{svi}\)</span>),</li>
<li>log of capsular penetration (<span class="math inline">\(\textsf{lcp}\)</span>),</li>
<li>Gleason score (<span class="math inline">\(\textsf{gleason}\)</span>), and</li>
<li>percent of Gleason score 4 or 5 (<span class="math inline">\(\textsf{pgg45}\)</span>).</li>
</ul>
<pre class="python"><code>%matplotlib inline
import math

import pandas as pd
import scipy
import scipy.stats
import matplotlib.pyplot as plt
import numpy as np</code></pre>
<pre class="python"><code>data = pd.read_csv(&#39;../../data/prostate/prostate.data&#39;, delimiter=&#39;\t&#39;,
                   index_col=0)
data.head(7)</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
lcavol
</th>
<th>
lweight
</th>
<th>
age
</th>
<th>
lbph
</th>
<th>
svi
</th>
<th>
lcp
</th>
<th>
gleason
</th>
<th>
pgg45
</th>
<th>
lpsa
</th>
<th>
train
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
1
</th>
<td>
-0.579818
</td>
<td>
2.769459
</td>
<td>
50
</td>
<td>
-1.386294
</td>
<td>
0
</td>
<td>
-1.386294
</td>
<td>
6
</td>
<td>
0
</td>
<td>
-0.430783
</td>
<td>
T
</td>
</tr>
<tr>
<th>
2
</th>
<td>
-0.994252
</td>
<td>
3.319626
</td>
<td>
58
</td>
<td>
-1.386294
</td>
<td>
0
</td>
<td>
-1.386294
</td>
<td>
6
</td>
<td>
0
</td>
<td>
-0.162519
</td>
<td>
T
</td>
</tr>
<tr>
<th>
3
</th>
<td>
-0.510826
</td>
<td>
2.691243
</td>
<td>
74
</td>
<td>
-1.386294
</td>
<td>
0
</td>
<td>
-1.386294
</td>
<td>
7
</td>
<td>
20
</td>
<td>
-0.162519
</td>
<td>
T
</td>
</tr>
<tr>
<th>
4
</th>
<td>
-1.203973
</td>
<td>
3.282789
</td>
<td>
58
</td>
<td>
-1.386294
</td>
<td>
0
</td>
<td>
-1.386294
</td>
<td>
6
</td>
<td>
0
</td>
<td>
-0.162519
</td>
<td>
T
</td>
</tr>
<tr>
<th>
5
</th>
<td>
0.751416
</td>
<td>
3.432373
</td>
<td>
62
</td>
<td>
-1.386294
</td>
<td>
0
</td>
<td>
-1.386294
</td>
<td>
6
</td>
<td>
0
</td>
<td>
0.371564
</td>
<td>
T
</td>
</tr>
<tr>
<th>
6
</th>
<td>
-1.049822
</td>
<td>
3.228826
</td>
<td>
50
</td>
<td>
-1.386294
</td>
<td>
0
</td>
<td>
-1.386294
</td>
<td>
6
</td>
<td>
0
</td>
<td>
0.765468
</td>
<td>
T
</td>
</tr>
<tr>
<th>
7
</th>
<td>
0.737164
</td>
<td>
3.473518
</td>
<td>
64
</td>
<td>
0.615186
</td>
<td>
0
</td>
<td>
-1.386294
</td>
<td>
6
</td>
<td>
0
</td>
<td>
0.765468
</td>
<td>
F
</td>
</tr>
</tbody>
</table>
</div>
<pre class="python"><code>data.describe()</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
lcavol
</th>
<th>
lweight
</th>
<th>
age
</th>
<th>
lbph
</th>
<th>
svi
</th>
<th>
lcp
</th>
<th>
gleason
</th>
<th>
pgg45
</th>
<th>
lpsa
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
count
</th>
<td>
97.000000
</td>
<td>
97.000000
</td>
<td>
97.000000
</td>
<td>
97.000000
</td>
<td>
97.000000
</td>
<td>
97.000000
</td>
<td>
97.000000
</td>
<td>
97.000000
</td>
<td>
97.000000
</td>
</tr>
<tr>
<th>
mean
</th>
<td>
1.350010
</td>
<td>
3.628943
</td>
<td>
63.865979
</td>
<td>
0.100356
</td>
<td>
0.216495
</td>
<td>
-0.179366
</td>
<td>
6.752577
</td>
<td>
24.381443
</td>
<td>
2.478387
</td>
</tr>
<tr>
<th>
std
</th>
<td>
1.178625
</td>
<td>
0.428411
</td>
<td>
7.445117
</td>
<td>
1.450807
</td>
<td>
0.413995
</td>
<td>
1.398250
</td>
<td>
0.722134
</td>
<td>
28.204035
</td>
<td>
1.154329
</td>
</tr>
<tr>
<th>
min
</th>
<td>
-1.347074
</td>
<td>
2.374906
</td>
<td>
41.000000
</td>
<td>
-1.386294
</td>
<td>
0.000000
</td>
<td>
-1.386294
</td>
<td>
6.000000
</td>
<td>
0.000000
</td>
<td>
-0.430783
</td>
</tr>
<tr>
<th>
25%
</th>
<td>
0.512824
</td>
<td>
3.375880
</td>
<td>
60.000000
</td>
<td>
-1.386294
</td>
<td>
0.000000
</td>
<td>
-1.386294
</td>
<td>
6.000000
</td>
<td>
0.000000
</td>
<td>
1.731656
</td>
</tr>
<tr>
<th>
50%
</th>
<td>
1.446919
</td>
<td>
3.623007
</td>
<td>
65.000000
</td>
<td>
0.300105
</td>
<td>
0.000000
</td>
<td>
-0.798508
</td>
<td>
7.000000
</td>
<td>
15.000000
</td>
<td>
2.591516
</td>
</tr>
<tr>
<th>
75%
</th>
<td>
2.127041
</td>
<td>
3.876396
</td>
<td>
68.000000
</td>
<td>
1.558145
</td>
<td>
0.000000
</td>
<td>
1.178655
</td>
<td>
7.000000
</td>
<td>
40.000000
</td>
<td>
3.056357
</td>
</tr>
<tr>
<th>
max
</th>
<td>
3.821004
</td>
<td>
4.780383
</td>
<td>
79.000000
</td>
<td>
2.326302
</td>
<td>
1.000000
</td>
<td>
2.904165
</td>
<td>
9.000000
</td>
<td>
100.000000
</td>
<td>
5.582932
</td>
</tr>
</tbody>
</table>
</div>
<p>The correlation matrix of the predictors given in TABLE 3.1 shows many strong correlations.</p>
<pre class="python"><code>&quot;&quot;&quot;TABLE 3.1. Correlations of predictors in the prostate cancer data

It shows many string correlations. For example, that both `lcavol` and
`lcp` show a strong relationship with the response `lpsa`, and with each
other. We need to fit the effects jointly to untangle the relationships
between the predictors and the response.&quot;&quot;&quot;
data.corr()</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
lcavol
</th>
<th>
lweight
</th>
<th>
age
</th>
<th>
lbph
</th>
<th>
svi
</th>
<th>
lcp
</th>
<th>
gleason
</th>
<th>
pgg45
</th>
<th>
lpsa
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
lcavol
</th>
<td>
1.000000
</td>
<td>
0.280521
</td>
<td>
0.225000
</td>
<td>
0.027350
</td>
<td>
0.538845
</td>
<td>
0.675310
</td>
<td>
0.432417
</td>
<td>
0.433652
</td>
<td>
0.734460
</td>
</tr>
<tr>
<th>
lweight
</th>
<td>
0.280521
</td>
<td>
1.000000
</td>
<td>
0.347969
</td>
<td>
0.442264
</td>
<td>
0.155385
</td>
<td>
0.164537
</td>
<td>
0.056882
</td>
<td>
0.107354
</td>
<td>
0.433319
</td>
</tr>
<tr>
<th>
age
</th>
<td>
0.225000
</td>
<td>
0.347969
</td>
<td>
1.000000
</td>
<td>
0.350186
</td>
<td>
0.117658
</td>
<td>
0.127668
</td>
<td>
0.268892
</td>
<td>
0.276112
</td>
<td>
0.169593
</td>
</tr>
<tr>
<th>
lbph
</th>
<td>
0.027350
</td>
<td>
0.442264
</td>
<td>
0.350186
</td>
<td>
1.000000
</td>
<td>
-0.085843
</td>
<td>
-0.006999
</td>
<td>
0.077820
</td>
<td>
0.078460
</td>
<td>
0.179809
</td>
</tr>
<tr>
<th>
svi
</th>
<td>
0.538845
</td>
<td>
0.155385
</td>
<td>
0.117658
</td>
<td>
-0.085843
</td>
<td>
1.000000
</td>
<td>
0.673111
</td>
<td>
0.320412
</td>
<td>
0.457648
</td>
<td>
0.566218
</td>
</tr>
<tr>
<th>
lcp
</th>
<td>
0.675310
</td>
<td>
0.164537
</td>
<td>
0.127668
</td>
<td>
-0.006999
</td>
<td>
0.673111
</td>
<td>
1.000000
</td>
<td>
0.514830
</td>
<td>
0.631528
</td>
<td>
0.548813
</td>
</tr>
<tr>
<th>
gleason
</th>
<td>
0.432417
</td>
<td>
0.056882
</td>
<td>
0.268892
</td>
<td>
0.077820
</td>
<td>
0.320412
</td>
<td>
0.514830
</td>
<td>
1.000000
</td>
<td>
0.751905
</td>
<td>
0.368987
</td>
</tr>
<tr>
<th>
pgg45
</th>
<td>
0.433652
</td>
<td>
0.107354
</td>
<td>
0.276112
</td>
<td>
0.078460
</td>
<td>
0.457648
</td>
<td>
0.631528
</td>
<td>
0.751905
</td>
<td>
1.000000
</td>
<td>
0.422316
</td>
</tr>
<tr>
<th>
lpsa
</th>
<td>
0.734460
</td>
<td>
0.433319
</td>
<td>
0.169593
</td>
<td>
0.179809
</td>
<td>
0.566218
</td>
<td>
0.548813
</td>
<td>
0.368987
</td>
<td>
0.422316
</td>
<td>
1.000000
</td>
</tr>
</tbody>
</table>
</div>
<p>FIGURE 1.1 of Chapter 1 is a scatterplot matrix showing every pairwise plot between the variables.</p>
<p>We see that <span class="math inline">\(\textsf{svi}\)</span> is a binary variable, and <span class="math inline">\(\textsf{gleason}\)</span> is an ordered categorical variable.</p>
<p>We see, for example, that both <span class="math inline">\(\textsf{lcavol}\)</span> and <span class="math inline">\(\textsf{lcp}\)</span> show a strong relationship with the response <span class="math inline">\(\textsf{lpsa}\)</span>, and with each other. We need to fit the effects jointly to untangle the relationships between the predictors and the response.</p>
<pre class="python"><code>&quot;&quot;&quot;FIGURE 1.1. Scatterplot matrix of the prostate cancer data&quot;&quot;&quot;
pd.plotting.scatter_matrix(data, figsize=(10, 10))
plt.show()</code></pre>
<div class="figure">
<img src="merged_files/merged_45_0.png" alt="" />
<p class="caption">png</p>
</div>
<p>We fit a linear model to <span class="math inline">\(\textsf{lpsa}\)</span>, after first standardizing the predictors to have unit variance. We randomly split the dataset into a training set of size 67 and a test set of size 30. We applied least squares estimation to the training set, producing the estimates, standard errors and <span class="math inline">\(Z\)</span>-scores shown in TABLE 3.2.</p>
<p>The <span class="math inline">\(Z\)</span>-scores measure the effect of dropping that variable from the model. A <span class="math inline">\(Z\)</span>-score greater than 2 in absolute value is approximately significant at the 5% level. For our example, we have 9 parameters, and the 0.025 tail quatiles of the <span class="math inline">\(t_{67-9}\)</span> distributions are <span class="math inline">\(\pm 2.002\)</span>!</p>
<pre class="python"><code>from scipy.stats import t
t.ppf(0.975, 67-9)</code></pre>
<pre><code>2.0017174830120923</code></pre>
<pre class="python"><code>&quot;&quot;&quot;Table 3.2. Linear model fit to the prostate cancer data.

Roughly a Z score larger than two in absolute value is significant nonzero
at the p = 0.05 level.

We fit a linear model to the log of prostate-specific antigen, `lpsa`,
after first standardizing the predictors to have unit variance. We randomly
split the dataset into a training set of size 67 and a test set of size 30.
&quot;&quot;&quot;
data = pd.read_csv(&#39;../../data/prostate/prostate.data&#39;, delimiter=&#39;\t&#39;,
                   index_col=0)
data_y = data.pop(&#39;lpsa&#39;)
mask_train = data.pop(&#39;train&#39;)
data_x_normalized = data.apply(scipy.stats.zscore)

# data_normalized.describe()  # check it normalized!
data_x_normalized.describe()

data_x_train = data_x_normalized[mask_train == &#39;T&#39;]
data_y_train = data_y[mask_train == &#39;T&#39;]
data_x_test = data_x_normalized[mask_train == &#39;F&#39;]
data_y_test = data_y[mask_train == &#39;F&#39;]

size_train = sum(mask_train == &#39;T&#39;)
size_test = sum(mask_train == &#39;F&#39;)
size_predictor = len(data_x_train.columns)

mat_x = np.hstack((np.ones((size_train, 1)), data_x_train.values))
vec_y = data_y_train.values
mat_xt = np.transpose(mat_x)
mat_xx_inv = scipy.linalg.inv(mat_x.T @ mat_x)

ols_beta = mat_xx_inv @ mat_x.T @ vec_y
vec_y_fitted = mat_x @ ols_beta
est_sigma2 = sum((vec_y-vec_y_fitted)**2)/(size_train-size_predictor-1)

table_term = [&#39;Intercept&#39;] + list(data_x_train.columns)
table_coeff = ols_beta
table_stderr = [math.sqrt(mat_xx_inv[j, j]*est_sigma2)
                for j in range(size_predictor+1)]

print(&#39;{0:&gt;15} {1:&gt;15} {2:&gt;15} {3:&gt;15}&#39;.format(&#39;Term&#39;, &#39;Coefficient&#39;,
                                               &#39;Std. Error&#39;, &#39;Z Score&#39;))
print(&#39;-&#39;*64)
for term, coeff, stderr in zip(table_term, table_coeff, table_stderr):
    print(&#39;{0:&gt;15} {1:&gt;15f} {2:&gt;15f} {3:&gt;15f}&#39;.format(term, coeff,
                                                      stderr, coeff/stderr))</code></pre>
<pre><code>           Term     Coefficient      Std. Error         Z Score
----------------------------------------------------------------
      Intercept        2.464933        0.089315       27.598203
         lcavol        0.676016        0.125975        5.366290
        lweight        0.261694        0.095134        2.750789
            age       -0.140734        0.100819       -1.395909
           lbph        0.209061        0.101691        2.055846
            svi        0.303623        0.122962        2.469255
            lcp       -0.287002        0.153731       -1.866913
        gleason       -0.021195        0.144497       -0.146681
          pgg45        0.265576        0.152820        1.737840</code></pre>
<pre class="python"><code># RSS_1:
sum((vec_y-vec_y_fitted)**2)</code></pre>
<pre><code>29.42638445990841</code></pre>
<p>The predictor <span class="math inline">\(\textsf{lcavol}\)</span> shows the strongest effect, with <span class="math inline">\(\textsf{lweight}\)</span> and <span class="math inline">\(\textsf{svi}\)</span> also strong.</p>
<p>Notice that <span class="math inline">\(\textsf{lcp}\)</span> is not significant, once <span class="math inline">\(\textsf{lcavol}\)</span> is in the model (when used in a model without <span class="math inline">\(\textsf{lcavol}\)</span>, <span class="math inline">\(\textsf{lcp}\)</span> is strongly significant).</p>
<p>We can also test for the exclusion of a number of terms at once, using the <span class="math inline">\(F\)</span>-statistic. For example, we consider dropping all the non-significant terms in TABLE 3.2, namely <span class="math inline">\(\textsf{age}\)</span>, <span class="math inline">\(\textsf{gleason}\)</span>, and <span class="math inline">\(\textsf{pgg45}\)</span>. We get</p>
<p><span class="math display">\[\begin{equation}
F = \frac{(\text{RSS}_0-\text{RSS}_1)/(p_1-p_0)}{\text{RSS}_1/(N-p_1-1)}\\
=\frac{(32.81 - 29.43)/(9-5)}{29.43/(67-9)} = 1.67,
\end{equation}\]</span></p>
<p>which has a <span class="math inline">\(p\)</span>-value of</p>
<p><span class="math display">\[\begin{equation}
\text{Pr}(F_{4,58} \gt 1.67) = 0.17,
\end{equation}\]</span></p>
<p>and hence is not significant.</p>
<pre class="python"><code>&quot;&quot;&quot;F test for the exclusion of a number of terms at once

For example, we cansider dropping all the non-significant terms, namely
age, lcp, gleason, and pgg45.&quot;&quot;&quot;
print(&quot;Null hypothesis: beta[3]=beta[6]=beta[7]=beta[8]=0&quot;)
data_x_train_alt = data_x_train.drop([&#39;age&#39;, &#39;lcp&#39;, &#39;gleason&#39;, &#39;pgg45&#39;],
                                     axis=1)

size_predictor_alt = len(data_x_train_alt.columns)

mat_x_alt = np.hstack((np.ones((size_train, 1)),
                          data_x_train_alt.values))
ols_beta_alt = scipy.linalg.solve(mat_x_alt.T @ mat_x_alt,
                                  mat_x_alt.T @ vec_y)

vec_y_fitted_alt = mat_x_alt @ ols_beta_alt

rss0 = sum((vec_y-vec_y_fitted_alt)**2)
rss1 = sum((vec_y-vec_y_fitted)**2)
F_stat = (rss0-rss1)/(size_predictor-size_predictor_alt)*(size_train-size_predictor-1)/rss1
print(&#39;F = {}&#39;.format(F_stat))
print(&#39;Pr(F({dfn},{dfd}) &gt; {fstat:&gt;.2f}) = {prob:.2f}&#39;.format(
    dfn=size_predictor-size_predictor_alt,
    dfd=size_train-size_predictor-1,
    fstat=F_stat,
    prob=1-scipy.stats.f.cdf(F_stat,
                             dfn=size_predictor-size_predictor_alt,
                             dfd=size_train-size_predictor-1),
))</code></pre>
<pre><code>Null hypothesis: beta[3]=beta[6]=beta[7]=beta[8]=0
F = 1.6697548846375123
Pr(F(4,58) &gt; 1.67) = 0.17</code></pre>
<p>The mean prediction error on the test data is 0.521. In contrast, prediction using the mean training value of <span class="math inline">\(\textsf{lpsa}\)</span> has a test error of 1.057, which is called the “base error rate.”</p>
<pre class="python"><code>&quot;&quot;&quot;Prediction error on the test data

We can see that the linear model reduces the base error rate by about 50%
&quot;&quot;&quot;

mean_y_train = sum(vec_y)/size_train

mat_x_test = np.hstack((np.ones((size_test, 1)),
                           data_x_test.values))
vec_y_test = data_y_test.values
vec_y_test_fitted = mat_x_test @ ols_beta

err_base = sum((vec_y_test-mean_y_train)**2)/size_test
err_ols = sum((vec_y_test-vec_y_test_fitted)**2)/size_test

print(&#39;Base error rate = {}&#39;.format(err_base))
print(&#39;OLS error rate = {}&#39;.format(err_ols))</code></pre>
<pre><code>Base error rate = 1.0567332280603818
OLS error rate = 0.5212740055076004</code></pre>
<p>Hence the linear model reduces the base error rate by about 50%.</p>
<p>We will return to this example later to compare various selection and shrinkage methods.</p>
</div>
<div id="s-3.2.2.-the-gauss-markov-theorem" class="section level2">
<h2><span class="math inline">\(\S\)</span> 3.2.2. The Gauss-Markov Theorem</h2>
<p>One of the most famous results in statistics asserts that</p>
<blockquote>
<p>the least squares estimates of the parameter <span class="math inline">\(\beta\)</span> have the smallest variance among all linear unbiased estimates.</p>
</blockquote>
<p>We will make this precise here, and also make clear that</p>
<blockquote>
<p>the restriction to unbiased estimates is not necessarily a wise one.</p>
</blockquote>
<p>This observation will lead us to consider biased estimates such as ridge regression later in the chapter.</p>
<div id="the-statement-of-the-theorem" class="section level3">
<h3>The statement of the theorem</h3>
<p>We focus on estimation of any linear combination of the parameters <span class="math inline">\(\theta=a^T\beta\)</span>. The least squares estimate of <span class="math inline">\(a^T\beta\)</span> is</p>
<p><span class="math display">\[\begin{equation}
\hat\theta = a^T\hat\beta = a^T\left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T\mathbf{y}.
\end{equation}\]</span></p>
<p>Considering <span class="math inline">\(\mathbf{X}\)</span> to be fixed and the linear model is correct, <span class="math inline">\(a^T\beta\)</span> is unbiased since</p>
<p><span class="math display">\[\begin{align}
\text{E}(a^T\hat\beta) &amp;= \text{E}\left(a^T(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}\right) \\
&amp;= a^T(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{X}\beta \\
&amp;= a^T\beta
\end{align}\]</span></p>
<p>The Gauss-Markov Theorem states that if we have any other linear estimator <span class="math inline">\(\tilde\theta = \mathbf{c}^T\mathbf{y}\)</span> that is unbiased for <span class="math inline">\(a^T\beta\)</span>, that is, <span class="math inline">\(\text{E}(\mathbf{c}^T\mathbf{y})=a^T\beta\)</span>, then</p>
<p><span class="math display">\[\begin{equation}
\text{Var}(a^T\hat\beta) \le \text{Var}(\mathbf{c}^T\mathbf{y}).
\end{equation}\]</span></p>
<p>The proof (Exercise 3.3) uses the triangle inequality.</p>
<p>For simplicity we have stated the result in terms of estimation of a single parameter <span class="math inline">\(a^T \beta\)</span>, but with a few more definitions one can state it in terms of the entire parameter vector <span class="math inline">\(\beta\)</span> (Exercise 3.3).</p>
</div>
<div id="implications-of-the-gauss-markov-theorem" class="section level3">
<h3>Implications of the Gauss-Markov theorem</h3>
<p>Consider the mean squared error of an estimator <span class="math inline">\(\tilde\theta\)</span> of <span class="math inline">\(\theta\)</span>:</p>
<p><span class="math display">\[\begin{align}
\text{MSE}(\tilde\theta) &amp;= \text{E}\left(\tilde\theta-\theta\right)^2 \\
&amp;= \text{E}\left(\tilde\theta-\text{E}(\tilde\theta)+\text{E}(\tilde\theta)-\theta\right)^2\\
&amp;= \text{E}(\tilde\theta-\text{E}(\tilde\theta))^2+\text{E}(\text{E}(\tilde\theta)-\theta)^2+2\text{E}(\tilde\theta-\text{E}(\tilde\theta))(\text{E}(\tilde\theta)-\theta)\\
&amp;= \text{Var}\left(\tilde\theta\right) + \left[\text{E}\left(\tilde\theta\right)-\theta\right]^2 \\
&amp;= \text{Var} + \text{Bias}^2
\end{align}\]</span></p>
<p>The Gauss-Markov theorem implies that the least squares estimator has the smallest MSE of all linear estimators with no bias. However there may well exist a biased estimator with smaller MSE. Such an estimator would trade a little bias for a larger reduction in variance.</p>
<p>Biased estimates are commonly used. Any method that shrinks or sets to zero some of the least squares coefficients may result in a biased estimate. We discuss many examples, including variable subset selection and ridge regression, later in this chapter.</p>
<p>From a more pragmatic point of view, most models are distortions of the truth, and hence are biased; picking the right model amounts to creating the right balance between bias and variance. We go into these issues in more detail in Chapter 7.</p>
</div>
<div id="relation-between-prediction-accuracy-and-mse" class="section level3">
<h3>Relation between prediction accuracy and MSE</h3>
<p>MSE is intimately related to prediction accuracy, as discussed in Chapter 2.</p>
<p>Consider the prediction of the new response at input <span class="math inline">\(x_0\)</span>,</p>
<p><span class="math display">\[\begin{equation}
Y_0 = f(x_0) + \epsilon.
\end{equation}\]</span></p>
<p>Then the expected prediction error of an estimate <span class="math inline">\(\tilde{f}(x_0)=x_0^T\tilde\beta\)</span> is</p>
<p><span class="math display">\[\begin{align}
\text{E}(Y_0 - \tilde{f}(x_0))^2 &amp;= \text{E}\left(Y_0 -f(x_0)+f(x_0) - \tilde{f}(x_0)\right)^2\\
&amp;= \sigma^2 + \text{E}\left(x_o^T\tilde\beta - f(x_0)\right)^2 \\
&amp;= \sigma^2 + \text{MSE}\left(\tilde{f}(x_0)\right).
\end{align}\]</span></p>
<p>Therefore, expected prediction error and MSE differ only by the constant <span class="math inline">\(\sigma^2\)</span>, representing the variance of the new observation <span class="math inline">\(y_0\)</span>.</p>
<p>The first error component <span class="math inline">\(\sigma^2\)</span> is unrelated to what model is used to describe our data. It cannot be reduced for it exists in the true data generation process. The second source of error corresponding to the term <span class="math inline">\(\text{MSE}(\tilde{f}(x_0))\)</span> represents the error in the model and is under
control of the statistician. Thus, based on the above expression, if we minimize the <span class="math inline">\(\text{MSE}\)</span> of our estimator <span class="math inline">\(\tilde{f}(x_0)\)</span> we are effectively minimizing the expected (quadratic) prediction error which is our ultimate goal anyway.</p>
<p>We will explore methods that minimize the mean square error. The mean square error can be broken down into two terms: a model variance term and a model bias
squared term. We will explore methods that seek to keep the total contribution of these two terms as small as possible by explicitly considering the trade-offs that come from methods that might increase one of the terms while decreasing the other.</p>
</div>
</div>
<div id="s-3.2.3.-multiple-regression-from-simple-univariate-regression" class="section level2">
<h2><span class="math inline">\(\S\)</span> 3.2.3. Multiple Regression from Simple Univariate Regression</h2>
<p>If there are <span class="math inline">\(n\)</span> points <span class="math inline">\((x_1,y_1),(x_2,y_3),...,(x_n,y_n)\)</span>, the straight line <span class="math inline">\(y=a+bx\)</span> minimizing the sum of the squares of the <span style="color: red;"><strong>vertical distances</strong></span> from the data points to the line <span class="math inline">\(L=\sum_{i=1}^{n}(y_i-a-bx_i)^2\)</span>, then we take partial derivatives of L with respect to <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> and let them equal to <span class="math inline">\(0\)</span> to get least squares coefficients <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>:
<span class="math display">\[\frac{\partial L}{\partial b}=-2\sum_{i=1}^{n}(y_i-a-bx_i)x_i=0\]</span>, then <span class="math display">\[\sum_{i=1}^{n}x_iy_i=a\sum_{i=1}^{n}x_i+b\sum_{i=1}^{n}x_i^2\]</span><br />
And, <span class="math display">\[\frac{\partial L}{\partial a}=-2\sum_{i=1}^{n}(y_i-a-bx_i)=0\]</span>, then
<span class="math display">\[\sum_{i=1}^{n}y_i=na+b\sum_{i=1}^{n}x_i\]</span>
these 2 equations are:
<span class="math display">\[
\begin{bmatrix}
\displaystyle\sum_{i=1}^{n}x_i &amp; \displaystyle\sum_{i=1}^{n}x_i^2\\
n &amp; \displaystyle\sum_{i=1}^{n}x_i\\
\end{bmatrix}
\begin{bmatrix}
a\\
b
\end{bmatrix}
=
\begin{bmatrix}
\displaystyle\sum_{i=1}^{n}x_iy_i\\
\displaystyle\sum_{i=1}^{n}y_i
\end{bmatrix}
\]</span>
then, using <span style="color: red;"><strong>Cramer’s rule</strong></span>
<span class="math display">\[\begin{align}
b&amp;=\frac{\begin{bmatrix}
\displaystyle\sum_{i=1}^{n}x_i &amp; \displaystyle\sum_{i=1}^{n}x_iy_i\\
n &amp; \displaystyle\sum_{i=1}^{n}y_i\\
\end{bmatrix}}{\begin{bmatrix}
\displaystyle\sum_{i=1}^{n}x_i &amp; \displaystyle\sum_{i=1}^{n}x_i^2\\
n &amp; \displaystyle\sum_{i=1}^{n}x_i\\
\end{bmatrix}}\\
&amp;=\frac{(\displaystyle\sum_{i=1}^{n}x_i)(\displaystyle\sum_{i=1}^{n}y_i)-n(\displaystyle\sum_{i=1}^{n}x_iy_i)}{(\displaystyle\sum_{i=1}^{n}x_i)^2-n\displaystyle\sum_{i=1}^{n}x_i^2}\\
&amp;=\frac{n(\displaystyle\sum_{i=1}^{n}x_iy_i)-(\displaystyle\sum_{i=1}^{n}x_i)(\displaystyle\sum_{i=1}^{n}y_i)}{n\displaystyle\sum_{i=1}^{n}x_i^2-(\displaystyle\sum_{i=1}^{n}x_i)^2}\\
&amp;=\frac{(\displaystyle\sum_{i=1}^{n}x_iy_i)-\frac{1}{n}(\displaystyle\sum_{i=1}^{n}x_i)(\displaystyle\sum_{i=1}^{n}y_i)}{\displaystyle\sum_{i=1}^{n}x_i^2-\frac{1}{n}(\displaystyle\sum_{i=1}^{n}x_i)^2}\\
&amp;=\frac{\langle\mathbf x, \mathbf y\rangle-\frac{1}{n}\langle\mathbf x, \mathbf 1\rangle\langle\mathbf y, \mathbf 1\rangle}{\langle\mathbf x, \mathbf x\rangle-\frac{1}{n}\langle\mathbf x, \mathbf 1\rangle\langle\mathbf x, \mathbf 1\rangle}\\
&amp;=\frac{\langle\mathbf x, \mathbf y\rangle-\bar{x}\langle\mathbf y, \mathbf 1\rangle}{\langle\mathbf x, \mathbf x\rangle-\bar{x}\langle\mathbf x, \mathbf 1\rangle}\\
&amp;=\frac{\langle\mathbf x-\bar{x}\mathbf 1, \mathbf y\rangle}{\langle\mathbf x-\bar{x}\mathbf 1, \mathbf x\rangle}\\
&amp;=\frac{\langle\mathbf x-\bar{x}\mathbf 1, \mathbf y\rangle}{\langle\mathbf x-\bar{x}\mathbf 1, \mathbf x-\bar{x}\mathbf 1\rangle}
\end{align}\]</span></p>
<p>and <span class="math inline">\(a=\frac{\displaystyle\sum_{i=1}^{n}y_i-b\sum_{i=1}^{n}x_i}{n}=\bar y-b\bar x\)</span>, which shows point <span class="math inline">\((\bar x, \bar y)\)</span> is in the line.</p>
<blockquote>
<p>Thus we see that obtaining an estimate of the second coefficient <span class="math inline">\(b\)</span> is really two one-dimensional regressions followed in succession. We first regress <span class="math inline">\(\mathbf x\)</span> onto
<span class="math inline">\(\mathbf 1\)</span> and obtain the residual <span class="math inline">\(\mathbf z = \mathbf x − \bar{x}\mathbf 1\)</span>. We next regress <span class="math inline">\(\mathbf y\)</span> onto this residual <span class="math inline">\(\mathbf z\)</span>. The direct extension of these ideas results in Algorithm 3.1: <strong>Regression by Successive Orthogonalization</strong> or <strong>Gram-Schmidt</strong> for multiple regression.</p>
</blockquote>
<p>We call <span class="math inline">\(\hat y_i=a+bx_i\)</span> the <strong>predicted value</strong> of <span class="math inline">\(y_i\)</span>, and <span class="math inline">\(y_i-\hat y_i\)</span> the <span class="math inline">\(i^{th}\)</span> <strong>residual</strong>.</p>
<p>The linear model with <span class="math inline">\(p \gt 1\)</span> inputs is called <em>multiple linear regression model</em>.</p>
<p>The least squares estimates</p>
<p><span class="math display">\[\begin{equation}
\hat\beta = \left(\mathbf{X}^T \mathbf{X}\right)^{-1} \mathbf{X}^T \mathbf{y}
\end{equation}\]</span></p>
<p>for this model are best understood in terms of the estimates for the <em>univariate</em> (<span class="math inline">\(p=1\)</span>) linear model, as we indicate in this section.</p>
<p>Suppose a univariate model with no intercept, i.e.,</p>
<p><span class="math display">\[\begin{equation}
Y=X\beta+\epsilon.
\end{equation}\]</span></p>
<p>The least squares estimate and residuals are</p>
<p><span class="math display">\[\begin{align}
\hat\beta &amp;= \frac{\sum_1^N x_i y_i}{\sum_1^N x_i^2} = \frac{\langle\mathbf{x},\mathbf{y}\rangle}{\langle\mathbf{x},\mathbf{x}\rangle}, \\
r_i &amp;= y_i - x_i\hat\beta, \\
\mathbf{r} &amp;= \mathbf{y} - \mathbf{x}\hat\beta,
\end{align}\]</span></p>
<p>where
* <span class="math inline">\(\mathbf{y}=(y_1,\cdots,y_N)^T\)</span>,
* <span class="math inline">\(\mathbf{x}=(x_1,\cdots,x_N)^T\)</span> and
* <span class="math inline">\(\langle\cdot,\cdot\rangle\)</span> denotes the dot product notation.</p>
<p>As we will see,</p>
<blockquote>
<p>this simple univariate regression provides the building block for multiple linear regression.</p>
</blockquote>
<div id="building-blocks-for-multiple-linear-regression" class="section level3">
<h3>Building blocks for multiple linear regression</h3>
<p>Suppose next that the columns of the data matrix <span class="math inline">\(\mathbf{X} = \left[\mathbf{x}_1,\cdots,\mathbf{x}_p\right]\)</span> are orthogonal, i.e.,</p>
<p><span class="math display">\[\begin{equation}
\langle \mathbf{x}_j,\mathbf{x}_k\rangle = 0\text{ for all }j\neq k.
\end{equation}\]</span></p>
<p>Then it is easy to check that the multiple least squares estimates are equal to the univariate estimates:</p>
<p><span class="math display">\[\begin{equation}
\hat\beta_j = \frac{\langle\mathbf{x}_j,\mathbf{y}\rangle}{\langle\mathbf{x}_j,\mathbf{x}_j\rangle}, \forall j
\end{equation}\]</span></p>
<blockquote>
<p>In other words, when the inputs are orthogonal, they have no effect on each other’s parameter estimates in the model.</p>
</blockquote>
<p>Orthogonal inputs occur most often with balanced, designed experiments (where orthogonality is enforced), but almost never with observational data. Hence we will have to orthogonalize them in order to carry this idea further.</p>
<div id="orthogonalization" class="section level4">
<h4>Orthogonalization</h4>
<p>Suppose next that we have an intercept and a single input <span class="math inline">\(\mathbf{x}\)</span>. Then the least squares coefficient of <span class="math inline">\(\mathbf{x}\)</span> has the form</p>
<p><span class="math display">\[\begin{equation}
\hat\beta_1 = \frac{\langle\mathbf{x}-\bar{x}\mathbf{1},\mathbf{y}\rangle}{\langle\mathbf{x}-\bar{x}\mathbf{1},\mathbf{x}-\bar{x}\mathbf{1}\rangle},
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\bar{x} = \sum x_i /N\)</span> and <span class="math inline">\(\mathbf{1} = \mathbf{x}_0\)</span>. And also note that</p>
<p><span class="math display">\[\begin{equation}
\bar{x}\mathbf{1} = \frac{\langle\mathbf{1},\mathbf{x}\rangle}{\langle\mathbf{1},\mathbf{1}\rangle}\mathbf{1},
\end{equation}\]</span></p>
<p>which means the fitted value in the case we regress <span class="math inline">\(\mathbf{x}\)</span> on <span class="math inline">\(\mathbf{x}_0=\mathbf{1}\)</span>. Therefore we can view <span class="math inline">\(\hat\beta_1\)</span> as the result of two application of the simple regression with the following steps:
1. Regress <span class="math inline">\(\mathbf{x}\)</span> on <span class="math inline">\(\mathbf{1}\)</span> to produce the residual <span class="math inline">\(\mathbf{z}=\mathbf{x}-\bar{x}\mathbf{1}\)</span>;
2. regress <span class="math inline">\(\mathbf{y}\)</span> on the residual <span class="math inline">\(\mathbf{z}\)</span> to give the coefficient <span class="math inline">\(\hat\beta_1\)</span>.</p>
<blockquote>
<p>In this procedure, “regreess <span class="math inline">\(\mathbf{b}\)</span> on <span class="math inline">\(\mathbf{a}\)</span>” means a simple univariate regression of <span class="math inline">\(\mathbf{b}\)</span> on <span class="math inline">\(\mathbf{a}\)</span> with no intercept, producing coefficient <span class="math inline">\(\hat\gamma=\langle\mathbf{a},\mathbf{b}\rangle/\langle\mathbf{a},\mathbf{a}\rangle\)</span> and residual vector <span class="math inline">\(\mathbf{b}-\hat\gamma\mathbf{a}\)</span>. We say that <span class="math inline">\(\mathbf{b}\)</span> is adjusted for <span class="math inline">\(\mathbf{a}\)</span>, or is “orthogonalized” w.r.t. <span class="math inline">\(\mathbf{a}\)</span>.</p>
</blockquote>
<p>In other words,
1. orthogonalize <span class="math inline">\(\mathbf{x}\)</span> w.r.t. <span class="math inline">\(\mathbf{x}_0=\mathbf{1}\)</span>;
2. just a simple univariate regression, using the orthogonal predictors <span class="math inline">\(\mathbf{1}\)</span> and <span class="math inline">\(\mathbf{z}\)</span>.</p>
<p>FIGURE 3.4 shows this process for two general inputs <span class="math inline">\(\mathbf{x}_1\)</span> and <span class="math inline">\(\mathbf{x}_2\)</span>. The orthogonalization does not change the subspace spanned by <span class="math inline">\(\mathbf{x}_0\)</span> and <span class="math inline">\(\mathbf{x}_1\)</span>, it simply produces an orthogonal basis for representing it.</p>
<p>This recipe gerenalizes to the case of <span class="math inline">\(p\)</span> inputs, as shown in ALGORITHM 3.1.</p>
</div>
<div id="algorithm-3.1.-regression-by-successive-orthogonalization-gram-schmidt-orthogonilization" class="section level4">
<h4>ALGORITHM 3.1. Regression by successive orthogonalization (<strong>Gram-Schmidt orthogonilization</strong>)</h4>
<ol style="list-style-type: decimal">
<li>Initialize <span class="math inline">\(\mathbf{z}_0=\mathbf{x}_0=\mathbf{1}\)</span>.</li>
<li>For <span class="math inline">\(j = 1, 2, \cdots, p\)</span>,<br />
regress <span class="math inline">\(\mathbf{x}_j\)</span> on <span class="math inline">\(\mathbf{z}_0,\mathbf{z}_1,\cdots,\mathbf{z}_{j-1}\)</span> to produce</li>
</ol>
<ul>
<li>coefficients <span class="math inline">\(\hat\gamma_{lj} = \langle\mathbf{z}_l,\mathbf{x}_j\rangle/\langle\mathbf{z}_l,\mathbf{z}_l\rangle\)</span>, for <span class="math inline">\(l=0,\cdots, j-1\)</span> and</li>
<li>residual vector <span class="math inline">\(\mathbf{z}_j=\mathbf{x}_j - \sum_{k=0}^{j-1}\hat\gamma_{kj}\mathbf{z}_k\)</span>.</li>
</ul>
<ol start="3" style="list-style-type: decimal">
<li>Regress <span class="math inline">\(\mathbf{y}\)</span> on the residual <span class="math inline">\(\mathbf{z}_p\)</span> to give the estimate <span class="math inline">\(\hat\beta_p\)</span>,</li>
</ol>
<p><span class="math display">\[\begin{equation}
  \hat\beta_p = \frac{\langle\mathbf{z}_p,\mathbf{y}\rangle}{\langle\mathbf{z}_p,\mathbf{z}_p\rangle}.
  \end{equation}\]</span></p>
<p>Note that the inputs <span class="math inline">\(\mathbf{z}_0,\cdots,\mathbf{z}_{j-1}\)</span> in step 2 are orthogonal, hence the simple regression coefficients computed there are in fact also the multiple regression coefficients.</p>
<p>The result of this algorithm is</p>
<p><span class="math display">\[\begin{equation}
\hat\beta_p = \frac{\langle \mathbf{z}_p, \mathbf{y} \rangle}{\langle \mathbf{z}_p, \mathbf{z}_p \rangle}.
\end{equation}\]</span></p>
<p>Re-arranging the residual in step 2, we can see that each of the <span class="math inline">\(\mathbf{x}_j\)</span> is</p>
<p><span class="math display">\[\begin{equation}
\mathbf{x}_j = \mathbf{z}_j + \sum_{k=0}^{j-1} \hat\gamma_{kj}\mathbf{z}_k,
\end{equation}\]</span></p>
<p>which is a linear combination of the <span class="math inline">\(\mathbf{z}_k\)</span>, <span class="math inline">\(k \le j\)</span>. Since the <span class="math inline">\(\mathbf{z}_j\)</span> are all orthogonal, they form a basis for the <span class="math inline">\(\text{col}(\mathbf{X})\)</span>, and hence the least squares projection onto this subspace is <span class="math inline">\(\hat{\mathbf{y}}\)</span>.</p>
<p>Since <span class="math inline">\(\mathbf{z}_p\)</span> alone involves <span class="math inline">\(\mathbf{x}_p\)</span> (with coefficient 1), we see that the coefficient <span class="math inline">\(\hat\beta_p\)</span> is indeed the multiple regression coefficient of <span class="math inline">\(\mathbf{y}\)</span> on <span class="math inline">\(\mathbf{x}_p\)</span>. This key result exposes the effect of correlated inputs in mutiple regression.</p>
<p>Note also that by rearranging the <span class="math inline">\(j\)</span>th multiple regression coefficient is the univariate regression coefficient of <span class="math inline">\(\mathbf{y}\)</span> on <span class="math inline">\(\mathbf{x}_{j\cdot 012\cdots(j-1)(j+1)\cdots p}\)</span>, the residual after regressing <span class="math inline">\(\mathbf{x}_j\)</span> on <span class="math inline">\(\mathbf{x}_0,\mathbf{x}_1,\cdots,\mathbf{x}_{j-1},\mathbf{x}_{j+1},\cdots,\mathbf{x}_p\)</span>:</p>
<blockquote>
<p>The multiple regression coefficient <span class="math inline">\(\hat\beta_j\)</span> represents the additional contribution of <span class="math inline">\(\mathbf{x}_j\)</span> on <span class="math inline">\(\mathbf{y}\)</span>, after <span class="math inline">\(\mathbf{x}_j\)</span> has been adjusted for <span class="math inline">\(\mathbf{x}_0,\mathbf{x}_1,\cdots,\mathbf{x}_{j-1},\mathbf{x}_{j+1},\cdots,\mathbf{x}_p\)</span>.</p>
</blockquote>
</div>
<div id="correlated-variables" class="section level4">
<h4>Correlated variables</h4>
<p>If <span class="math inline">\(\mathbf{x}_p\)</span> is highly correlated with some of the other <span class="math inline">\(\mathbf{x}_k\)</span>’s, the residual vector <span class="math inline">\(\mathbf{z}_p\)</span> will be close to zero, then the coefficient <span class="math inline">\(\hat\beta_p\)</span> will be very unstable. This will be true for all the varialbes in the correlated set. In such situations, we might have all the <span class="math inline">\(Z\)</span>-scores be small – any one of the set can be deleted – yet we cannot delete them all.</p>
<p>We also obtain an alternative formula for the variance estimates,</p>
<p><span class="math display">\[\begin{align}
\text{Var}(\hat\beta_p) &amp;= \text{Var}\left(\frac{\langle\mathbf{z}_p,\mathbf{y}\rangle}{\langle\mathbf{z}_p,\mathbf{z}_p\rangle}\right) \\
&amp;= \frac{\text{Var}(\langle\mathbf{z}_p,\mathbf{y}\rangle)}{\|\mathbf{z}_p\|^2}\\
&amp;= \frac{\mathbf{z}_p^T\text{Var}(\mathbf{y})\mathbf{z}_p}{\|\mathbf{z}_p\|^2}\\
&amp;= \frac{\mathbf{z}_p^T(\sigma^2\mathbf{I})\mathbf{z}_p}{\|\mathbf{z}_p\|^2}\\
&amp;= \frac{\sigma^2}{\|\mathbf{z}_p\|^2}.
\end{align}\]</span></p>
<p>In other words, the precision with which we can estimate with <span class="math inline">\(\hat\beta_p\)</span> depends on the length of the residual vector <span class="math inline">\(\mathbf{z}_p\)</span>; this represents how much of <span class="math inline">\(\mathbf{x}_p\)</span> is unexplained by the other <span class="math inline">\(\mathbf{x}_k\)</span>’s.</p>
</div>
</div>
<div id="gram-schmidt-procedure-and-qr-decomposition" class="section level3">
<h3>Gram-Schmidt procedure and QR decomposition</h3>
<p>Algorithm 3.1 is known as the <em>Gram-Schmidt</em> procedure for multiple regression, and is also a useful numerical strategy for computing the estimates. We can obtain from it not just <span class="math inline">\(\hat\beta_p\)</span>, but also the entire multiple least squares fit (Exercise 3.4).</p>
<p>We can represent step 2 of Algorithm 3.1 in matrix form:</p>
<p><span class="math display">\[\begin{equation}
\mathbf{X} = \mathbf{Z\Gamma},
\end{equation}\]</span></p>
<p>where</p>
<ul>
<li><span class="math inline">\(\mathbf{Z}\)</span> has as columns the <span class="math inline">\(\mathbf{z}_j\)</span> (in order)</li>
<li><span class="math inline">\(\mathbf{\Gamma}\)</span> is the upper triangular matrix with entries <span class="math inline">\(\hat\gamma_{kj}\)</span>.</li>
</ul>
<p>Introducing the diagonal matrix <span class="math inline">\(\mathbf{D}\)</span> with <span class="math inline">\(D_{jj}=\|\mathbf{z}_j\|\)</span>, we get</p>
<p><span class="math display">\[\begin{align}
\mathbf{X} &amp;= \mathbf{ZD}^{-1}\mathbf{D\Gamma} \\
&amp;= \mathbf{QR},
\end{align}\]</span></p>
<p>the so-called <em>QR decomposition</em> of <span class="math inline">\(\mathbf{X}\)</span>. Here</p>
<ul>
<li><span class="math inline">\(\mathbf{Q}\)</span> is an <span class="math inline">\(N\times(p+1)\)</span> orthogonal matrix s.t. <span class="math inline">\(\mathbf{Q}^T\mathbf{Q}=\mathbf{I}\)</span>,</li>
<li><span class="math inline">\(\mathbf{R}\)</span> is a <span class="math inline">\((p+1)\times(p+1)\)</span> upper triangular matrix.</li>
</ul>
<p>The QR decomposition represents a convenient orthogonal basis for the <span class="math inline">\(\text{col}(\mathbf{X})\)</span>. It is easy to see, for example, that the ordinary least squares (<strong>OLS</strong>) solution is given by</p>
<p><span class="math display">\[\begin{align}
\hat\beta &amp;= \left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T\mathbf{y}\\
&amp;=\left(\mathbf{R}^T\mathbf{Q}^T\mathbf{Q}\mathbf{R}\right)^{-1}\mathbf{R}^T\mathbf{Q}^T\mathbf{y}\\
&amp;=\left(\mathbf{R}^T\mathbf{R}\right)^{-1}\mathbf{R}^T\mathbf{Q}^T\mathbf{y}\\
&amp;=\mathbf{R}^{-1}\mathbf{R}^{-T}\mathbf{R}^T\mathbf{Q}^T\mathbf{y}\\
&amp;=\mathbf{R}^{-1}\mathbf{Q}^T\mathbf{y}, \\
\hat{\mathbf{y}} &amp;= \mathbf{X}\hat\beta =\mathbf{Q}\mathbf{R}\mathbf{R}^{-1}\mathbf{Q}^T\mathbf{y}=\mathbf{QQ}^T\mathbf{y}.
\end{align}\]</span></p>
<p>This last equation expresses the fact in ordinary least squares we obtain our fitted vector <span class="math inline">\(\mathbf y\)</span> by first computing the coefficients of <span class="math inline">\(\mathbf y\)</span> in terms of the basis spanned by the columns of <span class="math inline">\(\mathbf Q\)</span> (these coefficients are given by the vector <span class="math inline">\(\mathbf Q^T\mathbf y\)</span>). We next construct <span class="math inline">\(\hat{\mathbf y}\)</span> using these numbers as the coefficients of the column vectors in <span class="math inline">\(\mathbf Q\)</span> (this is the product <span class="math inline">\(\mathbf{QQ}^T\mathbf y\)</span>).</p>
<p>Note that the triangular matrix <span class="math inline">\(\mathbf{R}\)</span> makes it easy to solve (Exercise 3.4).</p>
</div>
</div>
<div id="s-3.2.4.-multiple-outputs" class="section level2">
<h2><span class="math inline">\(\S\)</span> 3.2.4. Multiple Outputs</h2>
<p>Suppose we have</p>
<ul>
<li>multiple outputs <span class="math inline">\(Y_1,Y_2,\cdots,Y_K\)</span></li>
<li>inputs <span class="math inline">\(X_0,X_1,\cdots,X_p\)</span></li>
<li>a linear model for each output<br />
<span class="math display">\[\begin{align}
Y_k &amp;= \beta_{0k} + \sum_{j=1}^p X_j\beta_{jk} + \epsilon_k \\
&amp;= f_k(X) + \epsilon_k
\end{align}\]</span></li>
</ul>
<blockquote>
<p>the coefficients for the <span class="math inline">\(k\)</span>th outcome are just the least squares estimates in the regression of <span class="math inline">\(y_k\)</span> on <span class="math inline">\(x_0,x_1,\cdots,x_p\)</span> . Multiple outputs do not affect one another’s least squares estimates.</p>
</blockquote>
<p>With <span class="math inline">\(N\)</span> training cases we can write the model in matrix notation</p>
<p><span class="math display">\[\begin{equation}
\mathbf{Y}=\mathbf{XB}+\mathbf{E},
\end{equation}\]</span></p>
<p>where</p>
<ul>
<li><span class="math inline">\(\mathbf{Y}\)</span> is <span class="math inline">\(N\times K\)</span> with <span class="math inline">\(ik\)</span> entry <span class="math inline">\(y_{ik}\)</span>,</li>
<li><span class="math inline">\(\mathbf{X}\)</span> is <span class="math inline">\(N\times(p+1)\)</span> input matrix,</li>
<li><span class="math inline">\(\mathbf{B}\)</span> is <span class="math inline">\((p+1)\times K\)</span> parameter matrix,</li>
<li><span class="math inline">\(\mathbf{E}\)</span> is <span class="math inline">\(N\times K\)</span> error matrix.</li>
</ul>
<p>A straightforward generalization of the univariate loss function is</p>
<p><span class="math display">\[\begin{align}
\text{RSS}(\mathbf{B}) &amp;= \sum_{k=1}^K \sum_{i=1}^N \left( y_{ik} - f_k(x_i) \right)^2 \\
&amp;= \text{trace}\left( (\mathbf{Y}-\mathbf{XB})^T(\mathbf{Y}-\mathbf{XB}) \right)
\end{align}\]</span></p>
<p>The least squares estimates have exactly the same form as before</p>
<p><span class="math display">\[\begin{equation}
\hat{\mathbf{B}} = \left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T\mathbf{Y}.
\end{equation}\]</span></p>
<div id="correlated-errors" class="section level3">
<h3>Correlated errors</h3>
<p>If the errors <span class="math inline">\(\epsilon = (\epsilon_1,\cdots,\epsilon_K)\)</span> are correlated with <span class="math inline">\(\text{Cov}(\epsilon)=\mathbf{\Sigma}\)</span>, then the multivariate weighted criterion</p>
<p><span class="math display">\[\begin{equation}
\text{RSS}(\mathbf{B};\mathbf{\Sigma}) = \sum_{i=1}^N (y_i-f(x_i))^T \mathbf{\Sigma}^{-1} (y_i-f(x_i))
\end{equation}\]</span></p>
<p>arises naturally from multivariate Gaussian theory. Here</p>
<ul>
<li><span class="math inline">\(f(x) = \left(f_1(x),\cdots,f_K(x)\right)^T\)</span> is the vector function,</li>
<li><span class="math inline">\(y_i\)</span> the vector of <span class="math inline">\(K\)</span> responses for observation <span class="math inline">\(i\)</span>.
However, the solution is again the same with ignoring the correlations as</li>
</ul>
<p><span class="math display">\[\begin{equation}
\hat{\mathbf{B}} = \left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T\mathbf{Y}.
\end{equation}\]</span></p>
<p>In Section 3.7 we pursue the multiple output problem and consider situations where it does pay to combine the regressions.</p>
</div>
</div>
</div>
<div id="s-3.3.-subset-selection" class="section level1">
<h1><span class="math inline">\(\S\)</span> 3.3. Subset Selection</h1>
<p>Two reasons why we are often not satisfied with the least squares estimates:
1. Prediction accuracy.<br />
The least squares estimate often have low bias but large variance. Prediction accuracy can sometimes be improved by shrinking or setting some coefficients to zero. By doing so we sacrifice a little bit of bias to reduce the variance of the predicted values, and hence may improve the overall prediction accuracy.
2. Interpretation.<br />
With a large number of predictiors, we often would like to determine a smaller subset that exhibit the strongest effects. In order to get the “big picture,” we are willing to sacrifice some of the small details.</p>
<p>In this section we describe a number of approaches to variable subset selection with linear regression. In later sections we discuss shrinkage and hybrid approaches for controlling variance, as well as other dimension-reduction strategies. These all fall under the general heading model selection. Model selection is not restricted to linear models; Chapter 7 covers this topic in some detail.</p>
<p>With subset selection we retain only a subset of the variables, and eliminate the rest from the model. Least squares regression is used to estimate the coefficients of the inputs that are retained. There are a number of different strategies for choosing the subset.</p>
<div id="s-3.3.1.-best-subset-selection" class="section level2">
<h2><span class="math inline">\(\S\)</span> 3.3.1. Best-Subset Selection</h2>
<p>Best subset regression finds for each <span class="math inline">\(k\in\lbrace0,1,2,\cdots,p\rbrace\)</span> the subset of size <span class="math inline">\(k\)</span> that gives the smallest residual sum of squares. An efficient algorithm – the <strong>leaps and bounds procedure</strong> (Furnival and Wilson, 1974) – makes this feasible for <span class="math inline">\(p\)</span> as large as 30 or 40.</p>
<pre class="python"><code>&quot;&quot;&quot;FIGURE 3.5. All possible subset models for the prostate cancer example

At each subset size is shown the residual sum-of-squares for each model of
that size.&quot;&quot;&quot;
import math
import collections
import itertools
import functools
import operator as op

import pandas as pd
import scipy
import scipy.stats
import matplotlib.pyplot as plt
import numpy as np</code></pre>
<pre class="python"><code>data = pd.read_csv(&#39;../../data/prostate/prostate.data&#39;, delimiter=&#39;\t&#39;,
                   index_col=0)
data_y = data[&#39;lpsa&#39;]
data_x_normalized = data.drop([&#39;train&#39;, &#39;lpsa&#39;], axis=1)\
                        .apply(scipy.stats.zscore)
# data_normalized.describe()  # check it normalized!
data_x_normalized.describe()</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
lcavol
</th>
<th>
lweight
</th>
<th>
age
</th>
<th>
lbph
</th>
<th>
svi
</th>
<th>
lcp
</th>
<th>
gleason
</th>
<th>
pgg45
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
count
</th>
<td>
9.700000e+01
</td>
<td>
9.700000e+01
</td>
<td>
9.700000e+01
</td>
<td>
9.700000e+01
</td>
<td>
9.700000e+01
</td>
<td>
9.700000e+01
</td>
<td>
9.700000e+01
</td>
<td>
9.700000e+01
</td>
</tr>
<tr>
<th>
mean
</th>
<td>
4.578239e-17
</td>
<td>
6.844468e-16
</td>
<td>
4.131861e-16
</td>
<td>
-2.432190e-17
</td>
<td>
-3.662591e-17
</td>
<td>
3.662591e-17
</td>
<td>
-2.174664e-17
</td>
<td>
5.636957e-17
</td>
</tr>
<tr>
<th>
std
</th>
<td>
1.005195e+00
</td>
<td>
1.005195e+00
</td>
<td>
1.005195e+00
</td>
<td>
1.005195e+00
</td>
<td>
1.005195e+00
</td>
<td>
1.005195e+00
</td>
<td>
1.005195e+00
</td>
<td>
1.005195e+00
</td>
</tr>
<tr>
<th>
min
</th>
<td>
-2.300218e+00
</td>
<td>
-2.942386e+00
</td>
<td>
-3.087227e+00
</td>
<td>
-1.030029e+00
</td>
<td>
-5.256575e-01
</td>
<td>
-8.676552e-01
</td>
<td>
-1.047571e+00
</td>
<td>
-8.689573e-01
</td>
</tr>
<tr>
<th>
25%
</th>
<td>
-7.139973e-01
</td>
<td>
-5.937689e-01
</td>
<td>
-5.219612e-01
</td>
<td>
-1.030029e+00
</td>
<td>
-5.256575e-01
</td>
<td>
-8.676552e-01
</td>
<td>
-1.047571e+00
</td>
<td>
-8.689573e-01
</td>
</tr>
<tr>
<th>
50%
</th>
<td>
8.264956e-02
</td>
<td>
-1.392703e-02
</td>
<td>
1.531086e-01
</td>
<td>
1.383966e-01
</td>
<td>
-5.256575e-01
</td>
<td>
-4.450983e-01
</td>
<td>
3.444069e-01
</td>
<td>
-3.343557e-01
</td>
</tr>
<tr>
<th>
75%
</th>
<td>
6.626939e-01
</td>
<td>
5.806076e-01
</td>
<td>
5.581506e-01
</td>
<td>
1.010033e+00
</td>
<td>
-5.256575e-01
</td>
<td>
9.762744e-01
</td>
<td>
3.444069e-01
</td>
<td>
5.566470e-01
</td>
</tr>
<tr>
<th>
max
</th>
<td>
2.107397e+00
</td>
<td>
2.701661e+00
</td>
<td>
2.043304e+00
</td>
<td>
1.542252e+00
</td>
<td>
1.902379e+00
</td>
<td>
2.216735e+00
</td>
<td>
3.128363e+00
</td>
<td>
2.695054e+00
</td>
</tr>
</tbody>
</table>
</div>
<pre class="python"><code>data_x_normalized.columns</code></pre>
<pre><code>Index([&#39;lcavol&#39;, &#39;lweight&#39;, &#39;age&#39;, &#39;lbph&#39;, &#39;svi&#39;, &#39;lcp&#39;, &#39;gleason&#39;, &#39;pgg45&#39;], dtype=&#39;object&#39;)</code></pre>
<pre class="python"><code>data_x_train = data_x_normalized[data[&#39;train&#39;] == &#39;T&#39;]
data_y_train = data_y[data[&#39;train&#39;] == &#39;T&#39;]
data_x_test = data_x_normalized[data[&#39;train&#39;] == &#39;F&#39;]
data_y_test = data_y[data[&#39;train&#39;] == &#39;F&#39;]
vec_y = data_y_train.values
vec_y_test = data_y_test.values

size_train = sum(data[&#39;train&#39;] == &#39;T&#39;)
size_test = sum(data[&#39;train&#39;] == &#39;F&#39;)
size_predictor = len(data_x_train.columns)</code></pre>
<p>Use function ‘ols_with_column_names’ to concatenate vector <span class="math inline">\(\mathbf 1\)</span> and columns of ‘data_x_train’ to get <span class="math inline">\(\mathbf X\)</span>, then solve <span class="math display">\[\hat{\mathbf{\beta}} = \left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T\mathbf{y}.\]</span> to get <span class="math inline">\(\hat{\mathbf{\beta}}\)</span>, then calculate ‘vec_y_fitted’ <span class="math display">\[\text{vec_y_fitted}=\mathbf{X}\hat{\mathbf{\beta}}\]</span></p>
<pre class="python"><code>def ols_with_column_names(df_x:pd.DataFrame, vec_y:np.ndarray,
                          *column_names) -&gt;float:
    if column_names:
        df_x_subset = df_x[list(column_names)]
        mat_x = np.hstack((np.ones((len(df_x), 1)),
                           df_x_subset.values))
    else:
        mat_x = np.ones((len(df_x), 1))
    ols_beta = scipy.linalg.solve(mat_x.T @ mat_x, mat_x.T @ vec_y)
    vec_y_fitted = mat_x @ ols_beta
    return ols_beta, vec_y_fitted</code></pre>
<pre class="python"><code># The reduce(fun,seq) function is used to apply a particular function passed 
# in its argument to all of the list elements mentioned in the sequence passed along
print (functools.reduce(lambda a,b : a*b, range(8, 2, -1)))</code></pre>
<pre><code>20160</code></pre>
<pre class="python"><code># calculate the factorial a!/b!
print(functools.reduce(op.mul, range(8, 2, -1)))</code></pre>
<pre><code>20160</code></pre>
<blockquote>
<p>Use function ‘ncr’ to calculate the Combinations: <span class="math display">\[{n \choose r}=\frac{n!}{(n-r)!r!}\]</span></p>
</blockquote>
<blockquote>
<p>Use ‘ols_with_subset_size’ function to calculate <span class="math inline">\(\hat{\mathbf{\beta}}\)</span> and <span class="math inline">\(\text{RSS}\)</span> of every combinations of the columns of every possible size.</p>
</blockquote>
<blockquote>
<p>Use ‘itertools.combinations(p,r)’ to iter r-length tuples, in sorted order, no repeated elements.</p>
</blockquote>
<pre class="python"><code>def ncr(n:int, r:int) -&gt;int:
    &quot;&quot;&quot;Compute combination number nCr&quot;&quot;&quot;
    r = min(r, n-r)
    if r == 0:
        return 1
    numer = functools.reduce(op.mul, range(n, n-r, -1))
    denom = functools.reduce(op.mul, range(1, r+1))
    return numer//denom


def ols_with_subset_size(df_x:pd.DataFrame, vec_y:np.ndarray,
                         k:int) -&gt;np.ndarray:
    if k == 0:
        ols_beta, vec_y_fitted = ols_with_column_names(df_x, vec_y)
        return [{
            &#39;column_names&#39;: &#39;constant&#39;,
            &#39;beta&#39;: ols_beta,
            &#39;rss&#39;: ((vec_y-vec_y_fitted)**2).sum(),
        }]
    column_combi = itertools.combinations(data_x_normalized.columns, k)
    result = []
    for column_names in column_combi:
        ols_beta, vec_y_fitted = ols_with_column_names(df_x, vec_y,
                                                       *column_names)
        result.append({
            &#39;column_names&#39;: column_names,
            &#39;beta&#39;: ols_beta,
            &#39;rss&#39;: ((vec_y-vec_y_fitted)**2).sum(),
        })
    return result</code></pre>
<pre class="python"><code># The number_of_combinations of different sizes
for i in range(1,size_predictor):
    print(ncr(size_predictor, i))</code></pre>
<pre><code>8
28
56
70
56
28
8</code></pre>
<pre class="python"><code>np.ones(28)*2</code></pre>
<pre><code>array([2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,
       2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.])</code></pre>
<pre class="python"><code>fig35 = plt.figure(figsize=(10, 10))
ax = fig35.add_subplot(1, 1, 1)

rss_min = []
for k in range(size_predictor+1):
    number_of_combinations = ncr(size_predictor, k)
    ols_list = ols_with_subset_size(data_x_train, vec_y, k)
    ax.plot(np.ones(number_of_combinations)*k, [d[&#39;rss&#39;] for d in ols_list],
            &#39;o&#39;, color=&#39;gray&#39;, markersize=3)
    ols_best = min(ols_list, key=op.itemgetter(&#39;rss&#39;))
    rss_min.append(ols_best[&#39;rss&#39;])
ax.plot(range(size_predictor+1), rss_min, &#39;o--&#39;, color=&#39;r&#39;, markersize=6)
ax.set_xlabel(&#39;Subset Size k&#39;)
ax.set_ylim(0, 100)
ax.set_ylabel(&#39;Residual Sum-of-Squares&#39;)
plt.show()</code></pre>
<div class="figure">
<img src="merged_files/merged_79_0.png" alt="" />
<p class="caption">png</p>
</div>
<pre class="python"><code>ols_list</code></pre>
<pre><code>[{&#39;column_names&#39;: (&#39;lcavol&#39;,
   &#39;lweight&#39;,
   &#39;age&#39;,
   &#39;lbph&#39;,
   &#39;svi&#39;,
   &#39;lcp&#39;,
   &#39;gleason&#39;,
   &#39;pgg45&#39;),
  &#39;beta&#39;: array([ 2.46493292,  0.67601634,  0.26169361, -0.14073374,  0.20906052,
          0.30362332, -0.28700184, -0.02119493,  0.26557614]),
  &#39;rss&#39;: 29.426384459908405}]</code></pre>
<p>Note that the best subset of size 2, for example, need not include the variable that was in the best subset if size 1. The best-subset curve is necessarily decreasing, so cannot be used to select the subsit size <span class="math inline">\(k\)</span>. The question of how to choose <span class="math inline">\(k\)</span> involves the tradeoff between bias and variance, along with more subjective desire for parsimony. There are a number of criteria that one may use; typically we choose the smallest model that minimizes an estimate of the expected prediction error.</p>
<p>Many of the other approaches that we discuss in this chapter are similar, in that they use the training data to produce a sequence of models varying in complexity and indexed by a single parameter. In the next section we use cross-validation to estimate prediction error and select <span class="math inline">\(k\)</span>; the <span class="math inline">\(\text{AIC}\)</span> criterion is a popular alternative.</p>
<pre class="python"><code>&quot;&quot;&quot;Hitters dataset from ISLR&quot;&quot;&quot;
%matplotlib inline
import pandas as pd
import numpy as np
import itertools
import time
import statsmodels.api as sm
import matplotlib.pyplot as plt</code></pre>
<pre class="python"><code># Here we apply the best subset selection approach to the Hitters data. 
# We wish to predict a baseball player’s Salary on the basis of various statistics associated with performance in the previous year.
hitters_df = pd.read_csv(&#39;../../data/Hitters.csv&#39;)
hitters_df.head()</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
Unnamed: 0
</th>
<th>
AtBat
</th>
<th>
Hits
</th>
<th>
HmRun
</th>
<th>
Runs
</th>
<th>
RBI
</th>
<th>
Walks
</th>
<th>
Years
</th>
<th>
CAtBat
</th>
<th>
CHits
</th>
<th>
…
</th>
<th>
CRuns
</th>
<th>
CRBI
</th>
<th>
CWalks
</th>
<th>
League
</th>
<th>
Division
</th>
<th>
PutOuts
</th>
<th>
Assists
</th>
<th>
Errors
</th>
<th>
Salary
</th>
<th>
NewLeague
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
-Andy Allanson
</td>
<td>
293
</td>
<td>
66
</td>
<td>
1
</td>
<td>
30
</td>
<td>
29
</td>
<td>
14
</td>
<td>
1
</td>
<td>
293
</td>
<td>
66
</td>
<td>
…
</td>
<td>
30
</td>
<td>
29
</td>
<td>
14
</td>
<td>
A
</td>
<td>
E
</td>
<td>
446
</td>
<td>
33
</td>
<td>
20
</td>
<td>
NaN
</td>
<td>
A
</td>
</tr>
<tr>
<th>
1
</th>
<td>
-Alan Ashby
</td>
<td>
315
</td>
<td>
81
</td>
<td>
7
</td>
<td>
24
</td>
<td>
38
</td>
<td>
39
</td>
<td>
14
</td>
<td>
3449
</td>
<td>
835
</td>
<td>
…
</td>
<td>
321
</td>
<td>
414
</td>
<td>
375
</td>
<td>
N
</td>
<td>
W
</td>
<td>
632
</td>
<td>
43
</td>
<td>
10
</td>
<td>
475.0
</td>
<td>
N
</td>
</tr>
<tr>
<th>
2
</th>
<td>
-Alvin Davis
</td>
<td>
479
</td>
<td>
130
</td>
<td>
18
</td>
<td>
66
</td>
<td>
72
</td>
<td>
76
</td>
<td>
3
</td>
<td>
1624
</td>
<td>
457
</td>
<td>
…
</td>
<td>
224
</td>
<td>
266
</td>
<td>
263
</td>
<td>
A
</td>
<td>
W
</td>
<td>
880
</td>
<td>
82
</td>
<td>
14
</td>
<td>
480.0
</td>
<td>
A
</td>
</tr>
<tr>
<th>
3
</th>
<td>
-Andre Dawson
</td>
<td>
496
</td>
<td>
141
</td>
<td>
20
</td>
<td>
65
</td>
<td>
78
</td>
<td>
37
</td>
<td>
11
</td>
<td>
5628
</td>
<td>
1575
</td>
<td>
…
</td>
<td>
828
</td>
<td>
838
</td>
<td>
354
</td>
<td>
N
</td>
<td>
E
</td>
<td>
200
</td>
<td>
11
</td>
<td>
3
</td>
<td>
500.0
</td>
<td>
N
</td>
</tr>
<tr>
<th>
4
</th>
<td>
-Andres Galarraga
</td>
<td>
321
</td>
<td>
87
</td>
<td>
10
</td>
<td>
39
</td>
<td>
42
</td>
<td>
30
</td>
<td>
2
</td>
<td>
396
</td>
<td>
101
</td>
<td>
…
</td>
<td>
48
</td>
<td>
46
</td>
<td>
33
</td>
<td>
N
</td>
<td>
E
</td>
<td>
805
</td>
<td>
40
</td>
<td>
4
</td>
<td>
91.5
</td>
<td>
N
</td>
</tr>
</tbody>
</table>
<p>
5 rows × 21 columns
</p>
</div>
<pre class="python"><code># the Salary variable is missing for some of the players
print(&quot;Number of null values:&quot;, hitters_df[&quot;Salary&quot;].isnull().sum())</code></pre>
<pre><code>Number of null values: 59</code></pre>
<pre class="python"><code># We see that Salary is missing for 59 players. 
# The dropna() function removes all of the rows that have missing values in any variable:
# Print the dimensions of the original Hitters data (322 rows x 20 columns)
print(&quot;Dimensions of original data:&quot;, hitters_df.shape)

# Drop any rows the contain missing values, along with the player names
hitters_df_clean = hitters_df.dropna(axis=0).drop(&#39;Unnamed: 0&#39;, axis=1)

# Print the dimensions of the modified Hitters data (263 rows x 20 columns)
print(&quot;Dimensions of modified data:&quot;, hitters_df_clean.shape)

# One last check: should return 0
print(&quot;Number of null values:&quot;, hitters_df_clean[&quot;Salary&quot;].isnull().sum())</code></pre>
<pre><code>Dimensions of original data: (322, 21)
Dimensions of modified data: (263, 20)
Number of null values: 0</code></pre>
<pre class="python"><code># Some of our predictors are categorical, so we&#39;ll want to clean those up as well. 
# We&#39;ll ask pandas to generate dummy variables for them, separate out the response variable, and stick everything back together again:
dummies = pd.get_dummies(hitters_df_clean[[&#39;League&#39;, &#39;Division&#39;, &#39;NewLeague&#39;]])

y = hitters_df_clean.Salary

# Drop the column with the independent variable (Salary), and columns for which we created dummy variables
X_ = hitters_df_clean.drop([&#39;Salary&#39;, &#39;League&#39;, &#39;Division&#39;, &#39;NewLeague&#39;], axis=1).astype(&#39;float64&#39;)

# Define the feature set X.
X = pd.concat([X_, dummies[[&#39;League_N&#39;, &#39;Division_W&#39;, &#39;NewLeague_N&#39;]]], axis=1)</code></pre>
<pre class="python"><code>X.head()</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
AtBat
</th>
<th>
Hits
</th>
<th>
HmRun
</th>
<th>
Runs
</th>
<th>
RBI
</th>
<th>
Walks
</th>
<th>
Years
</th>
<th>
CAtBat
</th>
<th>
CHits
</th>
<th>
CHmRun
</th>
<th>
CRuns
</th>
<th>
CRBI
</th>
<th>
CWalks
</th>
<th>
PutOuts
</th>
<th>
Assists
</th>
<th>
Errors
</th>
<th>
League_N
</th>
<th>
Division_W
</th>
<th>
NewLeague_N
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
1
</th>
<td>
315.0
</td>
<td>
81.0
</td>
<td>
7.0
</td>
<td>
24.0
</td>
<td>
38.0
</td>
<td>
39.0
</td>
<td>
14.0
</td>
<td>
3449.0
</td>
<td>
835.0
</td>
<td>
69.0
</td>
<td>
321.0
</td>
<td>
414.0
</td>
<td>
375.0
</td>
<td>
632.0
</td>
<td>
43.0
</td>
<td>
10.0
</td>
<td>
1
</td>
<td>
1
</td>
<td>
1
</td>
</tr>
<tr>
<th>
2
</th>
<td>
479.0
</td>
<td>
130.0
</td>
<td>
18.0
</td>
<td>
66.0
</td>
<td>
72.0
</td>
<td>
76.0
</td>
<td>
3.0
</td>
<td>
1624.0
</td>
<td>
457.0
</td>
<td>
63.0
</td>
<td>
224.0
</td>
<td>
266.0
</td>
<td>
263.0
</td>
<td>
880.0
</td>
<td>
82.0
</td>
<td>
14.0
</td>
<td>
0
</td>
<td>
1
</td>
<td>
0
</td>
</tr>
<tr>
<th>
3
</th>
<td>
496.0
</td>
<td>
141.0
</td>
<td>
20.0
</td>
<td>
65.0
</td>
<td>
78.0
</td>
<td>
37.0
</td>
<td>
11.0
</td>
<td>
5628.0
</td>
<td>
1575.0
</td>
<td>
225.0
</td>
<td>
828.0
</td>
<td>
838.0
</td>
<td>
354.0
</td>
<td>
200.0
</td>
<td>
11.0
</td>
<td>
3.0
</td>
<td>
1
</td>
<td>
0
</td>
<td>
1
</td>
</tr>
<tr>
<th>
4
</th>
<td>
321.0
</td>
<td>
87.0
</td>
<td>
10.0
</td>
<td>
39.0
</td>
<td>
42.0
</td>
<td>
30.0
</td>
<td>
2.0
</td>
<td>
396.0
</td>
<td>
101.0
</td>
<td>
12.0
</td>
<td>
48.0
</td>
<td>
46.0
</td>
<td>
33.0
</td>
<td>
805.0
</td>
<td>
40.0
</td>
<td>
4.0
</td>
<td>
1
</td>
<td>
0
</td>
<td>
1
</td>
</tr>
<tr>
<th>
5
</th>
<td>
594.0
</td>
<td>
169.0
</td>
<td>
4.0
</td>
<td>
74.0
</td>
<td>
51.0
</td>
<td>
35.0
</td>
<td>
11.0
</td>
<td>
4408.0
</td>
<td>
1133.0
</td>
<td>
19.0
</td>
<td>
501.0
</td>
<td>
336.0
</td>
<td>
194.0
</td>
<td>
282.0
</td>
<td>
421.0
</td>
<td>
25.0
</td>
<td>
0
</td>
<td>
1
</td>
<td>
0
</td>
</tr>
</tbody>
</table>
</div>
<pre class="python"><code># We can perform best subset selection by identifying the best model that contains a given number of predictors, where best is quantified using RSS. 
# We&#39;ll define a helper function to outputs the best set of variables for each model size:

def processSubset(feature_set):
    # Fit model on feature_set and calculate RSS
    model = sm.OLS(y,X[list(feature_set)])
    regr = model.fit()
    RSS = ((regr.predict(X[list(feature_set)]) - y) ** 2).sum()
    return {&quot;model&quot;:regr, &quot;RSS&quot;:RSS}</code></pre>
<pre class="python"><code>X.columns</code></pre>
<pre><code>Index([&#39;AtBat&#39;, &#39;Hits&#39;, &#39;HmRun&#39;, &#39;Runs&#39;, &#39;RBI&#39;, &#39;Walks&#39;, &#39;Years&#39;, &#39;CAtBat&#39;,
       &#39;CHits&#39;, &#39;CHmRun&#39;, &#39;CRuns&#39;, &#39;CRBI&#39;, &#39;CWalks&#39;, &#39;PutOuts&#39;, &#39;Assists&#39;,
       &#39;Errors&#39;, &#39;League_N&#39;, &#39;Division_W&#39;, &#39;NewLeague_N&#39;],
      dtype=&#39;object&#39;)</code></pre>
<pre class="python"><code>def getBest(k):
    
    tic = time.time()
    
    results = []
    
    for combo in itertools.combinations(X.columns, k):
        results.append(processSubset(combo))
    
    # Wrap everything up in a nice dataframe
    models = pd.DataFrame(results)
    
    # Choose the model with the highest RSS, 
    # pandas.DataFrame.loc() access a group of rows and columns by label(s) or a boolean array
    best_model = models.loc[models[&#39;RSS&#39;].argmin()]
    
    toc = time.time()
    print(&quot;Processed&quot;, models.shape[0], &quot;models on&quot;, k, &quot;predictors in&quot;, (toc-tic), &quot;seconds.&quot;)
    
    # Return the best model, along with some other useful information about the model
    return best_model</code></pre>
<pre class="python"><code># This returns a DataFrame containing the best model that we generated, along with some extra information about the model. 
# Now we want to call that function for each number of predictors  k :

models_best = pd.DataFrame(columns=[&quot;RSS&quot;, &quot;model&quot;])

tic = time.time()
for i in range(1,8):
    models_best.loc[i] = getBest(i)

toc = time.time()
print(&quot;Total elapsed time:&quot;, (toc-tic), &quot;seconds.&quot;)</code></pre>
<pre><code>Processed 19 models on 1 predictors in 0.05321931838989258 seconds.
Processed 171 models on 2 predictors in 0.30821847915649414 seconds.
Processed 969 models on 3 predictors in 1.750037431716919 seconds.
Processed 3876 models on 4 predictors in 6.91033411026001 seconds.
Processed 11628 models on 5 predictors in 21.13918924331665 seconds.
Processed 27132 models on 6 predictors in 52.62217926979065 seconds.
Processed 50388 models on 7 predictors in 107.20377969741821 seconds.
Total elapsed time: 190.4251265525818 seconds.</code></pre>
<pre class="python"><code># Now we have one big DataFrame that contains the best models we&#39;ve generated along with their RSS:
models_best</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
RSS
</th>
<th>
model
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
1
</th>
<td>
4.321393e+07
</td>
<td>
&lt;statsmodels.regression.linear_model.Regressio…
</td>
</tr>
<tr>
<th>
2
</th>
<td>
3.073305e+07
</td>
<td>
&lt;statsmodels.regression.linear_model.Regressio…
</td>
</tr>
<tr>
<th>
3
</th>
<td>
2.941071e+07
</td>
<td>
&lt;statsmodels.regression.linear_model.Regressio…
</td>
</tr>
<tr>
<th>
4
</th>
<td>
2.797678e+07
</td>
<td>
&lt;statsmodels.regression.linear_model.Regressio…
</td>
</tr>
<tr>
<th>
5
</th>
<td>
2.718780e+07
</td>
<td>
&lt;statsmodels.regression.linear_model.Regressio…
</td>
</tr>
<tr>
<th>
6
</th>
<td>
2.639772e+07
</td>
<td>
&lt;statsmodels.regression.linear_model.Regressio…
</td>
</tr>
<tr>
<th>
7
</th>
<td>
2.606413e+07
</td>
<td>
&lt;statsmodels.regression.linear_model.Regressio…
</td>
</tr>
</tbody>
</table>
</div>
<pre class="python"><code>#We can get a full rundown of a single model using the summary() function:
print(models_best.loc[2, &quot;model&quot;].summary())</code></pre>
<pre><code>                                 OLS Regression Results                                
=======================================================================================
Dep. Variable:                 Salary   R-squared (uncentered):                   0.761
Model:                            OLS   Adj. R-squared (uncentered):              0.760
Method:                 Least Squares   F-statistic:                              416.7
Date:                Tue, 16 Feb 2021   Prob (F-statistic):                    5.80e-82
Time:                        15:26:02   Log-Likelihood:                         -1907.6
No. Observations:                 263   AIC:                                      3819.
Df Residuals:                     261   BIC:                                      3826.
Df Model:                           2                                                  
Covariance Type:            nonrobust                                                  
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
Hits           2.9538      0.261     11.335      0.000       2.441       3.467
CRBI           0.6788      0.066     10.295      0.000       0.549       0.809
==============================================================================
Omnibus:                      117.551   Durbin-Watson:                   1.933
Prob(Omnibus):                  0.000   Jarque-Bera (JB):              654.612
Skew:                           1.729   Prob(JB):                    7.12e-143
Kurtosis:                       9.912   Cond. No.                         5.88
==============================================================================

Notes:
[1] R² is computed without centering (uncentered) since the model does not contain a constant.
[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.</code></pre>
<pre class="python"><code>print(models_best.loc[7, &quot;model&quot;].summary())</code></pre>
<pre><code>                                 OLS Regression Results                                
=======================================================================================
Dep. Variable:                 Salary   R-squared (uncentered):                   0.798
Model:                            OLS   Adj. R-squared (uncentered):              0.792
Method:                 Least Squares   F-statistic:                              144.2
Date:                Tue, 16 Feb 2021   Prob (F-statistic):                    4.76e-85
Time:                        15:27:23   Log-Likelihood:                         -1885.9
No. Observations:                 263   AIC:                                      3786.
Df Residuals:                     256   BIC:                                      3811.
Df Model:                           7                                                  
Covariance Type:            nonrobust                                                  
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
Hits           1.6800      0.490      3.426      0.001       0.714       2.646
Walks          3.4000      1.196      2.843      0.005       1.045       5.755
CAtBat        -0.3288      0.090     -3.665      0.000      -0.506      -0.152
CHits          1.3470      0.312      4.316      0.000       0.732       1.962
CHmRun         1.3494      0.415      3.248      0.001       0.531       2.167
PutOuts        0.2482      0.074      3.336      0.001       0.102       0.395
Division_W  -111.9438     36.786     -3.043      0.003    -184.386     -39.502
==============================================================================
Omnibus:                      108.568   Durbin-Watson:                   2.008
Prob(Omnibus):                  0.000   Jarque-Bera (JB):              808.968
Skew:                           1.457   Prob(JB):                    2.16e-176
Kurtosis:                      11.082   Cond. No.                     6.81e+03
==============================================================================

Notes:
[1] R² is computed without centering (uncentered) since the model does not contain a constant.
[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[3] The condition number is large, 6.81e+03. This might indicate that there are
strong multicollinearity or other numerical problems.</code></pre>
<pre class="python"><code># Show the best 19-variable model (there&#39;s actually only one)
print(getBest(19)[&quot;model&quot;].summary())</code></pre>
<pre><code>Processed 1 models on 19 predictors in 0.007192373275756836 seconds.
                                 OLS Regression Results                                
=======================================================================================
Dep. Variable:                 Salary   R-squared (uncentered):                   0.810
Model:                            OLS   Adj. R-squared (uncentered):              0.795
Method:                 Least Squares   F-statistic:                              54.64
Date:                Tue, 16 Feb 2021   Prob (F-statistic):                    1.31e-76
Time:                        15:30:00   Log-Likelihood:                         -1877.9
No. Observations:                 263   AIC:                                      3794.
Df Residuals:                     244   BIC:                                      3862.
Df Model:                          19                                                  
Covariance Type:            nonrobust                                                  
===============================================================================
                  coef    std err          t      P&gt;|t|      [0.025      0.975]
-------------------------------------------------------------------------------
AtBat          -1.5975      0.600     -2.663      0.008      -2.779      -0.416
Hits            7.0330      2.374      2.963      0.003       2.357      11.709
HmRun           4.1210      6.229      0.662      0.509      -8.148      16.390
Runs           -2.3776      2.994     -0.794      0.428      -8.276       3.520
RBI            -1.0873      2.613     -0.416      0.678      -6.234       4.059
Walks           6.1560      1.836      3.352      0.001       2.539       9.773
Years           9.5196     10.128      0.940      0.348     -10.429      29.468
CAtBat         -0.2018      0.135     -1.497      0.136      -0.467       0.064
CHits           0.1380      0.678      0.204      0.839      -1.197       1.473
CHmRun         -0.1669      1.625     -0.103      0.918      -3.367       3.033
CRuns           1.5070      0.753      2.001      0.047       0.023       2.991
CRBI            0.7742      0.696      1.113      0.267      -0.596       2.144
CWalks         -0.7851      0.329     -2.384      0.018      -1.434      -0.137
PutOuts         0.2856      0.078      3.673      0.000       0.132       0.439
Assists         0.3137      0.220      1.427      0.155      -0.119       0.747
Errors         -2.0463      4.350     -0.470      0.638     -10.615       6.522
League_N       86.8139     78.463      1.106      0.270     -67.737     241.365
Division_W    -97.5160     39.084     -2.495      0.013    -174.500     -20.532
NewLeague_N   -23.9133     79.361     -0.301      0.763    -180.234     132.407
==============================================================================
Omnibus:                       97.217   Durbin-Watson:                   2.024
Prob(Omnibus):                  0.000   Jarque-Bera (JB):              626.205
Skew:                           1.320   Prob(JB):                    1.05e-136
Kurtosis:                      10.083   Cond. No.                     2.06e+04
==============================================================================

Notes:
[1] R² is computed without centering (uncentered) since the model does not contain a constant.
[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[3] The condition number is large, 2.06e+04. This might indicate that there are
strong multicollinearity or other numerical problems.</code></pre>
<pre class="python"><code>#we can access just the parts we need using the model&#39;s attributes. For example, if we want the  R2  value:
models_best.loc[2, &quot;model&quot;].rsquared</code></pre>
<pre><code>0.7614950002332872</code></pre>
<pre class="python"><code>#We can examine these to try to select the best overall model. Let&#39;s start by looking at  R2  across all our models:
# Gets the second element from each row (&#39;model&#39;) and pulls out its rsquared attribute
models_best.apply(lambda row: row[1].rsquared, axis=1)</code></pre>
<pre><code>1    0.664637
2    0.761495
3    0.771757
4    0.782885
5    0.789008
6    0.795140
7    0.797728
dtype: float64</code></pre>
<pre class="python"><code>print(models_best.apply(lambda row: row[1].aic, axis=1))
print(models_best.apply(lambda row: row[1].aic, axis=1).argmin())
print(models_best.apply(lambda row: row[1].aic, axis=1).min())</code></pre>
<pre><code>1    3906.865252
2    3819.228530
3    3809.661852
4    3798.516052
5    3792.992461
6    3787.236460
7    3785.891731
dtype: float64
6
3785.8917313130005</code></pre>
<pre class="python"><code># Plotting RSS, adjusted  R2 , AIC, and BIC for all of the models at once will help us decide which model to select. 

plt.figure(figsize=(20,10))
plt.rcParams.update({&#39;font.size&#39;: 18, &#39;lines.markersize&#39;: 10})

# Set up a 2x2 grid so we can look at 4 plots at once
plt.subplot(2, 2, 1)

# We will now plot a red dot to indicate the model with the largest adjusted R^2 statistic.
# The argmax() function can be used to identify the location of the maximum point of a vector
plt.plot(models_best[&quot;RSS&quot;])
plt.xlabel(&#39;# Predictors&#39;)
plt.ylabel(&#39;RSS&#39;)

# We will now plot a red dot to indicate the model with the largest adjusted R^2 statistic.
# The argmax() function can be used to identify the location of the maximum point of a vector

rsquared_adj = models_best.apply(lambda row: row[1].rsquared_adj, axis=1)

plt.subplot(2, 2, 2)
plt.plot(rsquared_adj)
plt.plot(rsquared_adj.argmax()+1, rsquared_adj.max(), &quot;or&quot;)
plt.xlabel(&#39;# Predictors&#39;)
plt.ylabel(&#39;adjusted rsquared&#39;)

# We&#39;ll do the same for AIC and BIC, this time looking for the models with the SMALLEST statistic
aic = models_best.apply(lambda row: row[1].aic, axis=1)

plt.subplot(2, 2, 3)
plt.plot(aic)
plt.plot(aic.argmin()+1, aic.min(), &quot;or&quot;)
plt.xlabel(&#39;# Predictors&#39;)
plt.ylabel(&#39;AIC&#39;)

bic = models_best.apply(lambda row: row[1].bic, axis=1)

plt.subplot(2, 2, 4)
plt.plot(bic)
plt.plot(bic.argmin()+1, bic.min(), &quot;or&quot;)
plt.xlabel(&#39;# Predictors&#39;)
plt.ylabel(&#39;BIC&#39;)</code></pre>
<pre><code>Text(0, 0.5, &#39;BIC&#39;)</code></pre>
<div class="figure">
<img src="merged_files/merged_99_1.png" alt="" />
<p class="caption">png</p>
</div>
<p>We see that according to BIC, the best performer is the model with 6 variables. According to AIC and adjusted <span class="math inline">\(R^2\)</span> something a bit more complex might be better. Again, no one measure is going to give us an entirely accurate picture… but they all agree that a model with 5 or fewer predictors is insufficient.</p>
</div>
<div id="s-3.3.2.-forward--and-backward-stepwise-selection" class="section level2">
<h2><span class="math inline">\(\S\)</span> 3.3.2. Forward- and Backward-Stepwise Selection</h2>
<blockquote>
<p>Rather than search through all possible subsets, we can seek a good path through them.</p>
</blockquote>
<div id="forward-stepwise-selection" class="section level3">
<h3>Forward-stepwise selection</h3>
<p><em>Forward-stepwise selection</em> starts with the intercept, and the sequentially adds into the model the predictor that most improves the fit. Clever updating algorithms can exploit the QR decomposition for the current fit to rapidly establish the next candidate. Like best-subset regression, the subset size <span class="math inline">\(k\)</span> must be determined.</p>
<p>Forward-stepwise selection is a <em>greedy algorithm</em>, producing a nested sequence of models. In this sense it might seem sub-optimal compared to best-subset selection. However, there are several reasons why it might be preferred:
* Computational; we can always compute the forward stepwise sequence (even when <span class="math inline">\(p \gg N\)</span>).
* Statistical;forward stepwise is a more constrained search than the best subset selection, and will have lower variance, but perhaps more bias.</p>
</div>
<div id="backward-stepwise-selction" class="section level3">
<h3>Backward-stepwise selction</h3>
<p><em>Backward-stepwise selection</em> starts with the full model, and sequentially deletes the predictor that has the least impact on the fit. The candidate for dropping is the variable with the smallest Z-score. Backward selection can only be used when <span class="math inline">\(N&gt;p\)</span>, while forward selection can always be used.</p>
<p>On the prostate cancer example, best-subset, forward and backward selection all gave exactly the same sequence of terms.</p>
<pre class="python"><code># there are 19 predictors
X.columns</code></pre>
<pre><code>Index([&#39;AtBat&#39;, &#39;Hits&#39;, &#39;HmRun&#39;, &#39;Runs&#39;, &#39;RBI&#39;, &#39;Walks&#39;, &#39;Years&#39;, &#39;CAtBat&#39;,
       &#39;CHits&#39;, &#39;CHmRun&#39;, &#39;CRuns&#39;, &#39;CRBI&#39;, &#39;CWalks&#39;, &#39;PutOuts&#39;, &#39;Assists&#39;,
       &#39;Errors&#39;, &#39;League_N&#39;, &#39;Division_W&#39;, &#39;NewLeague_N&#39;],
      dtype=&#39;object&#39;)</code></pre>
<pre class="python"><code>[p for p in X.columns if p not in []]</code></pre>
<pre><code>[&#39;AtBat&#39;,
 &#39;Hits&#39;,
 &#39;HmRun&#39;,
 &#39;Runs&#39;,
 &#39;RBI&#39;,
 &#39;Walks&#39;,
 &#39;Years&#39;,
 &#39;CAtBat&#39;,
 &#39;CHits&#39;,
 &#39;CHmRun&#39;,
 &#39;CRuns&#39;,
 &#39;CRBI&#39;,
 &#39;CWalks&#39;,
 &#39;PutOuts&#39;,
 &#39;Assists&#39;,
 &#39;Errors&#39;,
 &#39;League_N&#39;,
 &#39;Division_W&#39;,
 &#39;NewLeague_N&#39;]</code></pre>
<pre class="python"><code>def forward(predictors):

    # Pull out predictors we still need to process
    remaining_predictors = [p for p in X.columns if p not in predictors]
    
    tic = time.time()
    
    results = []
    
    for p in remaining_predictors:
        results.append(processSubset(predictors+[p]))
    
    # Wrap everything up in a nice dataframe
    models = pd.DataFrame(results)
    
    # Choose the model with the highest RSS
    best_model = models.loc[models[&#39;RSS&#39;].argmin()]
    
    toc = time.time()
    print(&quot;Processed &quot;, models.shape[0], &quot;models on&quot;, len(predictors)+1, &quot;predictors in&quot;, (toc-tic), &quot;seconds.&quot;)
    
    # Return the best model, along with some other useful information about the model
    return best_model</code></pre>
<pre class="python"><code>models_fwd = pd.DataFrame(columns=[&quot;RSS&quot;, &quot;model&quot;])

tic = time.time()
predictors = []

for i in range(1,len(X.columns)+1):    
    models_fwd.loc[i] = forward(predictors)
    predictors = models_fwd.loc[i][&quot;model&quot;].model.exog_names

toc = time.time()
print(&quot;Total elapsed time:&quot;, (toc-tic), &quot;seconds.&quot;)</code></pre>
<pre><code>Processed  19 models on 1 predictors in 0.03515005111694336 seconds.
Processed  18 models on 2 predictors in 0.033811330795288086 seconds.
Processed  17 models on 3 predictors in 0.02893376350402832 seconds.
Processed  16 models on 4 predictors in 0.029803991317749023 seconds.
Processed  15 models on 5 predictors in 0.028636455535888672 seconds.
Processed  14 models on 6 predictors in 0.027070999145507812 seconds.
Processed  13 models on 7 predictors in 0.025038480758666992 seconds.
Processed  12 models on 8 predictors in 0.025738239288330078 seconds.
Processed  11 models on 9 predictors in 0.022580862045288086 seconds.
Processed  10 models on 10 predictors in 0.02110886573791504 seconds.
Processed  9 models on 11 predictors in 0.01943826675415039 seconds.
Processed  8 models on 12 predictors in 0.016745567321777344 seconds.
Processed  7 models on 13 predictors in 0.014856338500976562 seconds.
Processed  6 models on 14 predictors in 0.013204336166381836 seconds.
Processed  5 models on 15 predictors in 0.011202096939086914 seconds.
Processed  4 models on 16 predictors in 0.009953975677490234 seconds.
Processed  3 models on 17 predictors in 0.007590293884277344 seconds.
Processed  2 models on 18 predictors in 0.005232572555541992 seconds.
Processed  1 models on 19 predictors in 0.0031538009643554688 seconds.
Total elapsed time: 0.427997350692749 seconds.</code></pre>
<pre class="python"><code>print(models_fwd.loc[1, &quot;model&quot;].summary())
print(models_fwd.loc[2, &quot;model&quot;].summary())</code></pre>
<pre><code>                                 OLS Regression Results                                
=======================================================================================
Dep. Variable:                 Salary   R-squared (uncentered):                   0.665
Model:                            OLS   Adj. R-squared (uncentered):              0.663
Method:                 Least Squares   F-statistic:                              519.2
Date:                Tue, 16 Feb 2021   Prob (F-statistic):                    4.20e-64
Time:                        16:41:14   Log-Likelihood:                         -1952.4
No. Observations:                 263   AIC:                                      3907.
Df Residuals:                     262   BIC:                                      3910.
Df Model:                           1                                                  
Covariance Type:            nonrobust                                                  
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
Hits           4.8833      0.214     22.787      0.000       4.461       5.305
==============================================================================
Omnibus:                       90.075   Durbin-Watson:                   1.949
Prob(Omnibus):                  0.000   Jarque-Bera (JB):              293.080
Skew:                           1.469   Prob(JB):                     2.28e-64
Kurtosis:                       7.256   Cond. No.                         1.00
==============================================================================

Notes:
[1] R² is computed without centering (uncentered) since the model does not contain a constant.
[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.
                                 OLS Regression Results                                
=======================================================================================
Dep. Variable:                 Salary   R-squared (uncentered):                   0.761
Model:                            OLS   Adj. R-squared (uncentered):              0.760
Method:                 Least Squares   F-statistic:                              416.7
Date:                Tue, 16 Feb 2021   Prob (F-statistic):                    5.80e-82
Time:                        16:41:14   Log-Likelihood:                         -1907.6
No. Observations:                 263   AIC:                                      3819.
Df Residuals:                     261   BIC:                                      3826.
Df Model:                           2                                                  
Covariance Type:            nonrobust                                                  
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
Hits           2.9538      0.261     11.335      0.000       2.441       3.467
CRBI           0.6788      0.066     10.295      0.000       0.549       0.809
==============================================================================
Omnibus:                      117.551   Durbin-Watson:                   1.933
Prob(Omnibus):                  0.000   Jarque-Bera (JB):              654.612
Skew:                           1.729   Prob(JB):                    7.12e-143
Kurtosis:                       9.912   Cond. No.                         5.88
==============================================================================

Notes:
[1] R² is computed without centering (uncentered) since the model does not contain a constant.
[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.</code></pre>
<pre class="python"><code>#Let&#39;s see how the models stack up against best subset selection:
print(models_best.loc[6, &quot;model&quot;].summary())
print(models_fwd.loc[6, &quot;model&quot;].summary())</code></pre>
<pre><code>                                 OLS Regression Results                                
=======================================================================================
Dep. Variable:                 Salary   R-squared (uncentered):                   0.795
Model:                            OLS   Adj. R-squared (uncentered):              0.790
Method:                 Least Squares   F-statistic:                              166.3
Date:                Tue, 16 Feb 2021   Prob (F-statistic):                    1.79e-85
Time:                        16:42:03   Log-Likelihood:                         -1887.6
No. Observations:                 263   AIC:                                      3787.
Df Residuals:                     257   BIC:                                      3809.
Df Model:                           6                                                  
Covariance Type:            nonrobust                                                  
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
AtBat         -1.5488      0.477     -3.248      0.001      -2.488      -0.610
Hits           7.0190      1.613      4.352      0.000       3.843      10.195
Walks          3.7513      1.212      3.095      0.002       1.364       6.138
CRBI           0.6544      0.064     10.218      0.000       0.528       0.781
PutOuts        0.2703      0.075      3.614      0.000       0.123       0.418
Division_W  -104.4513     37.661     -2.773      0.006    -178.615     -30.287
==============================================================================
Omnibus:                      106.414   Durbin-Watson:                   1.986
Prob(Omnibus):                  0.000   Jarque-Bera (JB):              768.429
Skew:                           1.433   Prob(JB):                    1.37e-167
Kurtosis:                      10.869   Cond. No.                     1.29e+03
==============================================================================

Notes:
[1] R² is computed without centering (uncentered) since the model does not contain a constant.
[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[3] The condition number is large, 1.29e+03. This might indicate that there are
strong multicollinearity or other numerical problems.
                                 OLS Regression Results                                
=======================================================================================
Dep. Variable:                 Salary   R-squared (uncentered):                   0.795
Model:                            OLS   Adj. R-squared (uncentered):              0.790
Method:                 Least Squares   F-statistic:                              166.3
Date:                Tue, 16 Feb 2021   Prob (F-statistic):                    1.79e-85
Time:                        16:42:03   Log-Likelihood:                         -1887.6
No. Observations:                 263   AIC:                                      3787.
Df Residuals:                     257   BIC:                                      3809.
Df Model:                           6                                                  
Covariance Type:            nonrobust                                                  
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
Hits           7.0190      1.613      4.352      0.000       3.843      10.195
CRBI           0.6544      0.064     10.218      0.000       0.528       0.781
Division_W  -104.4513     37.661     -2.773      0.006    -178.615     -30.287
PutOuts        0.2703      0.075      3.614      0.000       0.123       0.418
AtBat         -1.5488      0.477     -3.248      0.001      -2.488      -0.610
Walks          3.7513      1.212      3.095      0.002       1.364       6.138
==============================================================================
Omnibus:                      106.414   Durbin-Watson:                   1.986
Prob(Omnibus):                  0.000   Jarque-Bera (JB):              768.429
Skew:                           1.433   Prob(JB):                    1.37e-167
Kurtosis:                      10.869   Cond. No.                     1.29e+03
==============================================================================

Notes:
[1] R² is computed without centering (uncentered) since the model does not contain a constant.
[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[3] The condition number is large, 1.29e+03. This might indicate that there are
strong multicollinearity or other numerical problems.</code></pre>
<p>For this data, the best one-variable through six-variable models are each identical for best subset and forward selection.</p>
<pre class="python"><code># Backward Selection
def backward(predictors):
    
    tic = time.time()
    
    results = []
    
    for combo in itertools.combinations(predictors, len(predictors)-1):
        results.append(processSubset(combo))
    
    # Wrap everything up in a nice dataframe
    models = pd.DataFrame(results)
    
    # Choose the model with the highest RSS
    best_model = models.loc[models[&#39;RSS&#39;].argmin()]
    
    toc = time.time()
    print(&quot;Processed &quot;, models.shape[0], &quot;models on&quot;, len(predictors)-1, &quot;predictors in&quot;, (toc-tic), &quot;seconds.&quot;)
    
    # Return the best model, along with some other useful information about the model
    return best_model</code></pre>
<pre class="python"><code>models_bwd = pd.DataFrame(columns=[&quot;RSS&quot;, &quot;model&quot;], index = range(1,len(X.columns)))

tic = time.time()
predictors = X.columns

while(len(predictors) &gt; 1):  
    models_bwd.loc[len(predictors)-1] = backward(predictors)
    predictors = models_bwd.loc[len(predictors)-1][&quot;model&quot;].model.exog_names

toc = time.time()
print(&quot;Total elapsed time:&quot;, (toc-tic), &quot;seconds.&quot;)</code></pre>
<pre><code>Processed  19 models on 18 predictors in 0.05166292190551758 seconds.
Processed  18 models on 17 predictors in 0.03813934326171875 seconds.
Processed  17 models on 16 predictors in 0.034967660903930664 seconds.
Processed  16 models on 15 predictors in 0.03255939483642578 seconds.
Processed  15 models on 14 predictors in 0.03638648986816406 seconds.
Processed  14 models on 13 predictors in 0.0325469970703125 seconds.
Processed  13 models on 12 predictors in 0.0324857234954834 seconds.
Processed  12 models on 11 predictors in 0.02887129783630371 seconds.
Processed  11 models on 10 predictors in 0.026468515396118164 seconds.
Processed  10 models on 9 predictors in 0.02369976043701172 seconds.
Processed  9 models on 8 predictors in 0.021591901779174805 seconds.
Processed  8 models on 7 predictors in 0.020296335220336914 seconds.
Processed  7 models on 6 predictors in 0.017458677291870117 seconds.
Processed  6 models on 5 predictors in 0.017169475555419922 seconds.
Processed  5 models on 4 predictors in 0.014002323150634766 seconds.
Processed  4 models on 3 predictors in 0.01012420654296875 seconds.
Processed  3 models on 2 predictors in 0.005807399749755859 seconds.
Processed  2 models on 1 predictors in 0.0037310123443603516 seconds.
Total elapsed time: 0.4661753177642822 seconds.</code></pre>
<p>For this data, the best one-variable through six-variable models are each identical for best subset and forward selection. However, the best seven-variable models identified by forward stepwise selection, backward stepwise selection, and best subset selection are different:</p>
<pre class="python"><code>print(&quot;------------&quot;)
print(&quot;Best Subset:&quot;)
print(&quot;------------&quot;)
print(models_best.loc[7, &quot;model&quot;].params)</code></pre>
<pre><code>------------
Best Subset:
------------
Hits            1.680029
Walks           3.399961
CAtBat         -0.328835
CHits           1.347017
CHmRun          1.349373
PutOuts         0.248166
Division_W   -111.943760
dtype: float64</code></pre>
<pre class="python"><code>print(&quot;-----------------&quot;)
print(&quot;Foward Selection:&quot;)
print(&quot;-----------------&quot;)
print(models_fwd.loc[7, &quot;model&quot;].params)</code></pre>
<pre><code>-----------------
Foward Selection:
-----------------
Hits            7.277149
CRBI            0.652415
Division_W   -110.656338
PutOuts         0.259787
AtBat          -1.644651
Walks           3.684324
League_N       49.978410
dtype: float64</code></pre>
<pre class="python"><code>print(&quot;-------------------&quot;)
print(&quot;Backward Selection:&quot;)
print(&quot;-------------------&quot;)
print(models_bwd.loc[7, &quot;model&quot;].params)</code></pre>
<pre><code>-------------------
Backward Selection:
-------------------
AtBat         -1.601655
Hits           6.148449
Walks          5.866033
CRuns          1.097453
CWalks        -0.650614
PutOuts        0.310125
Division_W   -95.027171
dtype: float64</code></pre>
</div>
</div>
<div id="s-3.3.3.-forward-stagewise-regression" class="section level2">
<h2><span class="math inline">\(\S\)</span> 3.3.3. Forward-Stagewise Regression</h2>
<p>Forward-stagewise regression (FS) is even more constrained than forwardstepwise
regression. It starts like forward-stepwise regression, with an intercept
equal to <span class="math inline">\(\bar{y}\)</span>, and centered predictors with coefficients initially all <span class="math inline">\(0\)</span>.
At each step the algorithm identifies the variable most correlated with the
current residual. It then computes the simple linear regression coefficient
of the residual on this chosen variable, and then adds it to the current coefficient for that variable. This is continued till none of the variables have
correlation with the residuals—i.e. the least-squares fit when <span class="math inline">\(N &gt; p\)</span>.</p>
<p>Unlike forward-stepwise regression, none of the other variables are adjusted
when a term is added to the model. As a consequence, forward stagewise can take many more than <span class="math inline">\(p\)</span> steps to reach the least squares fit, and historically has been dismissed as being inefficient. It turns out that this “slow fitting” can pay dividends in high-dimensional problems. We see in Section 3.8.1 that both forward stagewise and a variant which is slowed down even further are quite competitive, especially in very highdimensional problems.</p>
<p>Forward-stagewise regression is included in Figure 3.6. In this example it
takes over <span class="math inline">\(1000\)</span> steps to get all the correlations below <span class="math inline">\(10^{−4}\)</span>. For subset size <span class="math inline">\(k\)</span>, we plotted the error for the last step for which there where <span class="math inline">\(k\)</span> nonzero coefficients. Although it catches up with the best fit, it takes longer to do so.</p>
</div>
<div id="s-3.3.4.-prostate-cancer-data-example" class="section level2">
<h2><span class="math inline">\(\S\)</span> 3.3.4. Prostate Cancer Data Example</h2>
<p>With subset size <span class="math inline">\(k\)</span> determined via tenfold cross-validation, best-subset selection chose to use the two predictors <span class="math inline">\(\texttt{lcvol}\)</span> and <span class="math inline">\(\texttt{lweight}\)</span>.</p>
<div id="cross-validation-briefly" class="section level3">
<h3>Cross-validation, briefly</h3>
<ol style="list-style-type: decimal">
<li>Cross-validation works by dividing the training data randomly into ten equal parts.</li>
<li>The learning method is fit – for a range of values of the complexity parameter – to nine-tenths of the data.</li>
<li>The prediction error is computed on the remaining one-tenth.</li>
<li>Repeat step 2 - step 3 for each one-tenth of the data, and the ten prediction error estimates are averaged.</li>
<li>Then we obtain an estimated prediction error curve as a function of the complexity parameter so that a proper complexity parameter can be chosen.</li>
</ol>
<p>We have used the “one-standard-error” rule – we pick the most parsimonious model within one standard error of the minimun. Such a rule acknowledges the fact that the tradeoff curve is estimated with error, and hence takes a conservative approach.</p>
<p>Note that we have already divided these data into a training set of size
67 and a test set of size 30. Cross-validation is applied to the training set,
since selecting the shrinkage parameter is part of the training process. The
test set is there to judge the performance of the selected model.</p>
<pre class="python"><code>np.arange(67)</code></pre>
<pre><code>array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,
       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,
       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,
       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66])</code></pre>
<pre class="python"><code>divmod(67, 10)</code></pre>
<pre><code>(6, 7)</code></pre>
<pre class="python"><code>div, mod = divmod(67, 10)
unit_sizes = [div for _ in range(10)]
for i in range(mod):
    unit_sizes[i] += 1
print(unit_sizes)
sum(unit_sizes)</code></pre>
<pre><code>[7, 7, 7, 7, 7, 7, 7, 6, 6, 6]





67</code></pre>
<pre class="python"><code>for k, unit_size in enumerate(unit_sizes):
    print(k, unit_size)</code></pre>
<pre><code>0 7
1 7
2 7
3 7
4 7
5 7
6 7
7 6
8 6
9 6</code></pre>
<pre class="python"><code>np.random.choice(np.arange(67), unit_size, replace=False)</code></pre>
<pre><code>array([35,  7, 64, 58,  4, 63])</code></pre>
<pre class="python"><code>&quot;&quot;&quot;FIGURE 3.7. Estimated prediction error curves and their standard errors
for the best-subset selection via tenfold cross-validation.&quot;&quot;&quot;

def index_tenfold(n:int) -&gt; np.ndarray:
    &quot;&quot;&quot;Produce index array for tenfold CV with dataframe length n.&quot;&quot;&quot;
    original_indices = np.arange(n)
    tenfold_indices = np.zeros(n)

    div, mod = divmod(n, 10)
    unit_sizes = [div for _ in range(10)]
    for i in range(mod):
        unit_sizes[i] += 1

    for k, unit_size in enumerate(unit_sizes):
        tenfold = np.random.choice(original_indices, unit_size,
                                   replace=False)
        tenfold_indices[tenfold] = k
        original_indices = np.delete(
            original_indices,
            [np.argwhere(original_indices == val) for val in tenfold],
        )
        # print(tenfold, original_indices)
    return tenfold_indices</code></pre>
<pre class="python"><code>print(index_tenfold(67))
len(index_tenfold(67))</code></pre>
<pre><code>[9. 7. 9. 8. 5. 1. 7. 5. 8. 3. 2. 4. 7. 4. 9. 9. 0. 0. 3. 4. 3. 4. 3. 0.
 1. 1. 8. 7. 1. 2. 3. 5. 6. 8. 2. 9. 2. 8. 2. 0. 2. 6. 5. 4. 5. 7. 4. 1.
 0. 1. 6. 6. 6. 5. 9. 6. 5. 0. 0. 2. 6. 4. 3. 7. 3. 8. 1.]





67</code></pre>
<pre class="python"><code>data_x_train = data_x_normalized[data[&#39;train&#39;] == &#39;T&#39;]
data_y_train = data_y[data[&#39;train&#39;] == &#39;T&#39;]
data_x_test = data_x_normalized[data[&#39;train&#39;] == &#39;F&#39;]
data_y_test = data_y[data[&#39;train&#39;] == &#39;F&#39;]
vec_y = data_y_train.values
vec_y_test = data_y_test.values

size_train = sum(data[&#39;train&#39;] == &#39;T&#39;)
size_test = sum(data[&#39;train&#39;] == &#39;F&#39;)
size_predictor = len(data_x_train.columns)</code></pre>
<pre class="python"><code>indices_tenfold = index_tenfold(size_train)
cv_results = collections.defaultdict(list)

for cv in range(10):
    mask_cv = indices_tenfold != cv
    size_cv_train = sum(mask_cv == True)

    df_x = data_x_train[mask_cv]
    v_y = vec_y[mask_cv]
    #All Subsets OLS
    for k in range(size_predictor+1):
        ols = ols_with_subset_size(df_x, v_y, k)
        ols_best = min(ols, key=op.itemgetter(&#39;rss&#39;))
        cv_results[k].append(ols_best[&#39;rss&#39;]/size_cv_train)

rss_average = [sum(rss)/10 for _, rss in cv_results.items()]
rss_stderr = [math.sqrt((sum(np.array(rss)**2)-10*rss_average[k]**2)/9)
              for k, rss in cv_results.items()]</code></pre>
<pre class="python"><code>rss_average</code></pre>
<pre><code>[1.435705040941674,
 0.6627739101582181,
 0.55114165895448,
 0.5119002341038311,
 0.48542751353551605,
 0.4682757004380743,
 0.44811692523738395,
 0.43369589944674913,
 0.4329883352963549]</code></pre>
<pre class="python"><code>list(cv_results.keys())</code></pre>
<pre><code>[0, 1, 2, 3, 4, 5, 6, 7, 8]</code></pre>
<pre class="python"><code>fig37 = plt.figure(figsize=(10, 10))
ax = fig37.add_subplot(1, 1, 1)
ax.plot(list(cv_results.keys()), rss_average, &#39;o-&#39;, color=&#39;C1&#39;)
for k, (ave, stderr) in enumerate(zip(rss_average, rss_stderr)):
    ax.plot([k, k], [ave-stderr, ave+stderr], color=&#39;C0&#39;, linewidth=1)
    ax.plot([k-.1, k+.1], [ave-stderr, ave-stderr], color=&#39;C0&#39;, linewidth=1)
    ax.plot([k-.1, k+.1], [ave+stderr, ave+stderr], color=&#39;C0&#39;, linewidth=1)
ax.set_xlabel(&#39;Subset Size k&#39;)
ax.set_ylabel(&#39;RSS_average of cv&#39;)
plt.show()</code></pre>
<div class="figure">
<img src="merged_files/merged_129_0.png" alt="" />
<p class="caption">png</p>
</div>
<p>FIGURE 3.7. Estimated prediction error curves and their standard errors for
the various selection and shrinkage methods. Each curve is plotted as a function
of the corresponding complexity parameter for that method. The horizontal axis
has been chosen so that the model complexity increases as we move from left to
right. The estimates of prediction error and their standard errors were obtained by
tenfold cross-validation; full details are given in Section 7.10.</p>
<pre class="python"><code>ols_k2 = ols_with_subset_size(data_x_train, vec_y, k=2)
ols_k2_best = min(ols_k2, key=op.itemgetter(&#39;rss&#39;))
[list(ols_k2_best[&#39;column_names&#39;])]</code></pre>
<pre><code>[[&#39;lcavol&#39;, &#39;lweight&#39;]]</code></pre>
<pre class="python"><code>ols_k2_best[&#39;beta&#39;]</code></pre>
<pre><code>array([2.47735734, 0.73589083, 0.31469341])</code></pre>
<pre class="python"><code>&quot;&quot;&quot;Table 3.3. Estimated coefficients ad test error result for the
best-subset selection applied to the prostate data&quot;&quot;&quot;

ols_k2 = ols_with_subset_size(data_x_train, vec_y, k=2)
ols_k2_best = min(ols_k2, key=op.itemgetter(&#39;rss&#39;))

df_x_test = data_x_test[list(ols_k2_best[&#39;column_names&#39;])]
mat_x_test = np.hstack((np.ones((size_test, 1)), df_x_test.values))
vec_y_test_fitted = mat_x_test @ ols_k2_best[&#39;beta&#39;]

print(&#39;{0:&gt;15} {1:&gt;15}&#39;.format(&#39;Term&#39;, &#39;Best Subset&#39;))
print(&#39;-&#39;*31)
print(&#39;{0:&gt;15} {1:&gt;15.3f}&#39;.format(&#39;Intercept&#39;, ols_k2_best[&#39;beta&#39;][0]))
for idx, col_name in enumerate(ols_k2_best[&#39;column_names&#39;]):
    print(&#39;{0:&gt;15} {1:&gt;15.3f}&#39;.format(col_name, ols_k2_best[&#39;beta&#39;][idx+1]))
print(&#39;-&#39;*31)
print(&#39;{0:&gt;15} {1:&gt;15.3f}&#39;.format(
    &#39;Test Error&#39;,
    sum((vec_y_test-vec_y_test_fitted)**2)/size_test))
print(&#39;{0:&gt;15} {1:&gt;15.3f}&#39;.format(
    &#39;Std Error&#39;,
    math.sqrt((sum(np.array(vec_y_test-vec_y_test_fitted)**2)-size_test*(np.mean(vec_y_test-vec_y_test_fitted))**2)/(size_test-1)),
))</code></pre>
<pre><code>           Term     Best Subset
-------------------------------
      Intercept           2.477
         lcavol           0.736
        lweight           0.315
-------------------------------
     Test Error           0.492
      Std Error           0.714</code></pre>
</div>
</div>
</div>
<div id="s-3.4.-shrinkage-methods" class="section level1">
<h1><span class="math inline">\(\S\)</span> 3.4. Shrinkage Methods</h1>
<p>By retaining a subset of the predictors and discarding the rest, subset selection produces a model that is interpretable and has possibly lower prediction error than the full model. However, because it is a discrete process – variables are either retained or discarded – it often exhibits high variance, and so doesn’t reduce the prediction error of the full model. Shrinkage methods are more continuous, and don’t suffer as much from high variability.</p>
<div id="s-3.4.1.-ridge-regression" class="section level2">
<h2><span class="math inline">\(\S\)</span> 3.4.1. Ridge Regression</h2>
<p>Ridge regression shrinks the regression coefficients by imposing a penalty on their size. The ridge coefficients minimize a penalized residual sum of squares,</p>
<p><span class="math display">\[\begin{equation}
\hat\beta^{\text{ridge}} = {\arg\min}_{\beta}\left\lbrace \sum_{i=1}^N\left( y_i - \beta_0 - \sum_{j=1}^p x_{ij}\beta_j \right)^2 +\lambda\sum_{j=1}^p \beta_j^2 \right\rbrace,
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\lambda \ge 0\)</span> is a complexity parameter that controls the amount of shrinkage: the larger the <span class="math inline">\(\lambda\)</span>, the greater the amount of shrinkage. The idea of penalizing by the sum-of-squares of the parameters is also used in neural networks, a.k.a. <em>weight decay</em> (Chapter 11).</p>
<p>An equivalent way to write the ridge problem is to make explicit the size constraint on the parameters, as</p>
<p><span class="math display">\[\begin{equation}
\hat\beta^{\text{ridge}} = {\arg\min}_\beta \sum_{i=1}^N \left( y_i - \beta_0 - \sum_{j=1}^p x_{ij}\beta_j \right)^2 \text{ subject to } \sum_{j=1}^p \beta_j^2 \le t,
\end{equation}\]</span></p>
<p>where <span class="math inline">\(t\)</span> has a one-to-one correspondence with <span class="math inline">\(\lambda\)</span>.</p>
<p>When there are many correlated variables in a linear regression model, their coefficients can become poorly determined and exhibit high variance. A wildly large positive coefficient on one variable can be canceled by a similarly large negative coefficient on its correlated cousin. By imposing a size constraint on the coefficients, this problem is alleviated.</p>
<div id="scaling-and-centering" class="section level3">
<h3>Scaling and centering</h3>
<p>The ridge solutions are not equivariant under scaling of the inputs, and so one normally standardizes the inputs before solving the ridge problem.</p>
<p>Also notice that the intercept <span class="math inline">\(\beta_0\)</span> has been left out of the penalty term. Penalization of the intercept would make the procedure depend on the origin chosen for <span class="math inline">\(Y\)</span>; i.e., adding a constant <span class="math inline">\(c\)</span> to each of the targets <span class="math inline">\(y_i\)</span> (i.e. simply shifting) would not simply result in a shift of the predictions by the same constant <span class="math inline">\(c\)</span>.</p>
<p>It can be shown that the ridge solution can be separated into two parts, after reparametrization using <em>centered</em> inputs: Each <span class="math inline">\(x_{ij}\)</span> gets replaced by <span class="math inline">\(x_{ij}-\bar{x}_j\)</span>.
1. We estimate <span class="math inline">\(\beta_0\)</span> by the mean response <span class="math inline">\(\bar{y} = \frac{1}{N}\sum_1^N y_i\)</span>.
2. The remaining coefficients get estimated by a ridge regression without intercept, using centered <span class="math inline">\(x_{ij}\)</span>.</p>
<p>Henceforth we assume that this centering has been done, so that the input matrix <span class="math inline">\(\mathbf{X}\)</span> has <span class="math inline">\(p\)</span> columns rather than <span class="math inline">\(p+1\)</span>.</p>
</div>
<div id="from-the-bayesian-point-of-view" class="section level3">
<h3>From the Bayesian point of view</h3>
<p>Ridge regression can also be derived as the mean or mode of a posterior distribution, with a suitably chosen prior distribution.</p>
<p>Suppose</p>
<p><span class="math display">\[\begin{align}
y_i &amp;\sim N(\beta_0+x_i^T\beta, \sigma^2) \\
\beta_j &amp;\sim \text{ i.i.d. }N(0, \tau^2)
\end{align}\]</span></p>
<p>Then the log-posterior density of <span class="math inline">\(\beta\)</span>, with <span class="math inline">\(\tau^2\)</span> and <span class="math inline">\(\sigma^2\)</span> assumed known, is equal to the expression</p>
<p><span class="math display">\[\begin{equation}
\sum_{i=1}^N\left( y_i - \beta_0 - \sum_{j=1}^p x_{ij}\beta_j \right)^2 +\frac{\sigma^2}{\tau^2}\sum_{j=1}^p \beta_j^2,
\end{equation}\]</span></p>
<p>which is the penalized residual sum of squares with <span class="math inline">\(\lambda = \sigma^2/\tau^2\)</span>. Thus the ridge estimate is the mode of the posterior distribution; since the distribution is Gaussian, it is also the posterior mean.</p>
</div>
<div id="matrix-form" class="section level3">
<h3>Matrix form</h3>
<p>We write the criterion:</p>
<p><span class="math display">\[\begin{equation}
\hat\beta^{\text{ridge}} = {\arg\min}_{\beta}\left\lbrace \sum_{i=1}^N\left( y_i - \beta_0 - \sum_{j=1}^p x_{ij}\beta_j \right)^2 +\lambda\sum_{j=1}^p \beta_j^2 \right\rbrace,
\end{equation}\]</span></p>
<p>in matrix form:
<span class="math display">\[\begin{equation}
\text{RSS}(\lambda) = (\mathbf{y}-\mathbf{X}\beta)^T(\mathbf{y}-\mathbf{X}\beta) + \lambda\beta^T\beta
\end{equation}\]</span></p>
<p>This is a quadratic function in the <span class="math inline">\(p+1\)</span> parameters. Differentiating w.r.t. <span class="math inline">\(\beta\)</span> we obtain</p>
<p><span class="math display">\[\begin{align}
\frac{\partial\text{RSS}(\lambda)}{\partial\beta} &amp;= -2\mathbf{X}^T\left(\mathbf{y}-\mathbf{X}\beta\right) + 2\lambda\beta \\
\frac{\partial^2\text{RSS}(\lambda)}{\partial\beta\partial\beta^T} &amp;= 2\mathbf{X}^T\mathbf{X} + 2\lambda
\end{align}\]</span></p>
<p>we set the first derivative to zero</p>
<p><span class="math display">\[\begin{equation}
\mathbf{X}^T(\mathbf{y}-\mathbf{X}\beta) - \lambda\beta = \mathbf{X}^T\mathbf{y} - (\mathbf{X}^T\mathbf{X}+\lambda\mathbf{I})\beta = 0
\end{equation}\]</span></p>
<p>to obtain the unique solution</p>
<p><span class="math display">\[\begin{equation}
\hat\beta^{\text{ridge}} = \left( \mathbf{X}^T\mathbf{X} + \lambda\mathbf{I} \right)^{-1}\mathbf{X}^T\mathbf{y},
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\mathbf{I}\)</span> is the <span class="math inline">\(p\times p\)</span> identity matrix. Notice that the ridge solution is again a linear function of <span class="math inline">\(\mathbf{y}\)</span> by the choice of quadratic penalty <span class="math inline">\(\beta^T\beta\)</span>, resulting in addition of a positive constant to the diagonal of <span class="math inline">\(\mathbf{X}^T\mathbf{X}\)</span> before inversion. This makes the problem nonsingular, even if <span class="math inline">\(\mathbf{X}^T\mathbf{X}\)</span> is not of full rank. It was actually the main motivation for ridge regression when it was first introduced in statistics (Hoerl and Kennard, 1970).</p>
</div>
<div id="the-singular-value-decomposition-svd" class="section level3">
<h3>The singular value decomposition (SVD)</h3>
<p>The SVD of the centered input matrix <span class="math inline">\(\mathbf{X}\)</span> gives us some additional insight into the nature of ridge regression. The SVD of the <span class="math inline">\(N\times p\)</span> matrix <span class="math inline">\(\mathbf{X}\)</span> has the form</p>
<p><span class="math display">\[\begin{equation}
\mathbf{X} = \mathbf{UDV}^T,
\end{equation}\]</span></p>
<p>where</p>
<ul>
<li><span class="math inline">\(\mathbf{U}\)</span> is <span class="math inline">\(N\times p\)</span> orthogonal matrix, with the columns of <span class="math inline">\(\mathbf{U}\)</span> spanning the <span class="math inline">\(\text{col}(\mathbf{X})\)</span></li>
<li><span class="math inline">\(\mathbf{V}\)</span> is <span class="math inline">\(p\times p\)</span> orthogonal matrix, with the columns of <span class="math inline">\(\mathbf{V}\)</span> spanning the <span class="math inline">\(\text{row}(\mathbf{X})\)</span></li>
<li><span class="math inline">\(\mathbf{D}\)</span> is a <span class="math inline">\(p\times p\)</span> diagonal matrix, with diagonal entries<br />
<span class="math inline">\(d_1 \ge d_2 \ge \cdots \ge d_p \ge 0\)</span> called the singular values of <span class="math inline">\(\mathbf{X}\)</span>.</li>
<li>If one or more values <span class="math inline">\(d_j = 0\)</span>, <span class="math inline">\(\mathbf{X}\)</span> is singular.</li>
</ul>
<p>If we compute the singular value decomposition (SVD) of the <span class="math inline">\(N \times p\)</span> centered data matrix <span class="math inline">\(\mathbf X\)</span> as <span class="math display">\[\mathbf X=\mathbf U\mathbf D\mathbf V^T\]</span>
where <span class="math inline">\(\mathbf U\)</span> is a <span class="math inline">\(N × p\)</span> matrix with orthonormal columns that span the column space of <span class="math inline">\(\mathbf X\)</span>, <span class="math inline">\(\mathbf V\)</span> is a <span class="math inline">\(p\times p\)</span> orthogonal matrix, and <span class="math inline">\(\mathbf D\)</span> is a <span class="math inline">\(p\times p\)</span> diagonal matrix with elements <span class="math inline">\(d_j\)</span> ordered such
that <span class="math inline">\(d_1 \ge d_2 \ge \cdots d_p \ge 0\)</span>. From this representation of <span class="math inline">\(\mathbf X\)</span> we can derive a simple expression for <span class="math inline">\(\mathbf X^T\mathbf X\)</span>:
<span class="math display">\[\mathbf X^T\mathbf X=\mathbf V\mathbf D\mathbf U^T\mathbf U\mathbf D\mathbf V^T=\mathbf V\mathbf D^2\mathbf V^T\]</span>
Using this expression we can compute the least squares fitted values <span class="math inline">\(\hat{\mathbf y}^{ls} = \mathbf X \hat{\beta}^{ls}\)</span> as
<span class="math display">\[\begin{align}
\hat{y}^{ls} &amp;= \mathbf X \hat{\beta}^{ls}\\
&amp;=\mathbf X(\mathbf X^T\mathbf X)^{-1}\mathbf X^T\mathbf y\\
&amp;=\mathbf U\mathbf D\mathbf V^T(\mathbf V\mathbf D^2\mathbf V^T)^{-1}\mathbf V\mathbf D\mathbf U^T\mathbf y\\
&amp;=\mathbf U\mathbf D\mathbf V^T(\mathbf V^{-T}\mathbf D^{-2}\mathbf V^{-1})\mathbf V\mathbf D\mathbf U^T\mathbf y\\
&amp;=\mathbf U\mathbf U^T\mathbf y\\
&amp;=\sum_{j=1}^{p}u_ju_j^T\mathbf y
\end{align}\]</span></p>
<p>Note</p>
<ul>
<li><span class="math inline">\(\mathbf{U}^T\mathbf{y}\)</span> are the coordinates of <span class="math inline">\(\mathbf{y}\)</span> w.r.t. the orthonormal basis <span class="math inline">\(\mathbf{U}\)</span>.</li>
<li>the similarity with QR decomposition; <span class="math inline">\(\mathbf{Q}\)</span> and <span class="math inline">\(\mathbf{U}\)</span> are generally different orthogonal bases for <span class="math inline">\(\text{col}(\mathbf{X})\)</span>.</li>
</ul>
<p>To compare how the fitted values <span class="math inline">\(\hat{\mathbf y}^{ls}\)</span> obtained in ridge regression compare with ordinary least squares, we next consider the SVD expression for <span class="math inline">\(\hat{\beta}^{ridge}\)</span>. In the same way as for least squares we find:
<span class="math display">\[
\begin{align}
\hat\beta^{\text{ridge}} &amp;= \left( \mathbf{X}^T\mathbf{X} + \lambda\mathbf{I} \right)^{-1}\mathbf{X}^T\mathbf{y}\\
&amp;=(\mathbf V\mathbf D^2\mathbf V^T+\lambda\mathbf{V}\mathbf{V}^T)^{-1}\mathbf V\mathbf D\mathbf U^T\mathbf{y}\\
&amp;=(\mathbf V(\mathbf D^2+\lambda\mathbf{I})\mathbf{V}^T)^{-1}\mathbf V\mathbf D\mathbf U^T\mathbf{y}\\
&amp;=\mathbf{V}^{-T}(\mathbf D^2+\lambda\mathbf{I})^{-1}\mathbf D\mathbf U^T\mathbf{y}\\
&amp;=\mathbf{V}(\mathbf D^2+\lambda\mathbf{I})^{-1}\mathbf D\mathbf U^T\mathbf{y}\\
\end{align}
\]</span>
And
<span class="math display">\[
\begin{align}
\hat y^{\text{ridge}} &amp;=\mathbf X\hat\beta^{\text{ridge}}\\
&amp;=\mathbf U\mathbf D\mathbf V^T\mathbf{V}(\mathbf D^2+\lambda\mathbf{I})^{-1}\mathbf D\mathbf U^T\mathbf{y}\\
&amp;=\mathbf U\mathbf D(\mathbf D^2+\lambda\mathbf{I})^{-1}\mathbf D\mathbf U^T\mathbf{y}\\
\end{align}
\]</span>
Now note that in this last expression <span class="math inline">\(\mathbf D(\mathbf D^2+\lambda\mathbf{I})^{-1}\mathbf D\)</span> is a diagonal matrix with elements given by <span class="math display">\[\frac{d_j^2}{d_j^2+\lambda}\]</span> and the vector <span class="math inline">\(\mathbf U^T\mathbf y\)</span> is the coordinates of the vector <span class="math inline">\(\mathbf y\)</span> in the basis spanned by the <span class="math inline">\(p\)</span>-columns of <span class="math inline">\(\mathbf U\)</span>. Thus writing the expression by summing columns we
obtain:
<span class="math display">\[\begin{align}
\hat y^{\text{ridge}} &amp;= \mathbf X\hat\beta^{\text{ridge}}\\
&amp;=\mathbf U\mathbf D(\mathbf D^2+\lambda\mathbf{I})^{-1}\mathbf D\mathbf U^T\mathbf{y}\\
&amp;=\sum_{j=1}^{p}u_j\left(\frac{d_j^2}{d_j^2+\lambda}\right)u_j^T\mathbf y\\
\end{align}\]</span></p>
<p>Note that this result is similar to that found in Equation <span class="math display">\[\sum_{j=1}^{p}u_ju_j^T\mathbf y\]</span> derived for ordinary least squares regression but in ridge-regression the inner products <span class="math inline">\(u_j^T\mathbf y\)</span> are now scaled by the factors <span class="math inline">\(\left(\frac{d_j^2}{d_j^2+\lambda}\right)\)</span>. This means that a greater amount of shrinkage is applied to the coordinates of basis vectors with smaller <span class="math inline">\(d_j^2\)</span>. Then what does a small value of <span class="math inline">\(d_j^2\)</span> mean?</p>
</div>
<div id="the-svd-and-the-principal-components" class="section level3">
<h3>The SVD and the principal components</h3>
<p>The SVD of the centered matrix <span class="math inline">\(\mathbf{X}\)</span> is another way of expressing the <em>principal components</em> of the variables in <span class="math inline">\(\mathbf{X}\)</span>. The sample covariance matrix is given by</p>
<p><span class="math display">\[\begin{equation}
\mathbf{S} = \frac{1}{N}\mathbf{X}^T\mathbf{X},
\end{equation}\]</span></p>
<p>and via the SVD,</p>
<p><span class="math display">\[\begin{equation}
\mathbf{X}^T\mathbf{X} = \mathbf{VD}^2\mathbf{V}^T,
\end{equation}\]</span></p>
<p>which is the <em>eigen decomposition</em> of <span class="math inline">\(\mathbf{X}^T\mathbf{X}\)</span> (and of <span class="math inline">\(\mathbf{S}\)</span>, up to a factor <span class="math inline">\(N\)</span>). The eigenvectors <span class="math inline">\(v_j\)</span> (columns of <span class="math inline">\(\mathbf{V}\)</span>) are also called the <strong>principal components (or Karhunen-Loeve) directions</strong> of <span class="math inline">\(\mathbf{X}\)</span> (in the <span class="math inline">\(p\)</span>-dimension space). The first principal component direction <span class="math inline">\(\mathbf v_1\)</span> has the property that <span class="math inline">\(\mathbf{z}_1 = \mathbf{X}\mathbf v_1\)</span> has the larger sample projected variance than any other direction. This sample variance is easily seen to be</p>
<p><span class="math display">\[\begin{equation}
\mathbf{X}\mathbf{V}=\mathbf{U}\mathbf{D}
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
\text{Var}(\mathbf{z}_1) = \text{Var}(\mathbf{X}\mathbf v_1) = \text{Var}(\mathbf{u_1}d_1)=d_1^2\text{Var}(\mathbf{u_1})=\frac{d_1^2}{N}(\mathbf{u_1}^T\mathbf{u_1})= \frac{d_1^2}{N},
\end{equation}\]</span></p>
<p>and in fact <span class="math inline">\(\mathbf{z}_1 = \mathbf{X}\mathbf v_1 = \mathbf{u}_1 d_1\)</span>. The derived variable <span class="math inline">\(z_1\)</span> is called the first principal component of <span class="math inline">\(\mathbf{X}\)</span>, and hence <span class="math inline">\(\mathbf{u}_1\)</span> is the normalized first <strong>principal component</strong> (in the <span class="math inline">\(N\)</span>-dimension space). And subsequent principal components <span class="math inline">\(z_j\)</span> have maximum variance <span class="math inline">\(d_j^2/N\)</span>, subject to being orthogonal to the earlier ones. Conversely the last principal component has minimum variance. Hence the small singular values <span class="math inline">\(d_j\)</span> correspond to directions in the <span class="math inline">\(\text{col}(\mathbf{X})\)</span> having small variance, and ridge regression shrinks these directions the most. In simple words, it does not care what seems not worth. See FIGURE 3.9 for the graphical representation of the principal components.</p>
<p><span style="color:red"><strong>The 1st principal component compresses <span class="math inline">\(N\)</span> points in <span class="math inline">\(p\)</span>-dimention into <span class="math inline">\(N\)</span> points in <span class="math inline">\(1\)</span>-dimention which maintains the largest variance. The 2nd principal component compresses <span class="math inline">\(N\)</span> points in <span class="math inline">\(p\)</span>-dimention into <span class="math inline">\(N\)</span> points in <span class="math inline">\(1\)</span>-dimention which is perpendicular to the 1st principal component and maintains the largest remained variance. And so on with the <span class="math inline">\(N^{st}\)</span> principal component.</strong></span></p>
<p>Ridge regression protects against the potentially high variance of gradients estimated in the short directions. The implicit assumption is that the response will tend to vary most in the directions of high variance of the inputs. This is often a reasonable assumption, since predictors are often chosen for study because they vary with the response variable, but need not hold in general.</p>
</div>
<div id="the-effective-degrees-of-freedom" class="section level3">
<h3>The effective degrees of freedom</h3>
<p><span class="math display">\[\hat y^{\text{ridge}} =\mathbf X\hat\beta^{\text{ridge}}=\mathbf X\left( \mathbf{X}^T\mathbf{X} + \lambda\mathbf{I} \right)^{-1}\mathbf{X}^T\mathbf{y}\]</span>
In FIGURE 3.7 we have plotted the estimated prediction error versus the quantity.
The definition of the effective degrees of freedom <span class="math inline">\(df(\lambda)\)</span> in ridge regression is given by
<span class="math display">\[\begin{align}
\text{df}(\lambda) &amp;= \text{tr}\left( \mathbf{X}(\mathbf{X}^T\mathbf{X} + \lambda\mathbf{I})^{-1}\mathbf{X}^T \right)\\
&amp;= \text{tr}(\mathbf{H}_\lambda) \\
&amp;= \text{tr}\left(\mathbf U\mathbf D(\mathbf D^2+\lambda\mathbf{I})^{-1}\mathbf D\mathbf U^T\right)\\
&amp;=\text{tr}\left(\sum_{j=1}^{p}u_j\left(\frac{d_j^2}{d_j^2+\lambda}\right)u_j^T\right)\\
&amp;= \sum_{j=1}^p \frac{d_j^2}{d_j^2+\lambda}.
\end{align}\]</span>
Since the trace of a matrix can be shown to equal the sum of its eigenvalues.</p>
<p>This monotone decreasing function of <span class="math inline">\(\lambda\)</span> is the <em>effective degrees of freedom</em> of the ridge regression fit. Usually in a linear-regression fit, the degrees-of-freedom of the fit is <span class="math inline">\(p\)</span>, the number of free parameters. The idea is that although all <span class="math inline">\(p\)</span> coefficients in a ridge fit will be non-zero, they are fit in a restricted fashion controlled by <span class="math inline">\(\lambda\)</span>. Note that</p>
<p><span class="math display">\[\begin{align}
\text{df}(\lambda) &amp;= p \text{ when }\lambda = 0, \\
\text{df}(\lambda) &amp;\rightarrow 0 \text{ as }\lambda \rightarrow \infty.
\end{align}\]</span></p>
<p>Of course there is always an additional one degree of freedom for the intercept, which was removed <em>apriori</em>. This definition is motivated in more detail in Section 3.4.4 and Sections 7.4-7.6.</p>
<p>One important consequence of this expression is that we can use it to determine the values of <span class="math inline">\(\lambda\)</span> for which to use when applying cross validation. For example, the book discusses how
to obtain the estimate of y when using ridge regression and it is given by Equation <span class="math display">\[\hat y^{\text{ridge}} =\mathbf X\hat\beta^{\text{ridge}}=\mathbf X\left( \mathbf{X}^T\mathbf{X} + \lambda\mathbf{I} \right)^{-1}\mathbf{X}^T\mathbf{y}\]</span>
but no mention of the numerical values of <span class="math inline">\(\lambda\)</span> we should use in this expression to guarantee that we have accurate coverage of all possible regularized linear models. The approach taken in generating the ridge regression results in Figure 3.8 is to consider <span class="math inline">\(df\)</span> in Equation <span class="math display">\[\text{df}(\lambda)=\sum_{j=1}^p \frac{d_j^2}{d_j^2+\lambda}\]</span> a function of <span class="math inline">\(\lambda\)</span>. As such we set <span class="math inline">\(df(\lambda) = k\)</span> for <span class="math inline">\(k = 1, 2, \cdots , p\)</span> representing all of the possible values for the degree of freedom. We then use Newton’s root finding method to solve for <span class="math inline">\(\lambda\)</span> in the expression
<span class="math display">\[\text{df}(\lambda)=\sum_{j=1}^p \frac{d_j^2}{d_j^2+\lambda}=k, (k = 1, 2, \cdots , p)\]</span></p>
<blockquote>
<p><strong>Newton’s method (Newton-Raphson method)</strong> uses tangent lines of the graph of <span class="math inline">\(y = ƒ(x)\)</span> near the points where <span class="math inline">\(ƒ(x)=0\)</span> to estimate the solution. The goal of Newton’s method for estimating a solution of an equation <span class="math inline">\(ƒ(x) = 0\)</span> is to produce a sequence of approximations that approach the solution. The initial estimate, <span class="math inline">\(x_0\)</span>, may be found by just plain guessing. The method then uses the tangent to the curve <span class="math inline">\(y = ƒ(x)\)</span> at <span class="math inline">\((x_0, ƒ(x_0))\)</span> to approximate the curve, calling the point <span class="math inline">\(x_1\)</span> where the tangent meets the <span class="math inline">\(x\)</span>-axis. The number <span class="math inline">\(x_1\)</span> is usually a better approximation to the solution than is <span class="math inline">\(x_0\)</span>. The point <span class="math inline">\(x_2\)</span> where the tangent to the curve at <span class="math inline">\((x_1, ƒ(x_1))\)</span> crosses the <span class="math inline">\(x\)</span>-axis is the next approximation in the sequence. We continue on, using each approximation to generate the next, until we are close enough to the root to stop.<br />
Given the approximation <span class="math inline">\(x_n\)</span>, the point-slope equation for the tangent to the curve at <span class="math inline">\((x_n,ƒ(x_n))\)</span> is <span class="math display">\[y = ƒ(x_n) + ƒ&#39;(x_n)(x - x_n)\]</span> We can find where it crosses the <span class="math inline">\(x\)</span>-axis by setting <span class="math inline">\(y=0\)</span>, <span class="math display">\[0 = ƒ(x_n) + ƒ&#39;(x_n)(x - x_n)\]</span> <span class="math display">\[x - x_n=-\frac{ƒ(x_n)}{ƒ&#39;(x_n)}\]</span> <span class="math display">\[x=x_n-\frac{ƒ(x_n)}{ƒ&#39;(x_n)}\]</span> This value of <span class="math inline">\(x\)</span> is the next approximation <span class="math inline">\(x_{n+1}\)</span>. Then a solution of the equation <span class="math inline">\(f(x)=0\)</span> is a fixed point of the function <span class="math display">\[x \mapsto x-\frac{f(x)}{f&#39;(x)}\]</span> Newton’s method is the use of the fixed-point method to approximate a solution of the equation by finding a fixed point of the function <span class="math display">\[x-\frac{f(x)}{f&#39;(x)}\]</span></p>
</blockquote>
<p>Thus we define a function <span class="math inline">\(d(\lambda)\)</span> given by
<span class="math display">\[d(\lambda)=\sum_{j=1}^p \frac{d_j^2}{d_j^2+\lambda}-k\]</span>
and we want <span class="math inline">\(\lambda\)</span> such that<span class="math inline">\(d(\lambda) = 0\)</span>. We use Newton’s algorithm for this where we iterate given a starting value of <span class="math inline">\(\lambda_0\)</span>
<span class="math display">\[\lambda_{n+1}=\lambda_n-\frac{d(\lambda_n)}{d&#39;(\lambda_n)}\]</span>
Thus we need the derivative of <span class="math inline">\(d(\lambda)\)</span> which is given by
<span class="math display">\[d&#39;(\lambda)=-\sum_{j=1}^{p}\frac{d_j^2}{(d_j^2+\lambda)^2}\]</span>
and an initial guess for <span class="math inline">\(\lambda_0\)</span>. Since we are really looking for <span class="math inline">\(p\)</span> values of <span class="math inline">\(\lambda\)</span> (one for each value of <span class="math inline">\(k\)</span>) we will start by solving the problems for <span class="math inline">\(k = p, p−1, p−2, \cdots , 1\)</span>. When <span class="math inline">\(k = p\)</span> the value of <span class="math inline">\(\lambda\)</span> that solves <span class="math inline">\(df(\lambda) = p\)</span> is seen to be <span class="math inline">\(\lambda = 0\)</span>. For each subsequent value of <span class="math inline">\(k\)</span> we use the estimate of <span class="math inline">\(\lambda\)</span> found in the previous Newton solve as the initial guess for the current Newton solve.</p>
<pre class="python"><code>&quot;&quot;&quot;FIGURE 3.8. Profiles of ridge coefficients for the prostate cancer
example as tuning parameter lambda is varied.

Coefficients are plotted versus df(lambda), the effective degrees of
freedom. In case of orthonormal inputs, the ridge estimates are just a
scaled version of the least squares estimates;
beta_ridge = beta_ols/(1+lambda)
&quot;&quot;&quot;
import scipy
import scipy.stats
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt</code></pre>
<pre class="python"><code>data = pd.read_csv(&#39;../../data/prostate/prostate.data&#39;, delimiter=&#39;\t&#39;,
                   index_col=0)
data_y = data.pop(&#39;lpsa&#39;)
mask_train = data.pop(&#39;train&#39;)

data_x_train = data[mask_train == &#39;T&#39;]
data_y_train = data_y[mask_train == &#39;T&#39;]
beta_intercept = data_y_train.mean()
# Centering for the training data y by subtracting the mean
data_y_train_centered = data_y_train.subtract(beta_intercept)
# Centering and scaling for the training data x using zscore
data_x_train_normalized = data_x_train.apply(scipy.stats.zscore)
vec_y = data_y_train_centered.values

data_x_test = data[mask_train == &#39;F&#39;]
# Centering and scaling for the test data x using zscore
data_x_test_normalized = data_x_test.apply(scipy.stats.zscore)
data_y_test = data_y[mask_train == &#39;F&#39;]
vec_y_test = data_y_test.values

size_train = sum(mask_train == &#39;T&#39;)
size_test = sum(mask_train == &#39;F&#39;)
size_predictor = data_x_train.columns.size</code></pre>
<pre class="python"><code>def lambdas_from_edf(singular_squared:np.ndarray, interval:int) -&gt;np.ndarray:
    &quot;&quot;&quot;Given squared singular values of data matrix, calculate the lambdas
    with `interval` parameter to split unit intervals s.t. the resulting
    effective degrees of freedom are equidistant with 1/interval, via the
    Newton-Raphson method. e.g., if interval = 10, it produces lambdas for
    0, 0.5, 0.6, 0.7, ...&quot;&quot;&quot;
    p = singular_squared.size
    edfs = np.linspace(.5, p-.5, (p-1)*interval+1)
    threshold = 1e-3
    lambdas = []
    for edf in edfs:
        # Newton-Raphson
        lambda0 = (p-edf)/edf
        lambda1 = 1e6
        diff = lambda1 - lambda0
        while diff &gt; threshold:
            num = (singular_squared/(singular_squared+lambda0)).sum()-edf
            denom = (singular_squared/((singular_squared+lambda0)**2)).sum()
            lambda1 = lambda0 + num/denom
            diff = lambda1 - lambda0
            lambda0 = lambda1
        lambdas.append(lambda1)
    lambdas.append(0)
    edfs = np.concatenate(([0], edfs, [p]))
    return edfs, np.array(lambdas)</code></pre>
<pre class="python"><code># singular value decomposition of data_x_train_normalized
u, s, vh = scipy.linalg.svd(data_x_train_normalized, full_matrices=False)
s2 = s**2
edfs, lambdas = lambdas_from_edf(s2, 10)
# beta_ols = vh.T @ scipy.diag(scipy.reciprocal(s)) @ u.T @ vec_y
# print(beta_ols)
beta_ridge_array = [np.zeros(size_predictor)]
for lamb in lambdas:
    mat_diag = np.diag(s/(s2+lamb))
    beta_ridge = vh.T @ mat_diag @ u.T @ vec_y
    beta_ridge_array.append(beta_ridge)
beta_ridge_array = np.array(beta_ridge_array)

fig38 = plt.figure(figsize=(10, 10))
ax = fig38.add_subplot(1, 1, 1)
ax.plot(edfs, beta_ridge_array, &#39;o-&#39;, markersize=2)
ax.legend(data_x_train.columns)
ax.set_xlabel(r&#39;df ($\lambda$)&#39;)
ax.set_ylabel(&#39;Coefficients&#39;)
ax.set_xlim(-.2, 9)
ax.plot([-.2, 9], [0, 0], &#39;--&#39;, color=&#39;0&#39;, linewidth=0.5)
ax.plot([5, 5], [-0.35, 0.75], &#39;--&#39;, color=&#39;r&#39;, linewidth=0.5)
for x, y, s in zip(np.ones(8)*(8+0.1), beta_ridge_array[-1], data_x_train.columns):
    ax.text(x, y, s, color=&#39;0&#39;, fontsize=12)
ax.margins(0,0)
plt.show()</code></pre>
<div class="figure">
<img src="merged_files/merged_150_0.png" alt="" />
<p class="caption">png</p>
</div>
<pre class="python"><code>len(lambdas)</code></pre>
<pre><code>72</code></pre>
<p>FIGURE 3.8. Profiles of ridge coefficients for the prostate cancer example, as
the tuning parameter <span class="math inline">\(\lambda\)</span> is varied. Coefficients are plotted versus df(<span class="math inline">\(\lambda\)</span>), the effective degrees of freedom. A vertical line is drawn at df = 5.0, the value chosen by cross-validation.</p>
<pre class="python"><code>import numpy as np
import pandas as pd
# 10-folds cross-validation

def index_tenfold(n:int) -&gt;np.ndarray:
    &quot;&quot;&quot;Produce index array for tenfold CV with dataframe length n.&quot;&quot;&quot;
    original_indices = np.arange(n)
    tenfold_indices = np.zeros(n)

    div, mod = divmod(n, 10)
    unit_sizes = [div for _ in range(10)]
    for i in range(mod):
        unit_sizes[i] += 1

    for k, unit_size in enumerate(unit_sizes):
        tenfold = np.random.choice(original_indices, unit_size,
                                   replace=False)
        tenfold_indices[tenfold] = k
        original_indices = np.delete(
            original_indices,
            [np.argwhere(original_indices == val) for val in tenfold],
        )
        # print(tenfold, original_indices)
    return tenfold_indices</code></pre>
<pre class="python"><code>&quot;&quot;&quot;FIGURE 3.7. CV result of the ridge regression&quot;&quot;&quot;
import collections
import math

cv10_indices = index_tenfold(size_train)
cv_beta = collections.defaultdict(list)
cv_rss = collections.defaultdict(list)

for cv_idx in range(10):
    mask_cv = cv10_indices != cv_idx
    size_cv_train = (mask_cv == True).size
    # Use the zscore Centered and scaled training data x
    df_x = data_x_train_normalized[mask_cv]
    cv_x = df_x.values
    # Use the subtracting-mean Centered training data y
    v_y = vec_y[mask_cv]

    # Exceptional case for lambda = infinity, or edf = 0 (constant model)
    v_y_mean= v_y.mean()
    cv_rss[0].append(((v_y-v_y_mean)**2).sum()/size_cv_train)
    
    u, s, vh = np.linalg.svd(cv_x, full_matrices=False)
    s2 = s**2
    edfs, lambdas = lambdas_from_edf(s2, 2)

    for edf, lamb in zip(edfs[1:], lambdas):
        mat_diag = np.diag(s/(s2+lamb))
        beta_ridge = vh.T @ mat_diag @ u.T @ v_y
        cv_beta[edf].append(beta_ridge)
        cv_y_fitted = cv_x @ beta_ridge
        cv_rss[edf].append(((v_y-cv_y_fitted)**2).sum()/size_cv_train)

cv_rss_average = [sum(rss)/len(rss) for _, rss in cv_rss.items()]
cv_rss_stderr = [math.sqrt(((np.array(rss)**2).sum()-sum(rss)**2/10)/9)
                 for _, rss in cv_rss.items()]</code></pre>
<pre class="python"><code># For the 17 edfs, each edfs has 10 cv_rss and 1 cv_rss_average
len(cv_rss_average)</code></pre>
<pre><code>17</code></pre>
<pre class="python"><code>fig27 = plt.figure(figsize=(10, 10))
ax = fig27.add_subplot(1, 1, 1)
ax.plot(edfs, cv_rss_average, &#39;o-&#39;, color=&#39;C1&#39;)
for k, (ave, stderr) in enumerate(zip(cv_rss_average, cv_rss_stderr)):
    ax.plot([k/2, k/2], [ave-stderr, ave+stderr], color=&#39;C0&#39;, linewidth=1)
    ax.plot([k/2-.1, k/2+.1], [ave-stderr, ave-stderr],
            color=&#39;C0&#39;, linewidth=1)
    ax.plot([k/2-.1, k/2+.1], [ave+stderr, ave+stderr],
            color=&#39;C0&#39;, linewidth=1)
ax.set_xlabel(&#39;Degrees of Freedom&#39;)
ax.set_ylabel(&#39;CV Error&#39;)
ax.set_title(&#39;Ridge Regression&#39;)
ax.set_xlim(-.2, 8.2)
ax.plot([-.2, 8.2], [cv_rss_average[-1]+cv_rss_stderr[-1], cv_rss_average[-1]+cv_rss_stderr[-1]], &#39;--&#39;, color=&#39;purple&#39;, linewidth=0.5)
ax.plot([6, 6], [0.3, 1.4], &#39;--&#39;, color=&#39;purple&#39;, linewidth=0.5)
ax.margins(0,0)
plt.show()</code></pre>
<div class="figure">
<img src="merged_files/merged_156_0.png" alt="" />
<p class="caption">png</p>
</div>
<p>FIGURE 3.7. Estimated prediction error curves and their standard errors for
the ridge shrinkage methods. The curve is plotted as a function
of the corresponding complexity parameter for the method. The horizontal axis
has been chosen so that the model complexity increases as we move from left to
right. The estimates of prediction error and their standard errors were obtained by tenfold cross-validation; full details are given in Section 7.10. The least complex model within one standard error of the best (the horizontal dash line) is chosen, indicated by the purple vertical broken lines.</p>
<pre class="python"><code># The data_y is the &#39;lpsa&#39; column of the original data 
data_y</code></pre>
<pre><code>1    -0.430783
2    -0.162519
3    -0.162519
4    -0.162519
5     0.371564
        ...   
93    4.385147
94    4.684443
95    5.143124
96    5.477509
97    5.582932
Name: lpsa, Length: 97, dtype: float64</code></pre>
<pre class="python"><code># vec_y_test is the &#39;lpsa&#39; column of the original data with train==&#39;F&#39;
data_y_test = data_y[mask_train == &#39;F&#39;]
vec_y_test = data_y_test.values
vec_y_test</code></pre>
<pre><code>array([0.7654678, 1.047319 , 1.047319 , 1.3987169, 1.6582281, 1.7316555,
       1.7664417, 1.8164521, 2.008214 , 2.0215476, 2.0856721, 2.3075726,
       2.3749058, 2.5687881, 2.5915164, 2.5915164, 2.6844403, 2.6912431,
       2.7047113, 2.7880929, 2.8535925, 2.8820035, 2.8820035, 2.8875901,
       3.0563569, 3.0750055, 3.5130369, 3.5709402, 5.1431245, 5.5829322])</code></pre>
<pre class="python"><code>&quot;&quot;&quot;Table 3.3. for the ridge regression. Use lambda = 5 as the textbook does.
&quot;&quot;&quot;
lambda_from_cv = lambdas[np.where(edfs == 5)[0]-1]  # lambda with edf = 5

u, s, vh = scipy.linalg.svd(data_x_train_normalized, full_matrices=False)
s2 = s**2

mat_diag = np.diag(s/(s2+lambda_from_cv))
beta_ridge = vh.T @ mat_diag @ u.T @ vec_y

# Use the zscore Centered and scaled test data x
mat_x_test = data_x_test_normalized.values
vec_y_test_fitted = mat_x_test @ beta_ridge

print(&#39;{0:&gt;15} {1:&gt;15}&#39;.format(&#39;Term&#39;, &#39;Best Subset&#39;))
print(&#39;-&#39;*31)
print(&#39;{0:&gt;15} {1:&gt;15.3f}&#39;.format(&#39;Intercept&#39;, beta_intercept))
for idx, col_name in enumerate(data.columns):
    print(&#39;{0:&gt;15} {1:&gt;15.3f}&#39;.format(col_name, beta_ridge[idx]))
print(&#39;-&#39;*31)
print(&#39;{0:&gt;15} {1:&gt;15.3f}&#39;.format(
    &#39;Test Error&#39;,
    #uncenter vec_y_test_fitted by adding beta_intercept
    ((vec_y_test-(vec_y_test_fitted+beta_intercept))**2).sum()/size_test),
)</code></pre>
<pre><code>           Term     Best Subset
-------------------------------
      Intercept           2.452
         lcavol           0.443
        lweight           0.255
            age          -0.051
           lbph           0.171
            svi           0.238
            lcp          -0.005
        gleason           0.041
          pgg45           0.137
-------------------------------
     Test Error           0.503</code></pre>
<h3>
Singular Value Decomposition
</h3>
<pre class="python"><code>from scipy.stats import multivariate_normal
# multivariate_normal(mean=array, cov=array)
rv = multivariate_normal([0, 0], [[10, 6], [6, 10]])
# .rvs() Draw random samples from a multivariate normal distribution.
X = rv.rvs(100)
sum(X)
# X is 100X2 array</code></pre>
<pre><code>array([-45.25513411, -54.32794596])</code></pre>
<pre class="python"><code># np.linalg.svd() Singular Value Decomposition. mXn = (mXm)@(mXn)@(nXn)
# When a is a 2D array, it is factorized as u @ np.diag(s) @ vh = (u * s) @ vh, 
# where u and vh are 2D unitary arrays and s is a 1D array of a’s singular values. 
# When a is higher-dimensional, SVD is applied in stacked mode.
# full_matricesbool, optional If True (default), u and vh have the shapes (..., M, M) and (..., N, N), respectively. 
# Otherwise, the shapes are (..., M, K) and (..., K, N), respectively, where K = min(M, N).

U, d, Vt = np.linalg.svd(X, full_matrices=True)
# U is 100X100 array, d is 100X2 array, V is 2X2 array

V = Vt.T
d = np.sqrt(d)
print(U, &#39;\n\n&#39;, d, &#39;\n\n&#39;, V)</code></pre>
<pre><code>[[ 0.01  0.09 -0.07 ...  0.08  0.08  0.  ]
 [-0.03 -0.04  0.23 ...  0.02  0.11 -0.01]
 [ 0.06  0.24  0.94 ... -0.   -0.02  0.  ]
 ...
 [ 0.08 -0.02  0.   ...  0.99 -0.01 -0.  ]
 [ 0.12  0.06 -0.02 ... -0.01  0.98  0.  ]
 [-0.   -0.01  0.   ... -0.    0.    1.  ]] 

 [6.45 4.62] 

 [[ 0.71 -0.7 ]
 [ 0.7   0.71]]</code></pre>
<pre class="python"><code>V[0, 0]*d[0]</code></pre>
<pre><code>4.611849897053329</code></pre>
<pre class="python"><code>fig, ax1 = plt.subplots(figsize=(5, 5))
ax1.plot([0, V[0, 0]*d[0]], [0, V[1, 0]*d[0]])
ax1.plot([0, V[0, 1]*d[1]], [0, V[1, 1]*d[1]])
ax1.scatter(X[:,0], X[:,1],s=2)
ax1.set_xlim([-10, 10])
ax1.set_ylim([-10, 10])</code></pre>
<pre><code>(-10.0, 10.0)</code></pre>
<div class="figure">
<img src="merged_files/merged_165_1.png" alt="" />
<p class="caption">png</p>
</div>
<p>FIGURE 3.9. Principal components of some input data points. The largest prin-
cipal component is the direction that maximizes the variance of the projected data,
and the smallest principal component minimizes that variance. Ridge regression
projects y onto these components, and then shrinks the coefficients of the low–
variance components more than the high-variance components.</p>
</div>
</div>
<div id="s-3.4.2.-the-lasso" class="section level2">
<h2><span class="math inline">\(\S\)</span> 3.4.2. The Lasso</h2>
<p>The lasso estimate is defined by</p>
<p><span class="math display">\[\begin{equation}
\hat\beta^{\text{lasso}} = \arg\min_\beta \sum_{i=1}^N \left( y_i-\beta_0-\sum_{j=1}^p x_{ij}\beta_j \right)^2 \text{ subject to } \sum_{j=1}^p |\beta_j| \le t,
\end{equation}\]</span></p>
<p>Just as in ridge regression, we can re-parametrize the constant <span class="math inline">\(\beta_0\)</span> by standardizing the predictors; <span class="math inline">\(\hat\beta_0 = \bar{y}\)</span>, and thereafter we fit a model without an intercept.</p>
<p>In the signal processing literature, the lasso is a.k.a. <em>basis pursuit</em> (Chen et al., 1998).</p>
<p>Also the lasso problem has the equivalent <em>Lagrangian form</em></p>
<p><span class="math display">\[\begin{equation}
\hat\beta^{\text{lasso}} = \arg\min_\beta \left\lbrace \frac{1}{2}\sum_{i=1}^N \left( y_i-\beta_0-\sum_{j=1}^p x_{ij}\beta_j \right)^2 + \lambda\sum_{j=1}^p |\beta_j| \right\rbrace,
\end{equation}\]</span></p>
<p>which is similar to the ridge problem as the <span class="math inline">\(L_2\)</span> ridge penalty is replaced by the <span class="math inline">\(L_1\)</span> lasso penalty. This lasso constraint makes the solutions nonlinear in the <span class="math inline">\(y_i\)</span>, and there is no closed form expresssion as in ridge regression. And computing the above lasso solution is a quadratic programming problem, although efficient algorithms, introduced in <span class="math inline">\(\S\)</span> 3.4.4, are available for computing the entire path of solution as <span class="math inline">\(\lambda\)</span> varies, with the same computational cost as for ridge regression.</p>
<p>Note that</p>
<ul>
<li>If <span class="math inline">\(t \gt t_0 = \sum_1^p \lvert\hat\beta_j^{\text{ls}}\rvert\)</span>, then <span class="math inline">\(\hat\beta^{\text{lasso}} = \hat\beta^{\text{ls}}\)</span>.</li>
<li>Say, for <span class="math inline">\(t = t_0/2\)</span>, then the least squares coefficients are shrunk by about <span class="math inline">\(50\%\)</span> on average.<br />
However, the nature of the shrinkage is not obvious, and we investigate it further in <span class="math inline">\(\S\)</span> 3.4.4.</li>
</ul>
<p>In FIGURE 3.7, for ease of interpretation, we have plotted the lasso prediction error estimates versus the standardized parameter. The Shrinkage Factor <span class="math inline">\(s\)</span> is defined by:</p>
<p><span class="math display">\[\begin{equation}
s = \frac{t}{\sum_1^p \lvert\hat\beta_j\rvert}.
\end{equation}\]</span></p>
<p>A value <span class="math inline">\(\hat s \approx 0.36\)</span> was chosen by 10-fold cross-validation; this caused four coefficients to be set to zero (see Table 3.3). The resulting model has the second lowest test error, slightly lower than the full least squares model, but the standard errors of the test error estimates are fairly large.</p>
<p>FIGURE 3.10 is discussed after implementing the lasso algorithm in <span class="math inline">\(\S\)</span> 3.4.4.</p>
<pre class="python"><code>%load_ext rpy2.ipython</code></pre>
<pre class="r"><code>%%R
load_prostate_data &lt;- function(globalScale=FALSE,trainingScale=TRUE,responseScale=FALSE){ 
  X  = read.table(&quot;../../data/prostate/prostate.data&quot;)
  if( globalScale ){
    if( responseScale ){
      lpsa = X$lpsa - mean(X$lpsa) 
    }else{
      lpsa = X$lpsa 
    }
    train = X$train
    X$lpsa = NULL
    X$train = NULL 
    X = scale(X, TRUE, TRUE)
    Xf = data.frame(X)
    Xf$lpsa = lpsa
    Xf$train = train
    X = Xf
    rm(Xf)
    rm(lpsa)
  }

  # separate into training/testing sets
  # 
  XTraining = subset( X, train )
  XTraining$train = NULL # remove the training/testing column
  p = dim(XTraining)[2]-1
  
  XTesting  = subset( X, train==FALSE )
  XTesting$train = NULL # remove the training/testing column

  #
  # Sometime data is processed and stored in a certain order.  When doing cross validation
  # on such data sets we don&#39;t want to bias our results if we grab the first or the last samples.
  # Thus we randomize the order of the rows in the Training data frame to make sure that each
  # cross validation training/testing set is as random as possible.
  # 
  if( FALSE ){
    nSamples = dim(XTraining)[1] 
    inds = sample( 1:nSamples, nSamples )
    XTraining = XTraining[inds,]
  }

  #
  # In reality we have to estimate everything based on the training data only
  # Thus here we estimate the predictor statistics using the training set
  # and then scale the testing set by the same statistics
  # 
  if( trainingScale ){
    X = XTraining 
    if( responseScale ){
      meanLpsa = mean(X$lpsa) 
      lpsa = X$lpsa - meanLpsa 
    }else{
      lpsa = X$lpsa 
    }
    X$lpsa = NULL
    X = scale(X, TRUE, TRUE)
    means = attr(X,&quot;scaled:center&quot;)
    stds = attr(X,&quot;scaled:scale&quot;)
    Xf = data.frame(X)
    Xf$lpsa = lpsa
    XTraining = Xf

    # scale the testing predictors by the same amounts:
    # 
    DCVTest  = XTesting
    if( responseScale ){
      lpsaTest = DCVTest$lpsa - meanLpsa
    }else{
      lpsaTest = DCVTest$lpsa # in physical units (not mean adjusted)
    }
    DCVTest$lpsa = NULL 
    DCVTest  = t( apply( DCVTest, 1, &#39;-&#39;, means ) ) 
    DCVTest  = t( apply( DCVTest, 1, &#39;/&#39;, stds ) ) 
    DCVTestb = cbind( DCVTest, lpsaTest ) # append back on the response
    DCVTestf = data.frame( DCVTestb ) # a data frame containing all scaled variables of interest
    names(DCVTestf)[p+1] = &quot;lpsa&quot; # fix the name of the response
    XTesting = DCVTestf
  }

  return( list( XTraining, XTesting ) ) 
}

PD = load_prostate_data(globalScale=FALSE,trainingScale=TRUE,responseScale=TRUE) # read in unscaled data 
XTraining = PD[[1]]
XTesting = PD[[2]]

p        = dim(XTraining)[2]-1 # the last column is the response 
nSamples = dim(XTraining)[1] 

library(glmnet)

# alpha = 1 =&gt; lasso
fit = glmnet( as.matrix( XTraining[,1:p] ), XTraining[,p+1], family=&quot;gaussian&quot;, alpha=1 )

# alpha = 0 =&gt; ridge
# fit = glmnet( as.matrix( XTraining[,1:p] ), XTraining[,p+1], family=&quot;gaussian&quot;, alpha=0 )


plot(fit,xlim=c(0, 2.5))
text(x=c(rep(2.4,8)), y=fit$beta@x[393:400], colnames(XTraining)[1:8])
</code></pre>
<div class="figure">
<img src="merged_files/merged_172_0.png" alt="" />
<p class="caption">png</p>
</div>
<p>FIGURE 3.10. Profiles of lasso coefficients, as the tuning parameter t is varied. Coefficients are plotted versus <span class="math inline">\(L1 norm = \sum_{1}^{p}|\hat{\beta}_j |\)</span>. Compare Figure 3.8; the lasso profiles hit zero, while those for ridge do not. The profiles are piece-wise linear, and so are computed only at the points displayed.</p>
<pre class="r"><code>%%R
# do crossvalidation to get the optimal value of lambda 
cvob = cv.glmnet( as.matrix( XTraining[,1:p] ), XTraining[,p+1], family=&quot;gaussian&quot;, alpha=1 )


plot( cvob )

# get the optimal value of lambda: 
lambdaOptimal = cvob$lambda.1se

# refit with this optimal value of lambda:
fitOpt = glmnet( as.matrix( XTraining[,1:p] ), XTraining[,p+1], family=&quot;gaussian&quot;, lambda=lambdaOptimal, alpha=1 )
print( coef(fitOpt), digit=3 )

</code></pre>
<pre><code>9 x 1 sparse Matrix of class &quot;dgCMatrix&quot;
                      s0
(Intercept)     2.29e-16
lcavol          5.72e-01
lweight         2.21e-01
age         .           
lbph            8.25e-02
svi             1.55e-01
lcp         .           
gleason     .           
pgg45           5.08e-02</code></pre>
<div class="figure">
<img src="merged_files/merged_174_1.png" alt="" />
<p class="caption">png</p>
</div>
<pre class="r"><code>%%R
# predict the testing data using this value of lambda: 
yPredict = predict( fit, newx=as.matrix(XTesting[,1:p]), s=lambdaOptimal )
NTest = dim(XTesting[,1:p])[1]
print( mean( ( XTesting[,p+1] - yPredict )^2 ), digit=3 ) 
print( sqrt( var( ( XTesting[,p+1] - yPredict )^2 )/NTest ), digit=3 ) </code></pre>
<pre><code>[1] 0.453
      1
1 0.153</code></pre>
</div>
<div id="s-3.4.3.-discussion-subset-selection-ridge-regression-and-the-lasso" class="section level2">
<h2><span class="math inline">\(\S\)</span> 3.4.3. Discussion: Subset Selection, Ridge Regression, and the Lasso</h2>
<div id="explicit-solutions-in-the-ideal-situation" class="section level3">
<h3>Explicit solutions in the ideal situation</h3>
<p>In the case of an orthonormal input matrix <span class="math inline">\(\mathbf{X}\)</span>, the three procecures have explicit solutions as below. <span class="math inline">\(M\)</span> and <span class="math inline">\(\lambda\)</span> are constants chosen by the corresponding techniques; sign denotes the sign of its argument (±1), and <span class="math inline">\(x_+\)</span> denotes “positive part” of <span class="math inline">\(x\)</span>. Below the table, estimators are shown by broken red lines. The <span class="math inline">\(45^\circ\)</span> line in gray shows the unrestricted estimate for reference.</p>
<ul>
<li>Best subset (size <span class="math inline">\(M\)</span>) drops all variables with coefficients smaller than the <span class="math inline">\(M\)</span>th largest; this is a form of “hard-thresholding.”<br />
<span class="math display">\[\begin{equation}
\hat\beta_j\cdot I\left( \lvert\hat\beta_j\rvert \ge \lvert\hat\beta_{(M)}\rvert \right)
\end{equation}\]</span></li>
<li>Ridge does a proportional shrinkage.<br />
<span class="math display">\[\begin{equation}
\frac{\hat\beta_j}{1+\lambda}
\end{equation}\]</span></li>
<li>Lasso translates each coefficient by a constant factor <span class="math inline">\(\lambda\)</span>, truncating at zero. This is called “soft-thresholding.”<br />
<span class="math display">\[\begin{equation}
\text{sign}\left( \hat\beta_j \right)\left( \lvert\hat\beta_j\rvert - \lambda \right)_+
\end{equation}\]</span></li>
</ul>
<pre class="python"><code>import scipy
import numpy as np
import matplotlib.pyplot as plt

x = np.linspace(-1, 1, 100)

# Set \hat\beta_{M}=0.33
y_best_subset = x.copy()
y_best_subset[np.absolute(y_best_subset) &lt;= .33] = 0

# Set \lambda=0.5
y_ridge = x/1.5

# Set \lambda=0.33
y_lasso = x.copy()
y_lasso[np.absolute(y_lasso) &lt;= .33] = 0
y_lasso[y_lasso &gt; .33] -= .33
y_lasso[y_lasso &lt; -.33] += .33
        

fig34 = plt.figure(figsize=(9, 3))
ax1 = fig34.add_subplot(1, 3, 1)
ax1.plot(x, x, color=&#39;gray&#39;)
ax1.plot(x, y_best_subset, &#39;r--&#39;)
ax1.text(.4, .1, r&#39;$|\hat\beta_{(M)}|$&#39;)
ax1.set_title(&#39;Best Subset&#39;, color = &#39;b&#39;)
ax1.grid()

ax2 = fig34.add_subplot(1, 3, 2)
ax2.plot(x, x, color=&#39;gray&#39;)
ax2.plot(x, y_ridge, &#39;r--&#39;)
ax2.set_title(&#39;Ridge&#39;, color = &#39;b&#39;)
ax2.grid()

ax3 = fig34.add_subplot(1, 3, 3)
ax3.plot(x, x, color=&#39;gray&#39;)
ax3.plot(x, y_lasso, &#39;r--&#39;)
ax3.plot([1, 1], [y_lasso[-1], 1.0], &#39;--&#39;, color=&#39;b&#39;, linewidth=1)
ax3.text(1.0, .8, r&#39;$\lambda$&#39;)
ax3.set_title(&#39;Lasso&#39;, color = &#39;b&#39;)
ax3.grid()
plt.show()</code></pre>
<div class="figure">
<img src="merged_files/merged_177_0.png" alt="" />
<p class="caption">png</p>
</div>
</div>
<div id="back-to-the-reality-the-nonorthonormal-case" class="section level3">
<h3>Back to the reality; the nonorthonormal case</h3>
<p>Back to the nonorthogonal case; some pictures help understand their relationship. Figure 3.11 depicts the lasso (left) and ridge regression (right)
when there are only two parameters. The residual sum of squares has elliptical
contours, centered at the full least squares estimate. The constraint region for ridge regression is the disk <span class="math inline">\(\beta_1^2 + \beta_2^2 \leq t\)</span>, while that for lasso is the diamond <span class="math inline">\(|\beta_1| + |\beta_2| \leq t\)</span>. Both methods find the first point where the elliptical contours hit the constraint region. Unlike the disk, the diamond has corners; if the solution occurs at a corner, then it has one parameter <span class="math inline">\(\beta_j\)</span> equal to zero. When <span class="math inline">\(p &gt; 2\)</span>, the diamond becomes a rhomboid, and has many corners, flat edges and faces; there are many more opportunities for the estimated parameters to be zero.</p>
<pre class="python"><code>import scipy
import numpy as np
import matplotlib.pyplot as plt

x = np.linspace(-1, 1, 100)

# lasso
y_0 = x.copy()
y_1 = 1-np.absolute(y_0)
y_2 = -(1-np.absolute(y_0))


# ridge
y_3 = np.sqrt(1-np.absolute(y_0)**2)
y_4 = -(np.sqrt(1-np.absolute(y_0)**2))


fig35 = plt.figure(figsize=(8, 4))
ax1 = fig35.add_subplot(1, 2, 1)
ax1.fill(x, y_1, color=&#39;cyan&#39;)
ax1.fill(x, y_2, color=&#39;cyan&#39;)
ax1.axis(&#39;off&#39;)
ax1.set_xlim(-2, 5)
ax1.set_ylim(-2, 5)
ax1.arrow(-2, 0, dx=6, dy=0, color=&#39;0&#39;, head_width=0.1,width=.01, linewidth=0.5)
ax1.arrow(0, -2, dx=0, dy=6, color=&#39;0&#39;, head_width=0.1,width=.01, linewidth=0.5)
ax1.text(4.0, -.6, r&#39;$\beta_1$&#39;)
ax1.text(-0.6, 4.0, r&#39;$\beta_2$&#39;)

ax2 = fig35.add_subplot(1, 2, 2)
ax2.fill(x, y_3, color=&#39;cyan&#39;)
ax2.fill(x, y_4, color=&#39;cyan&#39;)
ax2.axis(&#39;off&#39;)
ax2.set_xlim(-2, 5)
ax2.set_ylim(-2, 5)
ax2.arrow(-2, 0, dx=6, dy=0, color=&#39;0&#39;, head_width=0.1,width=.01, linewidth=0.5)
ax2.arrow(0, -2, dx=0, dy=6, color=&#39;0&#39;, head_width=0.1,width=.01, linewidth=0.5)
ax2.text(4.0, -.6, r&#39;$\beta_1$&#39;)
ax2.text(-0.6, 4.0, r&#39;$\beta_2$&#39;)

plt.show()</code></pre>
<div class="figure">
<img src="merged_files/merged_179_0.png" alt="" />
<p class="caption">png</p>
</div>
<p>FIGURE 3.11. Estimation picture for the lasso (left) and ridge regression
(right). Shown are contours of the error and constraint functions. The solid blue areas are the constraint regions <span class="math inline">\(|\beta_1| + |\beta_2| \leq t\)</span> and <span class="math inline">\(\beta_1^2 + \beta_2^2 \leq t^2\)</span>, respectively, while the red ellipses are the contours of the least squares error function.</p>
</div>
<div id="generalization-to-bayes-estimates" class="section level3">
<h3>Generalization to Bayes estimates</h3>
<p>Consider the criterion, for <span class="math inline">\(q \ge 0\)</span>,</p>
<p><span class="math display">\[\begin{equation}
\tilde\beta = \arg\min_\beta \left\lbrace \sum_{i=1}^N \left( y_i - \beta_0 - \sum_{j=1}^p x_{ij}\beta_j \right)^2 + \lambda\sum_{j=1}^p\lvert\beta_j\rvert^q\right\rbrace
\end{equation}\]</span></p>
<p>See FIGURE 3.12 for the contours of <span class="math inline">\(\sum_j\lvert\beta_j\rvert^q\)</span>.</p>
<p>Thinking of <span class="math inline">\(\lvert\beta_j\rvert^q\)</span> as the log-prior density for <span class="math inline">\(\beta_j\)</span>, these are also the equicontours of the prior distrobution of the parameters.</p>
<ul>
<li><span class="math inline">\(q=0\)</span> <span class="math inline">\(\Rightarrow\)</span> variable subset selction, as the penalty simply counts the number of nonzero parameters,</li>
<li><span class="math inline">\(q=1\)</span> <span class="math inline">\(\Rightarrow\)</span> corresponds to the lasso, Laplace distribution with density</li>
</ul>
<p><span class="math display">\[\begin{equation}
\frac{1}{2\tau}\exp\left(-\lvert\beta\rvert/\tau\right),
\end{equation}\]</span>
where <span class="math inline">\(\tau = 1/\lambda\)</span>. In this view, the lasso, ridge, and best subset selection are Bayes estimates with different priors. Note, however, that they are derived as posterior modes, that is, maximizers of the posterior. It is more common to use the mean of the posterior as the Bayes estimate. Ridge solution is also the posterior mean, but the lasso and best subset selection are not.</p>
<p>Note that the lasso case is the smallest <span class="math inline">\(q\)</span> such that the constraint region is convex; non-convext constraint regions make the optimization problem more difficult.</p>
<p>In this view, the lasso, ridge regression and best subset selection are
Bayes estimates with different priors. Note, however, that they are derived
as posterior modes, that is, maximizers of the posterior. It is more common
to use the mean of the posterior as the Bayes estimate. Ridge regression is
also the posterior mean, but the lasso and best subset selection are not.</p>
<p>We might try using other values of <span class="math inline">\(q\)</span> besides <span class="math inline">\(0\)</span>, <span class="math inline">\(1\)</span>, or <span class="math inline">\(2\)</span>. Although one might consider estimating <span class="math inline">\(q\)</span> from data, our experience is that it is not worth the effort for the extra variance incurred.</p>
<p>Although this is the case, with <span class="math inline">\(q \gt 1\)</span>, <span class="math inline">\(\lvert\beta_j\rvert^q\)</span> is differentiable at <span class="math inline">\(0\)</span>, and so does not share the ability of lasso (<span class="math inline">\(q=1\)</span>) for setting coefficients exactly to zero. Partly for this reason as well as for computational tractability, Zou and Hastie (2005) introduced the <em>elastic-net</em> penalty, introduced in <span class="math inline">\(\S\)</span> 18.4.</p>
<pre class="python"><code>x = np.linspace(-1, 1, 100)

# q=4
y_0 = x.copy()
y_1 = np.sqrt(np.sqrt((1-np.absolute(y_0)**4)))
y_2 = -np.sqrt(np.sqrt((1-np.absolute(y_0)**4)))

# ridge q=2
y_3 = np.sqrt(1-np.absolute(y_0)**2)
y_4 = -(np.sqrt(1-np.absolute(y_0)**2))

# lasso q=1
y_0 = x.copy()
y_5 = 1-np.absolute(y_0)
y_6 = -(1-np.absolute(y_0))


# q=0.5
y_7 = (1-np.sqrt(np.absolute(y_0)))**2
y_8 = -(1-np.sqrt(np.absolute(y_0)))**2

# q=0.1
y_9 = (1-(np.absolute(y_0))**0.1)**10
y_10 = -(1-(np.absolute(y_0))**0.1)**10


fig36 = plt.figure(figsize=(10, 2))
ax1 = fig36.add_subplot(1, 5, 1)
ax1.plot(x, y_1, color=&#39;cyan&#39;)
ax1.plot(x, y_2, color=&#39;cyan&#39;)
ax1.axis(&#39;off&#39;)
ax1.plot([-1.2,1.2],[0,0], color=&#39;0&#39;, linewidth=1)
ax1.plot([0,0],[-1.2,1.2], color=&#39;0&#39;, linewidth=1)
ax1.set_xlim(-2, 2)
ax1.set_ylim(-2, 2)
ax1.set_title(&#39;q=4&#39;)

ax2 = fig36.add_subplot(1, 5, 2)
ax2.plot(x, y_3, color=&#39;cyan&#39;)
ax2.plot(x, y_4, color=&#39;cyan&#39;)
ax2.axis(&#39;off&#39;)
ax2.plot([-1.2,1.2],[0,0], color=&#39;0&#39;, linewidth=1)
ax2.plot([0,0],[-1.2,1.2], color=&#39;0&#39;, linewidth=1)
ax2.set_xlim(-2, 2)
ax2.set_ylim(-2, 2)
ax2.set_title(&#39;q=2&#39;)

ax3 = fig36.add_subplot(1, 5, 3)
ax3.plot(x, y_5, color=&#39;cyan&#39;)
ax3.plot(x, y_6, color=&#39;cyan&#39;)
ax3.axis(&#39;off&#39;)
ax3.plot([-1.2,1.2],[0,0], color=&#39;0&#39;, linewidth=1)
ax3.plot([0,0],[-1.2,1.2], color=&#39;0&#39;, linewidth=1)
ax3.set_xlim(-2, 2)
ax3.set_ylim(-2, 2)
ax3.set_title(&#39;q=1&#39;)

ax4 = fig36.add_subplot(1, 5, 4)
ax4.plot(x, y_7, color=&#39;cyan&#39;)
ax4.plot(x, y_8, color=&#39;cyan&#39;)
ax4.axis(&#39;off&#39;)
ax4.plot([-1.2,1.2],[0,0], color=&#39;0&#39;, linewidth=1)
ax4.plot([0,0],[-1.2,1.2], color=&#39;0&#39;, linewidth=1)
ax4.set_xlim(-2, 2)
ax4.set_ylim(-2, 2)
ax4.set_title(&#39;q=0.5&#39;)

ax5 = fig36.add_subplot(1, 5, 5)
ax5.plot(x, y_9, color=&#39;cyan&#39;)
ax5.plot(x, y_10, color=&#39;cyan&#39;)
ax5.axis(&#39;off&#39;)
ax5.plot([-1.2,1.2],[0,0], color=&#39;0&#39;, linewidth=1)
ax5.plot([0,0],[-1.2,1.2], color=&#39;0&#39;, linewidth=1)
ax5.set_xlim(-2, 2)
ax5.set_ylim(-2, 2)
ax5.set_title(&#39;q=0.1&#39;)

plt.show()</code></pre>
<div class="figure">
<img src="merged_files/merged_182_0.png" alt="" />
<p class="caption">png</p>
</div>
<p>FIGURE 3.12. Contours of constant value of <span class="math inline">\(\sum_{j}|\beta_j|^q\)</span> for given values of <span class="math inline">\(q\)</span>.</p>
</div>
</div>
<div id="s-3.4.4.-least-angle-regression" class="section level2">
<h2><span class="math inline">\(\S\)</span> 3.4.4. Least Angle Regression</h2>
<p>Least Angle Regression (LAR) is a relative newcomer (Efron et al., 2004), and can be viewed as a kind of “democratic” version of forward stepwise regression (<span class="math inline">\(\S\)</span> 3.3.2). Forward stepwise regression builds a model sequentially, adding one variable at a time. At each step, it identifies the best variable to include in the <em>active set</em>, and then updates the least squares fit to include all the active variables.</p>
<p>LAR uses a similar strategy, but only enters “as much” of a predictor as it deserves.</p>
<ol style="list-style-type: decimal">
<li>Identify the variable most correlated with the response.</li>
<li>Rather than fit this variable completely, LAR moves the coefficient of this variable continuously toward its least-sqaures value (causing its correlation with the evolving residual to decrease in absolute value).</li>
<li>As soon as another variable “catches up” in terms of correlation with residual, the process is paused.</li>
<li>The second variable then joins the active set, and their coefficients are moved together in a way that keeps their correlations tied and decreasing.</li>
<li>This process is continued until all the variables are in the model, and ends at the full least-squares fit.</li>
</ol>
<p>Below is the algorithm in detail.</p>
<div id="algorithm-3.2.-least-angle-regression" class="section level3">
<h3>Algorithm 3.2. Least Angle Regression</h3>
<ol style="list-style-type: decimal">
<li><p>Standardize the predictors to have mean zero and unit norm.<br />
Start with the residual <span class="math inline">\(r = y - \bar{y}\)</span>,and coefficients <span class="math inline">\(\beta_0 = \beta_1 = \cdots = \beta_p = 0\)</span>. Since with all <span class="math inline">\(\beta_j = 0\)</span> and standardized predictors the constant coefficient <span class="math inline">\(\beta_0=\bar{y}\)</span>.</p></li>
<li><p>Set <span class="math inline">\(k = 1\)</span> and begin start the <span class="math inline">\(k\)</span>-th step. Since all values of <span class="math inline">\(\beta_j\)</span> are zero the first residual is <span class="math inline">\(r_1 = y - \bar{y}\)</span>. Find the predictor <span class="math inline">\(x_j\)</span> that is most correlated with this residual <span class="math inline">\(r_1\)</span>. Then as we begin this <span class="math inline">\(k = 1\)</span> step we have the active step given by <span class="math inline">\(A_1 = \{x_j\}\)</span> and the active
coefficients given by <span class="math inline">\(\beta_{A_1} = [0]\)</span>.</p></li>
<li><p>Move <span class="math inline">\(\beta_j\)</span> from <span class="math inline">\(0\)</span> towards its least-squares coefficient <span class="math display">\[\delta_1=\left(X_{A_1}^TX_{A_1}\right)^{-1}X_{A_1}^Tr_1=\frac{x_j^Tr_1}{x_j^Tx_j}=x_j^T r_1\]</span>
until some other competitor <span class="math inline">\(x_k\)</span> has as much correlation with the current residual as does <span class="math inline">\(x_j\)</span>.
The path taken by the elements in <span class="math inline">\(\beta_{A_1}\)</span> can be parameterized by <span class="math display">\[\beta_{A_1}(\alpha)\equiv\beta_{A_1}+\alpha\delta_1=0+\alpha x_j^T r_1=(x_j^T r_1)\alpha,\quad(0\leq\alpha\leq1)\]</span>
This path of the coefficients <span class="math inline">\(\beta_{A_1}(\alpha)\)</span> will produce a path of fitted values given by <span class="math display">\[\hat{f}_1(\alpha)=X_{A_1}\beta_{A_1}(\alpha)=(x_j^T r_1)\alpha x_j\]</span> and a residual of <span class="math display">\[r(\alpha)=y-\bar{y}-\hat{f}_1(\alpha)=y-\bar{y}-(x_j^T r_1)\alpha x_j=r_1-(x_j^T r_1)\alpha x_j\]</span>
Now at this point <span class="math inline">\(x_j\)</span> itself has a correlation with this residual as <span class="math inline">\(\alpha\)</span> varies given by <span class="math display">\[x_j^T(r_1-(x_j^T r_1)\alpha x_j)=x_j^Tr_1-(x_j^T r_1)\alpha=(1-\alpha)x_j^T r_1\]</span></p></li>
<li><p>Move <span class="math inline">\(\beta_j\)</span> and <span class="math inline">\(\beta_k\)</span> in the direction defined by their joint least squares coefficient of the current residual on <span class="math inline">\((x_j,x_k)\)</span>, until some other competitor <span class="math inline">\(x_l\)</span> has as much correlation with the current residual.</p></li>
<li><p>Continue in this way until all <span class="math inline">\(p\)</span> predictors have been entered. After <span class="math inline">\(\min(N-1,p)\)</span> steps, we arrive at the full least-squares solution.</p></li>
</ol>
<p>The termination condition in step 5 requires some explanation. If <span class="math inline">\(p &gt; N-1\)</span>, the LAR algorithm reaches a zero residual solution after <span class="math inline">\(N-1\)</span> steps (the <span class="math inline">\(-1\)</span> is because we have centered the data).</p>
<p>Suppose</p>
<ul>
<li><span class="math inline">\(\mathcal{A}_k\)</span> is the active set of variables at the beginning of the <span class="math inline">\(k\)</span>th step,</li>
<li><span class="math inline">\(\beta_{\mathcal{A}_k}\)</span> is the coefficient vector for these variables at this step.
There will be <span class="math inline">\(k-1\)</span> nonzero values, and the one just entered will be zero. If <span class="math inline">\(\mathbf{r}_k = \mathbf{y} - \mathbf{X}_{\mathcal{A}_k}\beta_{\mathcal{A}_k}\)</span> is the current residual, then the direction for this step is</li>
</ul>
<p><span class="math display">\[\begin{equation}
\delta_k = \left( \mathbf{X}_{\mathcal{A}_k}^T\mathbf{X}_{\mathcal{A}_k} \right)^{-1}\mathbf{X}_{\mathcal{A}_k}^T \mathbf{r}_k\\
=\left(\mathbf{V}\mathbf D^2\mathbf{V}^T\right)^{-1}\mathbf{V}\mathbf D\mathbf U^T\mathbf{r}_k\\
=\mathbf{V}^{-T}\mathbf D^{-2}\mathbf D\mathbf U^T\mathbf{r}_k\\
\end{equation}\]</span></p>
<p>The coefficient profile then evolves as</p>
<p><span class="math display">\[\begin{equation}
\beta_{\mathcal{A}_k}(\alpha) = \beta_{\mathcal{A}_k} + \alpha\delta_k,
\end{equation}\]</span></p>
<p>and this direction keeps the correlations tied and decreasing (See Exercise 3.23).</p>
<p>If the fit vector at the beginning of this step is <span class="math inline">\(\hat{\mathbf{f}}_k\)</span>, then it evolves as</p>
<p><span class="math display">\[\begin{equation}
\hat{\mathbf{f}}_k(\alpha) = \hat{\mathbf{f}}_k + \alpha\mathbf{u}_k,
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\mathbf{u}_k = \mathbf{X}_{\mathcal{A}_k}\delta_k\)</span> is the new fit direction.</p>
<p>The name “least angle” arises from a geometrical interpretation of this process; <span class="math inline">\(\mathbf{u}_k\)</span> makes the smallest (and equal) angle with each of the predictors in <span class="math inline">\(\mathcal{A}_k\)</span> (Exercise 3.24).</p>
<p>Note that, by construction, the coefficients in LAR change in a piecewise linear fashion, and we do not need to take small steps and recheck the correlations in step 3; using knowledge of the covariance of the predictors and the piecewise linearity of the algorithm, we can work out the exact step length at the beginning of each step (Exercise 3.25).</p>
<pre class="python"><code>import numpy as np
import matplotlib.pyplot as plt
import matplotlib.animation as animation

fig = plt.figure(figsize=(5, 5))
ax = fig.add_subplot(1, 1, 1)

x = np.arange(0, 2, 0.01)
ax.plot([0,1], [0,1], color=&#39;0&#39;, linewidth=1)
ax.plot([1, 0.5],[0, 0.5], color=&#39;0&#39;, linewidth=1)
ax.set_xlim(0,1)
ax.text(0.5, -0.04, &#39;r&#39;, size=15)
ax.arrow(0.01, 0.05, dx=0.4, dy=0.4, color=&#39;0&#39;, head_width=0.05,width=.01, linewidth=0.5)
ax.text(0.3, 0.45, &#39;u&#39;, size=15)
ax.set_title(&#39;Animation of the LAR moves the coefficient of \n a variable continuously toward its least-sqaures value&#39;)

for n in range(50):
    plt.plot([1, x[n]],[0, x[n]], color=&#39;cyan&#39;)
    
plt.show()</code></pre>
<div class="figure">
<img src="merged_files/merged_187_0.png" alt="" />
<p class="caption">png</p>
</div>
<pre class="python"><code>&quot;&quot;&quot;FIGURE 3.10. LARS(lasso) coefficients profile on prostate cancer data&quot;&quot;&quot;
import scipy
import scipy.stats
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt</code></pre>
<pre class="python"><code>df = pd.read_csv(&#39;../../data/prostate/prostate.data&#39;, delimiter=&#39;\t&#39;,
                 index_col=0)
dfy = df.pop(&#39;lpsa&#39;)
dfmask = df.pop(&#39;train&#39;)
size_predictor = df.columns.size

dfmask_train = dfmask == &#39;T&#39;
dfx_train = df[dfmask_train]
dfy_train = dfy[dfmask_train]
size_train = dfmask_train.sum()

dfmask_test = dfmask == &#39;F&#39;
dfx_test = df[dfmask_train]
dfy_test = dfy[dfmask_train]
size_test = dfmask_train.sum()

# Centering for the training data
meany_train = dfy_train.mean()
dfy_train_centered = dfy_train.subtract(meany_train)
dfx_train_centered = dfx_train.subtract(dfx_train.mean())

normx = np.sqrt((dfx_train_centered**2).sum())
dfx_train_scaled = dfx_train_centered.divide(normx)

matx = dfx_train_scaled.values
vecy = dfy_train_centered.values</code></pre>
<pre class="python"><code>&quot;&quot;&quot;Main loop for LAR

Reference:
Least Angle Regression, Efron et al., 2004, The Annals of Statistics
&quot;&quot;&quot;
# Initial data
signs = np.zeros(size_predictor)
betas = np.zeros(size_predictor)
indices_predictor = np.arange(size_predictor)
vecy_fitted = np.zeros_like(vecy)
beta_lars = [[0]*size_predictor]

for k in range(size_predictor):
    #vecc=X^T\bar{y}
    vecc = matx.T @ (vecy-vecy_fitted)
    vecc_abs = np.absolute(vecc)
    
    #maxc is x_j\bar{y} of the most correlated x_j
    maxc = vecc_abs.max()
    mask_maxc = np.isclose(vecc_abs, maxc)
    active = indices_predictor[mask_maxc]
    signs = np.where(vecc[active] &gt; 0, 1, -1)
    
    #get the active column (predictor) of matx
    matx_active = signs*matx[:, active]

    u, s, vh = scipy.linalg.svd(matx_active, full_matrices=False)
    
    # X^TX = VD^2V^T
    matg = vh.T @ np.diag(s**2) @ vh
    # (X^TX)^-1 = VD^-2V^T
    matg_inv = vh.T @ np.diag(np.reciprocal(s**2)) @ vh
    vec1 = np.ones(len(active))
    scalara = (matg_inv.sum())**(-.5)
    
    #sum in the rows of matg_inv, equal to (X^TX)^-1 @ 1
    vecw = scalara * matg_inv.sum(axis=1)
    vecu = matx_active @ vecw

    veca = matx.T @ vecu

    if k &lt; size_predictor-1:
        inactive = indices_predictor[np.invert(mask_maxc)]
        arr_gamma = np.concatenate([(maxc-vecc[inactive])/(scalara-veca[inactive]),
                                       (maxc+vecc[inactive])/(scalara+veca[inactive])])
        scalargamma = arr_gamma[arr_gamma &gt; 0].min()
    else:
        scalargamma = maxc/scalara

    vecy_fitted += scalargamma*vecu
    betas[active] += scalargamma*signs
    beta_lars.append(list(betas))</code></pre>
<pre class="python"><code>fig = plt.figure(figsize=(8, 8))
ax = fig.add_subplot(1, 1, 1)
l1length_ols_beta = sum(abs(coeff) for coeff in beta_lars[-1])
l1length_lars_beta = [sum(abs(coeff) for coeff in beta)/l1length_ols_beta for beta in beta_lars]
ax.plot(l1length_lars_beta, beta_lars, &#39;o-&#39;)
ax.grid()
ax.set_xlabel(&#39;Shrinkage Factor s&#39;)
ax.set_ylabel(&#39;Coefficients&#39;)
ax.legend(dfx_train.columns)
ax.set_xlim(-.2, 1.2)
ax.set_ylim(-2.2, 11.0)
for x, y, s in zip(np.ones(8)*(1+0.02), beta_lars[-1], df.columns):
    ax.text(x, y, s, color=&#39;0&#39;, fontsize=12)
ax.margins(0,0)
plt.show()</code></pre>
<div class="figure">
<img src="merged_files/merged_191_0.png" alt="" />
<p class="caption">png</p>
</div>
<p>For the prostate data, the LAR coefficient profile turns out to be identical to the lasso profile in FIGURE 3.10, which never crosses zero. These observations lead to a simple modification of the LAR algorithm that gives the entire lasso path, which is also piecewise-linear.</p>
</div>
<div id="algorithm-3.2a.-least-angle-regression-lasso-modification" class="section level3">
<h3>Algorithm 3.2a. Least Angle Regression: Lasso Modification</h3>
<p>4a. If a non-zero coefficient hits zero, drop its variable from the active set and recompute the current joint least squares direction.</p>
</div>
<div id="computational-efficiency" class="section level3">
<h3>Computational efficiency</h3>
<p>The LAR (lasso) algorithm is extremely efficient, requiring the same order of computation as that of a single least squares fit using the <span class="math inline">\(p\)</span> predictors. LAR always take <span class="math inline">\(p\)</span> steps to get to the full least squares estimates. The lasso path can have more than <span class="math inline">\(p\)</span> steps, although the two are often quite similar. Also the LAR algorithm with the lasso modification is efficient, especially when <span class="math inline">\(p \gg N\)</span>. Osborne et al. (2000) also discovered a piecewise-linear path for computing the lasso, which they called a <em>homotopy</em> algorithm.</p>
</div>
<div id="heuristic-argument-for-why-lar-and-lasso-are-so-similar" class="section level3">
<h3>Heuristic argument for why LAR and lasso are so similar</h3>
<p>Although the LAR algorithm is stated in terms of correlations, if the input features are standardized, it is equivalent and easier to work with inner-products.</p>
<p>Let <span class="math inline">\(\mathcal{A}\)</span> be the active set at some stage, tied in their absolute inner-product with the current residuals <span class="math inline">\(\mathbf{y}-\mathbf{X}\beta\)</span>, expressed as</p>
<p><span class="math display">\[\begin{equation}
\mathbf{x}_j^T\left( \mathbf{y}-\mathbf{X}\beta \right) = \gamma\cdot s_j, \forall j\in\mathcal{A},
\end{equation}\]</span></p>
<p>where <span class="math inline">\(s_j\in\lbrace -1, 1\rbrace\)</span> indicates the sign of the inner-product, and <span class="math inline">\(\gamma\)</span> is the common value. Also note that</p>
<p><span class="math display">\[\begin{equation}
\lvert \mathbf{x}_k^T \left( \mathbf{y}-\mathbf{X}\beta \right) \rvert \le \gamma, \forall k \notin \mathcal{A}
\end{equation}\]</span></p>
<p>Now consider the lasso criterion</p>
<p><span class="math display">\[\begin{equation}
\hat\beta^{\text{lasso}} = \arg\min_\beta \left\lbrace \frac{1}{2}\sum_{i=1}^N \left( y_i-\beta_0-\sum_{j=1}^p x_{ij}\beta_j \right)^2 + \lambda\sum_{j=1}^p |\beta_j| \right\rbrace,
\end{equation}\]</span></p>
<p>which we write in the vector form</p>
<p><span class="math display">\[\begin{equation}
R(\beta) = \frac{1}{2}\|\mathbf{y}-\mathbf{X}\beta\|^2_2 + \lambda\|\beta\|_1.
\end{equation}\]</span></p>
<p>Let <span class="math inline">\(\mathcal{B}\)</span> be the active set in the solution for a given value of <span class="math inline">\(\lambda\)</span>. For these variables <span class="math inline">\(R(\beta)\)</span> is differentiable, and the stationary conditions give</p>
<p><span class="math display">\[\begin{align*}
\frac{\partial R}{\partial\beta} &amp;= -\mathbf{X}^T(\mathbf{y}-\mathbf{X}\beta) + \lambda \frac{\partial \|\beta\|_1}{\partial\beta} = 0  \\
\mathbf{x}^T_j\left( \mathbf{y}-\mathbf{X}\beta \right) &amp;= \lambda\cdot\text{sign}(\beta_j), \forall j\in\mathcal{B}
\end{align*}\]</span></p>
<p>This is identical to the above inner-product expression only if the sign of <span class="math inline">\(\beta_j\)</span> matches the sign of the inner-product. This is why the LAR and lasso start to differ when an active coefficient passes through zero; the above condition is violated for that variable, and it is kicked out of the active set <span class="math inline">\(\mathcal{B}\)</span>. Exercise 3.23 shows that these equations imply a piecewise linear coefficient profile as <span class="math inline">\(\lambda\)</span> decreases.</p>
<p>The stationary conditions for the non-active variables require that</p>
<p><span class="math display">\[\begin{equation}
\lvert \mathbf{x}_k^T(\mathbf{y}-\mathbf{X}\beta) \rvert \le \lambda, \forall k\notin\mathcal{B},
\end{equation}\]</span></p>
<p>which again agrees with the LAR algorithm.</p>
</div>
<div id="degrees-of-freedom-formula-for-lar-and-lasso" class="section level3">
<h3>Degrees-of-freedom formula for LAR and lasso</h3>
<p>Suppose that we fit a linear model via the LAR procedure, stopping at some number of steps <span class="math inline">\(k&lt;p\)</span>, or equivalently using a lasso bound <span class="math inline">\(t\)</span> that produces a constrained version of the full least squares fit. How many parameters, or “degrees of freedom” have we used?</p>
<p>In classical statistics, the number of linearly independent parameters is what is meant by “degrees of freedom.” So a least squares model with <span class="math inline">\(k\)</span> features has degress of freedom to be <span class="math inline">\(k\)</span>. Alternatively, suppose that we carry out a best subset selection to determine the “optimal” set of <span class="math inline">\(k\)</span> predictors. Then the resulting model has <span class="math inline">\(k\)</span> parameters, but in some sense we have used up more than <span class="math inline">\(k\)</span> degrees of freedom.</p>
<p>We need a more general defintion for the effective degrees of freedom of an adaptively fitted model. Define the degrees of freedom of the fitted vector <span class="math inline">\(\hat{\mathbf{y}}\)</span> as</p>
<p><span class="math display">\[\begin{equation}
\text{df}(\hat{\mathbf{y}}) = \frac{1}{\sigma^2}\sum_{i=1}^N \text{Cov}(\hat{y}_i, y_i),
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\text{Cov}(\hat{y}_i, y_i)\)</span> refers to the sampling covariance between <span class="math inline">\(\hat{y}_i\)</span> and <span class="math inline">\(y_i\)</span>. This makes intuitive sense: The harder that we fit to the data, the larger this covariance and hence <span class="math inline">\(\text{df}(\hat{\mathbf{y}})\)</span>. This is a useful notion of degrees of freedom, one that can be applied to any model prediction <span class="math inline">\(\hat{\mathbf{y}}\)</span>, including models adaptively fitted to the training data. This definition is motivated and discussed further in <span class="math inline">\(\S\)</span> 7.4-7.6.</p>
<p>We relate this scalar expression into a vector inner product expression as
<span class="math display">\[\begin{align}
\text{df}(\hat{\mathbf{y}}) &amp;= \frac{1}{\sigma^2}\sum_{i=1}^N \text{Cov}(\hat{y}_i, y_i)\\
&amp;=\frac{1}{\sigma^2}\sum_{i=1}^N \text{Cov}(\mathbf{e}_i^T\hat{\mathbf{y}}, \mathbf{e}_i^T\mathbf{y})\\
&amp;=\frac{1}{\sigma^2}\sum_{i=1}^N \mathbf{e}_i^T\text{Cov}(\hat{\mathbf{y}}, \mathbf{y})\mathbf{e}_i\\
\end{align}\]</span></p>
<p>Now for ordinary least squares regression we have
<span class="math display">\[\hat{\mathbf{y}}=\mathbf{X}\hat{\beta}^{ls}=\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}\]</span>
so that the above expression for <span class="math inline">\(\text{Cov}(\hat{\mathbf{y}}, \mathbf{y})\)</span> becomes <span class="math display">\[\text{Cov}(\hat{\mathbf{y}}, \mathbf{y})=\text{Cov}(\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}, \mathbf{y})=\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\text{Cov}(\mathbf{y}, \mathbf{y})=\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\sigma^2\]</span>
Thus <span class="math display">\[\text{Cov}(\hat{y}_i, y_i)=\mathbf{e}_i^T\text{Cov}(\hat{\mathbf{y}}, \mathbf{y})\mathbf{e}_i^T=\mathbf{e}_i^T\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\sigma^2\mathbf{e}_i\]</span></p>
<p>Note that <span class="math inline">\(\mathbf{X}^T\mathbf{e}_i=\mathbf{x}_i\)</span> is the <span class="math inline">\(i\)</span>th samples feature vector for <span class="math inline">\(1\leq i\leq N\)</span> and we have <span class="math display">\[\text{Cov}(\hat{\mathbf{y}}, \mathbf{y})=\mathbf{x}_i^T(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{x}_i\sigma^2\]</span>
which when we sum for <span class="math inline">\(i = 1\)</span> to <span class="math inline">\(N\)</span> and divide by <span class="math inline">\(\sigma^2\)</span> gives</p>
<p><span class="math display">\[\begin{align}
\text{df}(\hat{\mathbf{y}}) &amp;= \frac{1}{\sigma^2}\sum_{i=1}^N \text{Cov}(\hat{y}_i, y_i)\\
&amp;=\frac{1}{\sigma^2}\sum_{i=1}^N \text{Cov}(\mathbf{e}_i^T\hat{\mathbf{y}}, \mathbf{e}_i^T\mathbf{y})\\
&amp;=\frac{1}{\sigma^2}\sum_{i=1}^N \mathbf{e}_i^T\text{Cov}(\hat{\mathbf{y}}, \mathbf{y})\mathbf{e}_i\\
&amp;=\sum_{i=1}^N \mathbf{x}_i^T(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{x}_i\\
&amp;=\sum_{i=1}^N \text{trace}\left(\mathbf{x}_i^T(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{x}_i\right)\\
&amp;=\sum_{i=1}^N \text{trace}\left(\mathbf{x}_i\mathbf{x}_i^T(\mathbf{X}^T\mathbf{X})^{-1}\right)\\
&amp;=\text{trace}\left(\sum_{i=1}^N \left(\mathbf{x}_i\mathbf{x}_i^T\right)(\mathbf{X}^T\mathbf{X})^{-1}\right)\\
\end{align}\]</span>
Since
<span class="math display">\[
\sum_{i=1}^N \left(\mathbf{x}_i\mathbf{x}_i^T\right)=\begin{bmatrix}
\mathbf{x}_1 &amp; \mathbf{x}_2 &amp; \cdots &amp; \mathbf{x}_N
\end{bmatrix}\begin{bmatrix}
\mathbf{x}_1^T \\
\mathbf{x}_2^T \\
\vdots \\
\mathbf{x}_N^T\\
\end{bmatrix}=\mathbf{X}^T\mathbf{X}
\]</span></p>
<p>Thus when there are <span class="math inline">\(k\)</span> predictors we get
<span class="math display">\[\text{df}(\hat{\mathbf{y}})=\text{trace}\left(\mathbf{X}^T\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\right)=\text{trace}\left(\mathbf{I}\right)=k\]</span></p>
<p>To do the same thing for ridge regression:
<span class="math display">\[\begin{equation}
\hat{\mathbf{y}}=\mathbf{X}\hat\beta^{\text{ridge}} = \mathbf{X}\left( \mathbf{X}^T\mathbf{X} + \lambda\mathbf{I} \right)^{-1}\mathbf{X}^T\mathbf{y},
\end{equation}\]</span></p>
<p>so that
<span class="math display">\[\text{Cov}(\hat{y}_i, y_i)=\mathbf{e}_i^T\text{Cov}(\hat{\mathbf{y}}, \mathbf{y})\mathbf{e}_i^T=\mathbf{e}_i^T\mathbf{X}\left( \mathbf{X}^T\mathbf{X} + \lambda\mathbf{I} \right)^{-1}\mathbf{X}^T\sigma^2\mathbf{e}_i\\
=\sigma^2\left(\mathbf{X}^T\mathbf{e}_i\right)^T\left( \mathbf{X}^T\mathbf{X} + \lambda\mathbf{I} \right)^{-1}\mathbf{X}^T\mathbf{e}_i\\
=\sigma^2\mathbf{x}_i^T\left( \mathbf{X}^T\mathbf{X} + \lambda\mathbf{I} \right)^{-1}\mathbf{x}_i\]</span>
Then summing for <span class="math inline">\(i = 1, 2, \cdots ,N\)</span> and dividing by <span class="math inline">\(\sigma^2\)</span> to get
<span class="math display">\[\begin{align}
\text{df}(\hat{\mathbf{y}})&amp;=\text{trace}\left(\mathbf{X}^T\mathbf{X}\left(\mathbf{X}^T\mathbf{X} + \lambda\mathbf{I} \right)^{-1}\right)\\
&amp;=\text{trace}\left(\mathbf{X}\left(\mathbf{X}^T\mathbf{X} + \lambda\mathbf{I} \right)^{-1}\mathbf{X}^T\right)
\end{align}\]</span></p>
<p>Now</p>
<ul>
<li>for a linear regression with <span class="math inline">\(k\)</span> fixed predictors, it is easy to show that <span class="math inline">\(\text{df}(\hat{\mathbf{y}}) = k\)</span>.</li>
<li>Likewise for ridge regression, <span class="math inline">\(\text{df}(\hat{\mathbf{y}}) = \text{trace}(\mathbf{S}_\lambda)\)</span>.</li>
</ul>
<p>In both these cases, <span class="math inline">\(\text{df}\)</span> is simple to evaluate because the fit</p>
<p><span class="math display">\[\begin{equation}
\hat{\mathbf{y}} = \mathbf{H}_\lambda \mathbf{y}
\end{equation}\]</span></p>
<p>is linear in <span class="math inline">\(\mathbf{y}\)</span>. If we think about the above definition of degrees of freedom in the context of a best subset selection of size <span class="math inline">\(k\)</span>, it seems clear that <span class="math inline">\(\text{df}(\hat{\mathbf{y}}) &gt; k\)</span>, and this can be verified by estimating <span class="math inline">\(\text{Cov}(\hat{y}_i,y_i)/\sigma^2\)</span> directly by simulation. However there is no closed form method for estimating <span class="math inline">\(\text{df}\)</span> for best subset selection.</p>
<p>For LAR and lasso, estimation of <span class="math inline">\(\text{df}\)</span> is more tractable since these techniques are adaptive in a smoother way than best subset selection.</p>
<ul>
<li>After the <span class="math inline">\(k\)</span>th step of the LAR procedure, <span class="math inline">\(\text{df} = k\)</span>.</li>
<li>For the lasso, the modified LAR procedure often takes more than <span class="math inline">\(p\)</span> steps, since predictors can drop out. Hence the definition is a little different; for the lasso, at any stage <span class="math inline">\(\text{df}\)</span> approximately equals the number of predictors in the model.<br />
While this approximation works reasonably well anywhere in the lasso path, for each <span class="math inline">\(k\)</span> it works best at the <em>last</em> model in the sequence that contains <span class="math inline">\(k\)</span> predictors.</li>
</ul>
<p>A detailed study of the effective degrees of freedom for the lasso may be found in Zou et al. (2007).</p>
</div>
</div>
</div>
<div id="s-3.5.-methods-using-derived-input-directions" class="section level1">
<h1><span class="math inline">\(\S\)</span> 3.5. Methods Using Derived Input Directions</h1>
<p>In many situations we have a large number of inputs, often very correlated. The methods in this section produce a small number of linear combinations <span class="math inline">\(Z_m\)</span>, <span class="math inline">\(m=1,\cdots,M\)</span> of the original inputs <span class="math inline">\(X_j\)</span>, and the <span class="math inline">\(Z_m\)</span> are then used in place of the <span class="math inline">\(X_j\)</span> as inputs in regression. The methods differ in how the linear combinations are constructed.</p>
<div id="s-3.5.1.-principal-components-regression" class="section level2">
<h2><span class="math inline">\(\S\)</span> 3.5.1. Principal Components Regression</h2>
<p>The linear combinations <span class="math inline">\(Z_m\)</span> used in principal component regression (PCR) are the principal components as defined in <span class="math inline">\(\S\)</span> 3.4.1.</p>
<p>PCR forms the derived input columns</p>
<p><span class="math display">\[\begin{equation}
\mathbf{z}_m = \mathbf{X} v_m,
\end{equation}\]</span></p>
<p>and then regress <span class="math inline">\(\mathbf{y}\)</span> on <span class="math inline">\(\mathbf{z}_1,\mathbf{z}_2,\cdots,\mathbf{z}_M\)</span> for some <span class="math inline">\(M\le p\)</span>. Since the <span class="math inline">\(\mathbf{z}_m\)</span> are orthogonal, this regression is just a sum of univariate regressions:</p>
<p><span class="math display">\[\begin{equation}
\hat{\mathbf{y}}_{(M)}^{\text{pcr}} = \bar{y}\mathbf{1} + \sum_{m=1}^M \hat\theta_m \mathbf{z}_m = \bar{y}\mathbf{1} + \mathbf{X}\mathbf{V}_M\hat{\mathbf{\theta}},
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\hat\theta_m = \langle\mathbf{z}_m,\mathbf{y}\rangle \big/ \langle\mathbf{z}_m,\mathbf{z}_m\rangle\)</span>. We can see from the last equality that, since the <span class="math inline">\(\mathbf{z}_m\)</span> are each linear combinations of the original <span class="math inline">\(\mathbf{x}_j\)</span>, we can express the solution in terms of coefficients of the <span class="math inline">\(\mathbf{x}_j\)</span>.</p>
<p><span class="math display">\[\begin{equation}
\hat\beta^{\text{pcr}}(M) = \sum_{m=1}^M \hat\theta_m v_m.
\end{equation}\]</span></p>
<p>As with ridge regression, PCR depends on the scaling of the inputs, so typically we first standardized them.</p>
<p>lambdas### Comparison with ridge regression</p>
<p>If <span class="math inline">\(M=p\)</span>, since the columns of <span class="math inline">\(\mathbf{Z} = \mathbf{UD}\)</span> span the <span class="math inline">\(\text{col}(\mathbf{X})\)</span>,</p>
<p><span class="math display">\[\begin{equation}
\hat\beta^{\text{pcr}}(p) = \hat\beta^{\text{ls}}.
\end{equation}\]</span></p>
<p>For <span class="math inline">\(M&lt;p\)</span> we get a reduced regression and we see that PCR is very similar to ridge regression: both operate via the principal components of the input matrix.
* Ridge regression shrinks the coefficients of the principal components (FIGURE 3.17), shrinking more depending on the size of the corresponding eigenvalue;
* PCR discards the <span class="math inline">\(p-M\)</span> smallest eigenvalue components. FIGURE 3.17 illustrates this.</p>
<pre class="python"><code>import scipy
import scipy.stats
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt</code></pre>
<pre class="python"><code>data = pd.read_csv(&#39;../../data/prostate/prostate.data&#39;, delimiter=&#39;\t&#39;,
                   index_col=0)
data_y = data.pop(&#39;lpsa&#39;)
mask_train = data.pop(&#39;train&#39;)

data_x_train = data[mask_train == &#39;T&#39;]
data_y_train = data_y[mask_train == &#39;T&#39;]
beta_intercept = data_y_train.mean()
# Centering for the training data y by subtracting the mean
data_y_train_centered = data_y_train.subtract(beta_intercept)
# Centering and scaling for the training data x using zscore
data_x_train_normalized = data_x_train.apply(scipy.stats.zscore)
vec_y = data_y_train_centered.values

data_x_test = data[mask_train == &#39;F&#39;]
# Centering and scaling for the test data x using zscore
data_x_test_normalized = data_x_test.apply(scipy.stats.zscore)
data_y_test = data_y[mask_train == &#39;F&#39;]
vec_y_test = data_y_test.values

size_train = sum(mask_train == &#39;T&#39;)
size_test = sum(mask_train == &#39;F&#39;)
size_predictor = data_x_train.columns.size</code></pre>
<pre class="python"><code>def lambdas_from_edf(singular_squared:np.ndarray, interval:int) -&gt;np.ndarray:
    &quot;&quot;&quot;Given squared singular values of data matrix, calculate the lambdas
    with `interval` parameter to split unit intervals s.t. the resulting
    effective degrees of freedom are equidistant with 1/interval, via the
    Newton-Raphson method. e.g., if interval = 10, it produces lambdas for
    0, 0.5, 0.6, 0.7, ...&quot;&quot;&quot;
    p = singular_squared.size
    edfs = np.linspace(.5, p-.5, (p-1)*interval+1)
    threshold = 1e-3
    lambdas = []
    for edf in edfs:
        # Newton-Raphson
        lambda0 = (p-edf)/edf
        lambda1 = 1e6
        diff = lambda1 - lambda0
        while diff &gt; threshold:
            num = (singular_squared/(singular_squared+lambda0)).sum()-edf
            denom = (singular_squared/((singular_squared+lambda0)**2)).sum()
            lambda1 = lambda0 + num/denom
            diff = lambda1 - lambda0
            lambda0 = lambda1
        lambdas.append(lambda1)
    lambdas.append(0)
    edfs = np.concatenate(([0], edfs, [p]))
    return edfs, np.array(lambdas)</code></pre>
<pre class="python"><code># singular value decomposition of data_x_train_normalized
u, s, vh = scipy.linalg.svd(data_x_train_normalized, full_matrices=False)
s2 = s**2
edfs, lambdas = lambdas_from_edf(s2, 10)
# beta_ols = vh.T @ scipy.diag(scipy.reciprocal(s)) @ u.T @ vec_y
# print(beta_ols)
beta_ridge_array = [np.zeros(size_predictor)]
shrinkage_factors = []
for lamb in lambdas:
    shrinkage = s2/(s2+lamb)
    shrinkage_factors.append(shrinkage)
    shrinkage_factors_array = np.array(shrinkage_factors)

ridge = shrinkage_factors_array.mean(axis=0)
pcr = np.array((1,1,1,1,1,1,1,0,0))


fig = plt.figure(figsize=(10, 6))
ax = fig.add_subplot(1, 1, 1)
ax.plot(range(1, 9), ridge, &#39;o-&#39;, markersize=2)
ax.plot(np.array((1,2,3,4,5,6,7,7,8)), pcr, &#39;o-&#39;, markersize=2)
ax.legend((&#39;ridge&#39;,&#39;pcr&#39;))
ax.set_xlabel(&#39;Index&#39;)
ax.set_ylabel(&#39;Shrinkage Factor&#39;)
ax.set_ylim(-0.1,1.1)
plt.show()</code></pre>
<div class="figure">
<img src="merged_files/merged_205_0.png" alt="" />
<p class="caption">png</p>
</div>
<p>FIGURE 3.17. Ridge regression shrinks the regression coefficients of the prin-
cipal components, using shrinkage factors <span class="math inline">\(d_j^2 /(d_j^2 + \lambda)\)</span> as in (3.47). Principal component regression truncates them. Shown are the shrinkage and truncation patterns corresponding to Figure 3.7, as a function of the principal component index.</p>
</div>
<div id="s-3.5.2.-partial-least-squares" class="section level2">
<h2><span class="math inline">\(\S\)</span> 3.5.2. Partial Least Squares</h2>
<p>Unlike PCR, partial least squares (PLS) uses <span class="math inline">\(\mathbf{y}\)</span> (in addition to <span class="math inline">\(\mathbf{X}\)</span>) for the construction for a set of linear combinations of the inputs.</p>
<p>PLS is not scale invariant like PCR, so we assume that each <span class="math inline">\(\mathbf{x}_j\)</span> is standardized to have mean <span class="math inline">\(0\)</span> and variance <span class="math inline">\(1\)</span>.</p>
<div id="algorithm-3.3.-partial-least-squares." class="section level3">
<h3>Algorithm 3.3. Partial least squares.</h3>
<ol style="list-style-type: decimal">
<li><p>Standardized each <span class="math inline">\(\mathbf{x}_j\)</span> to have mean <span class="math inline">\(0\)</span> and variance <span class="math inline">\(1\)</span>.<br />
Set
<span class="math display">\[\begin{align}
\hat{\mathbf{y}}^{(0)} &amp;= \bar{y}\mathbf{1} \\
\mathbf{x}_j^{(0)} &amp;= \mathbf{x}_j, \text{ for } j=1,\cdots,p.
\end{align}\]</span></p></li>
<li><p>For <span class="math inline">\(m = 1,2,\cdots,p\)</span></p></li>
</ol>
<ul>
<li><span class="math inline">\(\mathbf{z}_m = \sum_{j=1}^p \hat\rho_{mj}\mathbf{x}_j^{(m-1)}\)</span>, where <span class="math inline">\(\hat\rho_{mj} = \langle \mathbf{x}_j^{(m-1)},\mathbf{y}\rangle\)</span>.</li>
<li><span class="math inline">\(\hat\theta_m = \langle\mathbf{z}_m,\mathbf{y}\rangle \big/ \langle\mathbf{z}_m,\mathbf{z}_m\rangle\)</span>.</li>
<li><span class="math inline">\(\hat{\mathbf{y}}^{(m)} = \hat{\mathbf{y}}^{(m-1)} + \hat\theta_m \mathbf{z}_m\)</span>.</li>
<li>Orthogonalize each <span class="math inline">\(\mathbf{x}_j^{(m-1)}\)</span> w.r.t. <span class="math inline">\(\mathbf{z}_m\)</span>:<br />
<span class="math inline">\(\mathbf{x}_j^{(m)} = \mathbf{x}_j^{(m-1)} - \frac{\langle\mathbf{z}_m,\mathbf{x}_j^{(m-1)}\rangle}{\langle\mathbf{z}_m,\mathbf{y}\rangle}\mathbf{z}_m, \text{ for } j=1,2,\cdots,p\)</span>.</li>
</ul>
<ol start="3" style="list-style-type: decimal">
<li>Output the sequence of fitted vectors <span class="math inline">\(\left\lbrace \hat{\mathbf{y}}^{(m)}\right\rbrace_1^p\)</span>.<br />
Since the <span class="math inline">\(\left\lbrace \mathbf{z}_l \right\rbrace_1^m\)</span> are linear in the original <span class="math inline">\(\mathbf{x}_j\)</span>, so is<br />
<span class="math display">\[\begin{equation}
\hat{\mathbf{y}}^{(m)} = \mathbf{X}\hat\beta^{\text{pls}}(m).
\end{equation}\]</span>
These linear coefficients can be recovered from the sequence of PLS transformations.</li>
</ol>
</div>
<div id="gist-of-the-pls-algorithm" class="section level3">
<h3>Gist of the PLS algorithm</h3>
<p>PLS begins by computing the weights</p>
<p><span class="math display">\[\begin{equation}
\hat\rho_{1j} = \langle \mathbf{x}_j,\mathbf{y} \rangle, \text{ for each } j,
\end{equation}\]</span>
which are in fact the univariate regression coefficients, since <span class="math inline">\(\mathbf{x}_j\)</span> are standardized (only for the first step <span class="math inline">\(m=1\)</span>).</p>
<p>From this we construct derived input</p>
<p><span class="math display">\[\begin{equation}
\mathbf{z}_1 = \sum_j \hat\rho_{1j}\mathbf{x}_j,
\end{equation}\]</span></p>
<p>which is the first PLS direction. Hence in the construction of each <span class="math inline">\(\mathbf{z}_m\)</span>, the inputs are weighted by the strength of their univariate effect on <span class="math inline">\(\mathbf{y}\)</span>.</p>
<p>The outcome <span class="math inline">\(\mathbf{y}\)</span> is regressed on <span class="math inline">\(\mathbf{z}_1\)</span> giving coefficient <span class="math inline">\(\hat\theta_1\)</span>, and then we orthogonalize <span class="math inline">\(\mathbf{x}_1,\cdots,\mathbf{x}_p\)</span> w.r.t. <span class="math inline">\(\mathbf{z}_1\)</span>.</p>
<p>We continue this process, until <span class="math inline">\(M\le p\)</span> directions have been obtained. In this manner, PLS produces a sequence of derived, orthogonal inputs or directions <span class="math inline">\(\mathbf{z}_1,\cdots,\mathbf{z}_M\)</span>.</p>
<ul>
<li>As with PCR, if <span class="math inline">\(M=p\)</span>, then <span class="math inline">\(\hat\beta^{\text{pls}} = \hat\beta^{\text{ls}}\)</span>.</li>
<li>Using <span class="math inline">\(M&lt;p\)</span> directions produces a reduced regression.</li>
</ul>
</div>
<div id="relation-to-the-optimization-problem" class="section level3">
<h3>Relation to the optimization problem</h3>
<blockquote>
<p>PLS seeks direction that have high variance <em>and</em> have high correlation with the response, in contrast to PCR with keys only on high variance (Stone and Brooks, 1990; Frank and Friedman, 1993).</p>
</blockquote>
<p>Since it uses the response <span class="math inline">\(\mathbf{y}\)</span> to construct its directions, its solution path is a nonlinear function of <span class="math inline">\(\mathbf{y}\)</span>.</p>
<p>In particular, the <span class="math inline">\(m\)</span>th principal component direction <span class="math inline">\(v_m\)</span> solves:</p>
<p><span class="math display">\[\begin{equation}
\max_\alpha \text{Var}(\mathbf{X}\alpha)\\
\text{subject to } \|\alpha\| = 1, \alpha^T\mathbf{S} v_l = 0 \text{ for } l = 1,\cdots, m-1,
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\mathbf{S}\)</span> is the sample covariance matrix of the <span class="math inline">\(\mathbf{x}_j\)</span>. The condition <span class="math inline">\(\alpha^T\mathbf{S} v_l= 0\)</span> ensures that <span class="math inline">\(\mathbf{z}_m = \mathbf{X}\alpha\)</span> is uncorrelated with all the previous linear combinations <span class="math inline">\(\mathbf{z}_l = \mathbf{X} v_l\)</span>.</p>
<p>The <span class="math inline">\(m\)</span>th PLS direction <span class="math inline">\(\hat\rho_m\)</span> solves:</p>
<p><span class="math display">\[\begin{equation}
\max_\alpha \text{Corr}^2(\mathbf{y},\mathbf{S}\alpha)\text{Var}(\mathbf{X}\alpha)\\
\text{subject to } \|\alpha\| = 1, \alpha^T\mathbf{S}\hat\rho_l = 0 \text{ for } l=1,\cdots, m-1.
\end{equation}\]</span></p>
<p>Further analysis reveals that the variance aspect tends to dominate, and so PLS behaves much like ridge regression and PCR. We discuss further in the next section.</p>
<p>If the input matrix <span class="math inline">\(\mathbf{X}\)</span> is orthogonal, then PLS finds the least squares estimates after the first <span class="math inline">\(m=1\)</span> step, and subsequent steps have no effect since the <span class="math inline">\(\hat\rho_{mj} = 0\)</span> for <span class="math inline">\(m&gt;1\)</span> (Exercise 3.14).</p>
<p>It can be also shown that the sequence of PLS coefficients for <span class="math inline">\(m=1,2,\cdots,p\)</span> represents the conjugate gradient sequence for computing the least squares solutions (Exercise 3.18).</p>
</div>
</div>
</div>
<div id="s-3.6.-discussion-a-comparison-of-the-selection-and-shrinkage-methods" class="section level1">
<h1><span class="math inline">\(\S\)</span> 3.6. Discussion: A Comparison of the Selection and Shrinkage Methods</h1>
<blockquote>
<p>PLS, PCR and ridge regression tend to behave similarly. Ridge regression may be preferred because it shrinks smoothly, rather than in discrete steps. Lasso falls somewhere between ridge regression and best subset regression, and enjoys some of the properties of each.</p>
</blockquote>
<pre class="python"><code>%load_ext rpy2.ipython</code></pre>
<pre class="r"><code>%%R
## generate simulated data
genXY &lt;- function(rho = 0.5,  # correlation
                 N = 100, # number of sample
                 beta = c(4, 2)) # true coefficient
{
  # covariance matrix
  Sigma = matrix(c(1, rho,
                   rho, 1), 2, 2)
  library(MASS)
  X = mvrnorm(N, c(0, 0), Sigma)
  Y = X[, 1] * beta[1] + X[, 2] * beta[2]
  return(list(X=X, Y=Y))
}</code></pre>
<pre class="r"><code>%%R
## main function 
## return the beta calculated by 6 methods (ols, ridge, lasso, pcr (plus 
## mypcr which from scratch), pls, subset)
select.vs.shrink &lt;- function(X, Y)
{
  ## least square regressions
  ols.fit &lt;- lm(Y ~ 0 + X)
  ols.beta &lt;- coef(ols.fit)
  #plot(ols.fit)
  ols.beta &lt;- as.matrix(t(ols.beta))

  
  ## create grid to fit lasso/ridge path
  grid = 10^seq(10, -2, length = 100)
  

  ## lasso

  library(glmnet)
  ## use cross-validation to choose the best model
  ## lasso.fit = cv.glmnet(X, Y, alpha = 1)
  
  lasso.fit &lt;- glmnet(X, Y, alpha = 1, lambda = grid)
  #plot(lasso.fit)
  ## extract beta
  lasso.beta &lt;- as.matrix(lasso.fit$beta) # convert dsCMatrix to regular matrix
  #plot(lasso.beta[1,], lasso.beta[2,])
  lasso.beta &lt;- t(lasso.beta)
  attr(lasso.beta, &quot;dimnames&quot;) = list(NULL,
                                      c(&quot;X1&quot;,&quot;X2&quot;))
  

  ## ridge regression

  ridge.fit = glmnet(X, Y, alpha = 0, lambda = grid)
  ridge.beta = as.matrix(ridge.fit$beta) # convert dsCMatrix to regular matrix
  ridge.beta = t(ridge.beta)
  attr(ridge.beta, &quot;dimnames&quot;) = list(NULL,
                                      c(&quot;X1&quot;, &quot;X2&quot;))
  
  ## principal component regression (PCR)
  library(pls)
  pcr.fit = pcr(Y ~ X, scale = FALSE)
  pcr.beta = pcr.fit$coefficients
  pcr.beta = rbind(c(0, 0), pcr.beta[,,1], pcr.beta[,,2]) # c(0, 0) for zero PC

  ## get PCs
  pc = prcomp(X, scale = FALSE)
  pc.m = pc$rotation
  ## scores
  pc.z = pc$x
  ## use one pc
  mypcr.fit.1 = lm(Y ~ 0+pc.z[,1])
  ## use two pc
  mypcr.fit.2 = lm(Y ~ 0+pc.z)
  ## original beta
  mypcr.beta.1 = coef(mypcr.fit.1) * pc.m[, 1]
  mypcr.beta.2 = t(pc.m %*% coef(mypcr.fit.2))
  mypcr.beta = rbind(c(0, 0), mypcr.beta.1, mypcr.beta.2)
  attr(mypcr.beta, &quot;dimnames&quot;) = list(NULL,
                                      c(&quot;X1&quot;, &quot;X2&quot;))

  ## Partial Least Squares (PLS)
  pls.fit = plsr(Y ~ X, scale = FALSE)
  pls.beta = pls.fit$coefficients
  pls.beta = rbind(c(0, 0), pls.beta[,,1], pls.beta[,,2])
  ## Best Subset
  library(leaps)
  bs.fit = regsubsets(x = X, y = Y, intercept = FALSE)
  if (summary(bs.fit)$which[1, 1])
  {
    bs.beta = c(coef(bs.fit, 1), 0)
  } else {
    bs.beta = c(0, coef(bs.fit, 1))
  }
  bs.beta = rbind(c(0, 0), bs.beta, coef(bs.fit, 2))
  attr(bs.beta, &quot;dimnames&quot;) = list(NULL,
                                   c(&quot;X1&quot;,&quot;X2&quot;))  
  res = list(ols = ols.beta,
              ridge = ridge.beta,
              lasso = lasso.beta,
              pcr = pcr.beta,
              mypcr = mypcr.beta,
              pls = pls.beta,
              subset = bs.beta)
  class(res) = &quot;selectORshrink&quot;
  return(res)
}</code></pre>
<pre class="r"><code>%%R
## plot function
## #######################################################################
plot.selectORshrink &lt;- function(obj, rho = 0.5)
{
  plot(0, 0,
       type = &quot;n&quot;,
       xlab = expression(beta[1]),
       ylab = expression(beta[2]),
       main = substitute(paste(rho,&quot;=&quot;,r), list(r=rho)),
       xlim = c(0, 6),
       ylim = c(-1, 3))
  par(lwd = 1, cex = 1)
  lines(obj$ridge, col = &quot;red&quot;)
  lines(obj$lasso, col = &quot;green&quot;)
  lines(obj$pcr, col = &quot;purple&quot;)
  lines(obj$pls, col = &quot;orange&quot;)
  lines(obj$subset, col = &quot;blue&quot;)
  points(obj$ols, col = &quot;black&quot;, pch = 16)
  abline(h=0, lty = 2)
  abline(v=0, lty = 2)
  legend(4.8, 3,
         c(&quot;Ridge&quot;, &quot;Lasso&quot;, &quot;PCR&quot;, &quot;PLS&quot;, &quot;Best Subset&quot;, &quot;Least Squares&quot;),
         col = c(&quot;red&quot;, &quot;green&quot;, &quot;purple&quot;, &quot;orange&quot;, &quot;blue&quot;, &quot;black&quot;),
         lty = c(1,1,1,1,1,NA),
         pch =c(NA,NA,NA,NA,NA, 16),
         box.col = &quot;white&quot;,
         box.lwd = 0,
         bg = &quot;transparent&quot;)
}
</code></pre>
<pre class="r"><code>%%R
## case 1
set.seed(1234)
data = genXY()
X = data$X
Y = data$Y
res1 = select.vs.shrink(X, Y)
plot(res1, rho = 0.5)

## case 2
set.seed(1234)
data2 = genXY(rho = -0.5)
X2 = data2$X
Y2 = data2$Y
res2 = select.vs.shrink(X2, Y2)
plot(res2, rho = -0.5)</code></pre>
<div class="figure">
<img src="merged_files/merged_217_0.png" alt="" />
<p class="caption">png</p>
</div>
<div class="figure">
<img src="merged_files/merged_217_1.png" alt="" />
<p class="caption">png</p>
</div>
<p>FIGURE 3.18. Coefficient profiles from different methods for a simple problem:
two inputs with correlation <span class="math inline">\(\pm 0.5\)</span>, and the true regression coefficients <span class="math inline">\(\beta = (4, 2)\)</span>.</p>
<pre class="r"><code>%%R
prcomp(X, scale = FALSE) # get the variables Coordinates, row orthonormalized matrix V^T of X=UDV^T decomposition</code></pre>
<pre><code>Standard deviations (1, .., p=2):
[1] 1.2303554 0.7295039

Rotation (n x k) = (2 x 2):
           PC1        PC2
[1,] 0.7233374 -0.6904948
[2,] 0.6904948  0.7233374</code></pre>
<pre class="r"><code>%%R
library(factoextra)
X.pca &lt;- prcomp(X, scale = FALSE)
# Visualize eigenvalues (scree plot). Show the percentage of variances explained by each principal component.
fviz_eig(X.pca)</code></pre>
<div class="figure">
<img src="merged_files/merged_220_0.png" alt="" />
<p class="caption">png</p>
</div>
<pre class="r"><code>%%R
#Graph of individuals. Individuals with a similar profile are grouped together.
fviz_pca_ind(X.pca,)</code></pre>
<div class="figure">
<img src="merged_files/merged_221_0.png" alt="" />
<p class="caption">png</p>
</div>
<pre class="r"><code>%%R
#Graph of variables.
fviz_pca_var(X.pca)</code></pre>
<div class="figure">
<img src="merged_files/merged_222_0.png" alt="" />
<p class="caption">png</p>
</div>
<pre class="r"><code>%%R
#Biplot of individuals and variables
fviz_pca_biplot(X.pca, repel = TRUE,
                col.var = &quot;#2E9FDF&quot;, # Variables color
                col.ind = &quot;#696969&quot;  # Individuals color
                )</code></pre>
<div class="figure">
<img src="merged_files/merged_223_0.png" alt="" />
<p class="caption">png</p>
</div>
<pre class="r"><code>%%R
# Eigenvalues
eig.val &lt;- get_eigenvalue(X.pca)
eig.val
# Results for Variables
res.var &lt;- get_pca_var(X.pca)
res.var$coord          # Coordinates, row orthogonal matrix V^T of X=UDV^T decomposition </code></pre>
<pre><code>        Dim.1      Dim.2
[1,] 0.889962 -0.5037187
[2,] 0.849554  0.5276774</code></pre>
<pre class="r"><code>%%R
# Results for individuals
res.ind &lt;- get_pca_ind(X.pca)
head(res.ind$coord)        # Coordinates matrix XV</code></pre>
<pre><code>       Dim.1       Dim.2
1 -1.2921373  0.23400463
2  0.5401025 -0.35239212
3  1.5193405  0.05279929
4 -2.6712365 -0.44662387
5  0.7316091 -0.59640308
6  0.8094987  0.10774412</code></pre>
<pre class="r"><code>%%R
# PCA visualization
library(factoextra)
data(decathlon2)
decathlon2.active &lt;- decathlon2[1:23, 1:10]
head(decathlon2.active[, 1:6])</code></pre>
<pre><code>R[write to console]: Loading required package: ggplot2

R[write to console]: Welcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa



          X100m Long.jump Shot.put High.jump X400m X110m.hurdle
SEBRLE    11.04      7.58    14.83      2.07 49.81        14.69
CLAY      10.76      7.40    14.26      1.86 49.37        14.05
BERNARD   11.02      7.23    14.25      1.92 48.93        14.99
YURKOV    11.34      7.09    15.19      2.10 50.42        15.31
ZSIVOCZKY 11.13      7.30    13.48      2.01 48.62        14.17
McMULLEN  10.83      7.31    13.76      2.13 49.91        14.38</code></pre>
<pre class="r"><code>%%R
#Compute PCA
res.pca &lt;- prcomp(decathlon2.active, scale = TRUE)
# Visualize eigenvalues (scree plot). Show the percentage of variances explained by each principal component.
fviz_eig(res.pca)</code></pre>
<div class="figure">
<img src="merged_files/merged_227_0.png" alt="" />
<p class="caption">png</p>
</div>
<pre class="r"><code>%%R
#Graph of individuals. Individuals with a similar profile are grouped together.
fviz_pca_ind(res.pca,
             col.ind = &quot;cos2&quot;, # Color by the quality of representation
             gradient.cols = c(&quot;#00AFBB&quot;, &quot;#E7B800&quot;, &quot;#FC4E07&quot;),
             repel = TRUE     # Avoid text overlapping
             )</code></pre>
<div class="figure">
<img src="merged_files/merged_228_0.png" alt="" />
<p class="caption">png</p>
</div>
<pre class="r"><code>%%R
#Graph of variables. Positive correlated variables point to the same side of the plot. Negative correlated variables point to opposite sides of the graph.
fviz_pca_var(res.pca,
             col.var = &quot;contrib&quot;, # Color by contributions to the PC
             gradient.cols = c(&quot;#00AFBB&quot;, &quot;#E7B800&quot;, &quot;#FC4E07&quot;),
             repel = TRUE     # Avoid text overlapping
             )</code></pre>
<div class="figure">
<img src="merged_files/merged_229_0.png" alt="" />
<p class="caption">png</p>
</div>
<pre class="r"><code>%%R
#Biplot of individuals and variables
fviz_pca_biplot(res.pca, repel = TRUE,
                col.var = &quot;#2E9FDF&quot;, # Variables color
                col.ind = &quot;#696969&quot;  # Individuals color
                )</code></pre>
<div class="figure">
<img src="merged_files/merged_230_0.png" alt="" />
<p class="caption">png</p>
</div>
<pre class="r"><code>%%R
# Eigenvalues
eig.val &lt;- get_eigenvalue(res.pca)
eig.val</code></pre>
<pre><code>       eigenvalue variance.percent cumulative.variance.percent
Dim.1   4.1242133        41.242133                    41.24213
Dim.2   1.8385309        18.385309                    59.62744
Dim.3   1.2391403        12.391403                    72.01885
Dim.4   0.8194402         8.194402                    80.21325
Dim.5   0.7015528         7.015528                    87.22878
Dim.6   0.4228828         4.228828                    91.45760
Dim.7   0.3025817         3.025817                    94.48342
Dim.8   0.2744700         2.744700                    97.22812
Dim.9   0.1552169         1.552169                    98.78029
Dim.10  0.1219710         1.219710                   100.00000</code></pre>
</div>
<div id="s-exercises" class="section level1">
<h1><span class="math inline">\(\S\)</span> Exercises</h1>
<div id="s-ex.-3.1-the-f-statistic-is-equivalent-to-the-square-of-the-z-score" class="section level2">
<h2><span class="math inline">\(\S\)</span> Ex. 3.1 (the F-statistic is equivalent to the square of the Z-score)</h2>
<p>Show that the F statistic (3.13) for dropping a single coefficient from a model is equal to the square of the corresponding z-score (3.12).</p>
<p>The F statistic,
<span class="math display">\[
F_{p_1−p_0,N−p_1−1}=\frac{\left(\text{RSS}_0-\text{RSS}_1\right)/\left(p_1-p_0\right)}{\text{RSS}_1/\left(N-p_1-1\right)}
\]</span>
where <span class="math inline">\(\text{RSS}_1\)</span> is the residual sum-of-squares for the least squares fit of the bigger model with <span class="math inline">\(p_1+1\)</span> parameters, and <span class="math inline">\(\text{RSS}_0\)</span> the same for the nested smaller model with <span class="math inline">\(p_0+1\)</span> parameters, having <span class="math inline">\(p_1−p_0\)</span> parameters constrained to be zero. The <span class="math inline">\(F\)</span> statistic measures the change in residual sum-of-squares per additional parameter in the bigger model, and it is normalized by an estimate of <span class="math inline">\(\sigma^2\)</span>. Under the Gaussian assumptions, and the null hypothesis that the smaller model is correct, the <span class="math inline">\(F\)</span> statistic will have a <span class="math inline">\(F_{p_1−p_0,N−p_1−1}\)</span> distribution.</p>
<p>The <span class="math inline">\(Z-score\)</span>
<span class="math display">\[z_j=\frac{\hat{\beta}_j}{\hat{\sigma}\sqrt{v_j}}\]</span> where <span class="math inline">\(v_j\)</span> is the <span class="math inline">\(j\)</span>th diagonal element of <span class="math inline">\((\mathbf X^T\mathbf X)^{−1}\)</span>. Under the null hypothesis
that <span class="math inline">\(\beta_j = 0\)</span>, <span class="math inline">\(z_j\)</span> is distributed as <span class="math inline">\(t_{N−p−1}\)</span> (a <span class="math inline">\(t\)</span> distribution with <span class="math inline">\(N − p − 1\)</span> degrees of freedom), and hence a large (absolute) value of <span class="math inline">\(zj\)</span> will lead to rejection of this null hypothesis.</p>
<p>Since once <span class="math inline">\(\beta\)</span> is estimated we can compute <span class="math inline">\(\sigma^2\)</span>:
<span class="math display">\[\hat{\sigma}^2=\frac{1}{N-p-1}\sum_{i=1}^{N}(y_i-\hat{y}_i)^2=\frac{1}{N-p-1}\sum_{i=1}^{N}(y_i-x_i^T\beta)^2=\frac{\text{RSS}_1}{N-p-1}\]</span>
In addition, by just deleting one variable from our regression the difference in degrees of freedom between the two models is one i.e. <span class="math inline">\(p_1 − p_0 = 1\)</span>. Thus the <span class="math inline">\(F\)</span>-statistic when we delete the <span class="math inline">\(j\)</span>th term from the base model simplifies to
<span class="math display">\[F_{1,N−p_1−1}=\frac{\left(\text{RSS}_j-\text{RSS}_1\right)}{\text{RSS}_1/\left(N-p_1-1\right)}=\frac{\left(\text{RSS}_j-\text{RSS}_1\right)}{\hat{\sigma}^2}\]</span></p>
<p>Since <span class="math display">\[\text{RSS}_j-\text{RSS}_1=\left(\mathbf{y&#39;}-\mathbf{X&#39;}\left(\mathbf{X&#39;}^T\mathbf{X&#39;}\right)^{-1}\mathbf{X&#39;}^T\mathbf{y&#39;}\right)^T\left(\mathbf{y&#39;}-\mathbf{X&#39;}\left(\mathbf{X&#39;}^T\mathbf{X&#39;}\right)^{-1}\mathbf{X&#39;}^T\mathbf{y&#39;}\right)-\left(\mathbf{y}-\mathbf{X}\left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T\mathbf{y}\right)^T\left(\mathbf{y}-\mathbf{X}\left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T\mathbf{y}\right)\]</span> Where <span class="math inline">\(\mathbf{X&#39;}\)</span> is <span class="math inline">\(\mathbf{X}\)</span> with <span class="math inline">\(j\)</span>th column deleted and <span class="math inline">\(\mathbf{y&#39;}\)</span> is <span class="math inline">\(\mathbf{y}\)</span> with <span class="math inline">\(j\)</span>th element deleted.</p>
</div>
<div id="s-ex.-3.2-confidence-intervals-on-a-cubic-equation" class="section level2">
<h2><span class="math inline">\(\S\)</span> Ex. 3.2 (confidence intervals on a cubic equation)</h2>
<p>Given data on two variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> , consider fitting a cubic
polynomial regression model <span class="math inline">\(f(X) =\sum_{j=0}^{3}\beta_jX^j\)</span>. In addition to plotting the fitted curve, you would like a <span class="math inline">\(95%\)</span> confidence band about the curve. Consider the following two approaches:
1. At each point <span class="math inline">\(x_0\)</span>, form a 95% confidence interval for the linear function
<span class="math inline">\(a^T \beta =\sum_{j=0}^{3}\beta_jx_0^j\)</span>.
2. Form a 95% confidence set for <span class="math inline">\(\beta\)</span> as in (3.15), which in turn generates
confidence intervals for <span class="math inline">\(f(x_0)\)</span>.</p>
<p>How do these approaches differ? Which band is likely to be wider? Conduct
a small simulation experiment to compare the two methods.</p>
<p><span class="math display">\[\begin{equation}
C_\beta = \left\{ \beta \big| (\hat\beta-\beta)^T\mathbf{X}^T\mathbf{X}(\hat\beta-\beta) \le \hat\sigma^2{\chi^2_{p+1}}^{(1-\alpha)}\right\},\quad 3.15
\end{equation}\]</span></p>
<p>We fix value for the column vector <span class="math display">\[\beta=\begin{bmatrix}
\beta_0\\
\beta_1\\
\beta_2\\
\beta_3\\
\end{bmatrix}\]</span> and and examine random deviations from the curve <span class="math display">\[y=\begin{bmatrix}
1&amp;x&amp;x^2&amp;x^3
\end{bmatrix}\begin{bmatrix}
\beta_0\\
\beta_1\\
\beta_2\\
\beta_3\\
\end{bmatrix}\]</span></p>
<p>For a given value of <span class="math inline">\(x\)</span>, the value of <span class="math inline">\(y\)</span> is randomized by adding a normally distributed variable with mean <span class="math inline">\(0\)</span> and variance <span class="math inline">\(1\)</span>. For each <span class="math inline">\(x\)</span>, we have a row vector <span class="math inline">\(x = (1, x, x^2, x^3)\)</span>. We fix <span class="math inline">\(N\)</span> values of <span class="math inline">\(x\)</span>. We arrange the corresponding values of <span class="math inline">\(x\)</span> in an <span class="math inline">\(N \times 4\)</span>-matrix, which we call <span class="math inline">\(X\)</span>, as
in the text. Also we denote by <span class="math inline">\(y\)</span> the corresponding <span class="math inline">\(N \times 1\)</span> column vector, with independent entries. The standard least squares estimate of <span class="math inline">\(\beta\)</span> is given by <span class="math inline">\(\hat{\beta} =\left(X^TX\right)^{-1}X^Ty\)</span>. We now compute a 95% confidence region around this cubic in two different ways.</p>
<ul>
<li><p>In the first method, we find, for each <span class="math inline">\(x\)</span>, a 95% confidence interval for the one-dimensional random variable <span class="math inline">\(\hat{y} = x\hat{\beta}\)</span>. Now <span class="math inline">\(y\)</span> is a normally distributed random variable, and therefore so is <span class="math inline">\(\hat{\beta} = (X^TX)^{−1}X^Ty\)</span>. Therefore<span class="math display">\[\text{Var}(\hat{y})=E\left(x\hat{\beta}\hat{\beta}^Tx^T\right)-E\left(x\hat{\beta}\right) E\left(\hat{\beta}^Tx^T\right)\\
=x\text{Var}(\hat{\beta})x^T=x\left(X^TX\right)^{-1}x^T\sigma^2\]</span>This is the variance of a normally distributed one-dimensional variable, centered at <span class="math inline">\(E(\hat{y})=E(x\hat{\beta})=x\beta\)</span>, and the 95% confidence interval can be calculated as usual as 1.96 times the square root of the variance, which are shown as the red lines in the figure below.</p></li>
<li><p>In the second method, since
<span class="math display">\[\begin{equation}
C_\beta = \left\{ \beta \big| (\hat\beta-\beta)^T\mathbf{X}^T\mathbf{X}(\hat\beta-\beta) \le \hat\sigma^2{\chi^2_{p+1}}^{(1-\alpha)}\right\},
\end{equation}\]</span> and <span class="math inline">\(X=UDV^T\)</span> then
<span class="math display">\[
\begin{align}
(\hat\beta-\beta)^T\mathbf{X}^T\mathbf{X}(\hat\beta-\beta)&amp;=(\hat\beta-\beta)^T\left(\mathbf{V}\mathbf{D}\mathbf{U}^T\mathbf{U}\mathbf{D}\mathbf{V}^T\right)(\hat\beta-\beta)\\
&amp;=(\hat\beta-\beta)^T\left(\mathbf{V}\mathbf{D}^2\mathbf{V}^T\right)(\hat\beta-\beta)\\
&amp;=\left((\hat\beta-\beta)^T\mathbf{V}\right)\mathbf{D}^2\left((\hat\beta-\beta)^T\mathbf{V}\right)^T\\
\end{align}
\]</span></p></li>
</ul>
<p><span class="math display">\[
\begin{equation}
\left((\hat\beta-\beta)^T\mathbf{V}\right)\mathbf{D}^2\left((\hat\beta-\beta)^T\mathbf{V}\right)^T\le \hat\sigma^2{\chi^2_{p+1}}^{(1-\alpha)}
\end{equation}
\]</span></p>
<p><span class="math display">\[
\begin{equation}
\left((\hat\beta-\beta)^T\mathbf{V}\right)\left((\hat\beta-\beta)^T\mathbf{V}\right)^T\le \hat\sigma^2{\chi^2_{p+1}}^{(1-\alpha)}/\mathbf{D}^2
\end{equation}
\]</span></p>
<p><span class="math display">\[
\begin{equation}
-\sqrt{\hat\sigma^2{\chi^2_{p+1}}^{(1-\alpha)}}/\mathbf{D}\le \left((\hat\beta-\beta)^T\mathbf{V}\right)\le \sqrt{\hat\sigma^2{\chi^2_{p+1}}^{(1-\alpha)}}/\mathbf{D}
\end{equation}
\]</span>
which are shown as the blue lines in the figure below.</p>
<pre class="r"><code>%%R
set.seed(1234)
N = 100
p = 3
X = rnorm(N)
beta = c(1, 0.4, 0.5, 0.2)
Y = beta[4] * X^3 + beta[3] * X^2 + beta[2] * X + beta[1] + rnorm(N)
model = lm(Y ~ X + I(X^2) + I(X^3))
sigma2 = sum(model$residuals^2) / (N - p - 1)

Xfull = cbind(1, X, X^2, X^3)
# method 1
XX = t(Xfull) %*% Xfull
invXX = solve(XX) # calculate (X^TX)^{-1}
s2 = apply(Xfull, 1, function(x) t(x) %*% invXX %*% x) * sigma2 # calculate x(X^TX)^{-1}x^T\sigma^2
yhat = model$fitted.values
idx = order(X)
plot(X[idx], yhat[idx], type = &quot;o&quot;, xlab = &quot;x&quot;, ylab = &quot;y&quot;)
lines(X[idx], yhat[idx] + qnorm(.975) * sqrt(s2[idx]), col = &quot;red&quot;, lwd = 3)
lines(X[idx], yhat[idx] - qnorm(.975) * sqrt(s2[idx]), col = &quot;red&quot;, lwd = 3)

# method 2
# svd decomposition
s = svd(Xfull)
delta = solve(t(s$v), diag(sqrt(sigma2 * qchisq(.95, p+1))/s$d))
#delta==(t(s$v))^{-1}%*%diag(sqrt(sigma2 * qchisq(.95, p+1))/s$d)

lines(X[idx], Xfull[idx,] %*% (model$coefficients + delta[,1]), col = &quot;blue&quot;)
lines(X[idx], Xfull[idx,] %*% (model$coefficients + delta[,2]), col = &quot;blue&quot;)
lines(X[idx], Xfull[idx,] %*% (model$coefficients + delta[,3]), col = &quot;blue&quot;)
lines(X[idx], Xfull[idx,] %*% (model$coefficients + delta[,4]), col = &quot;blue&quot;)
lines(X[idx], Xfull[idx,] %*% (model$coefficients - delta[,1]), col = &quot;blue&quot;)
lines(X[idx], Xfull[idx,] %*% (model$coefficients - delta[,2]), col = &quot;blue&quot;)
lines(X[idx], Xfull[idx,] %*% (model$coefficients - delta[,3]), col = &quot;blue&quot;)
lines(X[idx], Xfull[idx,] %*% (model$coefficients - delta[,4]), col = &quot;blue&quot;)

legend(&quot;topleft&quot;, c(&quot;Method 1&quot;, &quot;Method 2&quot;), col= c(&quot;red&quot;, &quot;blue&quot;), lwd = c(3, 1))</code></pre>
<div class="figure">
<img src="merged_files/merged_236_0.png" alt="" />
<p class="caption">png</p>
</div>
<p>The middle black curve is the curve <span class="math display">\[y=1+0.4x+0.5x^2+0.2x^3\]</span>
The two red curves come from the upper and lower limits of the 95% confidence interval, taken separately at each value of <span class="math inline">\(x\)</span>. The blue curves are the result of sampling values from the boundary of the 4-dimensional 95% confidence region for values of <span class="math inline">\(\hat{\beta}\)</span>, as determined by the <span class="math inline">\(\chi_4^2\)</span> distribution, and then drawing the corresponding curve. Note that the blue curves do not lie entirely between the red curves.</p>
</div>
<div id="s-ex.-3.3-the-gauss-markov-theorem" class="section level2">
<h2><span class="math inline">\(\S\)</span> Ex. 3.3 (the Gauss-Markov theorem)</h2>
<ol style="list-style-type: lower-alpha">
<li>Prove the Gauss–Markov theorem: the least squares estimate of a parameter <span class="math inline">\(a^T \beta\)</span> has variance no bigger than that of any other linear unbiased estimate of <span class="math inline">\(a^T \beta\)</span> (Section 3.2.2).</li>
<li>The matrix inequality <span class="math inline">\(B \preceq A\)</span> holds if <span class="math inline">\(A − B\)</span> is positive semidefinite. Show that if <span class="math inline">\(\hat{V}\)</span> is the variance-covariance matrix of the least squares estimate of <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\tilde{V}\)</span> is the variance-covariance matrix of any other linear unbiased estimate, then <span class="math inline">\(\hat{V}\preceq\tilde{V}\)</span>.</li>
</ol>
<p>Suppose we have in matrix notation,
<span class="math display">\[y=\mathbf{X}\beta+\epsilon\]</span> where$ $ is a <span class="math inline">\(N\times p\)</span> matrix</p>
<p>The Gauss–Markov assumptions concern the set of error random variables,<span class="math inline">\(\epsilon\)</span></p>
<ul>
<li>They have mean zero:<span class="math inline">\(E(\epsilon_i)=0\)</span></li>
<li>They are homoscedastic, that is all have the same finite variance: <span class="math inline">\(\text{Var}(\epsilon_i)=\sigma^2&lt;\infty\)</span> for all <span class="math inline">\(i\)</span></li>
<li>Distinct error terms are uncorrelated: <span class="math inline">\(\text{Cov}(\epsilon_i,\epsilon_j)=0,\forall i\ne j\)</span></li>
</ul>
<p>The estimator is said to be unbiased if and only if <span class="math inline">\(\text{E}(\hat{\beta})=\beta\)</span> regardless of the values of <span class="math inline">\(\mathbf{X}\)</span>. The ols estimator of <span class="math inline">\(\beta\)</span> is <span class="math inline">\(\hat{\beta}^{ols}=\left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^Ty\)</span> If there is another estimator of <span class="math inline">\(\beta\)</span> is <span class="math inline">\(\hat{\beta}^{&#39;}=\left(\left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T+\mathbf{D}\right)y\)</span>, where <span class="math inline">\(\mathbf{D}\)</span> is a <span class="math inline">\(p\times N\)</span> non-zero matrix. As we’re restricting to unbiased estimators, minimum mean squared error implies minimum variance. The goal is therefore to show that such an estimator has a variance no smaller than that of <span class="math inline">\(\hat{\beta}^{ols}\)</span></p>
<p><span class="math display">\[\begin{align}
\text{E}(\hat{\beta}^{&#39;})&amp;=\text{E}\left[\left(\left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T+\mathbf{D}\right)y\right]\\
&amp;=\text{E}\left[\left(\left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T+\mathbf{D}\right)(\mathbf{X}\beta+\epsilon)\right]\\
&amp;=\text{E}\left[\left(\left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T+\mathbf{D}\right)\mathbf{X}\beta+\left(\left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T+\mathbf{D}\right)\epsilon\right]\\
&amp;=\left(\left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T+\mathbf{D}\right)\mathbf{X}\beta+\text{E}\left[\left(\left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T+\mathbf{D}\right)\epsilon\right]\\
&amp;=\left(\left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T+\mathbf{D}\right)\mathbf{X}\beta+\left[\left(\left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T+\mathbf{D}\right)\text{E}(\epsilon)\right]\\
&amp;=\left(\left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T+\mathbf{D}\right)\mathbf{X}\beta\\
&amp;=\left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T\mathbf{X}\beta+\mathbf{D}\mathbf{X}\beta\\
&amp;=\left(\mathbf{I}+\mathbf{D}\mathbf{X}\right)\beta
\end{align}
\]</span>
Therefore, since <span class="math inline">\(\beta\)</span> is unobservable, <span class="math inline">\(\hat{\beta}^{&#39;}\)</span> is unbiased if and only if <span class="math inline">\(=\mathbf{D}\mathbf{X}=0\)</span>. Then
<span class="math display">\[\begin{align}
\text{Var}(\hat{\beta}^{&#39;})&amp;=\text{Var}\left[\left(\left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T+\mathbf{D}\right)y\right]\\
&amp;=\left(\left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T+\mathbf{D}\right)\left(\left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T+\mathbf{D}\right)^T\sigma^2\\
&amp;=\left(\left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T+\mathbf{D}\right)\left(\mathbf{X}\left(\mathbf{X}^T\mathbf{X}\right)^{-T}+\mathbf{D}^T\right)\sigma^2\\
&amp;=\sigma^2\left[\left(\mathbf{X}^T\mathbf{X}\right)^{-1}+\left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T\mathbf{D}^T+\mathbf{D}\mathbf{X}\left(\mathbf{X}^T\mathbf{X}\right)^{-T}+\mathbf{D}\mathbf{D}^T\right]\\
&amp;=\sigma^2\left[\left(\mathbf{X}^T\mathbf{X}\right)^{-1}+\mathbf{D}\mathbf{D}^T\right]\\
&amp;=\text{Var}(\hat{\beta})+\sigma^2\mathbf{D}\mathbf{D}^T
\end{align}
\]</span>
Since <span class="math inline">\(\mathbf{D}\mathbf{D}^T\)</span> is a positive semidefinite matrix, <span class="math inline">\(\text{Var}(\hat{\beta}^{&#39;})\)</span> exceeds <span class="math inline">\(\text{Var}(\hat{\beta})\)</span> by a positive semidefinite matrix.</p>
</div>
<div id="s-ex.-3.4-the-vector-of-least-squares-coefficients-from-gram-schmidt" class="section level2">
<h2><span class="math inline">\(\S\)</span> Ex. 3.4 (the vector of least squares coefficients from Gram-Schmidt)</h2>
<p>Show how the vector of least squares coefficients can be obtained from a single pass of the Gram–Schmidt procedure (Algorithm 3.1). Represent your solution in terms of the <span class="math inline">\(QR\)</span> decomposition of <span class="math inline">\(\mathbf{X}\)</span>.</p>
<p>Since
<span class="math display">\[\begin{align}
\hat{\beta}&amp;=\left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T\mathbf{y}\\
&amp;=\left(\mathbf{R}^T\mathbf{Q}^T\mathbf{Q}\mathbf{R}\right)^{-1}\mathbf{R}^T\mathbf{Q}^T\mathbf{y}\\
&amp;=\left(\mathbf{R}^T\mathbf{R}\right)^{-1}\mathbf{R}^T\mathbf{Q}^T\mathbf{y}\\
&amp;=\mathbf{R}^{-1}\mathbf{R}^{-T}\mathbf{R}^T\mathbf{Q}^T\mathbf{y}\\
&amp;=\mathbf{R}^{-1}\mathbf{Q}^T\mathbf{y}\\
\end{align}\]</span> where <span class="math inline">\(\mathbf{Q}\)</span> and <span class="math inline">\(\mathbf{R}\)</span> are computed from the Gram-Schmidt procedure on <span class="math inline">\(\mathbf{X}\)</span>. then
<span class="math display">\[\begin{align}
\hat{y}&amp;=\mathbf{X}\beta\\
&amp;=\mathbf{Q}\mathbf{R}\mathbf{R}^{-1}\mathbf{Q}^T\mathbf{y}\\
&amp;=\mathbf{Q}\mathbf{Q}^T\mathbf{y}\\
\end{align}\]</span></p>
<p>As we compute the columns of the matrix <span class="math inline">\(\mathbf{Q}\)</span> in the
Gram-Schmidt procedure we can evaluate <span class="math inline">\(q_j^Ty\)</span> for each column <span class="math inline">\(q_j\)</span> of <span class="math inline">\(\mathbf{Q}\)</span>, and fill in the <span class="math inline">\(j\)</span>-th element of the vector <span class="math inline">\(\mathbf{Q}^Ty\)</span>. After the matrices <span class="math inline">\(\mathbf{Q}\)</span> and <span class="math inline">\(\mathbf{R}\)</span> are computed one can then solve
<span class="math display">\[\hat{\beta}=\mathbf{R}^{-1}\mathbf{Q}^T\mathbf{y}\]</span>
and
<span class="math display">\[\mathbf{R}\hat{\beta}=\mathbf{Q}^T\mathbf{y}\]</span>
This is simple to do since <span class="math inline">\(\mathbf{R}\)</span> is upper triangular
and is performed with back-substitution, first solving for <span class="math inline">\(\hat{\beta}_{p}\)</span>, then <span class="math inline">\(\hat{\beta}_{p-1}\)</span>, then <span class="math inline">\(\hat{\beta}_{p-2}\)</span>, and on
until <span class="math inline">\(\hat{\beta}_{0}\)</span>.</p>
</div>
<div id="s-ex.-3.5-an-equivalent-problem-to-ridge-regression" class="section level2">
<h2><span class="math inline">\(\S\)</span> Ex. 3.5 (an equivalent problem to ridge regression)</h2>
<p>Consider the ridge regression problem (3.41). Show that this problem is equivalent to the problem</p>
<p><span class="math display">\[
\begin{equation}
\hat\beta^c = \underset{\beta^c}{\arg\min}\left\lbrace \sum_{i=1}^N\left( y_i - \beta_0^c - \sum_{j=1}^p (x_{ij}-\bar{x}_j)\beta_j^c \right)^2 +\lambda\sum_{j=1}^p {\beta_j^c}^2 \right\rbrace
\end{equation}
\]</span>
Give the correspondence between <span class="math inline">\(\beta^c\)</span> and the original <span class="math inline">\(\beta\)</span> in (3.41):
<span class="math display">\[\begin{equation}
\hat\beta^{\text{ridge}} = \underset{\beta}{\arg\min}\left\lbrace \sum_{i=1}^N\left( y_i - \beta_0 - \sum_{j=1}^p x_{ij}\beta_j \right)^2 +\lambda\sum_{j=1}^p \beta_j^2 \right\rbrace,
\end{equation}\]</span></p>
<p>Characterize the solution to this modified criterion. Show that a similar result holds for the lasso.</p>
<p>Since the ridge expression problem can be written as</p>
<p><span class="math display">\[\begin{align}
\hat\beta^{\text{ridge}} &amp;= \underset{\beta}{\arg\min}\left\lbrace \sum_{i=1}^N\left( y_i - \beta_0 - \sum_{j=1}^p (x_{ij}+\bar{x}_j-\bar{x}_j)\beta_j \right)^2 +\lambda\sum_{j=1}^p \beta_j^2 \right\rbrace\\
&amp;=\underset{\beta}{\arg\min}\left\lbrace \sum_{i=1}^N\left( y_i - \beta_0 - \sum_{j=1}^p \bar{x}_j\beta_j - \sum_{j=1}^p (x_{ij}-\bar{x}_j)\beta_j \right)^2 +\lambda\sum_{j=1}^p \beta_j^2 \right\rbrace\\
\end{align}\]</span></p>
<p>We can define <span class="math display">\[\beta_0^c=\beta_0+\sum_{j=1}^p \bar{x}_j\beta_j\]</span>
<span class="math display">\[\beta_j^c=\beta_i\quad i=1,2,\cdots,p\]</span>
then the above equation can be wrote as</p>
<p><span class="math display">\[\begin{align}
\hat\beta^{\text{ridge}} &amp;= \underset{\beta}{\arg\min}\left\lbrace \sum_{i=1}^N\left( y_i - \beta_0 - \sum_{j=1}^p (x_{ij}+\bar{x}_j-\bar{x}_j)\beta_j \right)^2 +\lambda\sum_{j=1}^p \beta_j^2 \right\rbrace\\
&amp;=\underset{\beta}{\arg\min}\left\lbrace \sum_{i=1}^N\left( y_i - \beta_0 - \sum_{j=1}^p \bar{x}_j\beta_j - \sum_{j=1}^p (x_{ij}-\bar{x}_j)\beta_j \right)^2 +\lambda\sum_{j=1}^p \beta_j^2 \right\rbrace\\
&amp;=\underset{\beta}{\arg\min}\left\lbrace \sum_{i=1}^N\left( y_i - \beta_0^c - \sum_{j=1}^p (x_{ij}-\bar{x}_j)\beta_j^c \right)^2 +\lambda\sum_{j=1}^p {\beta_j^c}^2 \right\rbrace\\
\end{align}\]</span></p>
<p>The equivalence of the minimization results from the fact that if <span class="math inline">\(\beta_i\)</span> minimize its respective functional the <span class="math inline">\(\beta_i^c\)</span> will do the same.</p>
<p>By centering the <span class="math inline">\(x_i\)</span>’s to have zero mean we have translated all points to the origin. As such only the “intercept” of the data or <span class="math inline">\(\beta_0\)</span> is modified, the “slope” or <span class="math inline">\(\beta_j^c,\quad j=1,2,3,\cdots,p\)</span> are not modified.</p>
<p>We compute the value of <span class="math inline">\(\beta_0^c\)</span> in the above expression by setting the derivative with respect to this variable equal to zero. We obtain</p>
<p><span class="math display">\[\frac{\partial{\hat\beta^{\text{ridge}}}}{\partial{\beta_0^c}}=-2\sum_{i=1}^N\left( y_i - \beta_0^c - \sum_{j=1}^p (x_{ij}-\bar{x}_j)\beta_j^c \right)=0\]</span></p>
<p><span class="math display">\[\beta_0^c=\bar{y}\]</span></p>
<p>The same argument above can be used to show that the minimization required for the lasso can be written in the same way (with <span class="math inline">\({\beta_j^c}^2\)</span> replaced by <span class="math inline">\(|\beta_j^c|\)</span>). The intercept in the centered case continues to be <span class="math inline">\(\bar{y}\)</span>.</p>
</div>
<div id="s-ex.-3.6-the-ridge-regression-estimate" class="section level2">
<h2><span class="math inline">\(\S\)</span> Ex. 3.6 (the ridge regression estimate)</h2>
<p>Show that the ridge regression estimate is the mean (and mode)
of the posterior distribution, under a Gaussian prior <span class="math inline">\(\beta\sim N(0,\tau^2 \mathbf{I})\)</span>, and Gaussian sampling model <span class="math inline">\(y\sim N(\mathbf{X\beta,\sigma^2\mathbf{I}})\)</span>. Find the relationship between the regularization parameter <span class="math inline">\(\lambda\)</span> in the ridge formula, and the variances <span class="math inline">\(\tau\)</span> and <span class="math inline">\(\sigma^2\)</span>.</p>
<p>From Bayes’ rule we have <span class="math display">\[p(\beta|D)=p(D|\beta)p(\beta)=N(y-\mathbf{X\beta,\sigma^2\mathbf{I}}) N(0,\tau^2 \mathbf{I})\]</span>
Since a <span class="math inline">\(p\)</span>-dimensional normal density for the random vector <span class="math inline">\(\mathbf x^T=[x_1,x_2,\cdots,x_p]\)</span> has the form <span class="math display">\[
f_X(\mathbf x)=\frac{1}{(2\pi)^{p/2}|\mathbf\Sigma|^{1/2}}e^{-\frac{1}{2}(\mathbf x-\boldsymbol \mu)^T(\mathbf\Sigma)^{-1}(\mathbf x-\boldsymbol \mu)}=N_p(\boldsymbol \mu, \mathbf\Sigma)\]</span></p>
<p>Now from this expression we calculate
<span class="math display">\[\begin{align}
\log(p(\beta|D))&amp;=\log(p(D|\beta))+\log(p(\beta))\\
&amp;=\log\left(N(y-\mathbf{X\beta,\sigma^2\mathbf{I}})\right)+\log\left(N(0,\tau^2 \mathbf{I})\right)\\
&amp;=\log\left(\frac{1}{(2\pi)^{p/2}\sigma}+\frac{1}{(2\pi)^{p/2}\tau}\right)-\frac{1}{2}\frac{(y-\mathbf X \beta)^T(y-\mathbf X \beta)}{\sigma^2}-\frac{1}{2}\frac{\beta^T\beta}{\tau^2}
\end{align}\]</span></p>
<p>The mode and the mean of this distribution (with respect to <span class="math inline">\(\beta\)</span>) is the argument that maximizes this expression and is given by
<span class="math display">\[\hat{\beta}=\underset{\beta}{\arg\max}\left[-\frac{1}{2}\frac{(y-\mathbf X\boldsymbol \beta)^T(y-\mathbf X\boldsymbol \beta)}{\sigma^2}-\frac{1}{2}\frac{\beta^T\beta}{\tau^2}\right]\]</span> or minimizes <span class="math display">\[\hat{\beta}=\underset{\beta}{\arg\min}\left[\frac{(y-\mathbf X\beta)^T(y-\mathbf X\beta)}{\sigma^2}+\frac{\beta^T\beta}{\tau^2}\right]\\
=\underset{\beta}{\arg\min}\left[(y-\mathbf X\beta)^T(y-\mathbf X\beta)+\frac{\sigma^2\beta^T\beta}{\tau^2}\right]\]</span>
This is the equivalent to criterion of ridge regression
<span class="math display">\[
\begin{equation}
\hat\beta^{\text{ridge}} = {\arg\min}_{\beta}\left\lbrace \sum_{i=1}^N\left( y_i - \beta_0 - \sum_{j=1}^p x_{ij}\beta_j \right)^2 +\lambda\sum_{j=1}^p \beta_j^2 \right\rbrace,
\end{equation}
\]</span></p>
<p>or in matrix form:
<span class="math display">\[
\begin{equation}
\text{RSS}(\lambda) = (\mathbf{y}-\mathbf{X}\beta)^T(\mathbf{y}-\mathbf{X}\beta) + \lambda\beta^T\beta
\end{equation}
\]</span>
with the substitution <span class="math inline">\(\lambda = \sigma^2\tau^2\)</span>.</p>
</div>
<div id="s-ex.-3.7" class="section level2">
<h2><span class="math inline">\(\S\)</span> Ex. 3.7</h2>
<p>Assume <span class="math display">\[y_i \sim N(\beta_0 + x_i^T \beta, \sigma^2), \quad i = 1, 2,\cdots ,N\]</span> and the parameters <span class="math inline">\(\beta_j , j = 1, \cdots, p\)</span> are each distributed as <span class="math inline">\(N(0, \tau^2)\)</span>, independently of one another. Assuming <span class="math inline">\(\sigma^2\)</span> and <span class="math inline">\(\tau^2\)</span> are known, and <span class="math inline">\(\beta_0\)</span> is not governed by a prior (or has a flat improper prior), show that the (minus) log-posterior density of <span class="math inline">\(\beta\)</span> is proportional to
<span class="math display">\[\sum_{i=1}^{N}\left(y_i-\beta_0-\sum_{j}x_{ij}\beta_j\right)^2+\lambda\sum_{j=1}^{p}\beta_j^2\]</span>
where <span class="math inline">\(\lambda = \sigma^2/\tau^2\)</span>.</p>
<p>The same as Ex. 3.6</p>
</div>
<div id="s-ex.-3.8" class="section level2">
<h2><span class="math inline">\(\S\)</span> Ex. 3.8</h2>
<p>Consider the <span class="math inline">\(QR\)</span> decomposition of the uncentered <span class="math inline">\(N \times (p + 1)\)</span>
matrix <span class="math inline">\(X\)</span> (whose first column is all ones), and the <span class="math inline">\(SVD\)</span> of the <span class="math inline">\(N \times p\)</span> centered matrix <span class="math inline">\(\tilde{X}\)</span>. Show that <span class="math inline">\(Q_2\)</span> and <span class="math inline">\(U\)</span> span the same subspace, where <span class="math inline">\(Q_2\)</span> is the sub-matrix of <span class="math inline">\(Q\)</span> with the first column removed. Under what circumstances will they be the same, up to sign flips?</p>
<p>This exercise is true if <span class="math inline">\(X\)</span> has rank <span class="math inline">\(p + 1\)</span>, and is false otherwise. Let <span class="math inline">\(X = QR\)</span>, where <span class="math inline">\(Q = (q_0, \cdots, q_p)\)</span> is <span class="math inline">\(N \times (p + 1)\)</span> with orthonormal columns, and <span class="math inline">\(R=(r_0, \cdots, r_p)\)</span> is upper triangular <span class="math inline">\((p+1) \times (p + 1)\)</span> with strictly positive diagonal entries. Let <span class="math inline">\(\mathbf{1}\)</span> be the length <span class="math inline">\(N\)</span> column matrix consisting entirely of ones. Then <span class="math inline">\(\mathbf{1} = q_0^Tr_0\)</span>. We deduce that all the entries of <span class="math inline">\(q_0\)</span> are equal. Since <span class="math inline">\(\lVert q_0\rVert = 1\)</span> and <span class="math inline">\(r_{00} &gt; 0\)</span>, where <span class="math inline">\(r_{00}\)</span> is the <span class="math inline">\(0\)</span>th row <span class="math inline">\(0\)</span>-th column of <span class="math inline">\(R\)</span>, we see that <span class="math inline">\(q_0 = \mathbf{1}/\sqrt{N}\)</span> and that <span class="math inline">\(r_{00} = \sqrt{N}\)</span>. The columns of <span class="math inline">\(Q\)</span> form a basis for the column-space of <span class="math inline">\(X\)</span>. Therefore the columns of <span class="math inline">\(Q_2\)</span> form a basis for the orthogonal complement of <span class="math inline">\(\mathbf 1\)</span> in the column-space of <span class="math inline">\(X\)</span>. For <span class="math inline">\(1 \le j \le p\)</span>, we have
<span class="math display">\[\bar{q}_j=\sum_{i=1}^{N}q_{ij}/N=\mathbf{1}^Tq_i/N=q_0^T\sqrt{N}q_i/N=q_0^Tq_i/\sqrt{N}=0\]</span>
Let <span class="math inline">\(X=(\mathbf{1}, x_1,\cdots,x_p)=QR\)</span> then the <span class="math inline">\(j\)</span>-th column of <span class="math inline">\(X\)</span> is <span class="math inline">\(x_j=Qr_j\)</span> and so <span class="math inline">\(\bar{x}_j=\mathbf{1}^TQr_j/N\)</span> and so <span class="math display">\[x_j-\bar{x}_j\mathbf{1}=x_j-Qr_j/N=Qr_j-Qr_j/N\]</span>
Let <span class="math display">\[Q=\begin{bmatrix}
\frac{1}{\sqrt{N}}&amp;\frac{\mathbf{1}^T}{\sqrt{N}}\\
\frac{\mathbf{1}}{\sqrt{N}}&amp;Q_2\\
\end{bmatrix}
\]</span>
and let <span class="math inline">\(R_2\)</span> be the lower right <span class="math inline">\(p \times p\)</span> submatrix of <span class="math inline">\(R\)</span>. Then
<span class="math display">\[\underset{(N\times (p+1))}{R}=\begin{bmatrix}
\sqrt{N}&amp;\sqrt{N}(\bar{x}_1,\cdots,\bar{x}_p)\\
0&amp;R_2\\
\end{bmatrix}
\]</span>
Then <span class="math display">\[Q_2R_2=\tilde{X}=UDV^T\]</span> Since <span class="math inline">\(X\)</span> is assumed to have rank <span class="math inline">\((p+1)\)</span>, <span class="math inline">\(DV^T\)</span> is a non-singular <span class="math inline">\((p\times p)\)</span> matrix. It follows that <span class="math inline">\(U, \tilde{X}, Q_2\)</span> have the same column space.</p>
<p>If <span class="math inline">\(Q_2\)</span> and <span class="math inline">\(U\)</span> are the same up to the sign, then <span class="math display">\[R_2=D_1DV^T\]</span> where <span class="math inline">\(D_1\)</span> is diagonal matrix with entries <span class="math inline">\(\pm 1\)</span>. Since <span class="math inline">\(V\)</span> is an orthogonal matrix, then <span class="math inline">\(R_2\)</span> has orthogonal rows. Also <span class="math inline">\(R_2\)</span> has strictly positive diagonal entries. It follows that <span class="math inline">\(R_2\)</span> is a diagonal matrix, and so the columns of <span class="math inline">\(\tilde{X}\)</span> are orthogonal to each other. Therefore <span class="math inline">\(V\)</span> is also diagonal, and the entries must all be <span class="math inline">\(\pm 1\)</span>.</p>
</div>
<div id="s-ex.-3.9-using-the-qr-decomposition-for-fast-forward-stepwise-selection" class="section level2">
<h2><span class="math inline">\(\S\)</span> Ex. 3.9 (using the QR decomposition for fast forward-stepwise selection)</h2>
<p>Forward stepwise regression. Suppose we have the <span class="math inline">\(QR\)</span> decomposition
for the <span class="math inline">\(N\times q\)</span> matrix <span class="math inline">\(X_1\)</span> in a multiple regression problem with response <span class="math inline">\(y\)</span>, and we have an additional <span class="math inline">\(p−q\)</span> predictors in the matrix <span class="math inline">\(X_2\)</span>. Denote the current residual by <span class="math inline">\(r\)</span>. We wish to establish which one of these additional variables will reduce the residual-sum-of squares the most when included with those in <span class="math inline">\(X_1\)</span>. Describe an efficient procedure for doing this.</p>
<pre class="python"><code></code></pre>
</div>
</div>
<div id="references" class="section level1 unnumbered">
<h1>References</h1>
<div id="refs" class="references csl-bib-body">
<div id="ref-friedman2001elements" class="csl-entry">
1. Friedman J, Hastie T, Tibshirani R, others. The elements of statistical learning. Springer series in statistics New York; 2009.
</div>
</div>
</div>

    </div>
  </article>

  


</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
          </li>
          <li>
          </li>
        </ul>
      </footer>

    </div>
    



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/r.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/yaml.min.js"></script>
<script>hljs.configure({languages: []}); hljs.initHighlightingOnLoad();</script>



    
<script src="../../../../js/math-code.js"></script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
  </body>
</html>

