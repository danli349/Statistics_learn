<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Distribution on A Hugo website</title>
    <link>/tags/distribution/</link>
    <description>Recent content in Distribution on A Hugo website</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 22 May 2021 00:00:00 +0000</lastBuildDate><atom:link href="/tags/distribution/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>AOS chapter24 Stochastic Processes</title>
      <link>/2021/05/22/aos-chapter24-stochastic-processes/</link>
      <pubDate>Sat, 22 May 2021 00:00:00 +0000</pubDate>
      
      <guid>/2021/05/22/aos-chapter24-stochastic-processes/</guid>
      <description>24. Stochastic Processes24.1 Introduction24.2 Markov Chains24.3 Poisson Process24.6 ExercisesReferences24. Stochastic Processes24.1 IntroductionA stochastic process \(\{ X_t : t \in T \}\) is a collection of random variables. We shall sometimes write \(X(t)\) instead of \(X_t\). The variables \(X_t\) take values in some set \(\mathcal{X}\) called the state space. The set \(T\) is called the index set and for our purposes can be thought of as time.</description>
    </item>
    
    <item>
      <title>AOS chapter23 Classification</title>
      <link>/2021/05/20/aos-chapter23-classification/</link>
      <pubDate>Thu, 20 May 2021 00:00:00 +0000</pubDate>
      
      <guid>/2021/05/20/aos-chapter23-classification/</guid>
      <description>23. Classification23.1 Introduction23.2 Error Rates and The Bayes Classifier23.3 Gaussian and Linear Classifiers23.4 Linear Regression and Logistic Regression23.5 Relationship Between Logistic Regression and LDA23.6 Density Estimation and Naive Bayes23.7 Trees23.8 Assessing Error Rates and Choosing a Good Classifier23.9 Support Vector Machines23.10 Kernelization23.11 Other Classifiers23.13 ExercisesReferences23. Classification23.1 IntroductionThe problem of predicting a discrete variable \(Y\) from another random variable \(X\) is called classfication, supervised learning, discrimination or pattern recognition.</description>
    </item>
    
    <item>
      <title>AOS chapter22 Smoothing Using Orthogonal Functions</title>
      <link>/2021/05/17/aos-chapter22-smoothing-using-orthogonal-functions/</link>
      <pubDate>Mon, 17 May 2021 00:00:00 +0000</pubDate>
      
      <guid>/2021/05/17/aos-chapter22-smoothing-using-orthogonal-functions/</guid>
      <description>22. Smoothing Using Orthogonal Functions22.1 Orthogonal Functions and \(L_2\) Spaces22.2 Density Estimation22.3 Regression22.4 Wavelets22.6 ExercisesReferences22. Smoothing Using Orthogonal FunctionsIn this Chapter we study a different approach to nonparametric curve estimation based on orthogonal functions. We begin with a brief introduction to the theory of orthogonal functions. Then we turn to density estimation and regression.
22.1 Orthogonal Functions and \(L_2\) SpacesLet \(v = (v_1, v_2, v_3)\) denote a three dimensional vector.</description>
    </item>
    
    <item>
      <title>AOS chapter21 Nonparametric Curve Estimation</title>
      <link>/2021/05/09/aos-chapter21-nonparametric-curve-estimation/</link>
      <pubDate>Sun, 09 May 2021 00:00:00 +0000</pubDate>
      
      <guid>/2021/05/09/aos-chapter21-nonparametric-curve-estimation/</guid>
      <description>21. Nonparametric Curve Estimation21.1 The Bias-Variance Tradeoff21.2 Histograms21.3 Kernel Density Estimation21.4 Nonparametric Regression21.5 Appendix: Confidence Sets and Bias21.7 ExercisesReferences21. Nonparametric Curve EstimationIn this Chapter we discuss the nonparametric estimation of probability density functions and regression functions, which we refer to a curve estimation.
In Chapter 8 we saw it is possible to consistently estimate a cumulative distribution function \(F\) without making any assumptions about \(F\).</description>
    </item>
    
    <item>
      <title>Density Estimation</title>
      <link>/2021/05/05/density-estimation/</link>
      <pubDate>Wed, 05 May 2021 00:00:00 +0000</pubDate>
      
      <guid>/2021/05/05/density-estimation/</guid>
      <description>1. INTROUCTION2. SURVEY OF EXISTING METHODS2.1 Introduction2.2. Histograms2.3. The naive estimator2.4. The kernel estimator2.5. The nearest neighbour method2.6. The variable kernel method2.7. Orthogonal series estimators2.8. Maximum penalized likelihood estimators2.9. General weight function estimators1. INTROUCTIONReferences1. INTROUCTIONThe probability density function is a fundamental concept in statistics. Consider any random quantity \(X\) that has probability density function \(f\).</description>
    </item>
    
    <item>
      <title>AOS chapter20 Directed Graphs</title>
      <link>/2021/05/03/aos-chapter20-directed-graphs/</link>
      <pubDate>Mon, 03 May 2021 00:00:00 +0000</pubDate>
      
      <guid>/2021/05/03/aos-chapter20-directed-graphs/</guid>
      <description>20. Directed Graphs20.1 Introduction20.2 DAG’s20.3 Probability and DAG’s20.4 More Independence Relations20.5 Estimation for DAG’s20.6 Causation Revisited20.8 Exercises20. Directed Graphs20.1 IntroductionDirected graphs are similar to undirected graphs, but there are arrows between vertices instead of edges. Like undirected graphs, directed graphs can be used to represent independence relations. They can also be used as an alternative to counterfactuals to represent causal relationships.</description>
    </item>
    
    <item>
      <title>AOS chapter19 Causal Inference</title>
      <link>/2021/05/01/aos-chapter19-causal-inference/</link>
      <pubDate>Sat, 01 May 2021 00:00:00 +0000</pubDate>
      
      <guid>/2021/05/01/aos-chapter19-causal-inference/</guid>
      <description>19. Causal Inference19.1 The Counterfactual Model19.2 Beyond Binary Treatments19.3 Observational Studies and Confounding19.4 Simpson’s Paradox19.6 ExercisesReferences19. Causal InferenceIn this chapter we discuss causation. Roughly speaking “\(X\) causes \(Y\)” means that changing the value of \(X\) will change the distribution of \(Y\). When \(X\) causes \(Y\), \(X\) and \(Y\) will be associated but the reverse is not, in general, true.</description>
    </item>
    
    <item>
      <title>AOS chapter18 Loglinear Models</title>
      <link>/2021/04/30/aos-chapter18-loglinear-models/</link>
      <pubDate>Fri, 30 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>/2021/04/30/aos-chapter18-loglinear-models/</guid>
      <description>18. Loglinear Models18.1 The Loglinear Model18.2 Graphical Log-Linear Models18.3 Hierarchical Log-Linear Models18.4 Model Generators18.5 Lattices18.6 Fitting Log-Linear Models to Data18.8 ExercisesReferences18. Loglinear Models18.1 The Loglinear ModelLet \(X = (X_1, \dots, X_m)\) be a random vector with probability
\[ f(x) = \mathbb{P}(X = x) = \mathbb{P}(X_1 = x_1, \dots, X_m = x_m) \]</description>
    </item>
    
    <item>
      <title>AOS chapter17 Undirected Graphs and Conditional Independence</title>
      <link>/2021/04/26/aos-chapter16-undirected-graphs-and-conditional-independence/</link>
      <pubDate>Mon, 26 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>/2021/04/26/aos-chapter16-undirected-graphs-and-conditional-independence/</guid>
      <description>17. Undirected Graphs and Conditional Independence17.1 Conditional Independence17.2 Undirected Graphs17.3 Probability and Graphs17.4 Fitting Graphs to Data17.6 ExercisesReferences17. Undirected Graphs and Conditional Independence\(k\) binary variables \(Y_1, \dots, Y_k\) correspond to a multinomial with \(N = 2^k\) categories. Even for moderately large \(k\), \(2^k\) will be huge. It can be shown in this case that the MLE is a poor estimator, because the data are sparse.</description>
    </item>
    
    <item>
      <title>AOS chapter16 Inference about Independence</title>
      <link>/2021/04/25/aos-chapter16-inference-about-independence/</link>
      <pubDate>Sun, 25 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>/2021/04/25/aos-chapter16-inference-about-independence/</guid>
      <description>16. Inference about Independence16.1 Two Binary Variables16.2 Interpreting the Odds Ratios16.3 Two Discrete Variables16.4 Two Continuous Variables16.5 One Continuous Variable and One Discrete16.7 ExercisesReferences16. Inference about IndependenceThis chapter addresses two questions:
How do we test if two random variables are independent?How do we estimate the strength of dependence between two random variables?Recall we write \(Y \text{ ⫫ } Z\) to mean that \(Y\) and \(Z\) are independent.</description>
    </item>
    
    <item>
      <title>AOS chapter15 Multivariate Models</title>
      <link>/2021/04/24/aos-chapter15-multivariate-models/</link>
      <pubDate>Sat, 24 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>/2021/04/24/aos-chapter15-multivariate-models/</guid>
      <description>15. Multivariate Models15.1 Random Vectors15.2 Estimating the Correlation15.3 Multinomial15.4 Multivariate Normal15.5 Appendix15.6 Exercisesbox_mullerReferences15. Multivariate ModelsReview of notation from linear algebra:
If \(x\) and \(y\) are vectors, then \(x^T y = \sum_j x_j y_j\).If \(A\) is a matrix then \(\text{det}(A)\) denotes the determinant of \(A\), \(A^T\) denotes the transpose of A, and \(A^{-1}\) denotes the inverse of \(A\) (if the inverse exists).</description>
    </item>
    
    <item>
      <title>AOS chapter14 Linear Regression</title>
      <link>/2021/04/23/aos-chapter14-linear-regression/</link>
      <pubDate>Fri, 23 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>/2021/04/23/aos-chapter14-linear-regression/</guid>
      <description>14. Linear Regression14.1 Simple Linear Regression14.2 Least Squares and Maximum Likelihood14.3 Properties of the Least Squares Estimators14.4 Prediction14.5 Multiple Regression14.6 Model Selection14.7 The Lasso14.8 Technical Appendix14.9 ExercisesReferences14. Linear RegressionRegression is a method for studying the relationship between a response variable \(Y\) and a covariates \(X\). The covariate is also called a predictor variable or feature.</description>
    </item>
    
    <item>
      <title>AOS chapter13 Statistical Decision Theory</title>
      <link>/2021/04/22/aos-chapter13-statistical-decision-theory/</link>
      <pubDate>Thu, 22 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>/2021/04/22/aos-chapter13-statistical-decision-theory/</guid>
      <description>13. Statistical Decision Theory13.1 Preliminaries13.2 Comparing Risk Functions13.3 Bayes Estimators13.4 Minimax Rules13.5 Maximum Likelihood, Minimax and Bayes13.6 Admissibility13.7 Stein’s Paradox13.9 ExercisesReferences13. Statistical Decision Theory13.1 PreliminariesDecision theory is a formal theory for comparing between statistical procedures.
In the language of decision theory, a estimator is sometimes called a decision rule and the possible values of the decision rule are called actions.</description>
    </item>
    
    <item>
      <title>AOS chapter12 Bayesian Inference</title>
      <link>/2021/04/21/aos-chapter12-bayesian-inference/</link>
      <pubDate>Wed, 21 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>/2021/04/21/aos-chapter12-bayesian-inference/</guid>
      <description>12. Bayesian Inference12.1 Bayesian Philosophy12.2 The Bayesian Method12.3 Functions of Parameters12.4 Simulation12.5 Large Sample Properties for Bayes’ Procedures12.6 Flat Priors, Improper Priors and “Noninformative” Priors12.7 Multiparameter Problems12.8 Strenghts and Weaknesses of Bayesian Inference12.9 Appendix12.11 ExercisesReferences12. Bayesian Inference12.1 Bayesian PhilosophyPostulates of frequentist (or classical) inference:
Probabilty refers to limiting relative frequencies.</description>
    </item>
    
    <item>
      <title>AOS Chapter11 Hypothesis Testing and p-values</title>
      <link>/2021/04/20/aos-chapter11-hypothesis-testing-and-p-values/</link>
      <pubDate>Tue, 20 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>/2021/04/20/aos-chapter11-hypothesis-testing-and-p-values/</guid>
      <description>11. Hypothesis Testing and p-values11.1 The Wald Test11.2 p-values11.3 The \(\chi^2\) distribution11.4 Pearson’s \(\chi^2\) Test for Multinomial Data11.5 The Permutation Test11.6 Multiple Testing11.7 Technical Appendix11.9 ExercisesReferences11. Hypothesis Testing and p-valuesSuppose we partition the parameters space \(\Theta\) into two disjoint sets \(\Theta_0\) and \(\Theta_1\) and we wish to test
\[H_0: \theta \in \Theta_0\quad \text{versus} \quadH_1: \theta \in \Theta_1\]</description>
    </item>
    
    <item>
      <title>AOS chapter10  Parametric Inference</title>
      <link>/2021/04/18/aos-chapter10-parametric-inference/</link>
      <pubDate>Sun, 18 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>/2021/04/18/aos-chapter10-parametric-inference/</guid>
      <description>10. Parametric Inference10.1 Parameter of interest10.2 The Method of Moments10.3 Maximum Likelihood10.4 Properties of Maximum Likelihood Estimators10.5 Consistency of Maximum Likelihood Estimator10.6 Equivalence of the MLE10.7 Asymptotic Normality10.8 Optimality10.9 The Delta Method10.10 Multiparameter Models10.11 The Parametric Bootstrap10.12 Technical Appendix10.13 ExercisesReferences10. Parametric InferenceParametric models are of the form</description>
    </item>
    
    <item>
      <title>AOS Chapter09 The Bootstrap</title>
      <link>/2021/04/17/aos-chapter09-the-bootstrap/</link>
      <pubDate>Sat, 17 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>/2021/04/17/aos-chapter09-the-bootstrap/</guid>
      <description>9. The Bootstrap9.1 Simulation9.2 Bootstrap Variance Estimation9.3 Bootstrap Confidence Intervals9.5 Technical Appendix9.6 ExercisesReferences9. The BootstrapLet \(X_1, \dots, X_n \sim F\) be random variables distributed according to \(F\), and
\[ T_n = g(X_1, \dots, X_n)\]
be a statistic, that is, any function of the data. Suppose we want to know \(\mathbb{V}_F(T_n)\), the variance of \(T_n\).
For example, if \(T_n = n^{-1}\sum_{i=1}^nX_i\) then \(\mathbb{V}_F(T_n) = \sigma^2/n\) where \(\sigma^2 = \int (x - \mu)^2dF(x)\) and \(\mu = \int x dF(x)\).</description>
    </item>
    
    <item>
      <title>AOS Chapter08 Estimating the CDF and Statistical Functionals</title>
      <link>/2021/04/16/aos-chapter08-estimating-the-cdf-and-statistical-functionals/</link>
      <pubDate>Fri, 16 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>/2021/04/16/aos-chapter08-estimating-the-cdf-and-statistical-functionals/</guid>
      <description>8. Estimating the CDF and Statistical Functionals8.1 Empirical distribution function8.2 Statistical Functionals8.3 Technical Appendix8.5 ExercisesReferences8. Estimating the CDF and Statistical Functionals8.1 Empirical distribution functionThe empirical distribution function \(\hat{F_n}\) is the CDF that puts mass \(1/n\) at each data point \(X_i\). Formally,
\[\begin{align}\hat{F_n}(x) &amp;amp; = \frac{\sum_{i=1}^n I\left(X_i \leq x \right)}{n} \\&amp;amp;= \frac{\text{#}|\text{observations less than or equal to x}|}{n}\end{align}\]</description>
    </item>
    
    <item>
      <title>AOS Chapter07 Models, Statistical Inference and Learning</title>
      <link>/2021/04/15/aos-chapter07-models-statistical-inference-and-learning/</link>
      <pubDate>Thu, 15 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>/2021/04/15/aos-chapter07-models-statistical-inference-and-learning/</guid>
      <description>7. Models, Statistical Inference and Learning7.2 Parametric and Nonparametric Models7.3 Fundamental Concepts in Inference7.5 Technical AppendixReferences7. Models, Statistical Inference and Learning7.2 Parametric and Nonparametric ModelsA statistical model is a set of distributions \(\mathfrak{F}\).
A parametric model is a set \(\mathfrak{F}\) that may be parametrized by a finite number of parameters. For example, if we assume that data comes from a normal distribution then</description>
    </item>
    
    <item>
      <title>AOS Chapter06 Convergence of Random Variables</title>
      <link>/2021/04/14/aos-chapter05-convergence-of-random-variables/</link>
      <pubDate>Wed, 14 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>/2021/04/14/aos-chapter05-convergence-of-random-variables/</guid>
      <description>6. Convergence of Random Variables6.2 Types of convergence6.3 The Law of Large Numbers6.4 The Central Limit Theorem6.5 The Delta Method6.6 Technical appendix6.8 ExercisesReferences6. Convergence of Random Variables6.2 Types of convergence\(X_n\) converges to \(X\) in probability, written \(X_n \xrightarrow{\text{P}} X\), if, for every \(\epsilon &amp;gt; 0\),:
\[ \mathbb{P}( |X_n - X| &amp;gt; \epsilon ) \rightarrow 0 \]</description>
    </item>
    
    <item>
      <title>AOS Chapter05 Inequalities</title>
      <link>/2021/04/13/aos-chapter05-inequalities/</link>
      <pubDate>Tue, 13 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>/2021/04/13/aos-chapter05-inequalities/</guid>
      <description>5. Inequalities5.1 Markov and Chebyshev Inequalities5.2 Hoeffding’s Inequality5.3 Cauchy-Schwartz and Jensen Inequalities5.4 Technical Appendix: Proof of Hoeffding’s Inequality5.6 ExercisesReferences5. Inequalities5.1 Markov and Chebyshev InequalitiesTheorem 5.1 (Markov’s Inequality). Let \(X\) be a non-negative random variable and suppose that \(\mathbb{E}(X)\) exists. For any \(t &amp;gt; 0\),
\[ \mathbb{P}(X &amp;gt; t) \leq \frac{\mathbb{E}(X)}{t} \]
Proof.
\[ \mathbb{E}(X)=\int_0^\infty xf(x) dx=\int_0^t xf(x) dx + \int_t^\infty xf(x) dx\geq \int_t^\infty xf(x) dx\geq t \int_t^\infty f(x) dx= t \mathbb{P}(X &amp;gt; t)\]</description>
    </item>
    
    <item>
      <title>AOS chapter04 Expectation</title>
      <link>/2021/04/12/aos-chapter04-expectation/</link>
      <pubDate>Mon, 12 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>/2021/04/12/aos-chapter04-expectation/</guid>
      <description>4. Expectation4.1 Expectation of a Random Variable4.2 Properties of Expectations4.3 Variance and Covariance4.4 Expectation and Variance of Important Random Variables4.5 Conditional Expectation4.6 Technical Appendix4.7 ExercisesReferences4. Expectation4.1 Expectation of a Random VariableThe expected value, mean or first moment of \(X\) is defined to be
\[ \mathbb{E}(X) = \int x \; dF(x) = \begin{cases}\sum_x x f(x) &amp;amp;\text{if } X \text{ is discrete} \\\int x f(x)\; dx &amp;amp;\text{if } X \text{ is continuous}\end{cases} \]</description>
    </item>
    
    <item>
      <title>AOS chapter03 Random Variables</title>
      <link>/2021/04/11/aos-chapter03-random-variables/</link>
      <pubDate>Sun, 11 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>/2021/04/11/aos-chapter03-random-variables/</guid>
      <description>3. Random Variables3.1 Introduction3.2 Distribution Functions and Probability Functions3.3 Some Important Discrete Random Variables3.4 Some Important Continuous Random Variables3.5 Bivariate Distributions3.6 Marginal Distributions3.7 Independent Random Variables3.8 Conditional Distributions3.9 Multivariate Distributions and IID Samples3.10 Two Important Multivariate Distributions3.11 Transformations of Random Variables3.12 Transformation of Several Random Variables3.13 Technical Appendix3.14 ExercisesReferences3.</description>
    </item>
    
    <item>
      <title>The Multivariate Normal Density</title>
      <link>/2020/09/11/the-multivariate-normal-density/</link>
      <pubDate>Fri, 11 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/09/11/the-multivariate-normal-density/</guid>
      <description>The univariate normal pdf is:\[f_X(x)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{1}{2}(\frac{x-\mu}{\sigma})^2}, \quad -\infty&amp;lt;x&amp;lt;+\infty\] The term \((\frac{x-\mu}{\sigma})^2=(x-\mu)(\sigma^2)^{-1}(x-\mu)\) measures the square ofthe univariate distance from \(x\) to \(\mu\) in standard deviation units. This can be generalized to a \(p\times 1\) vector \(\mathbf x\) of observations on several variables as \((\mathbf X-\boldsymbol \mu)^T(\mathbf \Sigma)^{-1}(\mathbf X-\boldsymbol \mu)\), which is the square of the multivariate generalized distance from \(\mathbf X\) to \(\boldsymbol \mu\), the \(p\times p\) matrix \(\mathbf \Sigma\) is the variance–covariance matrix of \(\mathbf X\).</description>
    </item>
    
    <item>
      <title>The Bivariate Normal Distribution</title>
      <link>/2020/09/10/the-bivariate-normal-distribution/</link>
      <pubDate>Thu, 10 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/09/10/the-bivariate-normal-distribution/</guid>
      <description>The univariate normal pdf is:\[f_Y(y)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{1}{2}(\frac{y-\mu}{\sigma})^2}, \quad -\infty&amp;lt;y&amp;lt;+\infty\]
The bivariate normal pdf is, \[f_{X,Y}(x, y)=Ke^{-\frac{1}{2}c(x^2-2\nu xy+y^2)}, \quad -\infty&amp;lt;x, y&amp;lt;+\infty\] where \(c\) and \(\nu\) are constants.\[\begin{align}f_{X,Y}(x, y)&amp;amp;=Ke^{-\frac{1}{2}c(x^2-2\nu xy+y^2)}\\&amp;amp;=Ke^{-\frac{1}{2}c(x^2-\nu^2x^2+\nu^2x^2-2\nu xy+y^2)}\\&amp;amp;=Ke^{-\frac{1}{2}c(x^2-\nu^2x^2)+(\nu x-y)^2}\\&amp;amp;=Ke^{-\frac{1}{2}cx^2(1-\nu^2)}e^{-\frac{1}{2}c(\nu x-y)^2}\\\end{align}\] The exponents must be negative, so \(1-\nu^2&amp;gt;0\).
\[\begin{align}\int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty}f_{X,Y}(x, y)dxdy&amp;amp;=\int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty}Ke^{-\frac{1}{2}cx^2(1-\nu^2)}e^{-\frac{1}{2}c(\nu x-y)^2}dxdy\\&amp;amp;=K\int_{-\infty}^{+\infty}e^{-\frac{1}{2}cx^2(1-\nu^2)} \Biggl[\int_{-\infty}^{+\infty}e^{-\frac{1}{2}c(y-\nu x)^2}dy\Biggr]dx\\&amp;amp;=K\int_{-\infty}^{+\infty}e^{-\frac{1}{2}cx^2(1-\nu^2)}\frac{\sqrt{2\pi}}{\sqrt{c}}dx\\&amp;amp;=K\frac{\sqrt{2\pi}}{\sqrt{c}}\frac{\sqrt{2\pi}}{\sqrt{c(1-\nu^2)}}\\&amp;amp;=K\frac{2\pi}{c\sqrt{1-\nu^2}}\\&amp;amp;=1\end{align}\] Then \(K=\frac{c\sqrt{1-\nu^2}}{2\pi}\), if we choose \(c=\frac{1}{1-\nu^2}\), then \(K=\frac{1}{2\pi\sqrt{1-\nu^2}}\) and\[\begin{align}f_{X,Y}(x, y)&amp;amp;=\frac{1}{2\pi\sqrt{1-\nu^2}}e^{-\frac{1}{2}\frac{1}{1-\nu^2}(x^2-2\nu xy+y^2)}\\&amp;amp;=\frac{1}{2\pi\sqrt{1-\nu^2}}e^{-\frac{1}{2}\frac{1}{1-\nu^2}(x^2-\nu^2x^2+\nu^2x^2-2\nu xy+y^2)}\\&amp;amp;=\frac{1}{2\pi\sqrt{1-\nu^2}}e^{-\frac{1}{2}x^2}e^{-\frac{1}{2}\frac{1}{1-\nu^2}(\nu x-y)^2}\end{align}\] The marginal pdfs are sure the standard normal:\[\begin{align}f_{X}(x)&amp;amp;=\int_{-\infty}^{+\infty}f_{X,Y}(x, y)dy\\&amp;amp;=\int_{-\infty}^{+\infty}\frac{1}{2\pi\sqrt{1-\nu^2}}e^{-\frac{1}{2}x^2}e^{-\frac{1}{2}\frac{1}{1-\nu^2}(\nu x-y)^2}dy\\&amp;amp;=\frac{1}{2\pi\sqrt{1-\nu^2}}e^{-\frac{1}{2}x^2}\int_{-\infty}^{+\infty}e^{-\frac{1}{2}\frac{1}{1-\nu^2}(\nu x-y)^2}dy\\&amp;amp;=\frac{1}{2\pi\sqrt{1-\nu^2}}e^{-\frac{1}{2}x^2}\sqrt{2\pi}\sqrt{1-\nu^2}\\&amp;amp;=\frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}x^2}\end{align}\] and \(E(X)=E(Y)=0\) and \(\sigma_X=\sigma_Y=1\), then the correlation coefficient between X and Y is:\[\begin{align}\rho(X,Y)&amp;amp;=\frac{Cov(X,Y)}{\sigma_X\sigma_Y}\\&amp;amp;=\frac{E(XY) − E(X)E(Y)}{\sigma_X\sigma_Y}\\&amp;amp;=E(XY)\\&amp;amp;=\int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty}xyf_{X,Y}(x, y)dxdy\\&amp;amp;=\int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty}xy\frac{1}{2\pi\sqrt{1-\nu^2}}e^{-\frac{1}{2}x^2}e^{-\frac{1}{2}\frac{1}{1-\nu^2}(\nu x-y)^2}dxdy\\&amp;amp;=\int_{-\infty}^{+\infty}\frac{1}{\sqrt{2\pi}}xe^{-\frac{1}{2}x^2} \Biggl[\int_{-\infty}^{+\infty}\frac{1}{\sqrt{2\pi(1-\nu^2)}}ye^{-\frac{1}{2}\frac{1}{1-\nu^2}(\nu x-y)^2}dy\Biggr]dx\\&amp;amp;=\int_{-\infty}^{+\infty}\frac{1}{\sqrt{2\pi}}xe^{-\frac{1}{2}x^2}\nu x dx\\&amp;amp;=\nu\int_{-\infty}^{+\infty}\frac{1}{\sqrt{2\pi}}x^2e^{-\frac{1}{2}x^2}dx\\&amp;amp;=\nu Var(X)\\&amp;amp;=\nu \sigma_X\\&amp;amp;=\nu\end{align}\] So \(\nu\) is the correlation coefficient between X and Y.</description>
    </item>
    
    <item>
      <title>The square of Student t random variable is a F distribution with with 1 and n df</title>
      <link>/2020/08/30/the-square-of-student-t-random-variable-is-a-f-distribution-with-with-1-and-n-df/</link>
      <pubDate>Sun, 30 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/08/30/the-square-of-student-t-random-variable-is-a-f-distribution-with-with-1-and-n-df/</guid>
      <description>The Student t ratio with \(n\) degrees of freedom is denoted \(T_n\), where\(T_n=\frac{Z}{\sqrt{\frac{U}{n}}}\), \(Z\) is a standard normal random variable and \(U\) is a \(\chi^2\) random variable independent of \(Z\) with \(n\) degrees of freedom.
Because \(T_n^2= \frac{Z^2}{U/n}\) has an \(F\) distribution with \(1\) and \(n\) df, then,\[f_{T_n^2}(t)=\frac{\Gamma(\frac{1+n}{2})}{\Gamma(\frac{1}{2})\Gamma(\frac{n}{2})}\frac{n^{\frac{n}{2}}t^{-\frac{1}{2}}}{(n+t)^{\frac{1+n}{2}}},\quad t&amp;gt;0\]
Then,\[\begin{align}f_{T_n}(t)&amp;amp;=\frac{d}{dt}F_{T_n}(t)\\&amp;amp;=\frac{d}{dt}P(T_n\le t)\\&amp;amp;=\frac{d}{dt}(\frac{1}{2}+P(0\le T_n\le t))\\&amp;amp;=\frac{d}{dt}(\frac{1}{2}+\frac{1}{2}P(-t\le T_n\le t))\quad (t&amp;gt;0)\\&amp;amp;=\frac{d}{dt}(\frac{1}{2}+\frac{1}{2}P(T_n^2\le t^2))\\&amp;amp;=\frac{d}{dt}(\frac{1}{2}+\frac{1}{2}F_{T_n^2}(t^2))\\&amp;amp;=t\cdot f_{T_n^2}(t^2)\\&amp;amp;=t\cdot \frac{\Gamma(\frac{1+n}{2})}{\Gamma(\frac{1}{2})\Gamma(\frac{n}{2})}\frac{n^{\frac{n}{2}}t^{-1}}{(n+t^2)^{\frac{1+n}{2}}}\\&amp;amp;=\frac{\Gamma(\frac{1+n}{2})}{\Gamma(\frac{1}{2})\Gamma(\frac{n}{2})}\frac{1}{\sqrt{n}}\frac{1}{(1+\frac{t^2}{n})^{\frac{1+n}{2}}}\\&amp;amp;=\frac{\Gamma(\frac{1+n}{2})}{\Gamma(\frac{n}{2})}\frac{1}{\sqrt{n\pi}}\frac{1}{(1+\frac{t^2}{n})^{\frac{1+n}{2}}}\end{align}\]</description>
    </item>
    
    <item>
      <title>From the Cumulative distribution of standard normal distribution can get the chi square distribution </title>
      <link>/2020/08/29/from-the-cumulative-distribution-of-standard-normal-distribution-can-get-the-chi-square-distribution/</link>
      <pubDate>Sat, 29 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/08/29/from-the-cumulative-distribution-of-standard-normal-distribution-can-get-the-chi-square-distribution/</guid>
      <description>The Cumulative distribution function of standard Normal distribution in the region \((-x,x),x&amp;gt;0\) is:\[\begin{align}\Phi(x)&amp;amp;=\frac{1}{\sqrt{2\pi}}\int_{-x}^{x} e^{-\frac{1}{2}z^2}dz\\&amp;amp;=\frac{2}{\sqrt{2\pi}}\int_{0}^{x} e^{-\frac{1}{2}z^2}dz\\&amp;amp;=\frac{2}{\sqrt{2\pi}}\int_{0}^{\sqrt{x}} \frac{1}{2\sqrt{u}}e^{-\frac{1}{2}u}du \quad (u=z^2)\\&amp;amp;=\frac{1}{\sqrt{2\pi}}\int_{0}^{\sqrt{x}} \frac{1}{\sqrt{u}}e^{-\frac{1}{2}u}du\\&amp;amp;=\int_{0}^{\sqrt{x}}\frac{(\frac{1}{2})^{\frac{1}{2}}}{\Gamma(\frac{1}{2})}u^{(\frac{1}{2})-1}e^{-\frac{1}{2}u}du\end{align}\]
Here, the integrand\(f_U(u)=\frac{(\frac{1}{2})^{\frac{1}{2}}}{\Gamma(\frac{1}{2})}u^{(\frac{1}{2})-1}e^{-\frac{1}{2}u}\)is a special Gamma distribution with \(r=\frac{1}{2}, \lambda=\frac{1}{2}\). Here, the \(u=z^2\), where the \(z\) is independent standard normal random variable.
And the sum of several \(U=Z^2\) variables\[Y=\sum_{j=1}^{m} U_j=\sum_{j=1}^{m} Z_{j}^{2}\] is still a Gamma distribution:\[f_Y(y)=\frac{(\frac{1}{2})^{\frac{m}{2}}}{\Gamma(\frac{m}{2})}y^{(\frac{m}{2})-1}e^{-\frac{1}{2}y}\],and we give the special Gamma distribution with \(r=\frac{m}{2}, \lambda=\frac{1}{2}\) a new name: \(\chi^2\) distribution with \(m\) degrees of freedom.</description>
    </item>
    
    <item>
      <title>Ratio of 2 independent chi square random variables divided by their degrees of freedom is F distribution</title>
      <link>/2020/08/29/ratio-of-2-independent-chi-square-random-variables-divided-by-their-degrees-of-freedom-is-f-distribution/</link>
      <pubDate>Sat, 29 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/08/29/ratio-of-2-independent-chi-square-random-variables-divided-by-their-degrees-of-freedom-is-f-distribution/</guid>
      <description>When V and U are 2 \(\chi^2\) independent random variables: \(f_V(v)=\frac{(\frac{1}{2})^{\frac{m}{2}}}{\Gamma(\frac{m}{2})}v^{(\frac{m}{2})-1}e^{-\frac{1}{2}v}\)
\(f_U(u)=\frac{(\frac{1}{2})^{\frac{n}{2}}}{\Gamma(\frac{n}{2})}u^{(\frac{n}{2})-1}e^{-\frac{1}{2}u}\)
with \(m\) and \(n\) degrees of freedom, then, the pdf for \(W=V/U\) is:
\[\begin{align}f_{V/U}(\omega)&amp;amp;=\int_{0}^{+\infty}|u|f_U(u)f_V(u\omega)du\\&amp;amp;=\int_{0}^{+\infty}u\frac{(\frac{1}{2})^{\frac{n}{2}}}{\Gamma(\frac{n}{2})}u^{\frac{n}{2}-1}e^{-\frac{1}{2}u} \frac{(\frac{1}{2})^{\frac{m}{2}}}{\Gamma(\frac{m}{2})}(u\omega)^{\frac{m}{2}-1}e^{-\frac{1}{2}u\omega}du\\&amp;amp;=\frac{(\frac{1}{2})^{\frac{n}{2}}}{\Gamma(\frac{n}{2})}\frac{(\frac{1}{2})^{\frac{m}{2}}}{\Gamma(\frac{m}{2})} \omega^{\frac{m}{2}-1} \int_{0}^{+\infty}u^{\frac{n}{2}}u^{\frac{m}{2}-1} e^{-\frac{1}{2}u(1+\omega)}du\\&amp;amp;=\frac{(\frac{1}{2})^{\frac{n}{2}}}{\Gamma(\frac{n}{2})}\frac{(\frac{1}{2})^{\frac{m}{2}}}{\Gamma(\frac{m}{2})} \omega^{\frac{m}{2}-1} \int_{0}^{+\infty}u^{\frac{n+m}{2}-1} e^{-\frac{1}{2}u(1+\omega)}du\\&amp;amp;=\frac{(\frac{1}{2})^{\frac{n}{2}}}{\Gamma(\frac{n}{2})}\frac{(\frac{1}{2})^{\frac{m}{2}}}{\Gamma(\frac{m}{2})} \omega^{\frac{m}{2}-1} (\frac{\Gamma(\frac{n+m}{2})}{(\frac{1}{2}(1+\omega))^{\frac{n+m}{2}}})\\&amp;amp;=\frac{\Gamma(\frac{n+m}{2})}{\Gamma(\frac{n}{2})\Gamma(\frac{m}{2})}\frac{\omega^{\frac{m}{2}-1}}{(1+\omega)^{\frac{n+m}{2}}}\end{align}\]
Then, the pdf for \(W=\frac{V/m}{U/n}\) is:\[\begin{align}f_{\frac{V/m}{U/n}}&amp;amp;=f_{\frac{n}{m}V/U}\\&amp;amp;=\frac{m}{n}f_{V/U}(\frac{m}{n}\omega)\\&amp;amp;=\frac{m}{n}\frac{\Gamma(\frac{n+m}{2})}{\Gamma(\frac{n}{2})\Gamma(\frac{m}{2})}\frac{(\frac{m}{n}\omega)^{\frac{m}{2}-1}}{(1+\frac{m}{n}\omega)^{\frac{n+m}{2}}}\\&amp;amp;=\frac{\Gamma(\frac{n+m}{2})}{\Gamma(\frac{n}{2})\Gamma(\frac{m}{2})}\frac{m}{n}\frac{(\frac{m}{n}\omega)^{\frac{m}{2}-1}}{(n+m\omega)^{\frac{n+m}{2}}}n^{\frac{n+m}{2}}\\&amp;amp;=\frac{\Gamma(\frac{n+m}{2})}{\Gamma(\frac{n}{2})\Gamma(\frac{m}{2})}\frac{m^{\frac{m}{2}}n^{\frac{n}{2}}\omega^{\frac{m}{2}-1}}{(n+m\omega)^{\frac{n+m}{2}}}\end{align}\], which is a \(F\) distribution with \(m\) and \(n\) degrees of freedom.</description>
    </item>
    
    <item>
      <title>Geometric Distribution is the first success occurs on kth Bernoulli trial, Negative Binomial is the rth success occurs on kth Bernoulli trial</title>
      <link>/2020/08/25/geometric-distribution-is-the-first-success-occurs-on-kth-bernoulli-trial-negative-binomial-is-the-rth-success-occurs-on-kth-bernoulli-trial/</link>
      <pubDate>Tue, 25 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/08/25/geometric-distribution-is-the-first-success-occurs-on-kth-bernoulli-trial-negative-binomial-is-the-rth-success-occurs-on-kth-bernoulli-trial/</guid>
      <description>The Geometric variable X has a pdf like this:\[P_X(k)=P(X=k)=(1-p)^{k-1}p, \quad k=1,2,3,..\]
The moment-generating function for a Geometric random variable X is:\[\begin{align}M_X(t)=E(e^{tX})&amp;amp;=\sum_{all\ k}e^{tk}(1-p)^{k-1}p\\&amp;amp;=\frac{p}{1-p}\sum_{all\ k}(e^t(1-p))^{k}\\&amp;amp;=\frac{p}{1-p}(\frac{1}{1-e^t(1-p)}-1)\\&amp;amp;=\frac{pe^t}{1-(1-p)e^t}\end{align}\]
The expected value is:\[\begin{align}M_X^{(1)}(t)&amp;amp;=\frac{d}{dt}\frac{pe^t}{1-(1-p)e^t}\\&amp;amp;=\frac{pe^t}{1-(1-p)e^t}+\frac{pe^t(1-p)e^t}{(1-(1-p)e^t)^2}\Bigl|_{t=0}\\&amp;amp;=1+\frac{p(1-p)}{p^2}\\&amp;amp;=\frac{1}{p}\end{align}\]
\[\begin{align}M_X^{(2)}(t)&amp;amp;=\frac{d}{dt}\Bigl(\frac{pe^t}{1-(1-p)e^t}+\frac{pe^t(1-p)e^t}{(1-(1-p)e^t)^2}\Bigr)\\&amp;amp;=\frac{pe^t}{1-(1-p)e^t}+\frac{pe^t(1-p)e^t}{(1-(1-p)e^t)^2}+\frac{2pe^{2t}(1-p)}{(1-(1-p)e^t)^2}+\frac{2pe^{3t}(1-p)^2}{(1-(1-p)e^t)^3}\Biggl|_{t=0}\\&amp;amp;=1+(1/p-1)+2(1/p-1)+2(1/p-1)^2\\&amp;amp;=2/p^2-1/p\end{align}\]
Then, the Variance is:\(Var(X)=E(X^2)-(E(X))^2=2/p^2-1/p-1/p^2=1/p^2-1/p=\frac{1-p}{p^2}\)
Negative Binomial is the rth success occurs on kth Bernoulli trialThe Negative Binomial variable Y has a pdf like this:\[P_Y(k)=P(Y=k)=\binom{k-1}{r-1}p^r(1-p)^{k-r}, \quad k=r,r+1,r+2,.</description>
    </item>
    
    <item>
      <title>Exponential distribution is interval between consecutive Poisson events</title>
      <link>/2020/08/24/exponential-distribution-is-interval-between-consecutive-poisson-events/</link>
      <pubDate>Mon, 24 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/08/24/exponential-distribution-is-interval-between-consecutive-poisson-events/</guid>
      <description>Let’s denote the interval between consecutive Poisson events with random variable Y, during the interval that extends from a to a + y, the number of Poisson events k has the probability \(P(k)=e^{-\lambda y} \frac{(\lambda y)^k}{k!}\), if \(k=0\),\(e^{-\lambda y}\frac{(\lambda y)^0}{0!}=e^{-\lambda y}\) means there is no event during the (a,a+y) time period.
Because there will be no occurrences in the interval (a, a + y) only if Y &amp;gt; y,so \(P(Y &amp;gt; y)=e^{-\lambda y}\), then the cdf is \(F_Y(y)=P(Y \le y)=1-P(Y &amp;gt; y)=1-e^{-\lambda y}\).</description>
    </item>
    
    <item>
      <title>Poisson is a limit of Binomial when n goes to infinity with np maintained</title>
      <link>/2020/08/24/poisson-is-a-limit-of-binomial-when-n-goes-to-infinity-with-np-maintained/</link>
      <pubDate>Mon, 24 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/08/24/poisson-is-a-limit-of-binomial-when-n-goes-to-infinity-with-np-maintained/</guid>
      <description>The binomial random variable has a pdf like this:\(P_X(k)=\binom{n}{k}p^k(1-p)^{n-k},\quad k=0,1,2,...,n\)Its moment-generating function is:\[\begin{align}M_X(t)=E(e^{tX})&amp;amp;=\sum_{k=0}^{n}e^{tk}\binom{n}{k}p^k(1-p)^{n-k}\\&amp;amp;=\sum_{k=0}^{n}\binom{n}{k}(e^tp)^k(1-p)^{n-k}\\&amp;amp;=(1-p+pe^t)^n\end{align}\]
Then \(M_X^{(1)}(t)=n(1-p+pe^t)^{n-1}pe^t|_{t=0}=np=E(X)\)\[\begin{align}M_X^{(2)}(t)&amp;amp;=n(n-1)(1-p+pe^t)^{n-2}pe^tpe^t+n(1-p+pe^t)^{n-1}pe^t|_{t=0}\\&amp;amp;=n(n-1)p^2+np=E(X^2)\end{align}\]
Then \(Var(X)=E(X^2)-(E(X))^2=n(n-1)p^2+np-(np)^2=-np^2+np=np(1-p)\)
For the binomial random variable X:\(P_X(k)=\binom{n}{k}p^k(1-p)^{n-k},\quad k=0,1,2,...,n\), if \(n\to+\infty\) with \(\lambda=np\) remains constant, then\[\begin{align}\lim_{n\to+\infty}\binom{n}{k}p^k(1-p)^{n-k}&amp;amp;=\lim_{n\to+\infty}\frac{n!}{k!(n-k)!}(\frac{\lambda}{n})^k(1-\frac{\lambda}{n})^{n-k}\\&amp;amp;=\lim_{n\to+\infty}\frac{n!}{k!(n-k)!}\lambda^k(\frac{1}{n})^k(1-\frac{\lambda}{n})^n(1-\frac{\lambda}{n})^{-k}\\&amp;amp;=\frac{\lambda^k}{k!}\lim_{n\to+\infty}\frac{n!}{(n-k)!}(\frac{1}{n})^k(\frac{n}{n-\lambda})^k(1-\frac{\lambda}{n})^n\\&amp;amp;=e^{-\lambda}\frac{\lambda^k}{k!}\lim_{n\to+\infty}\frac{n!}{(n-k)!}(\frac{1}{n-\lambda})^k\\&amp;amp;=e^{-\lambda}\frac{\lambda^k}{k!}\lim_{n\to+\infty}\frac{n(n-1)...(n-k+1)}{(n-\lambda)(n-\lambda)...(n-\lambda)}\\&amp;amp;=e^{-\lambda}\frac{\lambda^k}{k!}\end{align}\]
The moment-generating function of Poisson random variable X is:\[\begin{align}M_X(t)=E(e^{tX})&amp;amp;=\sum_{k=0}^{n}e^{tk}e^{-\lambda}\frac{\lambda^k}{k!}\\&amp;amp;=e^{-\lambda}\sum_{k=0}^{n}\frac{(\lambda e^t)^k}{k!}\\&amp;amp;=e^{-\lambda}e^{\lambda e^t}\\&amp;amp;=e^{\lambda e^t-\lambda}\end{align}\]</description>
    </item>
    
    <item>
      <title>The Gamma random variable denotes the waiting time for a Poisson event also the sum of Exponential events</title>
      <link>/2020/08/24/the-gamma-random-variable-denotes-the-waiting-time-for-the-rth-poisson-event/</link>
      <pubDate>Mon, 24 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/08/24/the-gamma-random-variable-denotes-the-waiting-time-for-the-rth-poisson-event/</guid>
      <description>The Gamma random variable denotes the waiting time for the \(r^{th}\) Poisson event, and also denotes the sum of r Exponential random variables. The sum of m Gamma random variables (shared the same parameter \(\lambda\)) is a Gamma random variable, which denotes the waiting time for the \((\sum_{i=1}^{m} r_i)^{th}\) Poisson event, and also denotes the sum of \(\sum_{i=1}^{m} r_i\) Exponential random variables.
let Y denote the waiting time to the occurrence of the \(r^{th}\) Poisson event,the probability fewer than r Poisson events occur in [0, y] time period is\(P(Y&amp;gt;y)=\sum_{k=0}^{r-1}e^{-\lambda y}\frac{(\lambda y)^k}{k!</description>
    </item>
    
    <item>
      <title>The Gamma and Beta functions</title>
      <link>/2020/08/21/the-gamma-and-beta-functions/</link>
      <pubDate>Fri, 21 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/08/21/the-gamma-and-beta-functions/</guid>
      <description>The Gamma function:\[\Gamma(s)=\int_{0}^{+\infty}t^{s-1}e^{-t}dt\quad \Bigl(=(s-1)! \quad s\in \mathbb Z^+\Bigr) (0&amp;lt;s&amp;lt;\infty)\] Because \[\begin{align}\Gamma(s+1)&amp;amp;=\int_{0}^{+\infty}t^{s}e^{-t}dt\\&amp;amp;=-\int_{0}^{+\infty}t^{s}d(e^{-t})\\&amp;amp;=-\Biggl[t^{s}e^{-t}|_{0}^{\infty}-\int_{0}^{+\infty}st^{s-1}e^{-t}dt\Biggl]\\&amp;amp;=-\Biggl[0-s\Gamma(s)\Biggl]\\&amp;amp;=s\Gamma(s)\end{align}\] and \[\Gamma(1)=\int_{0}^{+\infty}t^{1-1}e^{-t}dt=\int_{0}^{+\infty}e^{-t}dt=1\]The product of two Gamma functions:\[\begin{align}\Gamma(x)\Gamma(y)&amp;amp;=\int_{0}^{+\infty}u^{x-1}e^{-u}du\int_{0}^{+\infty}v^{y-1}e^{-v}dv\\&amp;amp;=\int_{u=0}^{+\infty}\int_{v=0}^{+\infty}e^{-(u+v)}u^{x-1}v^{y-1}dudv \quad (let\quad u+v=z; \quad u/z=t; \quad v/z=1-t; \quad dudv=zdtdz)\\&amp;amp;=\int_{z=0}^{+\infty}\int_{t=0}^{t=1}e^{-z}(zt)^{x-1}(z(1-t))^{y-1}zdtdz\\&amp;amp;=\int_{z=0}^{+\infty}e^{-z}z^{(x+y-1)}dz\int_{t=0}^{t=1}t^{(x-1)}(1-t)^{(y-1)}dt\\&amp;amp;=\Gamma(x+y)\int_{t=0}^{t=1}t^{(x-1)}(1-t)^{(y-1)}dt\end{align}\]
We define this integral \(\int_{t=0}^{t=1}t^{(x-1)}(1-t)^{(y-1)}dt\) as \(B(x,y),\quad (x&amp;gt;0 ;\quad y&amp;gt;0)\), this is the Beta function.\(B(x,y)=\frac{\Gamma(x)\Gamma(y)}{\Gamma(x+y)}\) \(\Bigl(=\frac{(x-1)!(y-1)!}{(x+y-1)!}\quad x;y\in \mathbb Z^+ \Bigr)\) this is the complete Beta function.</description>
    </item>
    
    <item>
      <title>How to derive the beautiful probability density function (pdf) of Normal Distribution?</title>
      <link>/2020/08/13/distributions/</link>
      <pubDate>Thu, 13 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/08/13/distributions/</guid>
      <description>How can we derive the probability density function (pdf) of Normal Distribution?\[f_Y(y)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{1}{2}(\frac{y-\mu}{\sigma})^2}, \quad -\infty&amp;lt;y&amp;lt;+\infty\]
Let’s draw a normal pdf first
#draw normal pdfx &amp;lt;- seq(-5, 5, length.out = 201); dx &amp;lt;- diff(x)[1]y &amp;lt;- dnorm(x, mean = 0, sd = 1)base::plot(x, y, type = &amp;quot;l&amp;quot;, col = &amp;quot;skyblue&amp;quot;,xlab=&amp;quot;x&amp;quot; , ylab=&amp;quot;p(x)&amp;quot; , cex.lab=1.5,main=&amp;quot;Normal Probability Density&amp;quot; , cex.main=1.5,lwd=2)text( 0, .6*max(y) , bquote( paste(mu ,&amp;quot; = 0 &amp;quot;) ), cex=1.</description>
    </item>
    
  </channel>
</rss>
