<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Distribution on A Hugo website</title>
    <link>/tags/distribution/</link>
    <description>Recent content in Distribution on A Hugo website</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 16 Apr 2021 00:00:00 +0000</lastBuildDate><atom:link href="/tags/distribution/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>AOS Chapter08 Estimating the CDF and Statistical Functionals</title>
      <link>/2021/04/16/aos-chapter08-estimating-the-cdf-and-statistical-functionals/</link>
      <pubDate>Fri, 16 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>/2021/04/16/aos-chapter08-estimating-the-cdf-and-statistical-functionals/</guid>
      <description>8. Estimating the CDF and Statistical Functionals8.1 Empirical distribution function8.2 Statistical Functionals8.3 Technical Appendix8.5 ExercisesReferences8. Estimating the CDF and Statistical Functionals8.1 Empirical distribution functionThe empirical distribution function \(\hat{F_n}\) is the CDF that puts mass \(1/n\) at each data point \(X_i\). Formally,
\[\begin{align}\hat{F_n}(x) &amp;amp; = \frac{\sum_{i=1}^n I\left(X_i \leq x \right)}{n} \\&amp;amp;= \frac{\text{#}|\text{observations less than or equal to x}|}{n}\end{align}\]</description>
    </item>
    
    <item>
      <title>AOS Chapter07 Models, Statistical Inference and Learning</title>
      <link>/2021/04/15/aos-chapter07-models-statistical-inference-and-learning/</link>
      <pubDate>Thu, 15 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>/2021/04/15/aos-chapter07-models-statistical-inference-and-learning/</guid>
      <description>7. Models, Statistical Inference and Learning7.2 Parametric and Nonparametric Models7.3 Fundamental Concepts in Inference7.5 Technical AppendixReferences7. Models, Statistical Inference and Learning7.2 Parametric and Nonparametric ModelsA statistical model is a set of distributions \(\mathfrak{F}\).
A parametric model is a set \(\mathfrak{F}\) that may be parametrized by a finite number of parameters. For example, if we assume that data comes from a normal distribution then</description>
    </item>
    
    <item>
      <title>AOS Chapter06 Convergence of Random Variables</title>
      <link>/2021/04/14/aos-chapter05-convergence-of-random-variables/</link>
      <pubDate>Wed, 14 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>/2021/04/14/aos-chapter05-convergence-of-random-variables/</guid>
      <description>6. Convergence of Random Variables6.2 Types of convergence6.3 The Law of Large Numbers6.4 The Central Limit Theorem6.5 The Delta Method6.6 Technical appendix6.8 ExercisesReferences6. Convergence of Random Variables6.2 Types of convergence\(X_n\) converges to \(X\) in probability, written \(X_n \xrightarrow{\text{P}} X\), if, for every \(\epsilon &amp;gt; 0\),:
\[ \mathbb{P}( |X_n - X| &amp;gt; \epsilon ) \rightarrow 0 \]</description>
    </item>
    
    <item>
      <title>AOS Chapter05 Inequalities</title>
      <link>/2021/04/13/aos-chapter05-inequalities/</link>
      <pubDate>Tue, 13 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>/2021/04/13/aos-chapter05-inequalities/</guid>
      <description>5. Inequalities5.1 Markov and Chebyshev Inequalities5.2 Hoeffding’s Inequality5.3 Cauchy-Schwartz and Jensen Inequalities5.4 Technical Appendix: Proof of Hoeffding’s Inequality5.6 ExercisesReferences5. Inequalities5.1 Markov and Chebyshev InequalitiesTheorem 5.1 (Markov’s Inequality). Let \(X\) be a non-negative random variable and suppose that \(\mathbb{E}(X)\) exists. For any \(t &amp;gt; 0\),
\[ \mathbb{P}(X &amp;gt; t) \leq \frac{\mathbb{E}(X)}{t} \]
Proof.
\[ \mathbb{E}(X)=\int_0^\infty xf(x) dx=\int_0^t xf(x) dx + \int_t^\infty xf(x) dx\geq \int_t^\infty xf(x) dx\geq t \int_t^\infty f(x) dx= t \mathbb{P}(X &amp;gt; t)\]</description>
    </item>
    
    <item>
      <title>AOS chapter04 Expectation</title>
      <link>/2021/04/12/aos-chapter04-expectation/</link>
      <pubDate>Mon, 12 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>/2021/04/12/aos-chapter04-expectation/</guid>
      <description>4. Expectation4.1 Expectation of a Random Variable4.2 Properties of Expectations4.3 Variance and Covariance4.4 Expectation and Variance of Important Random Variables4.5 Conditional Expectation4.6 Technical Appendix4.7 ExercisesReferences4. Expectation4.1 Expectation of a Random VariableThe expected value, mean or first moment of \(X\) is defined to be
\[ \mathbb{E}(X) = \int x \; dF(x) = \begin{cases}\sum_x x f(x) &amp;amp;\text{if } X \text{ is discrete} \\\int x f(x)\; dx &amp;amp;\text{if } X \text{ is continuous}\end{cases} \]</description>
    </item>
    
    <item>
      <title>AOS chapter03 Random Variables</title>
      <link>/2021/04/11/aos-chapter03-random-variables/</link>
      <pubDate>Sun, 11 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>/2021/04/11/aos-chapter03-random-variables/</guid>
      <description>3. Random Variables3.1 Introduction3.2 Distribution Functions and Probability Functions3.3 Some Important Discrete Random Variables3.4 Some Important Continuous Random Variables3.5 Bivariate Distributions3.6 Marginal Distributions3.7 Independent Random Variables3.8 Conditional Distributions3.9 Multivariate Distributions and IID Samples3.10 Two Important Multivariate Distributions3.11 Transformations of Random Variables3.12 Transformation of Several Random Variables3.13 Technical Appendix3.14 ExercisesReferences3.</description>
    </item>
    
    <item>
      <title>The Multivariate Normal Density</title>
      <link>/2020/09/11/the-multivariate-normal-density/</link>
      <pubDate>Fri, 11 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/09/11/the-multivariate-normal-density/</guid>
      <description>The univariate normal pdf is:\[f_X(x)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{1}{2}(\frac{x-\mu}{\sigma})^2}, \quad -\infty&amp;lt;x&amp;lt;+\infty\] The term \((\frac{x-\mu}{\sigma})^2=(x-\mu)(\sigma^2)^{-1}(x-\mu)\) measures the square ofthe univariate distance from \(x\) to \(\mu\) in standard deviation units. This can be generalized to a \(p\times 1\) vector \(\mathbf x\) of observations on several variables as \((\mathbf X-\boldsymbol \mu)^T(\mathbf \Sigma)^{-1}(\mathbf X-\boldsymbol \mu)\), which is the square of the multivariate generalized distance from \(\mathbf X\) to \(\boldsymbol \mu\), the \(p\times p\) matrix \(\mathbf \Sigma\) is the variance–covariance matrix of \(\mathbf X\).</description>
    </item>
    
    <item>
      <title>The Bivariate Normal Distribution</title>
      <link>/2020/09/10/the-bivariate-normal-distribution/</link>
      <pubDate>Thu, 10 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/09/10/the-bivariate-normal-distribution/</guid>
      <description>The univariate normal pdf is:\[f_Y(y)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{1}{2}(\frac{y-\mu}{\sigma})^2}, \quad -\infty&amp;lt;y&amp;lt;+\infty\]
The bivariate normal pdf is, \[f_{X,Y}(x, y)=Ke^{-\frac{1}{2}c(x^2-2\nu xy+y^2)}, \quad -\infty&amp;lt;x, y&amp;lt;+\infty\] where \(c\) and \(\nu\) are constants.\[\begin{align}f_{X,Y}(x, y)&amp;amp;=Ke^{-\frac{1}{2}c(x^2-2\nu xy+y^2)}\\&amp;amp;=Ke^{-\frac{1}{2}c(x^2-\nu^2x^2+\nu^2x^2-2\nu xy+y^2)}\\&amp;amp;=Ke^{-\frac{1}{2}c(x^2-\nu^2x^2)+(\nu x-y)^2}\\&amp;amp;=Ke^{-\frac{1}{2}cx^2(1-\nu^2)}e^{-\frac{1}{2}c(\nu x-y)^2}\\\end{align}\] The exponents must be negative, so \(1-\nu^2&amp;gt;0\).
\[\begin{align}\int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty}f_{X,Y}(x, y)dxdy&amp;amp;=\int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty}Ke^{-\frac{1}{2}cx^2(1-\nu^2)}e^{-\frac{1}{2}c(\nu x-y)^2}dxdy\\&amp;amp;=K\int_{-\infty}^{+\infty}e^{-\frac{1}{2}cx^2(1-\nu^2)} \Biggl[\int_{-\infty}^{+\infty}e^{-\frac{1}{2}c(y-\nu x)^2}dy\Biggr]dx\\&amp;amp;=K\int_{-\infty}^{+\infty}e^{-\frac{1}{2}cx^2(1-\nu^2)}\frac{\sqrt{2\pi}}{\sqrt{c}}dx\\&amp;amp;=K\frac{\sqrt{2\pi}}{\sqrt{c}}\frac{\sqrt{2\pi}}{\sqrt{c(1-\nu^2)}}\\&amp;amp;=K\frac{2\pi}{c\sqrt{1-\nu^2}}\\&amp;amp;=1\end{align}\] Then \(K=\frac{c\sqrt{1-\nu^2}}{2\pi}\), if we choose \(c=\frac{1}{1-\nu^2}\), then \(K=\frac{1}{2\pi\sqrt{1-\nu^2}}\) and\[\begin{align}f_{X,Y}(x, y)&amp;amp;=\frac{1}{2\pi\sqrt{1-\nu^2}}e^{-\frac{1}{2}\frac{1}{1-\nu^2}(x^2-2\nu xy+y^2)}\\&amp;amp;=\frac{1}{2\pi\sqrt{1-\nu^2}}e^{-\frac{1}{2}\frac{1}{1-\nu^2}(x^2-\nu^2x^2+\nu^2x^2-2\nu xy+y^2)}\\&amp;amp;=\frac{1}{2\pi\sqrt{1-\nu^2}}e^{-\frac{1}{2}x^2}e^{-\frac{1}{2}\frac{1}{1-\nu^2}(\nu x-y)^2}\end{align}\] The marginal pdfs are sure the standard normal:\[\begin{align}f_{X}(x)&amp;amp;=\int_{-\infty}^{+\infty}f_{X,Y}(x, y)dy\\&amp;amp;=\int_{-\infty}^{+\infty}\frac{1}{2\pi\sqrt{1-\nu^2}}e^{-\frac{1}{2}x^2}e^{-\frac{1}{2}\frac{1}{1-\nu^2}(\nu x-y)^2}dy\\&amp;amp;=\frac{1}{2\pi\sqrt{1-\nu^2}}e^{-\frac{1}{2}x^2}\int_{-\infty}^{+\infty}e^{-\frac{1}{2}\frac{1}{1-\nu^2}(\nu x-y)^2}dy\\&amp;amp;=\frac{1}{2\pi\sqrt{1-\nu^2}}e^{-\frac{1}{2}x^2}\sqrt{2\pi}\sqrt{1-\nu^2}\\&amp;amp;=\frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}x^2}\end{align}\] and \(E(X)=E(Y)=0\) and \(\sigma_X=\sigma_Y=1\), then the correlation coefficient between X and Y is:\[\begin{align}\rho(X,Y)&amp;amp;=\frac{Cov(X,Y)}{\sigma_X\sigma_Y}\\&amp;amp;=\frac{E(XY) − E(X)E(Y)}{\sigma_X\sigma_Y}\\&amp;amp;=E(XY)\\&amp;amp;=\int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty}xyf_{X,Y}(x, y)dxdy\\&amp;amp;=\int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty}xy\frac{1}{2\pi\sqrt{1-\nu^2}}e^{-\frac{1}{2}x^2}e^{-\frac{1}{2}\frac{1}{1-\nu^2}(\nu x-y)^2}dxdy\\&amp;amp;=\int_{-\infty}^{+\infty}\frac{1}{\sqrt{2\pi}}xe^{-\frac{1}{2}x^2} \Biggl[\int_{-\infty}^{+\infty}\frac{1}{\sqrt{2\pi(1-\nu^2)}}ye^{-\frac{1}{2}\frac{1}{1-\nu^2}(\nu x-y)^2}dy\Biggr]dx\\&amp;amp;=\int_{-\infty}^{+\infty}\frac{1}{\sqrt{2\pi}}xe^{-\frac{1}{2}x^2}\nu x dx\\&amp;amp;=\nu\int_{-\infty}^{+\infty}\frac{1}{\sqrt{2\pi}}x^2e^{-\frac{1}{2}x^2}dx\\&amp;amp;=\nu Var(X)\\&amp;amp;=\nu \sigma_X\\&amp;amp;=\nu\end{align}\] So \(\nu\) is the correlation coefficient between X and Y.</description>
    </item>
    
    <item>
      <title>The square of Student t random variable is a F distribution with with 1 and n df</title>
      <link>/2020/08/30/the-square-of-student-t-random-variable-is-a-f-distribution-with-with-1-and-n-df/</link>
      <pubDate>Sun, 30 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/08/30/the-square-of-student-t-random-variable-is-a-f-distribution-with-with-1-and-n-df/</guid>
      <description>The Student t ratio with \(n\) degrees of freedom is denoted \(T_n\), where\(T_n=\frac{Z}{\sqrt{\frac{U}{n}}}\), \(Z\) is a standard normal random variable and \(U\) is a \(\chi^2\) random variable independent of \(Z\) with \(n\) degrees of freedom.
Because \(T_n^2= \frac{Z^2}{U/n}\) has an \(F\) distribution with \(1\) and \(n\) df, then,\[f_{T_n^2}(t)=\frac{\Gamma(\frac{1+n}{2})}{\Gamma(\frac{1}{2})\Gamma(\frac{n}{2})}\frac{n^{\frac{n}{2}}t^{-\frac{1}{2}}}{(n+t)^{\frac{1+n}{2}}},\quad t&amp;gt;0\]
Then,\[\begin{align}f_{T_n}(t)&amp;amp;=\frac{d}{dt}F_{T_n}(t)\\&amp;amp;=\frac{d}{dt}P(T_n\le t)\\&amp;amp;=\frac{d}{dt}(\frac{1}{2}+P(0\le T_n\le t))\\&amp;amp;=\frac{d}{dt}(\frac{1}{2}+\frac{1}{2}P(-t\le T_n\le t))\quad (t&amp;gt;0)\\&amp;amp;=\frac{d}{dt}(\frac{1}{2}+\frac{1}{2}P(T_n^2\le t^2))\\&amp;amp;=\frac{d}{dt}(\frac{1}{2}+\frac{1}{2}F_{T_n^2}(t^2))\\&amp;amp;=t\cdot f_{T_n^2}(t^2)\\&amp;amp;=t\cdot \frac{\Gamma(\frac{1+n}{2})}{\Gamma(\frac{1}{2})\Gamma(\frac{n}{2})}\frac{n^{\frac{n}{2}}t^{-1}}{(n+t^2)^{\frac{1+n}{2}}}\\&amp;amp;=\frac{\Gamma(\frac{1+n}{2})}{\Gamma(\frac{1}{2})\Gamma(\frac{n}{2})}\frac{1}{\sqrt{n}}\frac{1}{(1+\frac{t^2}{n})^{\frac{1+n}{2}}}\\&amp;amp;=\frac{\Gamma(\frac{1+n}{2})}{\Gamma(\frac{n}{2})}\frac{1}{\sqrt{n\pi}}\frac{1}{(1+\frac{t^2}{n})^{\frac{1+n}{2}}}\end{align}\]</description>
    </item>
    
    <item>
      <title>From the Cumulative distribution of standard normal distribution can get the chi square distribution </title>
      <link>/2020/08/29/from-the-cumulative-distribution-of-standard-normal-distribution-can-get-the-chi-square-distribution/</link>
      <pubDate>Sat, 29 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/08/29/from-the-cumulative-distribution-of-standard-normal-distribution-can-get-the-chi-square-distribution/</guid>
      <description>The Cumulative distribution function of standard Normal distribution in the region \((-x,x),x&amp;gt;0\) is:\[\begin{align}\Phi(x)&amp;amp;=\frac{1}{\sqrt{2\pi}}\int_{-x}^{x} e^{-\frac{1}{2}z^2}dz\\&amp;amp;=\frac{2}{\sqrt{2\pi}}\int_{0}^{x} e^{-\frac{1}{2}z^2}dz\\&amp;amp;=\frac{2}{\sqrt{2\pi}}\int_{0}^{\sqrt{x}} \frac{1}{2\sqrt{u}}e^{-\frac{1}{2}u}du \quad (u=z^2)\\&amp;amp;=\frac{1}{\sqrt{2\pi}}\int_{0}^{\sqrt{x}} \frac{1}{\sqrt{u}}e^{-\frac{1}{2}u}du\\&amp;amp;=\int_{0}^{\sqrt{x}}\frac{(\frac{1}{2})^{\frac{1}{2}}}{\Gamma(\frac{1}{2})}u^{(\frac{1}{2})-1}e^{-\frac{1}{2}u}du\end{align}\]
Here, the integrand\(f_U(u)=\frac{(\frac{1}{2})^{\frac{1}{2}}}{\Gamma(\frac{1}{2})}u^{(\frac{1}{2})-1}e^{-\frac{1}{2}u}\)is a special Gamma distribution with \(r=\frac{1}{2}, \lambda=\frac{1}{2}\). Here, the \(u=z^2\), where the \(z\) is independent standard normal random variable.
And the sum of several \(U=Z^2\) variables\[Y=\sum_{j=1}^{m} U_j=\sum_{j=1}^{m} Z_{j}^{2}\] is still a Gamma distribution:\[f_Y(y)=\frac{(\frac{1}{2})^{\frac{m}{2}}}{\Gamma(\frac{m}{2})}y^{(\frac{m}{2})-1}e^{-\frac{1}{2}y}\],and we give the special Gamma distribution with \(r=\frac{m}{2}, \lambda=\frac{1}{2}\) a new name: \(\chi^2\) distribution with \(m\) degrees of freedom.</description>
    </item>
    
    <item>
      <title>Ratio of 2 independent chi square random variables divided by their degrees of freedom is F distribution</title>
      <link>/2020/08/29/ratio-of-2-independent-chi-square-random-variables-divided-by-their-degrees-of-freedom-is-f-distribution/</link>
      <pubDate>Sat, 29 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/08/29/ratio-of-2-independent-chi-square-random-variables-divided-by-their-degrees-of-freedom-is-f-distribution/</guid>
      <description>When V and U are 2 \(\chi^2\) independent random variables: \(f_V(v)=\frac{(\frac{1}{2})^{\frac{m}{2}}}{\Gamma(\frac{m}{2})}v^{(\frac{m}{2})-1}e^{-\frac{1}{2}v}\)
\(f_U(u)=\frac{(\frac{1}{2})^{\frac{n}{2}}}{\Gamma(\frac{n}{2})}u^{(\frac{n}{2})-1}e^{-\frac{1}{2}u}\)
with \(m\) and \(n\) degrees of freedom, then, the pdf for \(W=V/U\) is:
\[\begin{align}f_{V/U}(\omega)&amp;amp;=\int_{0}^{+\infty}|u|f_U(u)f_V(u\omega)du\\&amp;amp;=\int_{0}^{+\infty}u\frac{(\frac{1}{2})^{\frac{n}{2}}}{\Gamma(\frac{n}{2})}u^{\frac{n}{2}-1}e^{-\frac{1}{2}u} \frac{(\frac{1}{2})^{\frac{m}{2}}}{\Gamma(\frac{m}{2})}(u\omega)^{\frac{m}{2}-1}e^{-\frac{1}{2}u\omega}du\\&amp;amp;=\frac{(\frac{1}{2})^{\frac{n}{2}}}{\Gamma(\frac{n}{2})}\frac{(\frac{1}{2})^{\frac{m}{2}}}{\Gamma(\frac{m}{2})} \omega^{\frac{m}{2}-1} \int_{0}^{+\infty}u^{\frac{n}{2}}u^{\frac{m}{2}-1} e^{-\frac{1}{2}u(1+\omega)}du\\&amp;amp;=\frac{(\frac{1}{2})^{\frac{n}{2}}}{\Gamma(\frac{n}{2})}\frac{(\frac{1}{2})^{\frac{m}{2}}}{\Gamma(\frac{m}{2})} \omega^{\frac{m}{2}-1} \int_{0}^{+\infty}u^{\frac{n+m}{2}-1} e^{-\frac{1}{2}u(1+\omega)}du\\&amp;amp;=\frac{(\frac{1}{2})^{\frac{n}{2}}}{\Gamma(\frac{n}{2})}\frac{(\frac{1}{2})^{\frac{m}{2}}}{\Gamma(\frac{m}{2})} \omega^{\frac{m}{2}-1} (\frac{\Gamma(\frac{n+m}{2})}{(\frac{1}{2}(1+\omega))^{\frac{n+m}{2}}})\\&amp;amp;=\frac{\Gamma(\frac{n+m}{2})}{\Gamma(\frac{n}{2})\Gamma(\frac{m}{2})}\frac{\omega^{\frac{m}{2}-1}}{(1+\omega)^{\frac{n+m}{2}}}\end{align}\]
Then, the pdf for \(W=\frac{V/m}{U/n}\) is:\[\begin{align}f_{\frac{V/m}{U/n}}&amp;amp;=f_{\frac{n}{m}V/U}\\&amp;amp;=\frac{m}{n}f_{V/U}(\frac{m}{n}\omega)\\&amp;amp;=\frac{m}{n}\frac{\Gamma(\frac{n+m}{2})}{\Gamma(\frac{n}{2})\Gamma(\frac{m}{2})}\frac{(\frac{m}{n}\omega)^{\frac{m}{2}-1}}{(1+\frac{m}{n}\omega)^{\frac{n+m}{2}}}\\&amp;amp;=\frac{\Gamma(\frac{n+m}{2})}{\Gamma(\frac{n}{2})\Gamma(\frac{m}{2})}\frac{m}{n}\frac{(\frac{m}{n}\omega)^{\frac{m}{2}-1}}{(n+m\omega)^{\frac{n+m}{2}}}n^{\frac{n+m}{2}}\\&amp;amp;=\frac{\Gamma(\frac{n+m}{2})}{\Gamma(\frac{n}{2})\Gamma(\frac{m}{2})}\frac{m^{\frac{m}{2}}n^{\frac{n}{2}}\omega^{\frac{m}{2}-1}}{(n+m\omega)^{\frac{n+m}{2}}}\end{align}\], which is a \(F\) distribution with \(m\) and \(n\) degrees of freedom.</description>
    </item>
    
    <item>
      <title>Geometric Distribution is the first success occurs on kth Bernoulli trial, Negative Binomial is the rth success occurs on kth Bernoulli trial</title>
      <link>/2020/08/25/geometric-distribution-is-the-first-success-occurs-on-kth-bernoulli-trial-negative-binomial-is-the-rth-success-occurs-on-kth-bernoulli-trial/</link>
      <pubDate>Tue, 25 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/08/25/geometric-distribution-is-the-first-success-occurs-on-kth-bernoulli-trial-negative-binomial-is-the-rth-success-occurs-on-kth-bernoulli-trial/</guid>
      <description>The Geometric variable X has a pdf like this:\[P_X(k)=P(X=k)=(1-p)^{k-1}p, \quad k=1,2,3,..\]
The moment-generating function for a Geometric random variable X is:\[\begin{align}M_X(t)=E(e^{tX})&amp;amp;=\sum_{all\ k}e^{tk}(1-p)^{k-1}p\\&amp;amp;=\frac{p}{1-p}\sum_{all\ k}(e^t(1-p))^{k}\\&amp;amp;=\frac{p}{1-p}(\frac{1}{1-e^t(1-p)}-1)\\&amp;amp;=\frac{pe^t}{1-(1-p)e^t}\end{align}\]
The expected value is:\[\begin{align}M_X^{(1)}(t)&amp;amp;=\frac{d}{dt}\frac{pe^t}{1-(1-p)e^t}\\&amp;amp;=\frac{pe^t}{1-(1-p)e^t}+\frac{pe^t(1-p)e^t}{(1-(1-p)e^t)^2}\Bigl|_{t=0}\\&amp;amp;=1+\frac{p(1-p)}{p^2}\\&amp;amp;=\frac{1}{p}\end{align}\]
\[\begin{align}M_X^{(2)}(t)&amp;amp;=\frac{d}{dt}\Bigl(\frac{pe^t}{1-(1-p)e^t}+\frac{pe^t(1-p)e^t}{(1-(1-p)e^t)^2}\Bigr)\\&amp;amp;=\frac{pe^t}{1-(1-p)e^t}+\frac{pe^t(1-p)e^t}{(1-(1-p)e^t)^2}+\frac{2pe^{2t}(1-p)}{(1-(1-p)e^t)^2}+\frac{2pe^{3t}(1-p)^2}{(1-(1-p)e^t)^3}\Biggl|_{t=0}\\&amp;amp;=1+(1/p-1)+2(1/p-1)+2(1/p-1)^2\\&amp;amp;=2/p^2-1/p\end{align}\]
Then, the Variance is:\(Var(X)=E(X^2)-(E(X))^2=2/p^2-1/p-1/p^2=1/p^2-1/p=\frac{1-p}{p^2}\)
Negative Binomial is the rth success occurs on kth Bernoulli trialThe Negative Binomial variable Y has a pdf like this:\[P_Y(k)=P(Y=k)=\binom{k-1}{r-1}p^r(1-p)^{k-r}, \quad k=r,r+1,r+2,.</description>
    </item>
    
    <item>
      <title>Exponential distribution is interval between consecutive Poisson events</title>
      <link>/2020/08/24/exponential-distribution-is-interval-between-consecutive-poisson-events/</link>
      <pubDate>Mon, 24 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/08/24/exponential-distribution-is-interval-between-consecutive-poisson-events/</guid>
      <description>Let’s denote the interval between consecutive Poisson events with random variable Y, during the interval that extends from a to a + y, the number of Poisson events k has the probability \(P(k)=e^{-\lambda y} \frac{(\lambda y)^k}{k!}\), if \(k=0\),\(e^{-\lambda y}\frac{(\lambda y)^0}{0!}=e^{-\lambda y}\) means there is no event during the (a,a+y) time period.
Because there will be no occurrences in the interval (a, a + y) only if Y &amp;gt; y,so \(P(Y &amp;gt; y)=e^{-\lambda y}\), then the cdf is \(F_Y(y)=P(Y \le y)=1-P(Y &amp;gt; y)=1-e^{-\lambda y}\).</description>
    </item>
    
    <item>
      <title>Poisson is a limit of Binomial when n goes to infinity with np maintained</title>
      <link>/2020/08/24/poisson-is-a-limit-of-binomial-when-n-goes-to-infinity-with-np-maintained/</link>
      <pubDate>Mon, 24 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/08/24/poisson-is-a-limit-of-binomial-when-n-goes-to-infinity-with-np-maintained/</guid>
      <description>The binomial random variable has a pdf like this:\(P_X(k)=\binom{n}{k}p^k(1-p)^{n-k},\quad k=0,1,2,...,n\)Its moment-generating function is:\[\begin{align}M_X(t)=E(e^{tX})&amp;amp;=\sum_{k=0}^{n}e^{tk}\binom{n}{k}p^k(1-p)^{n-k}\\&amp;amp;=\sum_{k=0}^{n}\binom{n}{k}(e^tp)^k(1-p)^{n-k}\\&amp;amp;=(1-p+pe^t)^n\end{align}\]
Then \(M_X^{(1)}(t)=n(1-p+pe^t)^{n-1}pe^t|_{t=0}=np=E(X)\)\[\begin{align}M_X^{(2)}(t)&amp;amp;=n(n-1)(1-p+pe^t)^{n-2}pe^tpe^t+n(1-p+pe^t)^{n-1}pe^t|_{t=0}\\&amp;amp;=n(n-1)p^2+np=E(X^2)\end{align}\]
Then \(Var(X)=E(X^2)-(E(X))^2=n(n-1)p^2+np-(np)^2=-np^2+np=np(1-p)\)
For the binomial random variable X:\(P_X(k)=\binom{n}{k}p^k(1-p)^{n-k},\quad k=0,1,2,...,n\), if \(n\to+\infty\) with \(\lambda=np\) remains constant, then\[\begin{align}\lim_{n\to+\infty}\binom{n}{k}p^k(1-p)^{n-k}&amp;amp;=\lim_{n\to+\infty}\frac{n!}{k!(n-k)!}(\frac{\lambda}{n})^k(1-\frac{\lambda}{n})^{n-k}\\&amp;amp;=\lim_{n\to+\infty}\frac{n!}{k!(n-k)!}\lambda^k(\frac{1}{n})^k(1-\frac{\lambda}{n})^n(1-\frac{\lambda}{n})^{-k}\\&amp;amp;=\frac{\lambda^k}{k!}\lim_{n\to+\infty}\frac{n!}{(n-k)!}(\frac{1}{n})^k(\frac{n}{n-\lambda})^k(1-\frac{\lambda}{n})^n\\&amp;amp;=e^{-\lambda}\frac{\lambda^k}{k!}\lim_{n\to+\infty}\frac{n!}{(n-k)!}(\frac{1}{n-\lambda})^k\\&amp;amp;=e^{-\lambda}\frac{\lambda^k}{k!}\lim_{n\to+\infty}\frac{n(n-1)...(n-k+1)}{(n-\lambda)(n-\lambda)...(n-\lambda)}\\&amp;amp;=e^{-\lambda}\frac{\lambda^k}{k!}\end{align}\]
The moment-generating function of Poisson random variable X is:\[\begin{align}M_X(t)=E(e^{tX})&amp;amp;=\sum_{k=0}^{n}e^{tk}e^{-\lambda}\frac{\lambda^k}{k!}\\&amp;amp;=e^{-\lambda}\sum_{k=0}^{n}\frac{(\lambda e^t)^k}{k!}\\&amp;amp;=e^{-\lambda}e^{\lambda e^t}\\&amp;amp;=e^{\lambda e^t-\lambda}\end{align}\]</description>
    </item>
    
    <item>
      <title>The Gamma random variable denotes the waiting time for a Poisson event also the sum of Exponential events</title>
      <link>/2020/08/24/the-gamma-random-variable-denotes-the-waiting-time-for-the-rth-poisson-event/</link>
      <pubDate>Mon, 24 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/08/24/the-gamma-random-variable-denotes-the-waiting-time-for-the-rth-poisson-event/</guid>
      <description>The Gamma random variable denotes the waiting time for the \(r^{th}\) Poisson event, and also denotes the sum of r Exponential random variables. The sum of m Gamma random variables (shared the same parameter \(\lambda\)) is a Gamma random variable, which denotes the waiting time for the \((\sum_{i=1}^{m} r_i)^{th}\) Poisson event, and also denotes the sum of \(\sum_{i=1}^{m} r_i\) Exponential random variables.
let Y denote the waiting time to the occurrence of the \(r^{th}\) Poisson event,the probability fewer than r Poisson events occur in [0, y] time period is\(P(Y&amp;gt;y)=\sum_{k=0}^{r-1}e^{-\lambda y}\frac{(\lambda y)^k}{k!</description>
    </item>
    
    <item>
      <title>The Gamma and Beta functions</title>
      <link>/2020/08/21/the-gamma-and-beta-functions/</link>
      <pubDate>Fri, 21 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/08/21/the-gamma-and-beta-functions/</guid>
      <description>The Gamma function:\[\Gamma(s)=\int_{0}^{+\infty}t^{s-1}e^{-t}dt\quad \Bigl(=(s-1)! \quad s\in \mathbb Z^+\Bigr) (0&amp;lt;s&amp;lt;\infty)\] Because \[\begin{align}\Gamma(s+1)&amp;amp;=\int_{0}^{+\infty}t^{s}e^{-t}dt\\&amp;amp;=-\int_{0}^{+\infty}t^{s}d(e^{-t})\\&amp;amp;=-\Biggl[t^{s}e^{-t}|_{0}^{\infty}-\int_{0}^{+\infty}st^{s-1}e^{-t}dt\Biggl]\\&amp;amp;=-\Biggl[0-s\Gamma(s)\Biggl]\\&amp;amp;=s\Gamma(s)\end{align}\] and \[\Gamma(1)=\int_{0}^{+\infty}t^{1-1}e^{-t}dt=\int_{0}^{+\infty}e^{-t}dt=1\]The product of two Gamma functions:\[\begin{align}\Gamma(x)\Gamma(y)&amp;amp;=\int_{0}^{+\infty}u^{x-1}e^{-u}du\int_{0}^{+\infty}v^{y-1}e^{-v}dv\\&amp;amp;=\int_{u=0}^{+\infty}\int_{v=0}^{+\infty}e^{-(u+v)}u^{x-1}v^{y-1}dudv \quad (let\quad u+v=z; \quad u/z=t; \quad v/z=1-t; \quad dudv=zdtdz)\\&amp;amp;=\int_{z=0}^{+\infty}\int_{t=0}^{t=1}e^{-z}(zt)^{x-1}(z(1-t))^{y-1}zdtdz\\&amp;amp;=\int_{z=0}^{+\infty}e^{-z}z^{(x+y-1)}dz\int_{t=0}^{t=1}t^{(x-1)}(1-t)^{(y-1)}dt\\&amp;amp;=\Gamma(x+y)\int_{t=0}^{t=1}t^{(x-1)}(1-t)^{(y-1)}dt\end{align}\]
We define this integral \(\int_{t=0}^{t=1}t^{(x-1)}(1-t)^{(y-1)}dt\) as \(B(x,y),\quad (x&amp;gt;0 ;\quad y&amp;gt;0)\), this is the Beta function.\(B(x,y)=\frac{\Gamma(x)\Gamma(y)}{\Gamma(x+y)}\) \(\Bigl(=\frac{(x-1)!(y-1)!}{(x+y-1)!}\quad x;y\in \mathbb Z^+ \Bigr)\) this is the complete Beta function.</description>
    </item>
    
    <item>
      <title>How to derive the beautiful probability density function (pdf) of Normal Distribution?</title>
      <link>/2020/08/13/distributions/</link>
      <pubDate>Thu, 13 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/08/13/distributions/</guid>
      <description>How can we derive the probability density function (pdf) of Normal Distribution?\[f_Y(y)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{1}{2}(\frac{y-\mu}{\sigma})^2}, \quad -\infty&amp;lt;y&amp;lt;+\infty\]
Let’s draw a normal pdf first
#draw normal pdfx &amp;lt;- seq(-5, 5, length.out = 201); dx &amp;lt;- diff(x)[1]y &amp;lt;- dnorm(x, mean = 0, sd = 1)base::plot(x, y, type = &amp;quot;l&amp;quot;, col = &amp;quot;skyblue&amp;quot;,xlab=&amp;quot;x&amp;quot; , ylab=&amp;quot;p(x)&amp;quot; , cex.lab=1.5,main=&amp;quot;Normal Probability Density&amp;quot; , cex.main=1.5,lwd=2)text( 0, .6*max(y) , bquote( paste(mu ,&amp;quot; = 0 &amp;quot;) ), cex=1.</description>
    </item>
    
  </channel>
</rss>
