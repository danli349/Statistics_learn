<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Regression on A Hugo website</title>
    <link>/tags/regression/</link>
    <description>Recent content in Regression on A Hugo website</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 24 Sep 2023 00:00:00 +0000</lastBuildDate><atom:link href="/tags/regression/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Common statistical tests are linear models</title>
      <link>/2023/09/24/common-statistical-tests-are-linear-models-or-how-to-teach-stats/</link>
      <pubDate>Sun, 24 Sep 2023 00:00:00 +0000</pubDate>
      
      <guid>/2023/09/24/common-statistical-tests-are-linear-models-or-how-to-teach-stats/</guid>
      <description>Interpretation of R’s lm() outputFive point summaryCoefficients and \(\hat{\beta_i}s\)\(t\)-statisticsResidual standard errorAdjusted \(R^2\)\(F\)-statisticThe simplicity underlying common testsSettings and toy dataPearson and Spearman correlationTheory: As linear modelsTheory: rank-transformationR code: Pearson correlationR code: Spearman correlationOne meanOne sample t-test and Wilcoxon signed-rankTheory: As linear modelsR code: One-sample t-testR code: Wilcoxon signed-rank testPaired samples t-test and Wilcoxon matched pairsTheory: As linear modelsR code: Paired sample t-testR code: Wilcoxon matched pairsTwo meansIndependent t-test and Mann-Whitney UTheory: As linear modelsTheory: Dummy codingTheory: Dummy coding (continued)R code: independent t-testR code: Mann-Whitney UWelch’s t-testThree or more meansOne-way ANOVA and Kruskal-WallisTheory: As linear modelsExample dataR code: one-way ANOVAR code: Kruskal-WallisTwo-way ANOVATheory: As linear modelsR code: Two-way ANOVAANCOVAProportions: Chi-square is a log-linear modelGoodness of fitTheory: As log-linear modelExample dataR code: Goodness of fitContingency tablesTheory: As log-linear modelExample dataR code: Chi-square testSources and further equivalencesExplicit GLM(M) Equivalents for Standard TestsExplicit GLM Test: PoissonBinomial Test: Logistic RegressionClassical Test: Exact Binomial TestExplicit GLM: Logit (or Probit)Probability density function of Logistic distributionProportion Test: (Multinomial) Logistic or Poisson ModelClassical Test: Test for Equality of ProportionsExplicit GLM: LogitExplicit GLM: PoissonClassical Test: Poisson TestReferences#share-buttons img {width: 40px;padding-right: 15px;border: 0;box-shadow: 0;display: inline;vertical-align: top;}# Options for building this documentknitr::opts_chunk$set(fig.</description>
    </item>
    
    <item>
      <title>ESL chapter 4 Linear Methods for Classification</title>
      <link>/2021/04/03/esl-chapter-4-linear-methods-for-classification/</link>
      <pubDate>Sat, 03 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>/2021/04/03/esl-chapter-4-linear-methods-for-classification/</guid>
      <description>Chapter 4. Linear Methods for Classification\(\S\) 4.1. IntroductionLinear regressionDiscriminant functionLogit transformationSeparating hyperplanesScope for generalization\(\S\) 4.2. Linear Regression of an Indicator MatrixRationaleA more simplistic viewpointMasked class with the regression approach\(\S\) 4.3. Linear Discriminant AnalysisLDA from multivariate GaussianEstimating parametersSimple correspondence between LDA and linear regression with two classesPractice beyond the Gaussian assumptionQuadratic Discriminant AnalysisWhy LDA and QDA have such a good track record?</description>
    </item>
    
    <item>
      <title>ESL chapter 3 exercises</title>
      <link>/2021/03/23/esl-chapter-3-exercises/</link>
      <pubDate>Tue, 23 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>/2021/03/23/esl-chapter-3-exercises/</guid>
      <description>Ex. 3.9 (using the QR decomposition for fast forward-stepwise selection)Ex. 3.10 (using the z-scores for fast backwards stepwise regression)Ex. 3.11 (multivariate linear regression with different \(\Sigma_i\))Ex. 3.12 (ordinary least squares to implement ridge regression)Ex. 3.13 (principal component regression)Ex. 3.14 (when the inputs are orthogonal PLS stops after m = 1 step)Ex. 3.15 (PLS seeks directions that have high variance and high correlation)Relation to the optimization problemEx.</description>
    </item>
    
    <item>
      <title>ESL chapter 3 Linear Methods for Regression</title>
      <link>/2021/02/24/esl-chapter-3-linear-methods-for-regression/</link>
      <pubDate>Wed, 24 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>/2021/02/24/esl-chapter-3-linear-methods-for-regression/</guid>
      <description>Chapter 3. Linear Methods for Regression\(\S\) 3.1. Introduction\(\S\) 3.2. Linear Regression Models and Least SquaresThe linear modelLeast squares fitSolution of least squaresGeometrical representation of the least squares estimateSampling properties of \(\hat{\beta}\)Inference and hypothesis testingConfidence intervals\(\S\) 3.2.1. Example: Prostate Cancer\(\S\) 3.2.2. The Gauss-Markov TheoremThe statement of the theoremImplications of the Gauss-Markov theoremRelation between prediction accuracy and MSE\(\S\) 3.</description>
    </item>
    
    <item>
      <title>ESL chapter 2 Overview of Supervised Learning</title>
      <link>/2021/02/12/esl-chapter-2-overview-of-supervised-learning/</link>
      <pubDate>Fri, 12 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>/2021/02/12/esl-chapter-2-overview-of-supervised-learning/</guid>
      <description>\(\S\) Supervised Learning\(\S\) 2.3. Two Simple Approaches to Prediction: Least Squares and Nearest Neighbors\(\S\) 2.3.3 From Least Squares to Nearest Neighbors\(\S\) 2.3.1. Linear Models and Least SquaresLinear ModelsHow to fit the model: Least squaresLinear model in a classification contextWhere the data came from?\(\S\) 2.3.2 Nearest-Neighbor MethodsDo not satisfy with the training resultsEffective number of parametersDo not appreciate the training errors\(\S\) 2.</description>
    </item>
    
    <item>
      <title>covariance and correlation coefficient</title>
      <link>/2020/09/05/covariance-and-correlation-coefficient/</link>
      <pubDate>Sat, 05 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/09/05/covariance-and-correlation-coefficient/</guid>
      <description>We define the covariance of any two random variables \(X\) and \(Y\), written \(Cov(X,Y)\), as: \[\begin{align}Cov(X,Y) &amp;amp;= E(X-\mu_X)(Y-\mu_Y)\\&amp;amp;= E(XY-X\mu_Y-Y\mu_X+\mu_X\mu_Y)\\&amp;amp;= E(XY)-\mu_X\mu_Y-\mu_X\mu_Y+\mu_X\mu_Y\\&amp;amp;= E(XY) - \mu_X\mu_Y\\&amp;amp;= E(XY) − E(X)E(Y)\\\end{align}\].If \(X\) and \(Y\) are independent random variables,\[\begin{align}E(XY)&amp;amp;=\int\int xy\cdot f_{X,Y}(x,y)dxdy\\&amp;amp;=\int\int xy\cdot f_X(x)f_Y(y)dxdy\\&amp;amp;=\int x\cdot f_X(x)dx\int y\cdot f_Y(y)dy\\&amp;amp;=E(X)E(Y)\end{align}\], then \(Cov(X,Y) = E(XY) − E(X)E(Y)=0\)
The Variance of the sum of two random variables \(aX + bY\) is:\[\begin{align}Var(aX + bY) &amp;amp;= E(aX + bY)^2-(E(aX + bY))^2\\&amp;amp;=E(aX + bY)^2-(a\mu_X+b\mu_Y)^2\\&amp;amp;=E(a^2X^2+2aXbY+b^2Y^2)-a^2\mu_X^2-2a\mu_Xb\mu_Y-b^2\mu_Y^2\\&amp;amp;=a^2(E(X^2)-\mu_X^2)+b^2(E(Y^2)-\mu_Y^2)+2ab(E(XY)-\mu_X\mu_Y)\\&amp;amp;=a^2Var(X)+b^2Var(Y)+2abCov(X,Y)\end{align}\].</description>
    </item>
    
    <item>
      <title>Regression random variable Y for a given value x</title>
      <link>/2020/09/04/regression-random-variable-y-for-a-given-value-x/</link>
      <pubDate>Fri, 04 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/09/04/regression-random-variable-y-for-a-given-value-x/</guid>
      <description>We want to make regression of a random variable \(Y\) for a given value \(x\), the function \(f_{Y|x}(y)\) denotes the pdf of the random variable \(Y\) for a given value \(x\), and the expected value associated with \(f_{Y|x}(y)\) is \(E(Y | x)\). The function\(y = E(Y | x)\) is called the regression curve of \(Y\) on \(x\). The regression model is called simple linear model if it satisfy the \(4\) assumptions:</description>
    </item>
    
    <item>
      <title>Linear Regression</title>
      <link>/2020/09/03/linear-regression/</link>
      <pubDate>Thu, 03 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/09/03/linear-regression/</guid>
      <description>If there are \(n\) points \((x_1,y_1),(x_2,y_3),...,(x_n,y_n)\), the straight line \(y=a+bx\) minimizing the sum of the squares of the vertical distances from the data points to the line \(L=\sum_{i=1}^{n}(y_i-a-bx_i)^2\), then we take partial derivatives of L with respect to \(a\) and \(b\) and let them equal to \(0\) to get least squares coefficients \(a\) and \(b\):\[\frac{\partial L}{\partial b}=-2\sum_{i=1}^{n}(y_i-a-bx_i)x_i=0\], then \[\sum_{i=1}^{n}x_iy_i=a\sum_{i=1}^{n}x_i+b\sum_{i=1}^{n}x_i^2\]
And, \[\frac{\partial L}{\partial a}=-2\sum_{i=1}^{n}(y_i-a-bx_i)=0\], then\[\sum_{i=1}^{n}y_i=na+b\sum_{i=1}^{n}x_i\]these 2 equations are:\[\begin{bmatrix}\displaystyle\sum_{i=1}^{n}x_i &amp;amp; \displaystyle\sum_{i=1}^{n}x_i^2\\n &amp;amp; \displaystyle\sum_{i=1}^{n}x_i\\\end{bmatrix}\begin{bmatrix}a\\b\end{bmatrix}=\begin{bmatrix}\displaystyle\sum_{i=1}^{n}x_iy_i\\\displaystyle\sum_{i=1}^{n}y_i\end{bmatrix}\]then, using Cramer’s rule\[\begin{align}b&amp;amp;=\frac{\begin{bmatrix}\displaystyle\sum_{i=1}^{n}x_i &amp;amp; \displaystyle\sum_{i=1}^{n}x_iy_i\\n &amp;amp; \displaystyle\sum_{i=1}^{n}y_i\\\end{bmatrix}}{\begin{bmatrix}\displaystyle\sum_{i=1}^{n}x_i &amp;amp; \displaystyle\sum_{i=1}^{n}x_i^2\\n &amp;amp; \displaystyle\sum_{i=1}^{n}x_i\\\end{bmatrix}}\\&amp;amp;=\frac{(\displaystyle\sum_{i=1}^{n}x_i)(\displaystyle\sum_{i=1}^{n}y_i)-n(\displaystyle\sum_{i=1}^{n}x_iy_i)}{(\displaystyle\sum_{i=1}^{n}x_i)^2-n\displaystyle\sum_{i=1}^{n}x_i^2}\\&amp;amp;=\frac{n(\displaystyle\sum_{i=1}^{n}x_iy_i)-(\displaystyle\sum_{i=1}^{n}x_i)(\displaystyle\sum_{i=1}^{n}y_i)}{n\displaystyle\sum_{i=1}^{n}x_i^2-(\displaystyle\sum_{i=1}^{n}x_i)^2}\\&amp;amp;=\frac{(\displaystyle\sum_{i=1}^{n}x_iy_i)-\frac{1}{n}(\displaystyle\sum_{i=1}^{n}x_i)(\displaystyle\sum_{i=1}^{n}y_i)}{\displaystyle\sum_{i=1}^{n}x_i^2-\frac{1}{n}(\displaystyle\sum_{i=1}^{n}x_i)^2}\end{align}\], and, \(a=\frac{\displaystyle\sum_{i=1}^{n}y_i-b\sum_{i=1}^{n}x_i}{n}=\bar y-b\bar x\), which shows point \((\bar x, \bar y)\) is in the line.</description>
    </item>
    
  </channel>
</rss>
