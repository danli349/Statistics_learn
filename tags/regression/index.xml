<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Regression on A Hugo website</title>
    <link>/tags/regression/</link>
    <description>Recent content in Regression on A Hugo website</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 05 Sep 2020 00:00:00 +0000</lastBuildDate><atom:link href="/tags/regression/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>covariance and correlation coefficient</title>
      <link>/2020/09/05/covariance-and-correlation-coefficient/</link>
      <pubDate>Sat, 05 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/09/05/covariance-and-correlation-coefficient/</guid>
      <description>We define the covariance of any two random variables \(X\) and \(Y\), written \(Cov(X,Y)\), as: \[\begin{align}Cov(X,Y) &amp;amp;= E(X-\mu_X)(Y-\mu_Y)\\&amp;amp;= E(XY-X\mu_Y-Y\mu_X+\mu_X\mu_Y)\\&amp;amp;= E(XY)-\mu_X\mu_Y-\mu_X\mu_Y+\mu_X\mu_Y\\&amp;amp;= E(XY) - \mu_X\mu_Y\\&amp;amp;= E(XY) − E(X)E(Y)\\\end{align}\].If \(X\) and \(Y\) are independent random variables,\[\begin{align}E(XY)&amp;amp;=\int\int xy\cdot f_{X,Y}(x,y)dxdy\\&amp;amp;=\int\int xy\cdot f_X(x)f_Y(y)dxdy\\&amp;amp;=\int x\cdot f_X(x)dx\int y\cdot f_Y(y)dy\\&amp;amp;=E(X)E(Y)\end{align}\], then \(Cov(X,Y) = E(XY) − E(X)E(Y)=0\)
The Variance of the sum of two random variables \(aX + bY\) is:\[\begin{align}Var(aX + bY) &amp;amp;= E(aX + bY)^2-(E(aX + bY))^2\\&amp;amp;=E(aX + bY)^2-(a\mu_X+b\mu_Y)^2\\&amp;amp;=E(a^2X^2+2aXbY+b^2Y^2)-a^2\mu_X^2-2a\mu_Xb\mu_Y-b^2\mu_Y^2\\&amp;amp;=a^2(E(X^2)-\mu_X^2)+b^2(E(Y^2)-\mu_Y^2)+2ab(E(XY)-\mu_X\mu_Y)\\&amp;amp;=a^2Var(X)+b^2Var(Y)+2abCov(X,Y)\end{align}\].</description>
    </item>
    
    <item>
      <title>Regression random variable Y for a given value x</title>
      <link>/2020/09/04/regression-random-variable-y-for-a-given-value-x/</link>
      <pubDate>Fri, 04 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/09/04/regression-random-variable-y-for-a-given-value-x/</guid>
      <description>We want to make regression of a random variable \(Y\) for a given value \(x\), the function \(f_{Y|x}(y)\) denotes the pdf of the random variable \(Y\) for a given value \(x\), and the expected value associated with \(f_{Y|x}(y)\) is \(E(Y | x)\). The function\(y = E(Y | x)\) is called the regression curve of \(Y\) on \(x\). The regression model is called simple linear model if it satisfy the \(4\) assumptions:</description>
    </item>
    
    <item>
      <title>Linear Regression</title>
      <link>/2020/09/03/linear-regression/</link>
      <pubDate>Thu, 03 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/09/03/linear-regression/</guid>
      <description>If there are \(n\) points \((x_1,y_1),(x_2,y_3),...,(x_n,y_n)\), the straight line \(y=a+bx\) minimizing the sum of the squares of the vertical distances from the data points to the line \(L=\sum_{i=1}^{n}(y_i-a-bx_i)^2\), then we take partial derivatives of L with respect to \(a\) and \(b\) and let them equal to \(0\) to get least squares coefficients \(a\) and \(b\):\[\frac{\partial L}{\partial b}=-2\sum_{i=1}^{n}(y_i-a-bx_i)x_i=0\], then \[\sum_{i=1}^{n}x_iy_i=a\sum_{i=1}^{n}x_i+b\sum_{i=1}^{n}x_i^2\]
And, \[\frac{\partial L}{\partial a}=-2\sum_{i=1}^{n}(y_i-a-bx_i)=0\], then\[\sum_{i=1}^{n}y_i=na+b\sum_{i=1}^{n}x_i\]these 2 equations are:\[\begin{bmatrix}\displaystyle\sum_{i=1}^{n}x_i &amp;amp; \displaystyle\sum_{i=1}^{n}x_i^2\\n &amp;amp; \displaystyle\sum_{i=1}^{n}x_i\\\end{bmatrix}\begin{bmatrix}a\\b\end{bmatrix}=\begin{bmatrix}\displaystyle\sum_{i=1}^{n}x_iy_i\\\displaystyle\sum_{i=1}^{n}y_i\end{bmatrix}\]then, using Cramer’s rule\[\begin{align}b&amp;amp;=\frac{\begin{bmatrix}\displaystyle\sum_{i=1}^{n}x_i &amp;amp; \displaystyle\sum_{i=1}^{n}x_iy_i\\n &amp;amp; \displaystyle\sum_{i=1}^{n}y_i\\\end{bmatrix}}{\begin{bmatrix}\displaystyle\sum_{i=1}^{n}x_i &amp;amp; \displaystyle\sum_{i=1}^{n}x_i^2\\n &amp;amp; \displaystyle\sum_{i=1}^{n}x_i\\\end{bmatrix}}\\&amp;amp;=\frac{(\displaystyle\sum_{i=1}^{n}x_i)(\displaystyle\sum_{i=1}^{n}y_i)-n(\displaystyle\sum_{i=1}^{n}x_iy_i)}{(\displaystyle\sum_{i=1}^{n}x_i)^2-n\displaystyle\sum_{i=1}^{n}x_i^2}\\&amp;amp;=\frac{n(\displaystyle\sum_{i=1}^{n}x_iy_i)-(\displaystyle\sum_{i=1}^{n}x_i)(\displaystyle\sum_{i=1}^{n}y_i)}{n\displaystyle\sum_{i=1}^{n}x_i^2-(\displaystyle\sum_{i=1}^{n}x_i)^2}\\&amp;amp;=\frac{(\displaystyle\sum_{i=1}^{n}x_iy_i)-\frac{1}{n}(\displaystyle\sum_{i=1}^{n}x_i)(\displaystyle\sum_{i=1}^{n}y_i)}{\displaystyle\sum_{i=1}^{n}x_i^2-\frac{1}{n}(\displaystyle\sum_{i=1}^{n}x_i)^2}\end{align}\], and, \(a=\frac{\displaystyle\sum_{i=1}^{n}y_i-b\sum_{i=1}^{n}x_i}{n}=\bar y-b\bar x\), which shows point \((\bar x, \bar y)\) is in the line.</description>
    </item>
    
  </channel>
</rss>
